arXiv:cs/0603063v3  [cs.SC]  21 Dec 2007The Journal of Symbolic Logic
Volume 00, Number 0, XXX 0000
UNARY PRIMITIVE RECURSIVE FUNCTIONS
DANIEL E. SEVERIN
Abstract. In this article, we study some new characterizations of prim itive recursive
functions based on restricted forms of primitive recursion , improving the pioneering work
of R. M. Robinson and M. D. Gladstone. We reduce certain recur sion schemes (mixed/pure
iteration without parameters) and we characterize one-arg ument primitive recursive func-
tions as the closure under substitution and iteration of cer tain optimal sets.
§1. Introduction. Prim, i.e. the set of primitive recursive functions , is the
closure under substitution and primitive recursion of zero, succes sor and projec-
tion functions. For a detailed deﬁnition, the reader is referred to a ny standard
work, for instance chapter 1 of [8]. A suitable subset is Prim(N,N), i.e. the set of
unary primitive recursive functions . It will be one of the objects of our research.
Recursion schemes have been studied intensively during the twentie th cen-
tury. In particular, R. M. Robinson[15, 16] and his wife J. Robinson[1 3, 14]
proved that it is suﬃcient to consider one-argument functions bec ause functions
of several arguments can be reduced to them using pairing strate gies. Later on,
Gladstone[6, 7] and Georgieva[3] made improvements to the recurs ion schemes.
At the same time as the study of recursive functions, several clas siﬁcations were
carried out over Prim(N,N). The ﬁrst one was Grzegorczyk hierarchy[9]. Since
then, other hierarchies have appeared (cf. [12, 1, 4, 2, 11]). Fina lly, some alge-
braic properties of Prim(N,N) were veriﬁed in [17]. Similar topics are covered in
[5, 10].
The present paper improves the work of Robinson[15] and Gladston e[7].
The paper is organizedas follows: In §2 we will give a useful symbolic notation
for writing functions. In §3 we will show previous results, and the facts to be
proved here. In §4 we will analyze a possible reduction in one of the recursion
schemes. More precisely, mixed iteration without parameters with aﬁxed is as
expressive as mixed iteration without parameters with avariable (the meaning
of these schemes and acan be found in §3). In§5 we will do the same thing
with pure iteration without parameters. And, in §6 we will characterize unary
primitive recursivefunctions asthe closureofthe set including x/ma√sto−→1 andx/ma√sto−→
x− ⌊√x⌋2with respect to substitution, iteration and the following operator:
f/ma√sto−→f+I, whereIis the identity function on natural numbers.
Key words and phrases. unary, primitive recursive, recursion scheme, reduction.
c/circlecopyrt0000, Association for Symbolic Logic
0022-4812/00/0000-0000/$00.00
1
2 DANIEL E. SEVERIN
§2. Notation. Todenotearbitraryfunctionsweshalluselettersinuppercase
such asF,GandH. To denote natural variableswe shall use x,y,z..., whereas
a,b,...are used to denote constants. Throughout the paper, the follow ing
functions will be used:
•Basic functions:
I(x) =x (identity)
n(x) =n (constants)
S(x) =x+1 (successor)
P(x) =x−·1 (predecessor)
In
k(x1,x2,... ,x n) =xk,for 1≤k≤n (projections)
•Arithmetic functions:
D(x) = 2x (double)
Sq(x) =x2(square)
Hf(x) =⌊x/2⌋ (half)
Pw(x) = 2x(power of two)
Rt(x) =⌊√x⌋ (integer square root)
•Cantor pairing functions:
A(x) =⌊(x2+x)/2⌋ (x-th triangular number)
V(x) =/floorleftbigg⌊√8x+1⌋−1
2/floorrightbigg
(inverse of A)
J(x,y) =A(x+y)+x (pairing function)
K(x) =x−A(V(x)) (ﬁrst inverse)
L(x) =A(V(x)+1)−x−1 (second inverse)
•Binary functions:
x−·y=/braceleftBigg
x−yifx≥y
0 otherwise(arithmetic diﬀerence)
|x−y|=/braceleftBigg
x−yifx≥y
y−xotherwise(distance)
UNARY PRIMITIVE RECURSIVE FUNCTIONS 3
•Other functions:1
O(x) =/braceleftBigg
1 ifx= 0
0 otherwise(power of zero, cosignum)
Sgn(x) =/braceleftBigg
0 ifx= 0
1 otherwise(signum)
N(x) =xmod 2 (characteristic of odd numbers)
E(x) =x−⌊√x⌋2(excess over a square)
Q(x) =/braceleftBigg
1 ifxis a square
0 otherwise(characteristic of square numbers)
LetF,G,G1,...,Gmbe functions, and x= (x1,x2,... ,x n), i.e. a n-tuple.
The following operators on natural number functions will be used:
•Substitution:
subst(F,G1,G2,... ,G m)(x) =F(G1(x),G2(x),... ,G m(x)).
A more special case is deﬁned for one-argument functions,
(F◦G)(x) =F(G(x)),
(FG)(x) =F(G(x)).
•Primitive recursion:
R[F,G](x,0) =F(x),
R[F,G](x,y+1) =G(x,y,R[F,G](x,y)).
•Restricted forms of primitive recursion:2
1)M[F](0) = 0,
M[F](x+1) =F(x,M[F](x)),
2)F/square(a)(0) =a,
F/square(a)(x+1) =F(F/square(a)(x)).
3)F/square(x) =F/square(0)(x).
•Power:
F0(x) =x,
Fn+1(x) =F(Fn(x)).
•Miscellaneous:
1) (F+G)(x) =F(x)+G(x),
2) (F−·G)(x) =F(x)−·G(x),
3)|F−G|(x) =|F(x)−G(x)|,
4)J(F,G)(x) =J(F(x),G(x)).
1Some authors write 0x,sg(x) or cosg( x) instead of O(x).
2Notations F/squareandF/square(a)are due to Szalkai[17].
4 DANIEL E. SEVERIN
Table 1. Precedence and associativity of operators.
Precedence Operators Associativity
First F+G,F−·GLeft
Second FG,F◦GAny
Third Fn,F/square,F/square(a)−
In order to decrease the size of this article and improve readability, we will
give a symbolic notation for representing functions. If the deﬁnitio n of a new
function F:Nn→Nis
F(x1,x2,... ,x n) =expression (x1,x2,... ,x n),
we will write
F≡expression ,
whereexpression is composed by the functions and operators previously deﬁned.
Precedence and associativity rules are shown in table 1. Here are so me examples
of well-formed expressions:
D≡M[S◦S◦I2
2], O ≡subst(R[1,P◦I3
3],I,I),
Pw≡S(I+I+1)/square, V ≡Hf P Rt S D D D.
A ﬁnite set of initial functions and of functional operators is called basis. We
will denote with
F=/an}⌊ra⌋ketle{tF1,F2,... ,F n,F⊕,... ,F⊗G,.../an}⌊ra⌋ketri}ht
the basis composed by the initial functions F1,F2,...,Fn, the unary operators
F⊕,..., the binary operators F⊗G,...and so on.
We will denote with closFthe closure of the basis F. An example is
Prim=clos/an}⌊ra⌋ketle{t0,S,In
k,subst,R[F,G]/an}⌊ra⌋ketri}ht.
§3. Preliminaries. In [15], some recursion schemes are introduced (all of
them are particular cases of primitive recursion):
1. Mixed recursion with one parameter:
F(x,0) =G(x), F(x,y+1) =H(x,y,F(x,y)).
2. Pure recursion with one parameter:
F(x,0) =G(x), F(x,y+1) =H(x,F(x,y)).
3. Mixed iteration with one parameter:
F(x,0) =x, F(x,y+1) =H(y,F(x,y)).
4. Pure iteration with one parameter:
F(x,0) =x, F(x,y+1) =H(F(x,y)).
5. Mixed iteration without parameters:
F(0) =a, F(y+1) =H(y,F(y)).
6. Pure iteration without parameters:
F(0) =a, F(y+1) =H(F(y)).
7. Mixed iteration without parameters, and a= 0:
F(0) = 0, F(y+1) =H(y,F(y)).
UNARY PRIMITIVE RECURSIVE FUNCTIONS 5
Table 2. Table of functions that must be added as initial functions.
One Parameter No Parameter
Recursion Iteration avariable a= 0 (ﬁxed)
x+y,Q[15]
Mixed −[15] −[15] x+y[7]|x−y|[15]
x+y,O§4
x+y,E[15]
x+y,K[16]
Pure −[6] −[7]|x−y|[7]x+y,L[16]
J,K[16]
x−·y[3, 11] J,L[16]
|x−y| §5
x−·y§5
8. Pure iteration without parameters, and a= 0 (or simply called iteration ):
F(0) = 0, F(y+1) =H(F(y)).
We will refer to these schemes as rec1,rec2,...,rec8in the same order listed
above. Note thatschemes rec1,rec7andrec8havesymbolicnotations: F≡H/square(a),
F≡M[H] andF≡H/square.
Robinson and Gladstone proved that the primitive recursion scheme can be
replaced by one of the cases with one parameter, i.e. rec1-rec4. They also
proved that the cases without parameters, i.e. rec5-rec8, are adequate but
certain functions must be added to the initial functions. Table 2 sum marizes
which functions are suﬃcient to be included as initial functions (the s ymbol−
denotes the null set). In this table, the references indicate wher e the proofs of
previous results can be found and the section references indicate s where are the
proofs of new results. Now, the tables that appeared on p. 929 of [15] and on p.
654 of [7] can be substituted by our table.
In the cases without parameters, it is not necessary to take zero function as an
initial function because it can be obtained from identity and iteration as follows:
0(0) = 0,0(x+1) =I(0(x)).
Moreover, in the pure cases without parameters, it is not necessa ry to take pro-
jectionfunctionsasinitialfunctionsifweareconsideringone-argu mentfunctions.
InPrim(N,N), there is only one projection, the identity function, which can be
obtained from successor and iteration as follows:
I(0) = 0, I(x+1) =S(I(x)).
Notice that allconstantfunctions belongtoeverycasegivenin tab le2, sincethey
can be generated using zero and successor functions: n(x) =Sn(0(x)). Constant
functions of more that one argument can be deﬁned composing a on e-argument
6 DANIEL E. SEVERIN
constant function with an arbitrary function of narguments (e.g. a projection).
At the end of §4 of [15], Robinson determined that Sq,O,Hf,Rt, addition
and substraction3(x−y) are suﬃcient to add as initial functions when we works
withrec5-rec8. We will rewrite this result in the next lemma.
Lemma3.1.Fori∈ {5,6,7,8},
Prim=clos/an}⌊ra⌋ketle{tS,In
k,Sq,O,Hf,Rt, +,−,subst,reci/an}⌊ra⌋ketri}ht.
In some sections, we just will work with unary primitive recursive fun ctions
and the scheme of iteration. The following deﬁnition will help us.
Definition 3.2.We say that a basis FissuitablewhenclosF=Prim(N,N).
Lemma3.3.The basis /an}⌊ra⌋ketle{tS,Sq,O,Hf,Rt,F +G,F−G,FG,F/square/an}⌊ra⌋ketri}htis suitable.
Proof. It follows from Lemma 3.1 (also see Theorem 2 of [15]) and I≡S/square.
Due to the impossibility of introducing binary functions, we must incor porate
operators such as F+GandF−G. /square
Furthermore, Robinsonprovedthat Primcan beobtained byaddingprojection
functions, addition and the substitution operator to Prim(N,N) (cf.§7 of [15]).
We write this result as another lemma.
Lemma3.4.LetFbe a suitable basis. Then, Prim=clos(F+/an}⌊ra⌋ketle{tIn
k,+,subst/an}⌊ra⌋ketri}ht).
Now, we will derive a list of suitable bases for the pure cases without p aram-
eters (see table 3, the format is the same as in table 2). Bases prov ided in§6
are simpler than Robinson’s bases. In fact, the successor can be s ubstituted by
1, and the addition operator can be substituted by a unary operat or of the form
f/ma√sto−→f+I.
§4. Mixed iteration without parameters. In§4 of [7], Gladstone showed
thatrec5is adequate if we include the addition function. Our aim is to verify
thatrec7is adequate too, but we must incorporate a function that is not non -
decreasing: cosignum. In order to do this, we need to follow the sam e steps as
[7] but keeping in mind that we must use rec7.
At the scope of this section, let F=/an}⌊ra⌋ketle{tS,In
k,O,+,subst,M[F]/an}⌊ra⌋ketri}ht.
Lemma4.1.P,N,D,Sq,Hf,Pw ∈closF.
Proof. In the ﬁrst place, P≡M[I2
1],N≡M[O◦I2
2] andD≡M[S◦S◦I2
2].
Furthermore, we have:
•Square:Sq(0) = 0,Sq(x+1) =Sq(x)+2x+1.
Sq≡M[subst(+,S◦I2
2,D◦I2
1)].
•Half:Hf(0) = 0,Hf(x+1) =Hf(x)+N(x).
Hf≡M[subst(+,I2
2,N◦I2
1)].
•Power of two: Let Fbe deﬁned as follows: F(0) = 0,F(x+1) = 2F(x)+1.
Therefore, F(x) = 2x−1 andPw≡S◦M[S◦D◦I2
2].
3The notation x−ywithout dot or vertical bars, will always be used in an ambigu ous sense,
to stand for any function F(x,y) which is equal to x−yforx≥y, regardless of its value when
x<y. Any diﬀerence function, such as x−·yor|x−y|, can substitute x−y.
UNARY PRIMITIVE RECURSIVE FUNCTIONS 7
Table 3. Initial functions for characterizations of Prim(N,N)
using pure iteration.
avariable a= 0 (ﬁxed)
S,E,F+G[15]
S,K,F+G[16]
S,L,F+G[16]
S,|F−G|[7]S,E,J(F,G) [16]
S,K,J(F,G) [16]
S,L,J(F,G) [16]
S,|F−G| §5
S,F−·G[3, 11] S,F−·G§5
1,E,F+I§6
1,K,F+I§6
1,L,F+I§6
/square
Lemma4.2.The function δ(x,y) =/braceleftBigg
1ifx=y
0otherwise(namely Kronecker delta
function) belongs to closF.
Proof. In Lemma 6 of §4 of [7], the following function fis deﬁned using
schemerec5:
f(0) = 2,
f(x+1) =N(z)+z+2x+O(N(z))+2x+2O(N(z)),
wherez=/floorleftbiggf(x)
2/floorrightbigg
. We can simulate this function by transferring the index in
one unit:
f′(0) = 0,
f′(x+1) =N(z′)+z′+2x+O(N(z′))−1+2x+2O(N(z′))−1+O(O(x)),
wherez′=/floorleftbiggf′(x)−1
2/floorrightbigg
. Thus,f(x) =f′(x+1)−1.
Now, let gbe deﬁned as g(0) = 0,g(x+1) =N/floorleftbiggf(x−1)
2/floorrightbigg
.
According to Gladstone,4
g(x) =/braceleftBigg
1 ifxis a power of two ,
0 otherwise .
Note that x=yiﬀ 2x+2yis a power of two, so δ(x,y) =g(2x+2y). /square
Lemma4.3.Rt,− ∈closF.
4In his paper, g(x) returns 0 when xis a power of two, and 1 if not.
8 DANIEL E. SEVERIN
Proof. Integer square root is computed as follows: Rt(0) = 0, Rt(x+1) =
Rt(x)+δ((Rt(x)+1)2,x+1). Symbolically,
Rt≡M[subst(+,I2
2,subst(δ,Sq◦S◦I2
2,S◦I2
1))].
LetHbe deﬁned as follows:
H(0) = 0,
H(x+1) =H(x)+2N(⌊√x⌋)−·1.
Hence,H(x) =E(x) when⌊√x⌋is an odd number (cf. part (4) of §6 of [15]),
so that
x−y=H((2x+2y)2+5x+3y+1)
whenever x≥y. The formula above deﬁnes the function substraction as a
functional operator. Finally, − ≡I2
1−I2
2. /square
Theorem 4.4.Prim=clos/an}⌊ra⌋ketle{tS,In
k,O,+,subst,M[F]/an}⌊ra⌋ketri}ht.
Proof. It follows from Lemma 3.1 and Lemmata 4.1-4.3. /square
We will prove two theorems which explain the reason we included cosign um
function in Theorem 4.4.
Theorem 4.5.LetF′=F− /an}⌊ra⌋ketle{tO/an}⌊ra⌋ketri}ht(the result of removing Ofrom the basis
F). Every function Fof one argument of closF′is non-decreasing:
∀x∈NF(x)≤F(x+1).
Proof. We will proceed by structural induction over functions deﬁned usin g
one argument. The fact is trivial for identity and successor funct ion. IfFandG
are non-decreasing functions, its substitution (i.e. F◦G) and its addition (i.e.
subst(+,F,G)) are non-decreasing too.
Now, let Fbe deﬁned as
F(0) = 0, F(x+1) =G(x,F(x)).
Clearly,Gis a function written in terms of I2
1,I2
2and non-decreasing functions.
So,Gsatisfy the following property:
∀a,b,x,y∈NG(x,y)≤G(x+a,y+b).
Suppose that F(x)≤F(x+1). Then,
G(x,F(x))≤G(x+1,F(x+1)).
Therefore, F(x+1)≤F(x+2). /square
Theorem 4.6.Prim=clos/an}⌊ra⌋ketle{tS,In
k,ˆF,+,subst,M[F]/an}⌊ra⌋ketri}ht, where ˆFis not non-
decreasing.
Proof. IfˆFis not non-decreasingthen exists a naturalnumber athat veriﬁes
ˆF(a)>ˆF(a+1). Let Gbe deﬁned as G(x) =ˆF(x+a), i.eG≡ˆF◦Sa. Thus,
G(0)> G(1). Let Hbe deﬁned as H(x) =G(x)−·G(1), i.e. H≡PG(1)◦G,
UNARY PRIMITIVE RECURSIVE FUNCTIONS 9
whereP≡M[I2
1]. Thus, H(0)> H(1) = 0.
Next, let Sgn≡M[1 ]. It follows easily that
Sgn(H(Sgn(0))) =Sgn(H(0)) = 1,
Sgn(H(Sgn(x+1))) = Sgn(H(1)) = 0.
Therefore, O≡Sgn◦H◦Sgn. And now, we can apply Theorem 4.4. /square
Remark 4.7.In this section, we ﬁxed the value of ato zero. However, we
could have ﬁxed the value of ato another number.5
We will show that scheme rec7can be expressed using rec5witha >0. First, we
deﬁne the functions below:
ˆP(0) =a,ˆP(x+1) =x,i.e.ˆP≡Ma[I2
1],
a(0) =a,a(x+1) =a(x),i.e.a≡Ma[I2
2],
0≡ˆPa◦a,
ˆO(0) =a,ˆO(x+1) = 0,i.e.ˆO≡Ma[0],
where Ma[F](0) =a,Ma[F](x+1) =F(x,Ma[F](x)).
Now, every function Fwhich satisﬁes F(0) = 0 and F(x+1) =H(x,F(x)) will
be written as follows:
G(0) =a,
G(x+1) =H(x,G(x)−a)+a.
By a simple induction, F(x) =G(x)−a, and
M[H]≡ˆPa◦Ma[Sa◦subst(H,I2
1,ˆPa◦I2
2)].
Note also that ˆOis not non-decreasing. So, applying Theorem 4.6 we prove that
Prim=clos/an}⌊ra⌋ketle{tS,In
k,+,subst,Ma[F]/an}⌊ra⌋ketri}ht.
§5. Iteration and diﬀerence. We will follow §5 of [7] (also see Lemma 1 of
[11]), replacing rec6byrec8:Primis generated using a diﬀerence function (may
be|x−y|orx−·y) as the unique initial function. However, we will propose an
equivalent statement. Let Fbe/an}⌊ra⌋ketle{tS,|F−G|,FG,F/square/an}⌊ra⌋ketri}htor/an}⌊ra⌋ketle{tS,F−·G,FG,F/square/an}⌊ra⌋ketri}ht.
Our intention is to prove that Fis suitable.
As much aspossible, we will try to use F−Ginsteadof |F−G|andF−·G, but
taking care of not subtracting two functions that render the exp ression mean-
ingless. In the ﬁrst place,
I≡S/square, D ≡(SS)/square,
0≡S−S, 1≡S0,
Pw≡S(SD)/square, Sgn ≡1/square,
P≡I−Sgn, O ≡1−Sgn.
5This diﬀers from Gladstone, because he used rec5with several values of a(more precisely,
witha∈ {0,1,2}). We show that it is suﬃcient to choose one value for a.
10 DANIEL E. SEVERIN
Next step is to construct the addition. The following sequence of fu nctions/braceleftBigg
f0≡S,
fn+1≡f/square(fn(1))
n
is a kind of Ackermann’s sequences (i.e. if f(x,n) =fn(x) thenfgrows faster
than any primitive recursive function; nevertheless, fnis primitive recursive).
Georgieva[3] discovered a method for constructing the addition be tween two
functions, based on this sequence.
LetF,G∈closF. According to Lemma 6 of [3], there exists i∈Nsuch that
F(x)≤fi(x) for every x(and there exists j∈Nsuch that G(x)≤fj(x)). Letk
be the maximumvalue between iandj. Hence, F(x)+G(x)≤2fk(x). Therefore
(cf. Lemma 7 of [3]),
F+G≡Dfk−((Dfk−F)−G).
Now, we will explain how to construct figivenFby means of the following
recursive deﬁnition:

A:F→N
A(S) = 0
A(|F−G|) = max( A(F),A(G))
A(F−·G) = A(F)
A(FG) = max( A(F),A(G))+2
A(F/square) = A(F)+1
To express F+Gusingrec8insteadof rec6, weneed onlyto generateasequence
that grows faster than fn.
Lemma5.1.The following sequence of functions/braceleftBigg
B0≡S,
Bn+1≡(Sfn(1)Bn)/square
satisﬁes
∀x,n∈NBn(x+1)≥fn(x).
Proof. First, we will try to rewrite fnwith iterations.


f′
0(x) = x,
f′
n+1(0) = 0 ,
f′
n+1(x+1) = gn(f′
n+1(x))
wheregn(x) =fn(1)O(x)+f′
n(x)Sgn(x). Hence, f′
n(x+1) =fn(x) (by a simple
induction on xandn). Consider the sequence


B0(x) = x+1,
Bn+1(0) = 0 ,
Bn+1(x+1) = hn(Bn+1(x))
wherehn(x) =fn(1)+Bn(x). Clearly, Bn+1(x)≥f′
n+1(x) ifBn(x)≥f′
n(x) (by
comparing gnandhn). We conclude that Bn(x+1)≥fn(x). /square
UNARY PRIMITIVE RECURSIVE FUNCTIONS 11
Lemma5.2.IfF,G∈closFthenF+G∈closF.
Proof. Remember that there exists i,j∈Nsuch that F(x)≤fi(x) and
G(x)≤fj(x). By virtue of the previous lemma, F(x)≤Bi(x+1) and G(x)≤
Bj(x+1). Let k= max(i,j), so
F(x)+G(x) = 2Bk(x+1)−((2Bk(x+1)−F(x))−G(x)).
In other words,
F+G≡DBmax( A(F),A(G))S−((DBmax( A(F),A(G))S−F)−G).
/square
Now, we only need to prove that Sq,Rt,Hf ∈closF. We will do this in the
next lemmata.
Lemma5.3.The following families of functions belong to closF:
•Characteristic of n:
On(x) =/braceleftBigg
1ifx=n,
0otherwise .
•Multiplication functions:
Mn(x) =nx.
•Cycle functions:
Cn+2(x) =/braceleftBigg
x+1ifx≤n,
0otherwise .
•Moduli functions:
Modn+2(x) =xmod (n+2).
•Division functions:
Divn+2(x) =⌊x/(n+2)⌋.
Proof. We will show the formulas of each one in the same order. All of them
can be proved easily by induction on n.6
Characteristic of n:
O0≡O, O1≡O(O+P), On+2≡On+1P.
Multiplication functions:
Mn≡(Sn)/square.
Cycle functions:
C2≡O, Cn+3≡Cn+2+Mn+2On+1.
Moduli functions:
Modn+2≡C/square
n+2.
Division functions:
Divn+2≡(S+O Mod n+3S S)/square−I.
6Some proofs can be consulted in §5 of [7].
12 DANIEL E. SEVERIN
/square
Definition 5.4.The conditional operator F→Gis deﬁned as follows
(F→G)(x) =/braceleftBigg
G(x) ifF(x) = 0,
0 otherwise .
Lemma5.5.IfF,G∈closFthenF→G∈closF.
Proof. Letα(x) = 2x+1+Mod2(x)−2x+1. Ifxis even,α(x) = 0. And if xis
odd,α(x) = 2x+1. In formal terms,
α≡Pw(S+Mod2)−Pw S.
Now, wewilldividetheproofintwocasesdependingonthesubstract ionoperator
which we are working:
•Distance: Let β≡(|α−(I+Pw)|+I)−Pw. Ifxis even,β(x) = 2x. And
ifxis odd,β(x) = 0.
•Arithmetic diﬀerence: Let β≡D−·α. Ifxis even,β(x) = 2x. And if xis
odd,β(x) = 0.
Finally, we will observe the behavior of w=β(2z+Sgn(y)). When yis zero,
w= 4z. And when yis positive, w= 0. So, w= 4.(F→G)(x) ify=F(x) and
z=G(x), and
(F→G)≡Div4β(DG+Sgn F).
/square
Lemma5.6.Q∈closF.
Proof. We follow Lemma 2.3 of [4]. Let Wbe deﬁned as follows:
W(x) =

2 if x= 0,
⌊3x/2⌋ifx/ne}ationslash= 0,xmod 10 = 0 ,
⌊2x/5⌋ifx/ne}ationslash= 0,xmod 2/ne}ationslash= 0,xmod 5 = 0 ,
⌊2x/3⌋ifx/ne}ationslash= 0,xmod 3 = 0, xmod 5/ne}ationslash= 0,
⌊15x/2⌋ifx/ne}ationslash= 0,xmod 3/ne}ationslash= 0,xmod 5/ne}ationslash= 0.
For allx >0,W/square(x) mod 3 /ne}ationslash= 0 if and only if xis a square. To write Wwe
must use the operator deﬁned above (see 5.4):
W1(x)≡DO,
W2(x)≡(O+Mod10→Div2M3),
W3(x)≡(O+O Mod 2+Mod5→Div5D),
W4(x)≡(O+Mod3+O Mod 5→Div3D),
W5(x)≡(O+O Mod 3+O Mod 5→Div2M15).
EachWirepresents one case (one line of the deﬁnition of W). The conditions
are mutually exclusive, so W(x) =Wi(x) for some ibetween 1 and 5.
Thus,W≡W1+W2+W3+W4+W5and
Q≡Sgn Mod 3W/square+O.
/square
UNARY PRIMITIVE RECURSIVE FUNCTIONS 13
Lemma5.7.Sq,Rt,Hf ∈closF.
Proof. We follow §5 of [15]. Suppose that R(x) =x+2⌊√x⌋.
Then,R≡(S+DQSSSS )/squareand
Sq≡(SR)/square,Rt≡Div2(R−I),Hf≡Div2.
/square
Theorem 5.8.The bases /an}⌊ra⌋ketle{tS,|F−G|,FG,F/square/an}⌊ra⌋ketri}htand/an}⌊ra⌋ketle{tS,F−·G,FG,F/square/an}⌊ra⌋ketri}htare
both suitable.
Proof. It follows from Lemma 3.3 and Lemmata 5.1-5.7. /square
We conclude this section with the following theorem, which is a consequ ence
of the previous results.
Theorem 5.9.Prim=clos/an}⌊ra⌋ketle{tS,In
k,⊖,subst,F/square/an}⌊ra⌋ketri}ht, where⊖(x,y)can bex−·yor
|x−y|.
Proof. LetF=/an}⌊ra⌋ketle{tS,F⊖G,FG,F/square/an}⌊ra⌋ketri}ht. Invirtue ofTheorem5.8, Fis suitable,
and therefore the operator addition belongs to closF. Note that + ≡I2
1+I2
2,
F⊖G≡subst(⊖,F,G) andFG≡subst(F,G), soclos/an}⌊ra⌋ketle{tS,In
k,⊖,subst,F/square/an}⌊ra⌋ketri}ht ⊇
clos/an}⌊ra⌋ketle{tS,In
k,F⊖G,subst,F/square/an}⌊ra⌋ketri}ht ⊇clos(F+/an}⌊ra⌋ketle{tIn
k,subst/an}⌊ra⌋ketri}ht)⊇clos(F+/an}⌊ra⌋ketle{tIn
k,+,subst/an}⌊ra⌋ketri}ht) =
Prim(use Lemma 3.4). Therefore, Prim=clos/an}⌊ra⌋ketle{tS,In
k,⊖,subst,F/square/an}⌊ra⌋ketri}ht. /square
Remark 5.10.In this section, we used scheme rec6witha= 0. However, we
could have ﬁxed the value of ato another number as we did in 4.7. In fact,
F/square≡ˆPa(SaFˆPa)/square(a)
whereˆPmay be|S−2|orS−·2, and2≡SS(S−S).
Remark 5.11.A further line of inquiry is to analyze if it is possible to rewrite
the operator F→Gusing the diﬀerence F−Ginstead of F−·Gand|F−G|.
For example, if we prove that Sq∈closF, then we can write
(F→G)≡Div2(Sq(OF+G)−(Sq O F)−(Sq G))
and replace F−·Gand|F−G|byF−Gin table 3.
§6. Iteration and unary operator. Robinson[15] proved that /an}⌊ra⌋ketle{tS,E,F+
G,FG,F/square/an}⌊ra⌋ketri}htis a suitable basis. In this section, we will simplify this result,
showing that /an}⌊ra⌋ketle{t1,E,F+,FG,F/square/an}⌊ra⌋ketri}htis suitable too. Let F=/an}⌊ra⌋ketle{t1,E,F+,FG,F/square/an}⌊ra⌋ketri}ht,
where the operator F+is deﬁned as
F+(x) =F(x)+x,
and its precedence is the same as in F/square. The aim of this section is to show that
it is not necessary to have a binary operator such as the addition (e xcept for,
of course, the substitution). In fact, the addition can be replace d by a unary
operator.
14 DANIEL E. SEVERIN
In the ﬁrst place, the following functions belong to closF:
S≡1+, Sgn ≡1/square,
0≡E1, Mod 3≡(ESS)/square,
D≡(0+)+, M 3≡D+,
O≡E D S Sgn, Q ≡OE,
R≡((DQSSS)+S)/square, Sq ≡(SR)/square.
Definition 6.1.In this section, the following operators on one argument
functions will be used:
F−(x) =F(x)−x, (F⊗G)(x) =F(x)G(x),
(F⊕G)(x) =F(x)+G2(x),(F⊖G)(x) =F(x)−G2(x),
whenever F(x)≥xandG(x)≥xfor every x∈N. In the deﬁnition of ⊖,
F(x)≥G2(x) must hold too.7
The precedencesof ⊕and⊖arethe sameasin the addition, while theprecedence
ofF−is the same as in F+. The precedence of ⊗is between addition and
substitution (like products in arithmetical expressions).
Lemma6.2.LetF∈closF. IfF(x)≥xfor every x, thenF−∈closF.
Proof. Robinson has proved that, if α≥β, then
α−β=E((α+β)2+3α+β+1).
If we take α=F(x) andβ=x, the formula becomes
F(x)−x=E((F(x)+x)2+3F(x)+x+1).
The following diagram shows how to compute F−:
x/ma√sto(Sq F+)+
−−−−−−→ F+(x)2+x/ma√sto(M3FE)+
− −−−−− → F+(x)2+3F(x)+x/ma√stoES− − →F−(x)
sinceE((F(x)+x)2+x) =x. Therefore,
F−≡ES(M3FE)+(Sq F+)+.
/square
Lemma6.3.Hf,Rt∈closF.
Proof. Cf.§5 of [15]:
Hf≡((S Mod 3+)/square)−,
Rt≡Hf R−.
/square
Lemma6.4.LetF,G∈closF. IfF(x)≥xandG(x)≥x, thenF⊕G∈
closF. IfF(x)≥G2(x)too, then F⊖G∈closF.
7G2(x) must be read as G(x)G(x), and not G(G(x)).
UNARY PRIMITIVE RECURSIVE FUNCTIONS 15
Proof. We will use the fact that if G(x)≥xthenE(G2(x)+x) =x. Hence,
x/ma√sto(Sq G)+
−−−−−→ G2(x)+x/ma√sto(F−E)+
−−−−− → F(x)+G2(x),
x/ma√sto(Sq G)+
−−−−−→ G2(x)+x/ma√sto(F+E)−
−−−−− → F(x)−G2(x).
Therefore,
F⊕G≡(F−E)+(Sq G)+,
F⊖G≡(F+E)−(Sq G)+.
/square
Lemma6.5.LetF,G∈closF. IfF(x)≥xandG(x)≥x, thenF⊗G∈
closF.
Proof. Note that
α2β2=/bracketleftbig
(α2+1)2+β2−α4−1/bracketrightbig2−β4
4−α4.
If we take α=F(x) andβ=G(x), we can reach αβwith
F⊗G≡Rt((Hf Hf(SqˆP((Sq S Sq F ⊕G)⊖Sq F)⊖Sq G))⊖Sq F),
whereˆP≡((Sq+)+)+⊖S, i.e.ˆP(x+1) =x. /square
Lemma6.6.LetF,G∈closF. So,F+G∈closF. IfF(x)≥G(x)for every
x, thenF−G∈closFtoo.8
Proof. First, we will compute the sum F+(x)+G+(x) by using the following
properties: ( α+β)2= 2αβ+α2+β2,F+(x)≥x,G+(x)≥x,
F+(x)+G+(x) =/radicalbig
2F+(x)G+(x)⊕F+(x)⊕G+(x).
Now, see that F(x)+G(x) =F+(x)+G+(x)−2x. Therefore,
F+G≡((Rt((D(F+⊗G+)⊕F+)⊕G+))−)−.
To compute F−G, we can use the same trick as in Lemma 6.2. Finally,
F−G≡ES(Sq(F+G)+M3F+G).
/square
Theorem 6.7./an}⌊ra⌋ketle{t1,E,F+,FG,F/square/an}⌊ra⌋ketri}htis a suitable basis.
Proof. It follows from Lemma 3.3 and Lemmata 6.2-6.6. /square
Now, we will prove that Fis suitable if we use K(orL) instead of E. Let
F′=/an}⌊ra⌋ketle{t1,K,F+,FG,F/square/an}⌊ra⌋ketri}ht. Following p. 664 of [16],
S≡1+, Sgn ≡1/square,
0≡K1, D ≡(0+)+,
Y≡((Sgn K)+S)/square, Z ≡(SSK+)/square,
8The formula of F−Gworks well even when F(x)< G(x) for some values of x. We can
use them regardless of the values which render the formula me aningless.
16 DANIEL E. SEVERIN
whereY(x) = 2x−⌊√x⌋andZ(x) =x(x+3)/2.
LetF∈closF′. Using the fact that
K((α+β)(α+β+3)/2+2α+3) =α−β,ifα≥β,
we see that
I−F≡KSSS((ZF+)+)+,ifx≥F(x),
and that
D−F≡KSSS(((((Z(F+)+)+)+)+)+),if 2x≥F(x).
Thus,
Rt≡D−Y,Sq≡(((1+)+Rt)+)/square,E≡I−Sq Rt.
The application of Theorem 6.7 makes F′suitable. If F′′=/an}⌊ra⌋ketle{t1,L,F+,FG,F/square/an}⌊ra⌋ketri}ht,
we may deﬁne Kby the formula K≡L1+((1+)+L)+, and F′′results to be
suitable too.
Theorem 6.8./an}⌊ra⌋ketle{t1,K,F+,FG,F/square/an}⌊ra⌋ketri}htand/an}⌊ra⌋ketle{t1,L,F+,FG,F/square/an}⌊ra⌋ketri}htare suitable bases.
Remark 6.9.In this section, we used scheme rec6witha= 0. However, we
could have ﬁxed the value of ato another number as we did in 5.10. We only
need a suitable predecessor ˆP, i.e.ˆP(x+1) =x.
Fora= 1, we can proceed as follows:
(I) Let F=/an}⌊ra⌋ketle{tE,F+,FG,F/square(1)/an}⌊ra⌋ketri}ht. Then,
O≡E/square(1), 0≡EO,
1≡O0, S≡1+,
D≡(0+)+, Q≡OE,
G≡(((DQSS)+S)/square(1))/square(1), ˆP≡ESS((G+)+)+,
whereG(x) = (x+1)2. Now, iteration can be deﬁned with
F/square≡ˆP(SFˆP)/square(1).
In virtue of Theorem 6.7, Fis suitable.
(II) Let F=/an}⌊ra⌋ketle{tK,F+,FG,F/square(1)/an}⌊ra⌋ketri}ht. Then,
O≡K/square(1), 0≡KO,
1≡O0, S ≡1+,
H≡S(SK+S)/square(1), ˆP≡KSSS(H+)+,
whereH(x) = (x+1)(x+4)/2. Now, iteration can be deﬁned as in (I). In virtue
of Theorem 6.8, Fis suitable.
(III) Let F=/an}⌊ra⌋ketle{tL,F+,FG,F/square(1)/an}⌊ra⌋ketri}ht. Then,
1≡L/square(1), K ≡L1+((1+)+L)+,
and by using (II) we prove that Fis suitable.
Fora >1 it is not known if /an}⌊ra⌋ketle{tX,F+,FG,F/square(a)/an}⌊ra⌋ketri}ht, forX∈ {E,K,L}, is suitable.
This question will be studied in the future.
UNARY PRIMITIVE RECURSIVE FUNCTIONS 17
§7. Acknowledgments. I would like to thank the referee for his helpful
comments and suggestions.
REFERENCES
[1]Paul Axt ,Iteration of relative primitive recursion ,Math. Ann. , vol. 167 (1966), pp.
53–55.
[2]Nelu Dima ,Sudan function is universal for the class of primitive recur sive functions ,
St. Cerc. Mat. , vol. 33 (1981), pp. 59–67.
[3]Nadejda Georgieva ,Another simpliﬁcation of the recursion scheme ,Arch. Math.
Logik, vol. 18 (1976), pp. 1–3.
[4] ,Classes of one-argument recursive functions ,Z. Math. Logik Grundlagen
Math., vol. 22 (1976), pp. 127–130.
[5]Giorgio Germano andStefano Mazzanti ,Primitive iteration and unary functions ,
Ann. Pure Appl. Logic , vol. 40 (1988), pp. 217–256.
[6]M. D. Gladstone ,A reduction of the recursion scheme ,J. Symbolic Logic , vol. 32
(1967), no. 4, pp. 505–508.
[7] ,Simpliﬁcations of the recursion scheme ,J. Symbolic Logic , vol. 36 (1971),
no. 4, pp. 653–665.
[8]Reuben L. Goodstein ,Recursive number theory: A development of recursive
arithmetic in a logic-free equation calculus , 1957.
[9]Andrzej Grzegorczyk ,Some classes of recursive functions ,Rozprawy Matematy-
czne, vol. 4 (1953), pp. 1–44.
[10]Stefano Mazzanti ,Bounded iteration and unary functions ,MLQ Math. Log. Q. ,
vol. 51 (2005), pp. 89–94.
[11]Jovan Naumovi ´c,A classiﬁcation of the one-argument primitive recursive fu nctions,
Arch. Math. Logik , vol. 23 (1983), pp. 161–174.
[12]Robert W. Ritchie ,Classes of recursive functions based on Ackermann’s functi on,
Paciﬁc J. Math. , vol. 15 (1965), no. 3, pp. 1027–1044.
[13]Julia Robinson ,General recursive functions ,Proc. Amer. Math. Soc. , vol. 1 (1950),
no. 6, pp. 703–718.
[14] ,A note on primitive recursive functions ,Proc. Amer. Math. Soc. , vol. 6
(1955), no. 4, pp. 667–670.
[15]Raphael M. Robinson ,Primitive recursive functions ,Bull. Amer. Math. Soc. , vol.
53 (1947), no. 10, pp. 925–942.
[16] ,Primitive recursive functions II ,Proc. Amer. Math. Soc. , vol. 6 (1955),
no. 4, pp. 663–666.
[17]Istv´an Szalkai ,On the algebraic structure of primitive recursive function s,Z. Math.
Logik Grundlagen Math. , vol. 31 (1985), pp. 551–556.
FACULTAD DE CIENCIAS EXACTAS, INGENIERIA Y AGRIMENSURA
UNIVERSIDAD NACIONAL DE ROSARIO
ROSARIO, SANTA FE, ARGENTINA
URL: http://www.fceia.unr.edu.ar/ ∼daniel
E-mail: daniel@fceia.unr.edu.ar

arXiv:1005.1987v1  [math.LO]  12 May 2010Iterating the recursively Mahlo operations
Toshiyasu Arai
Graduate School of Science
Chiba University
1-33, Yayoi-cho, Inage-ku, Chiba, 263-8522, JAPAN
Abstract
In this paper we address a problem: How far can we iterate lower
recursively Mahlo operations in higher reﬂecting universes? Or form ally:
How much can lower recursively Mahlo operations be iterated in set th e-
ories for higher reﬂecting universes?
It turns out that in Π N-reﬂecting universes the lowest recursively
Mahlooperationcanbe iteratedalongtowersofΣ 1-exponentialorderings
of height N−3, and that all we can do is such iterations. Namely the
set theory for Π N-reﬂecting universes is proof-theoretically reducible to
iterations of the operation along such a tower.
For set-theoretic formulas ϕ,
P|=ϕ:⇔(P,∈)|=ϕ.
In what follows, let L denote a transitive set, which is a univ erse in dis-
course.P,Q,... denotes transitive sets in L ∪{L}such that ω∈P.
LetXbe a ﬁrst-order class of transitive sets. This means that the re exists
a ﬁrst-order sentence ϕsuch that P∈ X ⇔P|=ϕ. Then a set theory T is
said to prove L ∈ Xiﬀ T⊢ϕ.
A Πi-recursively Mahlo operation for 2 ≤i < ω, is then deﬁned through a
universal Π i-formula Π i(a):
P∈Mi(X) :⇔ ∀b∈P[P|= Πi(b)→ ∃Q∈ X ∩P(b∈Q|= Πi(b))]
(read:Pis Πi-reﬂecting on X.)
Its iteration is deﬁned by transﬁnite recursion on ordinals β:
Mβ
i:=/intersectiondisplay
{Mi(Mν
i) :ν < β}.
Observe that Mi(X) is Πi+1, i.e., there exists a Π i+1-sentence mi(X) such that
P∈Mi(X) iﬀP|=mi(X) for any transitive (and admissible) set P.
A transitive set Pis said to be Π i-reﬂecting ifP∈Mi=M1
i.
Let us denote
X ≺iY:⇔ Y ⊆Mi(X), i.e.,∀P∈ Y(P∈Mi(X)).
1
P∈Mi+1is much stronger than P∈Mi: Assume P∈Mi+1andP|=
Πi(b) forb∈P. ThenP∈MiandP|=mi∧Πi(b) for the Π i+1-sentence
misuch that P∈MiiﬀP|=mi. Hence there exists a Q∈Psuch that
Q|=mi∧Πi(b), i.e.,Q∈Mi&Q|= Πi(b). This means P∈M2
i=Mi(Mi),
i.e.,Mi≺iMi+1. Moreover P∈M△
i, i.e.,P∈/intersectiontext{Mβ
i:β∈ord(P)},
M△
i≺iMi+1, and so on.
In particular a set theory KPΠ i+1for universes in Mi+1proves the consis-
tency of a set theory for universes in M△
i.
In this paper we address a problem: How far can we iterate lowe r re-
cursively Mahlo operations in higher reﬂecting universes? Or formally: How
much can lower recursively Mahlo operations be iterated in s et theories for
higher reﬂecting universes? Speciﬁcally: What kind of iter ations of the lowest
operations M2do we need to obtain equiconsistent theories for set theorie s for
higher reﬂecting universes?
1 Iterations of the operation MiinΠi+1-reﬂectings
In this section we see that iterations of the operation Mialong Σ 1-relations
onωare too short to resolve Π i+1-reﬂecting universes provided that the Σ 1-
relations are provably wellfounded in KPΠ i+1.
Deﬁnition 1.1 1.KPℓdenotesasettheoryforlimitsofadmissibles. KPΠ N
denotes a set theory for universes in MN.
2.For a deﬁnable relation ≺and set-theoretic universe P(admissibility
suﬃces) let
P∈Mi(a;≺) :⇔P∈/intersectiondisplay
{Mi(Mi(b;≺)) :b≺Pa},
whereb≺Pa:⇔P|=b≺a.
Note that Mi(a;≺) is a Π i+1-class for (set-theoretic) Σ i+1≺.
3.We say that a theory T is proof-theoretically reducible to another theory
S if T is a Π1
1(onω)-conservative extension of S, and the fact is provable
in a weak arithmetic, e.g., the elementary recursive arithm etic EA.
4.For a relation ≺onω,TI(a,≺) denotes the transﬁniteinduction schema
up toa∈ω:
{∀x∈ω[∀y≺xϕ(y)→ϕ(x)]→ ∀x≺aϕ(x) :ϕis a set-theoretic formula }
andTI(a,≺,Πn) its restriction to Π n-formulas ϕ.
Using a universal Π n-formula, TI(a,≺,Πn) is equivalent to a single
Πn+2-formula.
5.A relation ≺onωis said to be almost wellfounded in KPℓif KPℓproves
the transﬁnite induction schema TI(a,≺) up toeacha∈ω.
2
It is easy to see the following lemma using the fact that Mi(a;≺) is Πi+1.
Lemma 1.2 Let≺be aΣ1relation on ω. ThenKPΠi+1(i≥2)proves
∀a∈ω[TI(a,≺,Πi+1)→L∈Mi(a;≺)].
A fortiori KPΠi+1proves∀a∈ω[TI(a,≺,Πi+1)→L∈M2(Mi(a;≺))].
In other words, KPℓprovesP∈Mi+1→ ∀a∈ω[TI(a,≺P,ΠP
i+1)→P∈
Mi(a;≺)].
Therefore ∀a∈ω[L∈Mi(a;≺)] is too weak to reduce KPΠ i+1proof-
theoretically for any Σ 1relation≺onω, for example KPΠ i+1⊢CON(∀a∈
ω[L∈Mi(a;≺)]) if∀a∈ω[TI(a,≺)] is provable in KPΠ i+1.
Nonetheless Π i+1-reﬂecting universes can be approximated by iterations of
the operation Mialong well founded Σ 1relations on ω.
Theorem 1.3 For each i(2≤i < ω)there exists a Σ1almost wellfounded
relation ✁iinKPℓsuch that KPΠi+1is proof-theoretically reducible to the
theory
KPℓ+{L∈Mi(a;✁i) :a∈ω}.
Theorem 1.3 follows from Lemma 3.2 and Theorem 3.5 below.
The case i= 2 means that the set theory KPΠ 3for Π3-reﬂecting universes
can be resolved by iterations of the recursively Mahlo opera tionsM2.
Remark . Although KP ℓis weaker than KPΠ i+1, KPΠ i+1does not prove the
soundness of KP ℓ: Let Fund denote the axiom schema for Foundation. Then
for aϕ∈Σi+2and a standard provability predicate Pr Fundof Fund
KPΠi+1/\⌉}atio\slash⊢ ∀n∈ω[PrFund(⌈ϕ(˙n)⌉)→ϕ(n)]
since KPΠ i+1\Fund⊆Πi+2(i≥2).
Hence even if KPΠ i+1⊢ ∀a∈ω[PrKPℓ(⌈TI(˙a,✁i,Πi+1)⌉)], this does not
imply KPΠ i+1⊢ ∀a∈ωTI(a,✁i,Πi+1).
2Π3-reﬂecting on Π3-reﬂectings
Our goal is to approximate Π i+1-reﬂecting universes by iterations of the lowest
recursively Mahlo operations M2. Let us consider ﬁrst the simplest case:
Π3-reﬂecting universes on Π 3-reﬂectings, M2
3=M3(M3). Universes in M2
3
are seen to be resolved in terms of iterations of the operatio nM2along a
lexicographic ordering on pairs.
Deﬁnition 2.1 1.For a Σ 1relation ≺onω,W(≺) denotes the well-
founded part of≺:
a∈W(≺) :⇔ ∀f∈ωω∃n∈ω[f(0) =a→f(n+1)/\⌉}atio\slash≺f(n)].
3
W(≺) is Π1.
Note that W(≺Q) is asetin limits of admissibles Pfor any transitive
setQ∈P.
2.For two transitive relations <1,<0onω,<L:≡L(<1,<0) denotes the
lexicographic ordering:
/a\}brack⌉tl⌉{tn1,n0/a\}brack⌉tri}ht<L/a\}brack⌉tl⌉{tm1,m0/a\}brack⌉tri}ht:⇔n1<1m1or (n1=m1&n0<0m0).
L(<1,<0) is Σ1if<1and<0are Σ1.
<LWdenotes the restriction of <Lto the wellfounded part in the second
component:
/a\}brack⌉tl⌉{tn1,n0/a\}brack⌉tri}ht<LW/a\}brack⌉tl⌉{tm1,m0/a\}brack⌉tri}ht:⇔ /a\}brack⌉tl⌉{tn1,n0/a\}brack⌉tri}ht<L/a\}brack⌉tl⌉{tm1,m0/a\}brack⌉tri}ht&n0,m0∈W(<0).
<LWis ∆2if<1and<0are Σ1.
Proposition 2.2 LetPbe a limit of admissibles and <be aΣ1relation on
ω. Suppose P|=a∈W(<). Thena∈WP(<Q) =W(<Q)andQ|=TI(a,<)
for anyQ∈P, where
a∈WP(<Q) :⇔ ∀f∈ωω∩P∃n∈ω[f(0) =a→f(n+1)/\⌉}atio\slash<Qf(n)].
Proof. Since <is Σ1andQ⊆P, we have <Q⊆<P. Hencea∈WP(<P)⊆
WR(<Q) for any R⊆P. Therefore a∈WP(<Q) =WQ+(<Q) =W(<Q) for
the set<QinP, and the next admissible Q+∈PaboveQ. This yields the
transﬁnite induction schema TI(a,<Q) up toa. ✷
KPΠ3(Π3) denotes a set theory for universes in M3(M3).
Lemma 2.3 Let<1,<0be twoΣ1transitive relations on ω, and<LWthe
restriction of the lexicographic ordering deﬁned from thes e to the wellfounded
part in the second component.
ThenKPΠ3(Π3)proves
∀a,α∈ω[TI(a,<1,Π3)→L∈M2(/a\}brack⌉tl⌉{ta,α/a\}brack⌉tri}ht;<LW)].
Proof. Let L ∈M3(M3). By transﬁnite induction on aalong<1we show
∀α∈ω[L∈M2(/a\}brack⌉tl⌉{ta,α/a\}brack⌉tri}ht;<LW)]
where
P∈M2(/a\}brack⌉tl⌉{ta,α/a\}brack⌉tri}ht;<LW)⇔P∈/intersectiondisplay
{M2(M2(/a\}brack⌉tl⌉{tb,β/a\}brack⌉tri}ht;<LW)) :/a\}brack⌉tl⌉{tb,β/a\}brack⌉tri}ht<P
LW/a\}brack⌉tl⌉{ta,α/a\}brack⌉tri}ht}
and
/a\}brack⌉tl⌉{tb,β/a\}brack⌉tri}ht<P
LW/a\}brack⌉tl⌉{ta,α/a\}brack⌉tri}ht ⇔ /a\}brack⌉tl⌉{tb,β/a\}brack⌉tri}ht<P
L/a\}brack⌉tl⌉{ta,α/a\}brack⌉tri}ht&P|=α,β∈W(<0).
Suppose that ∀b <1a∀β∈ω[L∈M2(/a\}brack⌉tl⌉{tb,β/a\}brack⌉tri}ht;<LW)], and/a\}brack⌉tl⌉{tb,β/a\}brack⌉tri}ht<LW/a\}brack⌉tl⌉{ta,α/a\}brack⌉tri}ht.
We show L ∈M2(M2(/a\}brack⌉tl⌉{tb,β/a\}brack⌉tri}ht;<LW)).
4
IH yields the case b <1a. Assume b=aandβ <0α∈W(<0). Suppose
aϕ∈Π2holds in L ∈M3(M3). Pick a Q∈L∩M3so thatQ|=ϕand
Q∈/intersectiontext{M2(M2(/a\}brack⌉tl⌉{tb,γ/a\}brack⌉tri}ht;<LW)) :Q|=b <1a∧γ∈W(<0)}by IH.
We claim that Q∈M2(/a\}brack⌉tl⌉{ta,β/a\}brack⌉tri}ht;<LW). By Proposition 2.2 we have Q|=
TI(β,<0). Hence we have Q∈M2(/a\}brack⌉tl⌉{ta,β/a\}brack⌉tri}ht;<LW) by transﬁnite induction on
β. ✷
Theorem 2.4 There exist Σ1transitive relations <1,<0onωsuch that <1
is almost wellfounded in KPℓ, andKPΠ3(Π3)is proof-theoretically reducible
to the theory
KPℓ+{L∈/intersectiondisplay
{M2(M2(/a\}brack⌉tl⌉{ta,α/a\}brack⌉tri}ht;<LW)) :α∈W(<0)}:a∈ω}
for the restriction <LWof the lexicographic ordering <L=L(<1,<0)deﬁned
from these to the wellfounded part in the second components.
For a proof of Theorem 2.4, see [A ∞b].
3ΠN-reﬂection
As you expected, an exponential structure involves in resolving Π N-reﬂecting
universes L.
Deﬁnition 3.1 Let<1,<0be two transitive relations on ω.
1.The relation <E=E(<1,<0) is on sequences /a\}brack⌉tl⌉{t(n1
i,n0
i) :i < ℓ/a\}brack⌉tri}htof pairs
with<1-decreasing ﬁrst components ( n1
i+1<1n1
i), and is deﬁned by
/a\}brack⌉tl⌉{t(n1
i,n0
i) :i < ℓ0/a\}brack⌉tri}ht<E/a\}brack⌉tl⌉{t(m1
i,m0
i) :i < ℓ1/a\}brack⌉tri}htiﬀ
either
∃k∀i < k∀j <2[nj
i=mj
i&(n1
k,n0
k)<L(m1
k,m0
k)]
or
ℓ0< ℓ1&∀i < ℓ0∀j <2[nj
i=mj
i]
where<L=L(<1,<0) in Deﬁnition 2.1.2.
Write/summationtext
i<ℓπn1
in0
ifor/a\}brack⌉tl⌉{t(n1
i,n0
i) :i < ℓ/a\}brack⌉tri}ht.
2.Letdom(<E) denote the domain of the relation <E:
dom(<E) :={/summationdisplay
i<ℓπn1
in0
i:∀i < ℓ˙−1(n1
i+1<1n1
i)&n1
i,n0
i,ℓ∈ω}.
3.<EWdenotes the restriction of <Eto the wellfounded part in the second
components:
α=/summationdisplay
i<ℓ0πn1
in0
i<EW/summationdisplay
i<ℓ1πm1
im0
i=βiﬀ
α <Eβ&{n0
i:i < ℓ0}∪{m0
i:i < ℓ1} ⊆W(<0).
5
Lemma 3.2 Let<1,<0be two transitive relations on ω,<1is∆2,<0isΣ1,
and<EWthe restriction of the exponential ordering deﬁned from the se to the
wellfounded part in the second components. Then KPℓproves for each i≥2
∀P∈L∪{L}∀a∈ω∀α <Pa[P∈Mi+1(Mi+1(a;<1))→P∈Mi(α;<EW)]
where for α=/summationtext
i<ℓπn1
in0
i∈dom(<P
E),α <Pa:⇔n1
0<P
1a.
Proof. We show for any a∈ωand any β∈dom(<P
EW↑a)
P∈Mi+1(Mi+1(a;<1))&P∈Mi(β;<EW)→ ∀α <Pa{P∈Mi(β+α;<EW)}
by main induction on P∈L∪{L}with respect to the relation ∈, where for
β=/summationtext
i<ℓ1πm1
im0
iandα=/summationtext
i<ℓ0πn1
in0
i,
β∈dom(<P
EW↑a) :⇔β∈dom(<P
EW)&(ℓ1>0→a≤P
1m1
ℓ1−1)
andβ+α=/summationtext
i<ℓ1πm1
im0
i+/summationtext
i<ℓ0πn1
in0
i.
Suppose β∈dom(<P
EW↑a),P∈Mi+1(Mi+1(a;<1)) andP∈Mi(β;<EW
). Pick an α=πbx+α0∈dom(<P
EW) sothat α0<Pb <P
1aandx∈WP(<P
0).
We show P∈Mi(β+α;<EW). It suﬃces to show P∈Mi(Mi(β+γ;<EW))
for anyγ <P
EWαbyP∈Mi(β;<EW).
Ifγis the empty sequence, then P∈Mi(Mi(β;<EW)) follows from P∈
Mi(β;<EW), which is Π i+1, andP∈Mi+1(Mi+1(a;<1))⊆Mi+1.
Letγ=πcy+γ0withγ0<Pc≤P
1b, andP|=θfor aθ∈Πi. It suﬃces
to ﬁnd a Q∈Pso thatQ∈Mi(β+γ;<EW) andQ|=θ.
First consider the case when c <P
1b. ByP∈Mi+1(Mi+1(a;<1)), pick a
Q∈Pso thatQ∈Mi+1(a;<1),Q|=θ,β∈dom(<Q
EW↑a),Q∈Mi(β;<EW)
anddom(<Q
EW)∋γ <Qb <Q
1a.
ThenQ∈Mi+1(a;<1)⊆Mi+1(Mi+1(b;<1)), and hence MIH yields Q∈
Mi(β+γ;<EW).
Thus we have shown P∈/intersectiontext{Mi(β+δ;<EW) :δ <Pb}, which is Π i+1, and
hence
P∈Mi(Mi+1(a;<1)∩/intersectiondisplay
{Mi(β+δ;<EW) :δ < b}) (1)
Second consider the case when c=b.
We can ﬁnd a Q∈Pso thatQ∈Mi+1(a;<1),Q|=θ,β∈dom(<Q
EW↑a),
Q∈/intersectiontext{Mi(β+δ;<EW) :δ <Qb}by (1) and dom(<Q
EW)∋γ&b <Q
1a. We
havex∈WP(<P
0)⊆W(<Q
0) by Proposition 2.2.
Therefore it suﬃces to show
∀x∈W(<Q
0)∀b∈ω∀β∈dom(<Q
EW↑b)[Q∈P&Q∈Mi+1(Mi+1(b;<1))&
Q∈/intersectiondisplay
{Mi(β+δ;<EW) :δ <Qb}
=⇒ ∀γ0<Qb{Q∈Mi(β+πbx+γ0;<EW)}]
by subsidiary induction on x∈W(<Q
0).
6
First assume β+πby+δ0<Q
EWβ+πbx+γ0withy <Q
0x. SIH yields
Q∈Mi(β+πby+δ0;<EW), and this implies Q∈Mi(Mi(β+πby+δ0;<EW))
byQ∈Mi+1.
Therefore we have shown Q∈Mi(β+πbx;<EW) withγ0= 0. Now
letγ0=πcy+γ1withc <Q
1b. We have β+πbx∈dom(<Q
EW↑c),Q∈
Mi+1(Mi+1(b;<1))&Q∈Mi(β+πbx;<EW) andQ∈P. Hence MIH yields
Q∈Mi(β+πbx+γ0;<EW) forγ0<Qb. ✷
Deﬁnition 3.3 Let<i(2≤i≤N−1) be Σ 1relations on ω.Deﬁne a tower
relation<Tfrom these as follows.
Deﬁne inductively relations <Ei(2≤i≤N−1).
1.<EN−1:≡<N−1.
2.<Ei:≡E(<Ei+1,<i) for 2≤i≤N−2, cf. Deﬁnition 3.1.
Then let
<T:≡<E2.
<TWdenotes the restriction of <Tto the wellfounded parts in the second
components hereditarily. Namely <TW=<E2Wand
/summationdisplay
n<ℓπαnxn∈dom(<EiW) :⇔
∀n < ℓ˙−1(αn+1<Ei+1Wαn)&∀n < ℓ(xn∈W(<i))
with<EN−1W=<N−1.
Fora∈ωandα=/summationtext
n<ℓπαnxn∈dom(<T), deﬁne inductively
α < a:⇔ ∀n < ℓ(αn< a)
withαn< a:⇔αn<N−1aforαn∈ω.
Lemmas 3.2 and 1.2 yield the following for the set theory KPΠ Nfor uni-
verses in MN.
Theorem 3.4 Let<i(2≤i≤N−1< ω)beΣ1transitive relations on ω.
Let<TWdenote the restriction of the tower <Tof the exponential orderings
<Eideﬁned from these to the wellfounded parts in the second comp onents
hereditarily.
ThenKPΠNproves that
∀a∈ω∀α < a[TI(a,<N−1,ΠN)→L∈M2(α;<TW)]
and hence
∀a∈ω∀α < a[TI(a,<N−1,ΠN)→L∈M2(M2(α;<TW))].
We see an optimality of this resolving of Π N-reﬂecting universes in terms
of the lowest recursively Mahlo operation M2.
7
Theorem 3.5 For each N(2< N < ω )there exist Σ1transitive relations
<i(2≤i≤N−1)onωsuch that <N−1is almost wellfounded in KPℓ, and
KPΠNis proof-theoretically reducible to the theory
KPℓ+{L∈/intersectiondisplay
{M2(M2(α;<TW)) :dom(<TW)∋α < a}:a∈ω}
for the restriction <TWof the tower <Tof the exponential orderings <Eide-
ﬁned from these to the wellfounded parts in the second compon ents hereditarily.
Theorem 3.5 is extracted from proof-theoretic analyses of K PΠNin [A∞a]
and [A∞b] . Let me spend some words on ordinal analyses , an ordinal infor-
mative proof-theoretic investigations in generalities.
4 Background materials from proof theory
Let T be a recursive theory containing ACA 0[the predicative (and hence con-
servative) extension of the ﬁrst order arithmetic PA], and Π1
1-sound, i.e., any
T-provable Π1
1-sentence is true in the standard model.
Then its proof-theoretic ordinal |T|is deﬁned to be the supremum of the
order types of the provably recursive well orderings:
|T|:= sup{α < ωCK
1: T⊢WO[<]&
α= order type |<|of<for a recursive ordering <}
Remark . The ordinal |T|is stable if we consider Σ1
1-orderings and/or add
true Σ1
1-sentences to T ⊇ACA0, an anlogue to the C. Spector’s boundedness
theorem. For a proof see [A98].
It is seen that |T|is recursive, i.e., |T|< ωCK
1, and easy to cook up a recursive
well ordering <Twhose order type is equal to |T|.
For each p∈ωlet<pdenote a recursive well ordering deﬁned as follows:
1. The case when pis a G¨ odel number of a proof in T whose endformula is
WO[≺] for a recursive binary relation ≺: Then put <p:=≺.
2. Otherwise, let <pdenote an empty ordering, i.e., dom(<p) =∅.
Glue these orderings together to get a recursive ordering <T:
/a\}brack⌉tl⌉{tn,p/a\}brack⌉tri}ht<T/a\}brack⌉tl⌉{tm,q/a\}brack⌉tri}ht:⇔[p=q&n <pm]∨p < q
for a bijective pairing function /a\}brack⌉tl⌉{tn,p/a\}brack⌉tri}ht.
Then<Tis a recursive well ordering by the assumptions, and
|<T| ≤ |T|= sup{|<p|:p∈ω} ≤ |<T|< ωCK
1as desired.
Gentzen’scelebratedpioneeringworkyields |ACA0|=ε0. Theﬁrstachieve-
ment for proof theory of impredicative theory was done by G. T akeuti. He
8
designedarecursivenotation systemofordinals, whichdes cribestheproofthe-
oretic ordinal of, e.g., Π1
1-Comprehension Axiom. Nowadays Takeuti’s proof
is understood as for set theories of Π 2-reﬂecting universes, i.e., for the Kripke-
Platek set theory with the Axiom of Inﬁnity, KP ω.
Ordinal analyses for stronger theories are now obtained. Le t/a\}brack⌉tl⌉{tO(T),<T/a\}brack⌉tri}ht
denote a notation system of proof-theoretic ordinal of T = AC A0, KPω, KPM,
KPΠN, etc.
Ordinal analyses of theories T show not only the fact |O(T)|=|T|but
also more, i.e., some conservative extension results.
Theorem 4.1 LetEAdenote the elementary recursive arithmetic, a fragment
I∆0+∀x∃y(2x=y)ofPA.
1. If≺is an irreﬂexive, transitive and provably well founded rela tion in
T(not necessarily a total ordering), then there exists an ord inal term α∈
O(T)and an elementary recursive function fso thatEA+∀n,m,k[n/\⌉}atio\slash≺
n&(n≺m≺k→n≺k)]proves that
∀n,k[(n≺k→f(n)<Tf(k))&f(n)<Tα]
2. Over EA,WO[<T]is equivalent to the uniform reﬂection principle
RFNΠ1
1(T)ofTforΠ1
1-formulas.
3.TisΠ1
1-conservative over the theory ACA0∪ {WO[<T|n] :n∈ω},
which is an extension of ACA0by augmenting the wellfoundedness of
eachinitial segment <T|nof the ordering <T.
4. Over EA, the 1-consistency RFNΠ0
2(T)ofTis equivalent to the fact
ERWO[<T]that there is no elementary recursive descending chain of
ordinals in O(T).
5.TisΠ0
2-conservative over the theory EA∪{ERWO[<T|n] :n∈ω}.
Therefore provably recursive functions in Tare exactly the functions
deﬁned by ordinal recursions along initial segments <T|n(n∈ω).
6. Over EA, ﬁnitely iterated consistency statements CON(n)(T)ofT
CON(0)(T) :⇔ ∀x(0 = 0); CON(n+1)(T) :⇔CON(T+CON(n)(T))
is equivalent to the inference rule
[q(α)<Tα→A(q(α))]→A(α)
A(α)
whereαdenotes a variable ranging over O(T), andA[q] is an elementary
recursive relation [function], resp.
9
For a proof of Theorem 4.1.1, see [A98]. Theorem 4.1.6 is seen from The-
orem 4.1.4 through an Herbrand analysis and a result due to W. Tait[Tait65].
The rest of Theorem 4.1 is seen from Lemma 4.2 below, cf. [A96a ], [A96b],
[A97a], [A97b], [A99], [A00a], [A00b], [A03b], [A04a], [A0 4b] , [A∞a] and
[A∞b]. Also cf. [A02], [A03a], [A05a], [A05b] and [A06] for proo f theory
based on epsilon substitution method.
Lemma 4.2 1.Tproves that each initial segment <T|nis wellfounded.
The proof is uniform in the sense that
EA⊢ProofT(p(x),WO[<T|x])
for an elementary recursive function p(x)and a canonical proof predicate
ProofT(x,y)(read:xis a (code of a) T-proof of a (code of a) formula
y).
2. We can deﬁne a rewrite rule(cut-elimination step) r(p,n)on (ﬁnite) T-
proofspofΠ1
1-formulas, and an ordinal assignment o:p/mapsto→o(p)∈O(T)
so thatEAproves
∀n[o(r(p,n))<To(p)→TrΠ1
1(end(r(p,n)))]→TrΠ1
1(end(p))
whereTrΠ1
1denotes a partial truth deﬁnition for Π1
1-sentences, and end(p)
the end-formula of a proof p.
For proofs pofΣ0
1-sentences, the rewrite rule degenerates to be unary,
r(p,n) =r(p,m).
NB.
The size of proof-theoretic ordinals is by no means related t o consistency
strengths of theories. Only when we restrict to initial segm ents of notation
systemsO(T), the sizes are relevant. Cf. [Beklemishev00] and [Beckm ann02]
for some pathological examples on provably well orderings.
LetCON(T ,n) :⇔ ∀x≤n¬ProofT(x,⌈0 = 1⌉) denoteapartialconsistency
of T up to n.
1. ([Kreisel77])
Letn≺mdenote a recursive relation deﬁned as follows:
n≺m:⇔[CON(T,min{n,m})&n < m]∨[¬CON(T,min{n,m})&n > m].
Even though | ≺ |=ωsince T is assumed to be consistent, WO[≺]
implies CON(T) ﬁnitistically.
2. ModifyingtheaboveKreisel’spathological example, one seesthatforany
recursive and Bool(Π1
1)-sound theory T ( Bool(Π1
1) denotes the Boolean
10
combinations of Π1
1-sentences), there exists a recursive and Bool(Π1
1)-
sound theory T′such that |T|<|T′|but T′/\⌉}atio\slash⊢CON(T): let <Tbe any
recursive well ordering of type |T|, and let
n≺′m:⇔CON(T,max{n,m})&n <Tm.
Although | ≺′|=|<T|,≺′is a ﬁnite ordering if T is inconsistent. A
fortiori EA ⊢ ¬CON(T) →WO[≺′]. Hence T /\⌉}atio\slash⊢WO[≺′]→CON(T) by
the second incompleteness theorem. Therefore T′:= T∪{WO[≺′]}is a
desired one.
Note that if each initial segment of <Tis provably wellfounded in T,
then so is for ≺′.
5 Collapsing functions iterated
The essential step in cut-elimination for a set theory T is to analyse the axiom
expressing an ordinal σreﬂects any Π 2-formula ϕ:
ϕLσ(a)∧a∈Lσ→ ∃β < σ[ϕLβ∧a∈Lβ].
This means that given a proof ﬁgure Pof the premise, we have to ﬁnd an
ordinal term β < σ:
....P
ϕLσ(a)∧a∈Lσ=⇒....
ϕLβ(a)∧a∈Lβ
Thisisdonebyputting β=dσα < σ(o(P) =α∈Od(T))fora(Mostowski)
collapsing function d.
LetC(α)(α=o(P)) denote the set of ordinals which may occur in the
reducts of P. Ordinals in C(α) are on the solid lines with gaps here and there
in the following ﬁgure:
0
[dσα
)σ
[σ+dσα
) .........
By stuﬃng the gap below σin the set C(α) up,σis collapsed down to the
least indescribable ordinal dσα. Then ordinals in C(α) cannot discriminate
between σanddσα
γ < σ⇔γ < dσα(γ∈C(α)),
Thus the ordinal β=dσαcan be a substitute for σ.
To analyse larger ordinals, e.g., Π 3-reﬂecting ordinals, the collapsing pro-
cess has to be iterated.
A Π3-reﬂecting ordinal Kis understood to be < εK+1-recursively Mahlo,
LK∈/intersectiontext
µ<εK+1Mµ
2. FirstKis collapsed to a µ0-recursively Mahlo ordinal
for aµ0< εK+1:κ1=dµ0
Kα0< K. Then L κ1∈Mµ0
2is collapsed to a µ1-
recursively Mahlo ordinal: κ2=dµ1κ1α1< κ1(µ1< µ0), etc. In this way a
11
possibly inﬁnite collapsing process is generated: K=κ0> dµ0
Kα0=κ1>
dµ1κ1α1=κ2>···(εK+1> µ0> µ1>···).
We have designed a recursive notation system /a\}brack⌉tl⌉{tOd(ΠN),</a\}brack⌉tri}htof ordinals for
prooftheoretical analysis of KPΠ N, andshowed in[A ∞a] that KPΠ Nisproof-
theoretically reducible to the theory ACA 0+{WO[<|α] : Ω> α∈Od(ΠN)},
where Ω ∈Od(ΠN) denotes the least Π 2-reﬂecting ordinal ωCK
1and<|αthe
restriction of the ordering <inOd(ΠN) toα. ThusO(KPΠN) =Od(ΠN)|Ω.
On the other side in [A ∞b] we have shown that KPΠ NprovesWO[<|α]
foreachα <Ω. Indeed, this wellfoundedness proof is essentially forma lizable
in a theory KP ℓ+{L∈/intersectiontext{M2(M2(α;<TW)) :dom(<TW)∋α < a}:a∈ω}
for some Σ 1relations <i(2≤i≤N−1) onωsuch that <N−1is almost
wellfounded in KP ℓ. This shows Theorem 3.5.
In the next section we give a sketch of the wellfoundedness pr oof.
6 Wellfoundedness proof
Our wellfoundedness proof of Od(ΠN) is based on the maximal distinguished
classW[Buchholz75], a Σ 1-deﬁnable set of integers, and a proper classin
KPΠN.
To formalize the proof inKPΠN, we have to show for each η∈Od(ΠN)
there exists an η-Mahlo set on which the maximal distinguished class enjoys
the same closure properties as Wup to the given η. Theη-Mahlo sets are
deﬁned through a ramiﬁcation process to resolve the reﬂecti ng universes in
terms of iterations of lower Mahlo operations[A ∞b].
6.1 The notation system Od(ΠN)
The notation system Od(ΠN) (an element of Od(ΠN) is called an ordinal
diagram, which is abbreviated o.d.) contains the constants Ω for ωCK
1andπ
for the least Π N-reﬂecting ordinal.
Themainconstructoristoformano.d. dq
σα < σfromasymbol dando.d.’s
σ,q,α, whereσdenotes a recursively regular ordinal and qa ﬁnite sequence
of o.d.’s.
γ≺2σdenotes the transitive closure of {(β,σ) :∃α,q(β=dq
σα)}. The
set{τ:σ≺2τ}is ﬁnite and linearly ordered by ≺2for each σ, namely
{σ:σ/pr⌉c⌉⌈⌉s⌉qual2π}is a tree with its root π.
In the diagram dq
σα,qincludes some data telling us how the diagram dq
σα
is constructed from {τ:dq
σα≺2τ}={τ:σ/pr⌉c⌉⌈⌉s⌉qual2τ}.
The main task in wellfoundedness proofs is to show the tree {σ:σ/pr⌉c⌉⌈⌉s⌉qual2π}
to be wellfounded.
Speciﬁcally qinη=dq
σαincludes some data sti(η),pdi(η),rgi(η) for 2≤
i < N.stN−1(η) is an o.d. less than επ+1, andpd2(η) =σ.
A relation ≺iis deﬁned from pdi(η) as the transitive closure of {(η,κ) :
κ=pdi(η)}. This enjoys ≺i+1⊆≺i. Therefore the diagram pdi(η) is a proper
12
subdiagram of η.sti(η) is an o.d. less than the next admissible κ+to a
κ=rgi(η)≤pdi+1(η).rgN−1(η) =πfor any such η=dq
σα.
qdetermines a sequence {ηm
i:m < lh i(η)}of subdiagrams of ηwith its
lengthlhi(η) =n+1>0. The sequence enjoys the following property:
η/pr⌉c⌉⌈⌉s⌉quali+1η0
i≺i+1η1
i≺i+1··· ≺i+1ηn
i< π
withsti(ηm
i)<(rgi(ηm
i))+.
6.2 Towers derived from ordinal diagrams
Deﬁne relations ≪ifor 2≤i≤N−1 by
η≪iρ:⇔η≺iρ&rgi(η) =rgi(ρ)&sti(η)< sti(ρ).
Extend≪iby augmenting the least element 1:
1≪iη.
παdenotesπα·1.
Let✁i:≡<Eibe exponential ordering deﬁned from ≪i(2≤i≤N−1).
Namely✁N−1:≡≪N−1and✁i:≡E(✁i+1,≪i), cf. Deﬁnition 3.1.
Extend✁ito✁+
iby addingthe successor function +1. Namely the domain
is expanded to dom(✁+
i) :=dom(✁i)∪{a+1 :a∈dom(✁i)}, and deﬁne for
a,b∈dom(✁i)
a+1✁+
ib+1 :⇔a✁ib
a+1✁+
ib:⇔a✁ib
a✁+
ib+1 :⇔a✁ibora=b
From the sequence {ηm
i: 2≤i < N−1,m < lh i(η)}we deﬁne a tower
T(η) =E2(η). The elements of the form Ei(η)(+1) are understood to be
ordered by ✁+
i. Let✁T:≡✁+
2.
EN−1(η) :=η
Ei(η) :=/summationdisplay
1≤m<lhi(η)πEi+1(ηm
i)ηm−1
i+πEi+1(η0
i)+1+πEi+1(η)
The sequence {ηm
i:m < lh i(η)}is deﬁned so that, cf. [A ∞b] for a proof,
γ≺iη⇒Ei(γ)✁+
iEi(η).
In particular
γ≺2η⇒T(γ)✁TT(η) (2)
13
6.3 Distinguished classes
An elementary fact on the maximal distinguished class Wsays that Wis well
ordered by <onOd(ΠN), andW|Ω is included in the wellfounded part of
Od(ΠN). Therefore it suﬃces to show η∈ Wforeachη∈Od(ΠN).
Wis deﬁned to be the union of the distinguished sets,
W=/uniondisplay
{X⊆Od(T) :D[X]}
whereD[X](read:Xis a distinguished set) is a ∆ 1-formula on limits of admis-
sible sets. Hence Wis a Σ1-deﬁnable set of integers, and a proper classin
KPΠN.
SinceD[X] is ∆1on limits of admissibles, it is absolute: D[X]⇔P|=
D[X] for any X∈P∩P(ω). LetWP=/uniontext{X∈P:P|=D[X]}denote the
maximal distinguished class on P.
The following is a key on distinguished sets.
Lemma 6.1 There exists a Π2-formula g(η)(η∈Od(ΠN))for which the fol-
lowing holds for any limits Qof admissibles: Assume g(η)Qand
∀γ≺2η{g(γ)Q⇒γ∈ WQ} (3)
Then there exists a distinguished class Xsuch that η∈XandXis deﬁnable
inQ.
For some Σ 1classesUionω, theΣ 1transitive relations on ω,<imentioned
in Theorem 3.5 are now deﬁned to be
η <iρ:⇔η≪iρ&η,ρ∈Ui.
By deﬁnition 1 ∈Uifor anyi.<N−1is seen to be almost wellfounded in KP ℓ.
Let<TWdenote the restriction of the tower <Tof the exponential order-
ings<Eideﬁnedfrom theseΣ 1relations <i(2≤i≤N−1) to thewellfounded
parts in the second components hereditarily.
In other words,
T(η)<TT(ρ)⇔T(η)✁TT(ρ)&∀i[Ki(η)∪Ki(ρ)⊆Ui]
and
T(η)<TWT(ρ)⇔T(η)<TT(ρ)&∀i >0[Ki(η)∪Ki(ρ)⊆W(<i)]
where
1.K2(η) :={ηm
2:m < lh 2(η)}.
2. For 2 < i < N −1,Ki(η) :={ρm
i:m < lh i(ρ),ρ∈ Ki−1(η)}.
Lemma 6.2 IfP∈M2(M2(T(η);<TW)), theng(η)P→η∈ WP.
14
Proofby induction on ∈. Suppose P∈M2(M2(T(η);<TW)) andg(η)P. Pick
aQ∈P∩M2(T(η);<TW) so that g(η)Q.
We show (3). Assume γ≺2ηandg(γ)Q. (2) yields T(γ)✁TT(η). On the
other side the Π 2formulag(γ) is deﬁned so that
g(γ)Q→ ∀i[Ki(γ)⊆UQ
i]&∀i >0[Ki(γ)⊆WQ(<Q
i)].
Since/uniontext
iKi(η) is ﬁnite, we can assume ∀i[Ki(η)⊆UQ
i], and hence T(γ)<Q
TW
T(η). Therefore Q∈M2(M2(T(γ);<TW)). IH yields γ∈ WQ. This shows
(3).
By Lemma 6.1, let Xbe a distinguished class deﬁnable over Qsuch that
η∈X. ThusX∈P&D[X], andη∈ WP. ✷
Assuming L ∈M2(M2(T(η);<TW)) for each η, we have g(η)L→η∈
WL=Wby Lemma 6.2. On the other side, it is not hard to show g(η)Lfor
eachηin KPℓ.
Therefore the wellfoundedness of Od(ΠN) up to each η <Ω follows from
{L∈M2(M2(T(η);<TW)) :η∈Od(ΠN)}over KPℓ.
References
[A96a] T. Arai, Systems of ordinal diagrams, draft, 1996.
[A96b] T. Arai, Proof theory for theories of ordinals I: Reﬂe cting ordinals,
draft, 1996.
[A97a] T. Arai, Proof theory for theories of ordinals II: Σ 1-stability, draft,
1997.
[A97b] T. Arai, Proof theory for theories of ordinals III: Π 1-collection, draft,
1997.
[A98] T. Arai, Some results on cut-elimination, provable we ll-orderings, in-
duction and reﬂection, Ann. Pure Appl. Logic 95 (1998) 93-18 4.
[A99] T. Arai, Introduction to ﬁnitary analyses of proof ﬁgu res, In: Sets and
Proofs. Invited papers from Logic Colloquium ’97-European Meeting of
theAssociation for SymbolicLogic, Leeds, July1997. Ed. by S. B. Cooper
and J. K. Truss, London Mathematical Society Lecture Notes, vol. 258,
Cambridge University Press (1999), pp.1-25.
[A00a] T.Arai, OrdinaldiagramsforrecursivelyMahlouniv erses, Arch.Math.
Logic 39 (2000) 353-391.
[A00b] T. Arai, Ordinal diagrams for Π 3-reﬂection, Jour. Symb. Logic 65
(2000) 1375-1394.
[A02] T. Arai, Epsilon substitution method for theories of j ump hierarchies,
Arch. Math. Logic 41 (2002) 123-153.
15
[A03a] T. Arai, Epsilon substitution method for ID1(Π0
1∨Σ0
1), Ann. Pure
Appl. Logic 121 (2003) 163-208.
[A03b] T. Arai, Proof theory for theories of ordinals I:recu rsively Mahlo ordi-
nals, Ann. Pure Appl. Logic 122 (2003) 1-85.
[A04a] T. Arai, Proof theory for theories of ordinals II:Π 3-Reﬂection, Ann.
Pure Appl. Logic 129 (2004) 39-92.
[A04b] T. Arai, Wellfoundedness proofs by means of non-mono tonic inductive
deﬁnitions I: Π0
2-operators, Jour. Symb. Logic 69 (2004) 830-850.
[A05a] T. Arai, Ideas in the epsilon substitution method for Π0
1-FIX, Ann.
Pure Appl. Logic 136 (2005) 3-21.
[A05b] T. Arai, Epsilon substitution method for [Π0
1,Π0
1]-FIX, Arch. Math.
Logic 44 (2005) 1009-1043.
[A06] T. Arai, Epsilon substitution method for Π0
2-FIX, Jour. Symb. Logic 71
(2006) 1155-1188.
[A∞a] T. Arai, Proof theory for theories of ordinals III:Π N-reﬂection, sub-
mitted.
[A∞b] T. Arai, Wellfoundedness proofs by means of non-monotoni c inductive
deﬁnitions II: ﬁrst order operators, submitted.
[Beckmann02] A. Beckmann, A non-well-founded primitive re cursive tree
provably well-founded for co-r.e. sets, Arch. Math. Logic 4 1(2002) 251-
257.
[Beklemishev00] L. Beklemishev, Another pathological wel l-ordering, in Logic
Colloquium 98(Prague), 105-108, Lect. Notes Logic 13, Asso c. Symb.
Logic, 2000.
[Buchholz75] W. Buchholz, Normalfunktionen und konstrukt ive Systeme von
Ordinalzahlen. In: Diller, J., M¨ uller, G.H.(eds.) Proof T heory Sympo-
sion, Kiel 1974 (Lecture Notes in Mathematics, vol.500, pp. 4-25). Berlin:
Springer 1975
[Kreisel77] G. Kreisel, Wie die Beweistheoire zu ihren Ordi nalzahlen kam und
kommt, Jber. Deutsch. Math.-Verein 78(1977), 177-223.
[Richter-Aczel74] W.H. Richter and P. Aczel, Inductive deﬁ nitions and re-
ﬂecting properties of admissible ordinals, Generalized Re cursion Theory,
Studies in Logic, vol.79, North-Holland, 1974, pp.301-381 .
[Tait65] W. W. Tait, Functionals deﬁned by transﬁnite recur sion, Jour. Symb.
Logic 30 (1965) 155-174.
16

arXiv:1312.7275v2  [math.LO]  13 Apr 2015Arithmetical Foundations
Recursion.Evaluation.Consistency
Michael Pfender∗
June 7, 2021
Abstract
Primitiverecursion, mu-recursion, universal objectandu ni-
versetheories, complexity controlled iteration, codeeva luation,
soundness, decidability, G¨ odel incompleteness theorems , incon-
sistency provability for set theory, constructive consist ency.
Contents
1 Primitive Recursion 7
1.1 Theory PRof primitive recursion . . . . . . . . 7
1.2 Full scheme of primitive recursion . . . . . . . . 15
1.3 A monoidal presentation of theory PR. . . . . 16
1.4 Introduction of free variables . . . . . . . . . . 21
∗michael.pfender@alumni.tu-berlin.de
1
1.5 Goodstein FV arithmetic . . . . . . . . . . . . 23
1.6 Substitutivity and Peano induction . . . . . . . 25
1.7 Map deﬁnition by distinction of cases . . . . . . 26
1.8 Integer division and primes . . . . . . . . . . . 26
2 Predicate Abstraction 28
3 Partial Maps 34
3.1 Theory of partial maps . . . . . . . . . . . . . . 34
3.2 Structure theorem for partials . . . . . . . . . . 36
3.3µ-recursion without quantiﬁers . . . . . . . . . 37
3.4 Content driven loops . . . . . . . . . . . . . . . 38
4 Universal set 41
4.1 Strings as polynomials . . . . . . . . . . . . . . 41
4.2 Universal object /CGof numerals and nested pairs 42
4.3 Universe monoid PR /CG. . . . . . . . . . . . . . 45
4.4 Typed universe theory PR /CGa. . . . . . . . . . 48
5 Evaluation of p.r. map codes 50
5.1 Complexity controlled iteration . . . . . . . . . 51
5.2 PR code set . . . . . . . . . . . . . . . . . . . . 53
5.3 Iterative evaluation . . . . . . . . . . . . . . . . 53
5.4 Evaluation characterisation . . . . . . . . . . . 57
6 PR Decidability by Set Theory 61
6.1 PR soundness framed by set theory . . . . . . . 62
6.2 PR-predicate decision by set theory . . . . . . . 68
6.3 G¨ odel’s incompleteness theorems . . . . . . . . 71
2
7 Consistency Decision within πR 73
7.1 Termination conditioned soundness . . . . . . . 74
7.2 Framed consistency . . . . . . . . . . . . . . . . 84
7.3πRdecision . . . . . . . . . . . . . . . . . . . . 87
7.4 Consistency provability . . . . . . . . . . . . . 94
Introduction
Recursivemaps,nowadayscalled primitive recursive maps, have
beenintroducedby G¨odelinhis1931 article forthearithmeti-
sation,g¨ odelisation, of metamathematics.
For construction of his undecidable formula he introduces a
non-constructive, non-recursive predicate beweisbar, provable.
Staying within the area of (categorical) free-variables th e-
oryPRof primitive recursion or appropriate extensions opens
the chance to avoid the two G¨ odel’s incompleteness theorem s:
these are stated for Principia Mathematica und verwandte Sys-
teme,“related systems” such as in particular Zermelo-Fraenkel
settheoryZFand v. Neumann G¨ odel Bernays settheory
NGB.
On the basis of primitive recursion we consider µ-recursive
maps as partial p.r. maps. Specialterminating general re-
cursive maps considered are complexity controlled iterations.
Complexity takes values within appropriate (countable) or di-
nal /C6[ω] of polynomials in one indeterminate. Map code eval-
uationforPRis given in terms of such an iteration.
We discussiterative map codeevaluation indirection of ter-
mination conditioned soundness, and based on this µ-recursive
3
decisionofprimitiverecursivepredicates, withunexpect edcon-
sequences:
Inconsistency provability for the quantiﬁed theories, as well
asconsistency provability and logical soundness for the theory
πRof primitive recursion strengthened by an axiom scheme of
non-inﬁnite descent of complexity controlled iterations like in
particular (iterative) p.r. map-code evaluation.
Overview
We ﬁxconstructive foundations for arithmetic on a mapthe-
oretical, algorithmical level. In contrast to elementhood and
quantiﬁcation based traditional foundations such as Principia
Mathematica PMor Zermelo-Fraenkel set theory ZF,ourfun-
damental primitive recursive theory PRhas as its “undeﬁned”
terms just terms for objects and maps. On that language level
it isvariable free, and it is free from formal quantiﬁcation over
individuals such as numbers or number pairs.
This theory PRis a formal, combinatorial category with
cartesian i.e. universal productand a natural numbers object
(NNO) /C6,ap.r. cartesian category, cf.Rom`an1989.
The NNO /C6admitsiteration of endo maps and the full
scheme of primitive recursion. Such NNO has been introduced
in categorical terms by Freyd1972, on the basis of the NNO
ofLawvere 1964.
We will remain on the purely syntactical level of this cate-
gorical theory, and later extensions: no formal semantics nec-
essary into an outside, non-combinatorial world. Cf. Hilbe rt’s
formalistic program.
4
We introduce into our variable-free settingfree variables,
asnamesfor identities and projections. As a consequence, we
have in the present context ‘ free variable ’ as adeﬁnednotion.
We have object and map constants such as terminal object,
NNO, zero etc. and use free metavariables for objects and for
maps.
Fundamental arithmetic is further developed along Good-
stein’s 1971free variables Arithmetic whoseuniqueness rules
are derived as theorems of categorical theory PR,with its
“eliminable” notion of a free variable. This gives the expected
structure theorem for the algebra and order on NNO /C6.“On
the way”, via Goodstein’s truncated subtraction, and his com-
mutativity of maximum function, we obtain the Equality De-
ﬁnability theorem: Ifpredicative equality of two p.r. maps is
derivably true, then map equality between these maps is deri v-
able.
The subsequent section brings into the game an embed-
ding theory extension of PRbyabstraction ofpredicates into
“virtual” new objects. This enrichment makes emerging basic
theoryPRa=PR+ (abstr) more comfortable, in direction
tosettheories, with their setsandsubsets.
Section 3 introduces the general concept of a partialp.r.
map, states a structure theorem on theory P/hatwideRaof partials
and shows that µ-recursive maps and while-loop programs are
partial p.r. maps.
Section 4 exhibits within theory PRaauniversal object /CG,
of allnumerals and nested pairs of numerals, and constructs
by means of that object universe theories PR /CGandPR /CGa:
theoryPR /CGis good for a one-object map-code evaluation,
5
PR /CGacontains PRaas acartesian p.r.embeddedtheorywith
predicate extensions.
Section 5 on evaluation strengthens p.r. theory PR /CGainto
descent theory πR,by an axiom of non-inﬁnite iterative de-
scentwith order values in polynomial semiring /C6[ω] ordered
lexicographically (priority to higher powers of ω).
This theory is shown to derive the—free variable p.r.—
consistency formula for theories PR /CGa(andPR). The proof
relies on constructive, complexity controlled code evaluation,
which is extended to evaluation of argumented deduction trees:
Theorem on p.r. soundness withinsettheory as frame
(section 6), and termination conditioned soundness ofPRa⊂
PR /CGawithin theory πRtaken as frame (section 7).
Consequence is decidability of p.r. predicates by both the-
ories. Since consistency formulae Con of both theories can b e
expressed as (free variable) p.r. predicates, this leads to
1.Inconsistency provability ofsettheorybyG¨ odel’s second
incompleteness theorem, and to
2.Consistency provability (and soundness) of descent the-
oryπR,underassumption ofµ-consistency, a set theoreti-
cally equivalent) variant of ω-consistency.
You ﬁnd detailed proofs for the structure results of section s
1-5 in chapters 1-5 of (scratch) book Pfender 2012/14. This
book contains also a proof for (objective) soundness of descent
theoryπR.
6
1 Primitive Recursion
Almost everything in this long ﬁrst section is known from cla s-
sical free-variables Arithmetic, see Goodstein 1971. What we
need formally for our categorical p.r. free-variables Arit hmetic
is categorical schemes of iteration and primitive recursio n, cat-
egorical interpretation of free variables as identities an d projec-
tions out ofcartesian productsaswell as proofof Goodstein ’s
rules U 1-U4for the categorical theory PRof primitive recur-
sion developped here from scratch.
1.1 Theory PR of primitive recursion
We ﬁx here terms and axioms for the fundamental categori-
cal (formally variable-free) cartesian theory PRof primitive
recursion.
Thebasic objects of the theory PRarethenatural numbers
object(‘NNO’) /C6and theterminal object /BD.
Composed objects of PRcome in as “cartesian” products
(A×B) of objects already enumerated. Formally:
A,Bobjects
(ObjCart)
(A×B) object
[Here outmost brackets may be dropped]
Maps:Basic maps (“map constants”) of the theory PR
are
thezero map 0 : /BD→ /C6,and
thesuccessor map s : /C6→ /C6
7
Structure of PR as a category:
•generation—enumeration—of identity maps
Aan object
(id generation)
idA:A→Amap
•composition:
f:A→B, g:B→Cmaps
(◦)
(g◦f) :A→Cmap.
Here are the axioms making PRinto a category:
•associativity of composition:
f:A→B, g:B→C, h:C→Dmaps
(◦ass)
h◦(g◦f) = (h◦g)◦f:A→D
•neutrality of identities.
•map equality f=g:A→Bis to satisfy the axioms of
reﬂexivity, symmetry, andtransitivity.
•composition is to be compatible with equality:
f=f′:A→B, g:B→C
(◦=1st)
(g◦f) = (g◦f′) :A→B→C
8
f:A→B, g=g′:B→C
(◦=2nd)
(g◦f) = (g′◦f) :A→B→C
Cartesian map structure:
•generation of terminal maps
Aobject
Π = Π A:A→ /BDmap
•uniqueness axiom for terminal map family:
Aobject,f:A→ /BDmap
(Π!)
f= ΠA:A→ /BD
•generation of left and right projections:
A, Bobjects
(proj)
ℓ=ℓA,B:A×B→Aleft projection,
r = rA,B:A×B→Bright projection
9
•generation of induced maps into products:
f:C→A, g:C→Bmaps
(ind)
(f,g) :C→A×Bmap,
the map inducedbyfandg
•compatibility of induced map formation with equality:
f=f′:C→A, g=g′:C→Bmaps
(ind=)
(f,g) = (f′,g′) :C→A×B
•characteristic ( Godement ) equations
f:C→A, g:C→B
(GODE ℓ)
ℓ◦(f,g) =f:C→A
as well as
f:C→A, g:C→B
(GODE r)
r◦(f,g) =g:C→B
10
incommutative diagram form:
A
Cf/d49/d49
=
(f,g)/d47/d47
=
g /d45/d45A×Bℓ/d79/d79
r
/d15/d15
B
•uniqueness of induced map is equivalent to the following
equational axiom of Surjective Pairing , see Lambek-Scott
1986:
h:C→A×B
(SP)
(ℓ◦h,r◦h) =h:C→A×B.
Use compatibility of formingtheinducedmapwith equal-
ity.
•we eventually replace equivalently, given the other ax-
ioms, inferential axiom (ind =) bydistributivity equation
h:D→C, f:C→A, g:C→B
(distr◦)
(f,g)◦h= (f◦h,g◦h) :D→A×B,
see again Lambek-Scott 1986.
11
This ends the preliminary list of axioms—to be varied in
its induced-map part by monoidal category type axioms in
overnext subsection.
Deﬁnition: for a mapg:B→B′,Eilenberg-Elgot 1970
deﬁnecylindriﬁcation with object Aby
A×g=defidA×g=def(idA◦ℓ,g◦r) :A×B→A×B′.
Diagram:
Aid/d47/d47
=A
A×Bℓ/d79/d79
A×g/d47/d47
r
/d15/d15=A×B′ℓ/d79/d79
r
/d15/d15
Bg/d47/d47B′
Axioms for the iteration of endo maps
f:A→A(endo) map
(§)
f§:A× /C6→Aiterated of f,satisﬁes
f§◦(idA,0) = id A:A→A(anchor),[0 := 0Π],
f§◦(A×s) =f◦f§:A× /C6→A(step).
Infree-variables notation (see below):
f§(a,0) =a (anchor),
f§(a,sn) =f(fn(a)) =bydeff(f§(a,n)).(step)
As a ﬁrst example for an iterated endo map take addition
12
+ : /C6× /C6→ /C6characterised by equations
a+0 =a: /C6→ /C6,
a+sn= s(a+n) = (a+n)+1 : /C6× /C6→ /C6,
where 1 = defs◦0 : /BD→ /C6.
uniqueness axiom for the iterated:
f:A→A(endo map)
h:A× /C6→A,
h◦(idA,0) = id Aand
h◦(A×s) =f◦h“as well”
(§!)
h=f§:A× /C6→A
Lemma (compatibility of iteration with equality):
uniqueness axiom ( §!) infers
f=g:A→A
(§=)
f§=g§:A× /C6→A.
These axioms give all objects and maps of theory PR.
Freyd’suniquenessschemewhichcompletes theaxiomscon-
13
stituting theory PR,reads in free variables notation:
f:A→B(init map), g:B→B(endo to be iterated)
h:A× /C6→Bcomparison map:
h(a,0) =f(a) (init)
h(a,sn) =g(h(a,n)),(step)
h(a,n) =gn(f(a)) (uniqueness),
without use of free variables:
f:A→B, g:B→B, h:A× /C6→B,
h◦(idA,0◦ΠA) =f:A→B,(init)
h◦(A×s) =g◦h:A× /C6→B,(step)
(FR!)
h=g§◦(f× /C6) :A× /C6→B× /C6→B,
Remark: This uniqueness of the initialised iterated obvi-
ously specialises to axiom ( §!) of uniqueness of “simple” iter-
atedf§:A× /C6→Aand so makes that uniqueness axiom
redundant.
Problem: Is, conversely, stronger Freyd’s uniqueness ax-
iom already covered by uniqueness ( §!) of “simply” iterated
f§:A× /C6→A? My guess is “no”.
(FR!) above is just required as an axiom, ﬁnal axiom of
theoryPR.
14
1.2 Full scheme of primitive recursion
Already for deﬁnition and characterisation of multiplication
and moreover for proof of “the” laws of arithmetic, the fol-
lowingfull scheme (pr) of primitive recursion is needed:1
Theorem (Full scheme of primitive rec.): PR admits
scheme
g=g(a) :A→B(init map )
h=h(a,n) : (A× /C6)×B→B(step map )
(pr)
f=f(a,n) :A× /C6→B
is given such that
f(a,0) =g(a) and
f(a,sn) =h((a,n),f(a,n))
as well as
(pr!) :fisuniquewith these properties.
1in pure categorical form see Freyd1972, and (then) Pfender ,Kr¨oplin,
andPape1994, not to forget its uniqueness clause
15
Same without use of free variables:
g:A→B,
h: (A× /C6)×B→B
(pr)
pr[g,h] :=f:A× /C6→B,
f(idA,0) =g:A→B,
f(idA×s) =h(idA× /C6,f) :
(A× /C6)→(A× /C6)×B→B,
(pr!) :funique.
This scheme is an axiom in the classical theory of primitive r e-
cursion. The categorical proof out of existence and uniquen ess
of the initialised iterated is given in Rom`an1989.
1.3 A monoidal presentation of theory PR
We present the cartesian axioms of fundamental theory PR
of primitive recursion in terms of primitive recursive diagonal
symmetric half-cartesian monoidal structure. [“half” means
that the mentioned substitution families, here terminals and
projections, need not to be natural transformations.] Plus
cartesianness proper, the latter expressed by
uniqueness of terminal map family Π A:A→ /BDand
Godement ’s equations
f=ℓA,B◦(f,g)≡ℓA,B◦(f×g)◦∆C,
g= rA,B◦(f,g)≡rA,B◦(f×g)◦∆C,
16
cf. the diagonal symmetric half-terminal Categories of Bu-
dach&Hoehnke 1975, “realised” in particular as (classical)
categories of (sets and) partial maps.
Main reason for this alternative presentation is:
Theories P/hatwideRa⊏P/hatwideR /CGaof (genuine) partialp.r. maps to
be introduced in section 2 inherit the structure of a PR sym-
metric diagonal half-cartesian theory from basicp.r. theories
PR /CGa⊐PRato be discussed below.
[Theory PRais embedding extension of PRbypredicate-
into-object abstraction. ]
Godement ’s equations are equivalent to naturality of pro-
jection family for Bifunctor×:T×T−→T,Ta cartesian
theory.
So here is—alternative—presentation of cartesian part of
theoryPRas aPR symmetric diagonal half-terminal theory
with projections:
Replace in the cartesian part of presentation of theory PR
aboveformation of the induced and itsuniqueness equation
(SP) by introduction of the map constants and schemes pro-
ducing equations below.
17
Substitution maps:
Π = Π A:A→ /BD,terminal map for objectA,
Θ = Θ A,B:A×B∼=−→B×A,transposition
∆ = ∆ A:A→A2=A×A,diagonal, duplicate
ℓ=ℓA,B:A×B→Aleft projection,
r = rA,B=ℓB,A◦ΘA,B:A×B→B×A→B
right projection.
Fundamental for this structure of our theory PRis the gener-
ation ofcartesian product of maps by axiom
f:A→A′, g:B→B′maps
(×)
(f×g) : (A×B)→(A′×B′) map,
thecartesian product offandg.
As in case of composition, we state an axiom of compatibil-
ity of cartesian product of maps with (map) equality, namely
f=f′:A→A′, g=g′:B→B′maps
(×=)
(f×g) = (f′×g′) :A×B→A′×B′.
Deﬁnability of Θ and ∆ by the projections reads
18
A, Bobjects
(Θ−proj)
ΘA,B= (rA,B,ℓB,A) :A×B→B×A,
Cobject
(∆−proj)
∆C= (id,id) :C→C×C,i.e.
ℓC,C◦∆ = id C= rC,C◦∆ :
C→C×C→C,
naturality ofprojections reads:
Af/d47/d47
=A′
A×Bℓ/d79/d79
f×g/d47/d47
r
/d15/d15=A′×B′ℓ/d79/d79
r
/d15/d15
Bg/d47/d47B′
cartesian product of maps
We now show the availability of the induced map (f,g) :
C→A×Bfor givenf:C→Aandg:C→B:
Deﬁne
(f,g) =def(f×g)◦∆C:C→C×C→A×B.
19
Then this inducedobviously fullﬁlls Godement ’s equations
A
Cf/d50/d50
=
(f,g)/d47/d47
=
g/d44/d44A×Bℓ/d79/d79
r
/d15/d15
B
uniqueness of the induced map is guaranteed by the earlier
equational axiom (SP) of surjective pairing.
Aconsequenceis compatibility ofinduced map withequality:
it follows from compatibility of composition and of cartesi an
product with equality, combined with the uniqueness of the
induced or with distributivity of composition over forming the
induced map ( Lambek ).
Cartesian product “×” introduced above, becomes a bi-
functor
×:PR×PR−→PR.
This follows from the compatibilities with map equation by
uniqueness of the induced map: see the following commuting
4 squares rectangular diagram:
Af/d47/d47A′f′/d47/d47A′′
A×Bℓ/d79/d79
r
/d15/d15f×g/d47/d47
f′f×g′g/d51/d51A′×B′ℓ/d79/d79
r
/d15/d15f′×g′/d47/d47A′′×B′′ℓ/d79/d79
r
/d15/d15
Bg/d47/d47B′g′/d47/d47B′′
20
Furthermore follows naturality of thesubstitution transfor-
mations Π A,ΘA,B,∆A.
These are the map term and map-term equality construc-
tions for the cartesian part of theory PR,and some of their
immediate consequences.
1.4 Introduction of free variables
We start with a (“generic”) example of elimination of free vari-
ables by their interpretation into (possibly nested) projections:
a distributive law a·(b+c) =a·b+a·cgets the map
interpretation
a·(b+c) = (a·b)+(a·c) :
R3=bydefR2×R=bydef(R×R)×R→R,
withsystematic interpretation of variables:
a:=ℓ ℓ, b:= rℓ, c:= r :R3= (R×R)×R→R,
and inﬁx writing of operations op:R×R→Rpreﬁx inter-
preted as
·◦(a,+◦(b,c)) = +◦(·◦(a,b),·◦(a,c)) :R3→R.
Aniteratedf§:A× /C6may be written in free-variables
notation as
f§=f§(a,n) =fn(a) :A× /C6→A
witha:=ℓ:A× /C6→A,andn:= r :A× /C6→ /C6.
Systematic map interpretation of free-variables Equa-
tions:
21
1. extract the common codomain (domain of values), say
B,of both sides of the equation (this codomain may be
implicit);
2. “expand” operator priority into additional bracket pair s;
3. transform inﬁx into preﬁx notation, on both sides of the
equation;
4. order the (ﬁnitely many) variables appearing in the equa-
tion, e.g lexically;
5. if these variables a1,a2,...,amrange over the objects
A1,A2,...,A m,thenﬁxascommon domain object (source
of commuting diagram), the object
A=A1×A2×...×Am=def(...((A1×A2)×...)×Am);
6. interpret the variables as identities or (possibly nested)
projections, will say: replace, within the equation, all the
occurencesofa variable, bythecorresponding—ingeneral
binary nested —projection;
7. replace each symbol “0” by “0 Π D” where “D” is the
(common) domain of (both sides) of the equation;
8. insert composition symbol ◦between terms which are not
bound together by an induced map operator as in (f1,f2);
9. By the above, we have the following two-maps-cartesian-
Product rule, forth and back: For
a:=ℓA,B: (A×B)→A, b:= rA,B: (A×B)→B,and
f:A→A′as well asg:B→B′,the following identity
22
holds:
(f×g)(a,b) = (f×g)◦(ℓA,B,rA,B)
= (f×g)◦id(A×B)= (f×g)
= (f◦ℓA,B,g◦rA,B)
= (f◦a,g◦b) = (f(a),g(b)) :A×B→A′×B′;
10. for free variables a∈A, n∈ /C6interpret the term fn(a)
as the map f§(a,n) :A× /C6→A.
These 10 interpretation steps transform a (PR) free-variab les
equation into a variable-free, categorical equation of the ory
PR:
Elimination of (free) variables by their interpretation
asprojections, and vice versa: Introduction of free variables as
namesfor projections. We allow for mixed notation too, all
this, for the time being, only in the context of a cartesian (! )
theory.
Allofourtheoriesarefreefromclassical, (axiomatic) for mal
quantiﬁcation. free variables equations are understood na ively
asuniversally quantiﬁed. But a free variable ( a∈A) occurring
only inthepremiseof an implication takes (in suitablecontext)
the meaning
for any given a∈A: premise(a,...) =⇒conclusion,i.e.
if existsa∈As.t.premise(a,...),thenconclusion.
1.5 Goodstein FV arithmetic
In “Development of Mathematical Logic” (Logos Press 1971)
R.L.Goodsteingivesfourbasicuniqueness-rulesforfree- variable
23
Arithmetics. These rules are theorems for theory PR,and
they are suﬃcient for proof of the commutative and associati ve
laws for multiplication and the distributive law, for addit ion as
well as for truncated subtraction a.−n,e.g. 5.−3 = 2,but
3.−5 = 0.
Equality deﬁnability
As basic logical structures, /C6admitsnegation
¬=¬n: /C6→ /C6,as well as
sign = sign n=¬¬n: /C6→ /C6,
sign0 = def0≡false,signsn=def1≡s0≡true :
signn: /C6→ /C6PR decides on positiveness.
(Linear) orderis given by
[m≤n] =def¬[m.−n],[m<n] =defsign(m.−n) :/C6× /C6→ /C6.
Furthermore, we have a fundamental equalitypredicate
[m.=n] =bydef[m≤n]∧[m≥n] : /C6× /C6→ /C6,
[a∧b=defsign(a·b) logical ‘and’] ,
which is an equivalence predicate, and which makes up a tri-
chotomy with strict order above.
Object /C6admits deﬁnition of (Boolean) “logical functions”
bytruth tables, as does set /BEclassically—and below in theory
PRa=PR+ (abstr) of primitive recursion with predicate
abstraction.
24
Equality deﬁnability theorem
f=f(a) :A→B, g=g(a) :A→BinPR,
PR⊢trueA= [f(a).=Bg(a)] :
A∆− →A×Af×g−−→B×B.=B− − → /BE
(EqDef)
PR⊢f=g:A→B,i.e.f=PRg:A→B.
1.6 Substitutivity and Peano induction
Leibniz substitutivity theorem forpredicativeequalityreads:
f:A→BPR-map
a.=a′=⇒f(a).=f(a′) :
A×A→ /C6.
Proofby structural induction on f.
Peano-induction ,derived fromuniqueness part (pr!) of
fullscheme of primitive recursion:
ϕ=ϕ(a,n) :A× /C6→ /C6predicate
ϕ(a,0) = true A(a) (anchor)
[ϕ(a,n) =⇒ϕ(a,sn)] = true A× /C6(induction step )
(P5)
ϕ(a,n) = true A× /C6(conclusio ).
25
1.7 Map deﬁnition by distinction of cases
We have map deﬁnition by case distinction in variable-free
manner,
f=f(a) = if[χ,(g|h)](a) =

g(a) ifχ(a)
h(a) if¬χ(a)(otherwise).:
A→B,
for alternative f,g:A→Band conditioning predicate χ:
A→ /BE.
We use a sumdiagram, “Hilbert’s inﬁnite hotel” /C6∼=/BD+ /C6,more general A× /C6∼=A+A× /C6,
A× /BD /d111/d111∼=/d47/d47
a×0
/d35/d35❍❍❍❍❍❍❍❍❍❍❍❍❍❍A
(a,0)
/d15/d15f
/d32/d32
A× /C6(f|g)/d47/d47B
A× /C6a×s/d79/d79
g/d63/d63
1.8 Integer division and primes
Integer division with remainder (Euclide)
(a÷b,aremb) : /C6× /C6>→ /C6× /C6
is characterised by
a÷b= max{c≤a:b·c≤a}: /C6× /C6>→ /C6,
aremb=a.−(a÷b)·b: /C6× /C6>→ /C6.
26
The predicate a|b: /C6>× /C6→ /C6,ais a divisor of b,adivides
bis deﬁned by
a|b= [(brema).= 0].
Exercise: Construct the Gaussian algorithm for determi-
nation of the gcdofa,b∈ /C6>deﬁned as
gcd(a,b) = max{c≤min(a,b) :c|a∧c|b}: /C6>× /C6>→ /C6>
by iteration of mutual rem .
Primes
Deﬁnethe predicate is a prime by
P(p) =p
∧
m=1[m|p⇒m.= 1∨m.=p] : /C6→ /BE:
Only 1 and pdividep.
The (euclidean) count pn: /C6֌ /C6of all primes is given by
p0= 2,
pn+1= min{p∈ /C6:P(p),pn<p≤/productdisplay
q[q≤pn∧P(q)]}+1
= min{p∈ /C6:P(p),p<2pn}:P֌P,
iterated binary product and iterated binary minimum.
The latter presentation is given by Bertrand ’s theorem.
Notes
(a) An NNO, within a cartesian closed category of sets, was
ﬁrst studied by Lawvere 1964.
27
(b) Eilenberg-Elgot 1970 iteration, here special case of on e-
successoriterationtheory PR,isbecauseofFreyd’sunique-
ness scheme (FR!) a priori stronger than classical free-
variables primitive recursive arithmetic PRAinthesense
ofSmorynski 1977. If viewed as a conservative subsys-
tem ofPM,ZForNGB,thatPRAis stronger than our
PR.
(c) WithinTopoi(withtheircartesianclosedstructure),F reyd
1970 characterised Lawvere’s NNO by unique initialised
iteration. Such Freyd’s NNO has been called later, e.g.
in Maietti 2010, parametrised NNO.
(d) Lambek-Scott1986considerinparallela weak NNO: unique-
ness of Lawvere’s sequences a: /C6→Anot required. We
need here uniqueness (of the initialised iterated) for proo f
of Goodstein’s 1971 uniqueness rules basic for his devel-
opment of p.r. arithmetic. Without the latter uniqueness
requirement, the deﬁnition of parametrised (weak) NNO
is equational.
2 Predicate Abstraction
We extend the fundamental theory PRof primitive recursion
deﬁnitionally by predicate abstraction objects {A:χ}={a∈
A:χ(a)}.We get an (embedding) extension PR⊏PRahav-
ing all of the expected properties.
We discuss a p.r. abstraction scheme as a deﬁnitional en-
richment of PR,into theory PRaofPR decidable objects and
PR maps in between, decidable subobjects of the objects of
28
PR.The objects of PRare, up to isomorphism,/BD, /C61=def
/C6, /C6m+1=def( /C6m× /C6).
[mis a free metavariable, over the NNO constants 0 ,1 =
s0,2 = ss0,...∈ /C6.]
The extension PRais given by adding schemes (Ext Obj),
(ExtMap),and (Ext =) below. Together they correspond to
thescheme of abstraction insettheory, and they are referred
below as schemes ofPR abstraction.
Our ﬁrst predicate-into-object abstraction scheme is
χ:A→ /C6aPR-predicate:
sign◦χ=χ:A→ /C6→ /C6
(ExtObj)
{A:χ}object (of emerging theory PRa)
Subobject {A:χ} ⊆A∼=
/C6nmay be written alternatively,
withboundvariablea,as
{A:χ}={a∈A:χ(a)}.
ThemapsofPRa=PR+(abstr) come in by
{A:χ},{B:ϕ}PRa-objects,
f:A→BaPR-map,
PR⊢χ(a) =⇒ϕf(a)
(ExtMap)
fis aPRa-mapf:{A:χ} → {B:ϕ}
29
In particular, if for predicates χ′, χ′′:A→ /C6
PR⊢χ′(a) =⇒χ′′(a) :A→ /C6× /C6→ /C6,
then id A:{A:χ′} → {A:χ′′}inPRais called an inclusion,
and written ⊆:A′={A:χ′} →A′′={A:χ′′}orA′⊆A′′.
Nota bene: For predicate (terms!) χ, ϕ:A→ /C6such that
PR⊢χ=ϕ:A→ /C6(logically: such that PR⊢[χ⇐⇒
ϕ]) we have
{A:χ} ⊆ {A:ϕ}and{A:ϕ} ⊆ {A:χ},
but—in general—not equality of objects. We only get in this
case
idA:{A:χ}∼=− → {A:ϕ}
as anPRaisomorphism.
A posteriori, we introduce as Reiterdoes, the formal truth
Algebra /BEas/BE=def{n∈ /C6:n≤s0},
with proto Boolean operations on /C6restricting—in codomain
and domain—to booleanoperations on /BEresp./BE× /BE=def{(m,n)∈ /C6× /C6:m,n≤s0},
bydeﬁnitionbelowofcartesianProductofobjectswithin PRa.
PRa-maps with common PRadomain and codomain are
considered equal, if their values are equal on their deﬁning
30
domain predicate. This is expressed by the scheme
f, g:{A:χ} → {B:ϕ}PRa-maps,
PR⊢χ(a) =⇒f(a).=Bg(a)
(Ext=)
f=g:{A:χ} → {B:ϕ}.
Structure Theorem for the theory PRaofprimitive re-
cursion with predicate abstraction:2
PRais a cartesian p.r. theory. The theory PRis cartesian
p.r. embedded. The theory PRahas universal extensions of
all of its predicates and a boolean truth object as codomain o f
these predicates, as well as map deﬁnition by case distincti on.
In detail:
(i)PRainherits associative map composition and identities
fromPR.
(ii)PRahasPRfully embedded by
/angbracketleftf:A→B/angbracketright /mapsto→ /angbracketleftf:{A: trueA} → {B: trueB}/angbracketright
(iii)PRahas cartesian product
{A:χ}×{B:ϕ}=def{A×B:χ∧ϕ},
with projections and universal property inherited from
PR.We abbreviate {A: trueA}byA.
2cf.Reiter1980
31
(iv) object /BEcomes as a sum /BDfalse
0/d47/d47/BE∼=
/BD+ /BD /BDtrue
1/d111/d111over
which cartesian product A×distributes.
This allows in fact for the usual truth-table deﬁnitions
of all boolean operations on object /BEand for p.r. map
deﬁnition by case distinction.
(v) The embedding ⊏:PR−→PRais acartesian func-
tor :it preserves products and their cartesian universal
property with respect to the projections inherited from
PR.
(vi)PRahasextensions of its predicates, namely
Ext[ϕ:{A:χ} → /BE] =def{A:χ∧ϕ} ⊆ {A:χ},
characterised as ( PRa)-equalisers
Equ(χ∧ϕ,trueA) :{A:χ} → /BE.
(vii)PRahas allequalisers, namely equalisers
Equ[f,g] =def{a∈A:χ(a)∧f(a).=Bg(a)}
of arbitrary PRamap pairsf,g:{A:χ} → {B:ϕ},
and hence all ﬁnite projective limits,in particular pull-
backswhich we will rely on later.
Apullback, of a mapf:A→Calonga mapg:B→C,
32
also ofgalongf,is the square in
Dk
/d30/d30
h
/d33/d33(h,k)
/d32/d32
Pg′/d47/d47
f′
/d15/d15=A
f
/d15/d15
Bg/d47/d47C
The embedding preserves such limits as far as available
already in PR.Equality predicate extends to cartesian
products componentwise as
[(a,b).=A×B(a′,b′)]
=def[a.=Aa′]∧[b.=Bb′] :
(A×B)2→ /BE,
and to (predicative) subobjects {A:χ}by restriction.
(viii) arithmetical structure extends from PRtoPRa,i.e.
PRaadmits the iteration scheme as well as Freyd’s
uniqueness scheme: the iterated
f§:{A:χ}×{ /C6: true/C6} → {A:χ}
is just the restricted PR-mapf§:A× /C6→A.
(ix) equality predicate.=A:A2→ /C6,restricted to subobjects
A′={A:χ} ⊆A,inheritsall ofthepropertiesofequality
on /C6and the other fundamental objects.
33
(x) countability: Each fundamental object Ai.e.Aa ﬁnite
power of /C6≡ { /C6: true/C6},admits, by Cantor ’s isomor-
phism
ct = ct/C6× /C6(n) : /C6∼=−→ /C6× /C6,
a retractive count ct A(n) : /C6→A.Same for pointed
objectsAofPRa,admitting a pointα: /BD→A.
3 Partial Maps
We introduce µ-recursive maps as partial p.r. maps, coming as
a p.r. enumeration of deﬁned arguments together with a p.r.
rulemapping the enumeration index of a deﬁned argument
into thevalueof that argument. This covers µ-recursive maps
and content driven loops as in particular while-loops. Code
evaluation will be deﬁnable as such a while-loop.
3.1 Theory of partial maps
Deﬁnition: A partial map f:A⇀Bis a pair
f=/angbracketleftdf:Df→A,/hatwidef:Df→B/angbracketright:A⇀B,
Df
df
/d15/d15/hatwidef
/d32/d32❅❅❅❅❅❅❅❅❅❅❅❅
Af/d47B
The pairf=/angbracketleftdf,/hatwidef/angbracketrightis to fullﬁll the right-uniqueness con-
dition
df(ˆa).=Adf(ˆa′) =⇒/hatwidef(ˆa).=B/hatwidef(ˆa′) :
34
We now deﬁne the theory /hatwideSofpartialS-mapsf:A⇀B.
Objects of /hatwideSare those of S,i.e. ofPRa.Themorphisms
of/hatwideSare thepartialS-mapsf:A⇀B.
Deﬁnition: Givenf′,f:A ⇀ B in/hatwideS,we say that f
extendsf′or thatf′is arestriction off,writtenf′/hatwide⊆f,if
there is given a map i:Df′→DfinSsuch that
(f′/hatwide⊆f)Df′
df′
/d15/d15i
/d15/d15/hatwidef′
/d15/d15=S=SDf
df
/d126/d126⑥⑥⑥⑥⑥⑥⑥⑥⑥⑥⑥/hatwidef
/d32/d32❆❆❆❆❆❆❆❆❆❆❆
Af′
f/d47B
The partial maps fandf′areequalin/hatwideS,iffextendsf′
andf′extendsf:
f′/hatwide⊆f, f/hatwide⊆f′:A⇀B
(/hatwide=S)
f′/hatwide=f:A⇀B.
Deﬁnition: Composition h=g/hatwide◦f:A ⇀ B ⇀ C of/hatwideS
maps
f=/angbracketleft(df,/hatwidef) :Df→A×B/angbracketright:A⇀Band
g=/angbracketleft(dg,/hatwideg) :Dg→B×C/angbracketright:B ⇀C
is deﬁned by the diagram
35
Dh
dh=
/d28/d28πl
/d15/d15πr
/d32/d32❆❆❆❆❆❆❆❆❆❆❆
/hatwideh
/d17/d17Df
df
/d15/d15/hatwidef
/d32/d32❇❇❇❇❇❇❇❇❇❇❇p.b.Dg
dg
/d15/d15/hatwideg
/d32/d32❅❅❅❅❅❅❅❅❅❅=
Af/d47
h=g/hatwide◦f/hatwide=/d59Bg/d47C
Composition diagram for/hatwideS
[The idea is from Brinkmann-Puppe 1969: They con-
struct composition of relations this way via pullback]
3.2 Structure theorem for partials
(i)/hatwideScarries a canonical structure of a diagonal symmetric
monoidal category , with composition /hatwide◦and identities in-
troduced above, monoidal product ×extending ×ofS,
association ass : (A×B)×C∼=−→A×(B×C),symmetry
Θ :A×B∼=−→B×A,anddiagonal ∆ :A→A×A
inherited from S.
(ii)“section lemma:” The ﬁrst factor f:A ⇀ Bin an/hatwideS
composition
h=g/hatwide◦f:A⇀B⇀C,
when giving an (embedded) Smaph:A→C,is itself
an (embedded) Smap:
36
a ﬁrst composition factor of a total map is total.
So each section (“coretraction”) of theory /hatwideSis anSmap,
in particular an /hatwideSsection of an Smap belongs to S.
(iii)/hatwideSinherits from Ssurjective pairing (SP).
3.3µ-recursion without quantiﬁers
We deﬁneµ-recursion within the free-variables framework of
partial p.r. maps as follows:
Given a PRpredicateϕ=ϕ(a,n) :A× /C6→ /BE,the/hatwideS
morphism
µϕ=/angbracketleft(dµϕ,/hatwideµϕ) :Dµϕ→A× /C6/angbracketright:A⇀ /C6
is to have ( S) components
Dµϕ=def{A× /C6:ϕ} ⊆A× /C6,
dµϕ=dµϕ(a,n) =defa=ℓ◦ ⊆:
{A× /C6:ϕ}⊆−→A× /C6ℓ−→A,and
/hatwideµϕ=/hatwideµϕ(a,n) =defmin{m≤n:ϕ(a,m)}:
{A× /C6:ϕ} ⊆A× /C6→ /C6.
Comment: This deﬁnition of µϕ:A⇀ /C6is astaticone,
by enumeration ( ℓ,/hatwideµϕ) :{A× /C6:ϕ} →A× /C6of itsgraph,as
is the case in general here for partialp.r. maps: we start with
givenpairs inenumeration domain {A× /C6:ϕ},and getdeﬁned
argumentsa“only” asdµϕ-enumerated “elements” ( dependent
variable). No need—and in general no “direct” possibility—to
decide,for a given a∈A,ifais of forma=dµϕ(a,n) i.e. if
existsn∈ /C6such thatϕ(a,n).
37
µ-Lemma: /hatwideSadmits the following (free-variables) scheme
(µ) combined with ( µ!)—uniqueness —as a characterisation of
theµ-operator /angbracketleftϕ:A× /C6→ /BE/angbracketright /mapsto→ /angbracketleftµϕ:A⇀ /C6/angbracketrightabove:
ϕ=ϕ(a,n) :A× /C6→ /BES−map (“predicate”) ,
(µ)
µϕ=/angbracketleft(dµϕ,/hatwideµϕ) :Dµϕ→A× /C6/angbracketright:A⇀ /C6
is an/hatwideS-map such that
S⊢ϕ(dµϕ(ˆa),/hatwideµϕ(ˆa)) = true Dµϕ:Dµϕ→ /BE,
+ “argumentwise” minimality:
S⊢[ϕ(dµϕ(ˆa),n) =⇒/hatwideµϕ(ˆa)≤n] :Dµϕ× /C6→ /BE
as well as uniqueness—by maximal extension :
f=f(a) :A⇀ /C6in/hatwideSsuch that
S⊢ϕ(df(ˆa),/hatwidef(ˆa)) = true Df:Df→ /BE,
S⊢ϕ(df(ˆa),n) =⇒/hatwidef(ˆa)≤n:Df× /C6→ /BE
(µ!)
S⊢f/hatwide⊆µϕ:A⇀ /C6(inclusion of graphs)
3.4 Content driven loops
By acontent driven loop we mean an iteration of a given step
endo map, whose number of performed steps is not known at
entry time into the loop—as is the case for a PR iteration
38
f§(a,n) :A× /C6→Awithiteration number n∈ /C6—, but
whose (re) entry into a “new” endo step f:A→Adepends
oncontenta∈Areached so far:
This(re) entry orexitfrom the loop is now controlled by
a(control) predicate χ=χ(a) :A→ /BE.
First example: a while loop wh[χ:f] :A ⇀ A, for given
p.r.controlpredicateχ=χ(a) :A→ /BE,and(looping) step
endof:A→A,both inS,bothS-maps for the time being, S
as always in our present context an extension of PRa,admit-
ting the scheme of (predicate) abstraction. Examples for the
moment: PRa=PR+(abstr) itself, Universe theory PR /CGa
as well as PA↾PR,restriction of PAto its p.r. terms, with
inheritance of all PA-equations for this term-restriction.
Classically, withvariables, such wh = wh[ χ:f] would be
“deﬁned”—in pseudocode —by
wh(a) := [a′:=a;
whileχ(a′) doa′:=f(a′) od;
wh(a) :=a′].
The formal version of this—within a classical, element based
setting—, is the following partial-map characterisation:
wh(a) = wh[χ:f](a) =

aif¬χ(a)
wh(f(a)) ifχ(a):A⇀A.
But can this dynamical, bottom up “deﬁnition” be converted
into a p.r. enumeration of a suitable graph“of allargument-
value pairs ” in terms of an /hatwideS-morphism
wh = wh[χ:f] =/angbracketleft(dwh,/hatwiderwh) :Dwh→A×A/angbracketright:A⇀A?
39
In fact, we can give such suitable, static Deﬁnition of wh within
/hatwideS⊐Sas follows:
wh =deff§/hatwide◦(idA,µϕ[χ|f])
=bydeff§/hatwide◦(A×µϕ[χ:f])/hatwide◦∆A:
A→A×A⇀A× /C6→A,where
ϕ=ϕ[χ:f](a,n) =def¬χf§(a,n) :A× /C6→A→ /BE→ /BE.
Withinaquantiﬁedarithmeticaltheorylike PA,this/hatwideS-deﬁnition
of wh[χ:f] :A ⇀ A fullﬁlls the classical characterisation
quoted above, as is readily shown by Peano-Induction “on”
n:=µϕ[χ:f](a) :A ⇀ /C6,at least within PAand its exten-
sions.
In this generalised sense, we have—within theories /hatwideS⊐
S—all while loops, forthetimebeingat least thosewith control
χ:A→ /BEandstependof:A→AwithinS.
It is obvious that such wh[ χ:f] :A×Ais in general
“only”partial—as is trivially exempliﬁed by integer division
bydivisor0,which would be endlessly subtracted from the
dividend, although in this case controlandstepare both PR.
By the classical characterisation of these while loops above,
we are motivated for its generalisation to the S//hatwideScase:
Characterisation Theorem forwhile loopsoverS,within
theory/hatwideS: Forχ:A→ /BE(control) andf:A→A(step),
both—for the time being— S-maps, while loop wh = wh[ χ:
f] :A⇀A(as deﬁnedabove), ischaracterised bythefollowing
implications within/hatwideS:
/hatwideS⊢ ¬χ◦a=⇒wh/hatwide◦a.=a:A⇀ /BE,and
/hatwideS⊢χ◦a=⇒wh/hatwide◦a.= wh/hatwide◦f◦a.
40
where use of free variable ais to help intuition, formallyais
just another name for id A.
4 Universal set
Within theory PRaof primitive recursion with predicate ab-
straction we construct (in a categorical way) a universal object/CGof allnested pairs of natural numbers in which all objects of
PRa—subobjects of ﬁnite powers of NNO /C6—are embedded.
This gives rise to the theories PR /CG⊏PRaofPRaugmented
by universal object /CG,andPR /CGa,namelyPR /CGwith predi-
cate abstraction which has all properties wanted, see Universal
embedding theorem. These two theories will be basic for the
logical sections to come, on evaluation, soundness, decisi on,
and consistency.
4.1 Strings as polynomials
Stringsa0a1... anof natural numbers (in set /C6+= /C6∗/integerdivide{/square}
of non-empty strings) are coded as prime power products
2a0·3a1·...·pann∈ /C6>0⊂ /C6, pjthejth prime number.
Stringsa0a1... an≡pa0
0·...·pannare identiﬁed with (the
coeﬃcient lists of) “their” polynomials
p(X) =a0+a1X1+...+anXnas well as
p(ω) =a0+a1ω1+...+anωn,
inindeterminate Xresp.ω.
41
Componentwise addition (and truncated subtraction), as
well as
p(ω)·ω=n/summationdisplay
j=0ajωj+1≡n/productdisplay
j=0paj
j+1,
special case of Cauchy product of polynomials.
Lexicographical Orderof NNO strings and polynomials: or-
der priority of (coeﬃcient-)strings from right to left, has —
intuitively, and formally within sets—onlyﬁnite descending
chains.
This applies in particular to descending complexities of
CCI’s:Complexity Controlled Iterations below, with complex-
ity values in /C6[ω]; p.r. map code evaluation will be resolved
into such a CCI .
4.2 Universal object /CGof numerals and
nested pairs
We begin the construction of Universal object by internal nu-
meralisation of all objective natural numbers, of objective nu-
merals
num(0)≡0 : /BD→ /C6,
num(1)≡1 =def(s(0)) : /BD→ /C6→ /C6,
num(2)≡2 =def(s(s(0)) : /BD→ /C6
num(n+1)≡n+1 = def(s(n)) : /BD→ /C6,
n∈ /C6meta-variable.
Internal numerals, numeralisation
ν=ν(n) : /C6→ /C6+≡ /C6∗/integerdivide{0} ≡ /C6>⊂ /C6:
42
ν(0) =def/rightanglenw0/rightanglene: /BD→ /C6code (goedel number ) of 0,
ν(1) =def/a\}b∇ack⌉tl⌉{t/rightanglenws/rightanglene⊙ν(0)/a\}b∇ack⌉t∇i}ht=bydef/a\}b∇ack⌉tl⌉{t/rightanglenws/rightanglene/rightanglenw◦/rightanglene/rightanglenw0/rightanglene/a\}b∇ack⌉t∇i}ht: /BD→ /C6,
abbreviation for (string) goedelisation, here in particul ar for
LaTeXsource code
/rightanglenw(/rightanglene/rightanglenws/rightanglene/rightanglenw◦/rightangleneν(0)/rightanglenw)/rightanglene=/rightanglenw(/rightanglene/rightanglenws/rightanglene/rightanglenw◦/rightanglene/rightanglenw0/rightanglene/rightanglenw)/rightanglene
≡pASCII[(]
0pASCII[s]
1pASCII[\circ]
2 pASCII[0]
3pASCII[)]
4
≡24031155ASCII[\circ]7481141: /BD→ /C6,
an element of /C6,aconstant of /C6,
ν(2) =def/a\}b∇ack⌉tl⌉{t/rightanglenws/rightanglene⊙ν(1)/a\}b∇ack⌉t∇i}ht=/a\}b∇ack⌉tl⌉{t/rightanglenws/rightanglene⊙/a\}b∇ack⌉tl⌉{t/rightanglenws/rightanglene⊙ν(0)/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}htetc. PR:
ν(n+1) = def/a\}b∇ack⌉tl⌉{t/rightanglenws/rightanglene⊙ν(n)/a\}b∇ack⌉t∇i}ht ∈ /C6.
ν(n) hasnclosing brackets (at end).
This internal numeralisation distributes the “elements”, num-
bers of the NNO /C6,with suitable gaps over /C6: the gaps then
will receive in particular codes of any other symbols of ob-
ject Languages PRandPRaas well as of Universe Languages
PR /CGandPR /CGato come.
ν-Predicate lemma: Enumeration ν: /C6→ /C6deﬁnes
a characteristic image predicate im[ ν] : /C6→ /BE,and by this
object
ν /C6={ /C6: im[ν]} ⊂ /C6+
of internal numerals ν /C6∼=
/C6.
Proof:Use ﬁnite ∃—iterative ‘ ∨’—for deﬁnition of im[ ν].
For aPR-mapf: /C6→ /C6deﬁne its numeral twin
˙f=defν◦f◦ν−1:ν /C6ν−1
− − → /C6f− → /C6ν− →ν /C6.
43
Extension of numeral sets and numeralisation to all objects
ofPR(and ofPRa) is straightforward.
Universal objects /CG, /CG⊥of numerals and (nested) pairs
of numerals:
As code for waste symbol we take
⊥=def/rightanglenw⊥/rightanglene≡/rightanglenw\bot/rightanglene: /BD→ /C6.
Deﬁnesets/CG, /CG⊥={ /C6: /CG, /CG⊥: /C6→ /BE} ⊂ /C6
of all (codes of)
•undeﬁned value ⊥,
•numeralsν(n)∈ν /C6,and
•(possibly nested) pairs
/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht=bydef/rightanglenw(/rightanglenex/rightanglenw,/rightangleney/rightanglenw)/rightangleneof numerals
as follows:
•ν /C6⊂ /CG⊂ /C6,numerals proper; further recursively enu-
merated:
• /a\}b∇ack⌉tl⌉{t /CG˙× /CG/a\}b∇ack⌉t∇i}ht=def{/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht:x,y∈ /CG} ⊂ /CG,
set of(nested) pairs of numerals, general numerals, in
particular
/a\}b∇ack⌉tl⌉{t /CG˙×ν /C6/a\}b∇ack⌉t∇i}ht={/a\}b∇ack⌉tl⌉{tx;νn/a\}b∇ack⌉t∇i}ht:x∈ /CG,n∈ /C6} ⊂ /CG;
• /CG⊥=def
/CG∪{⊥} ⊂ /C6+./CG-Predicative Lemma: /CGhas predicative form/CG={ /C6:χ/CG},and /CG⊥={ /C6:χ/CG∨{/rightanglenw⊥/rightanglene}}.
Proofas (technically advanced) Exercise.
44
4.3 Universe monoid PR /CG
The endomorphism set PR( /C6, /C6)⊂PRis itself a monoid, a
categorical theory with just one object.
Embedded “cartesian p.r. Monoid” PR /CG:
•the basic, “super” object of PR /CGis/CG⊥= /CG˙∪{⊥}= /CG˙∪{/rightanglenw⊥/rightanglene} ⊂ /C6,/CG: /C6→ /C6inPR( /C6, /C6) predicate/set of (internal) nu-
merals and nested pairs of numerals.
•therˆ oleoftheNNOwillbetaken bytheabovepredicative
subsetν /C6⊂ /CGof the internal numerals.
•the basic “universe” map constants of PR /CG,
ba∈bas set of those maps, are
–“identity”˚ ıd = id/CG: /C6⊃ /CG→ /CG⊥,/CG∋x/mapsto→x∈ /CG,/C6/integerdivide /CG∋z/mapsto→ ⊥(trash),
trash cases below analogously,
–“zero” (redeﬁned for PR /CG)˚0 : /CG→ /CG⊥,/CG∋ν0/mapsto→ν0∈ν /C6⊂ /CG,
–“successor” ˚ s: /CG⊥→ /CG⊥:
νn/mapsto→ν(sn) =bydef/a\}b∇ack⌉tl⌉{t/rightanglenws/rightanglene⊙ν(n)/a\}b∇ack⌉t∇i}ht,
–“terminal map”: ˚Π : /CG→ν /BD⊂ /CG,/CG∋x/mapsto→ν0∈ν /BD={ν0} ⊂ /CG,
–“left projection”:
˚ℓ: /C6⊃ /CG⊃ /a\}b∇ack⌉tl⌉{t /CG˙× /CG/a\}b∇ack⌉t∇i}ht → /CG⊥,
/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht /mapsto→x∈ /CG, ν /C6∋νn/mapsto→ ⊥.
–“right projection” analogous.
45
•close Monoid PR /CGunder composition of theory PR:
f,ginPR /CG⊂PR( /C6, /C6)
(◦)
(g◦f) inPR /CG,
trash propagation clear.
•“induced map”:
f,ginPR /CG
(ind)
/a\}b∇ack⌉tl⌉{tf.g/a\}b∇ack⌉t∇i}htinPR /CG,deﬁned by/CG∋x/mapsto→ /a\}b∇ack⌉tl⌉{tfx;gx/a\}b∇ack⌉t∇i}ht ∈ /CG.
•“product map”:
f,ginPR /CG
(˙×)
/a\}b∇ack⌉tl⌉{tf˙×g/a\}b∇ack⌉t∇i}htinPR /CG,deﬁned by/CG∋ /a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht /mapsto→ /a\}b∇ack⌉tl⌉{tfx;gy/a\}b∇ack⌉t∇i}ht ∈ /CG,/C6/integerdivide/a\}b∇ack⌉tl⌉{t /CG˙× /CG/a\}b∇ack⌉t∇i}ht ∋z/mapsto→ ⊥.
46
•“iterated” (formally interesting, see last lines):
f: /CG→ /CGPR /CGmap, in particular ⊥/mapsto→ ⊥
(it)
f˙§: /CG⊃ /a\}b∇ack⌉tl⌉{t /CG˙×ν /C6/a\}b∇ack⌉t∇i}ht → /CGinPR /CG,
/a\}b∇ack⌉tl⌉{tx; ˙n/a\}b∇ack⌉t∇i}ht /mapsto→fn(x)∈ /CG,
n=ν−1(˙n),˙n∈˙/C6=ν /C6=bydef{ /C6: im[ν]}free,/C6∋z/mapsto→ ⊥forznot of form /a\}b∇ack⌉tl⌉{tx; ˙n/a\}b∇ack⌉t∇i}ht.
[Predicates ν /C6and/a\}b∇ack⌉tl⌉{t /CG˙×ν /C6/a\}b∇ack⌉t∇i}ht: /C6→ /C6work as auxiliary
objects, subobjects of /CG: /C6→ /C6.]
•Notion of map equality for theory PR /CGis inherited(!)
fromPR( /C6, /C6) i.e. from theory PR.
PR /CGStructure theorem: With emerging (predicative)
objects /CG,ν /BD,ν /C6,
A,Bobjects
/a\}b∇ack⌉tl⌉{tA˙×B/a\}b∇ack⌉t∇i}htobject,
constants, maps, composition above,
•ν /BD={ν0}taken as “terminal object”,
•˚Π : /CG→ν /BDtaken as “terminal map,”
•“Product” taken
/angbracketleft˚ℓ:/a\}b∇ack⌉tl⌉{tA˙×B/a\}b∇ack⌉t∇i}ht →A:/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht →x,
˚r:/a\}b∇ack⌉tl⌉{tA˙×B/a\}b∇ack⌉t∇i}ht →B,/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht →y/angbracketright,
47
• /a\}b∇ack⌉tl⌉{tf.g/a\}b∇ack⌉t∇i}ht:C→ /a\}b∇ack⌉tl⌉{tA˙×B/a\}b∇ack⌉t∇i}ht, x/mapsto→ /a\}b∇ack⌉tl⌉{tfx;gx/a\}b∇ack⌉t∇i}ht,
taken as “induced map,”
• /a\}b∇ack⌉tl⌉{tf˙×g/a\}b∇ack⌉t∇i}ht:/a\}b∇ack⌉tl⌉{tA˙×B/a\}b∇ack⌉t∇i}ht → /a\}b∇ack⌉tl⌉{tA′˙×B′/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht /mapsto→ /a\}b∇ack⌉tl⌉{tfx;gy/a\}b∇ack⌉t∇i}ht,
taken as “map product,”
•/angbracketleftν /BD˚0− →ν /C6˚s− →ν /C6/angbracketrighttaken as NNO,
•andf˙§:/a\}b∇ack⌉tl⌉{t /CG˙×ν /C6/a\}b∇ack⌉t∇i}ht → /CGas iterated of
PR /CGendomapf: /CG→ /CG,/a\}b∇ack⌉tl⌉{tx;νn/a\}b∇ack⌉t∇i}ht /mapsto→fn(x) =f§(x,n),
PR /CGbecomes a cartesian p.r. category with universal ob-
ject.
•Fundamental theory PRis naturally embedded into the-
oryPR /CG,by faithful functor Isay.
4.4 Typed universe theory PR /CGa
Let emergewithinuniversemonoid/universecartesian p.r. the-
ory allPRaobjects{A:χ}as additional objects ν{A:χ}and
get this way a p.r. cartesian theory PR /CGawith extensions of
predicates, ﬁnite limits, ﬁnite sums, coequalisers of equi valence
predicates, as well as with (formal, “including”) universa l ob-
ject /CG,of numerals and (nested) pairs of numerals.
Universal embedding theorem:
(i)I:PR−→PR /CG⊂PR( /C6, /C6) above is a faithful func-
tor .
(ii) theory PR /CGa“inherits” from category PRaall of its
(categorically described) structure: cartesian p.r. cate -
gory structure, equality predicates on all objects, scheme
of predicate abstraction, equalisers, and—trivially—the
48
whole algebraic, logic and order structure on NNO ν /C6
and truth object ν /BE.
(iii)PRmap embedding I“canonically” extends into a carte-
sian p.r. functorial embedding (!)
I:PRa−→PR /CGa⊂PR( /C6, /C6)
of theory PRa=PR+ (abstr) into emerging universe
theoryPR /CGawith predicate abstraction.
(iv) Embedding Ideﬁnes a p.r. isomorphism of categories
I:PRa∼=−→I[PRa]⊏PR /CGa.
(v) (internal) code set is
⌈ /CG, /CG⌉=bydef⌈ /CG, /CG⌉PR /CGa=⌈ /CG, /CG⌉PR /CG= PR /CG.
Internal notion ˇ = of equality is in both cases inherited
frominternalnotionofequalityoftheories PR,PR( /C6, /C6),
given as enumeration of internally equal pairs
ˇ = = ˇ = k: /C6→PR /CG×PR /CG⊂ /C6× /C6,
as well as predicatively as
ˇ = =uˇ =kv: /C6×(PR×PR)→ /BE:
kthinternal equality instance equals pair (u,v)of internal
maps.
(vi) put things together into the following diagram:
49
{A:χ}f/d47/d47
ν{A:χ}∼=
/d15/d15={B:ϕ}
∼=ν{B:ϕ}
/d15/d15
ν{A:χ}I{A:χ}
⊂
/d15/d15If/d47/d47I{B:ϕ}⊂/d47/d47I{B:ϕ}˙∪{⊥}
⊂
/d15/d15/CG⊥˙f=bydefIPRf/d47/d47
⊂
/d15/d15=
/CG⊥
⊂
/d15/d15/C6˙f/d47/d47/C6
PRaembedding diagram forIfq.e.d.
5 Evaluation of p.r. map codes
Double recursive evaluation of p.r. map codes and arguments
in universal set /CGof nested pairs of natural numbers can be
resolved into an iteration of elementary evaluation steps o n
code/argument pairs. Such a step evaluates the actual basic
code particle on the actual argument and simpliﬁes the code
accordingly. Each step diminishes a suitably deﬁned comple x-
ity out of the canonically ordered semiring /C6[ω] of polynomials
until complexity 0andthecode/argument pair /rightanglenwid/rightanglene/evaluation
result is reached.
Evaluation, deﬁned as this complexity controlled iteration,
satisﬁes thecharacteristic doublerecursiveequations, a ndeval-
uates concrete map codes /rightanglenwf/rightangleneinto ev(/rightanglenwf/rightanglene,a) =f(a) (objectiv-
ity).
Descending complexity of codes is introduced for to make
sure termination—intuitively in P/hatwideR /CGaand formally in set
50
theoryT.Theory-strengthening πRofPR /CGais introduced by
an additional axiom ( π) stating the impossibility of inﬁnitely
descending complexity controlled iterations, the way we ar rive
to circumscribe termination within our constructive conte xt.
On this theory πRwill bear our positive assertion about con-
sistency provability.
5.1 Complexity controlled iteration
The data of such a CCI are an endomap p=p(a) :A→A
(predecessor ), and a complexity mapc=c(a) :A→ /C6[ω] on
p’sdomain. Complexity valuesaretaken incanonically ordered
polynomial semiring /C6[ω]≡ /C6+≡ /C6∗/integerdivide{/square} ≡ /C6>.(Priority
to coeﬃcients of higher powers of ω.)
Deﬁnition: [c:A→ /C6[ω],p:A→A] constitute the data
of aComplexity Controlled Iteration CCI = CCI[ c,p],if
•(a∈A)[c(a)>0 =⇒cp(a)<c(a)](descent)
as well as, for commodity,
•(a∈A)[c(a).= 0 =⇒p(a).=a](stationarity).
Such data deﬁne a while loop
wh[c>0,p] :A⇀A, more explicetly written
whilec(a)>0doa:=p(a)od.
We rely on the following axiom scheme ofnon-inﬁnite
51
iterative descent:
CCI[c=c(a) :A→ /C6[ω], p=p(a) :A→A] :
c,pmake up a complexity controlled iteration;
ψ=ψ(a) :A→ /BE“negative” test predicate:
[ψ(a) =⇒cpn(a)>0], a∈A,n∈ /C6both free,
(“alln”,to be excluded )
(π)
ψ(a) = false A(a) :A→ /BE.
The scheme says: A predicate ψwhich implies a CCI to
(overall) inﬁnitely descend must be (overall) false.
•centralexample: general recursive, Ackermann type
PR-code evaluation ev to be resolved into such a CCI .
•scheme (π) is a theorem for settheoryTwith its quanti-
ﬁers∃and∀,and with its having /C6[ω]≡ωωas a (count-
able)ordinal: existential guarantee of ﬁniteness of de-
scending chains within ωω.
•withoutquantiﬁcation, namelyfortheorieslike PRa,PR /CGa,
we are lead to this inference-of-equations scheme guar-
anteeing (intuitively) termination of CCIs, in particular
termination of iterative p.r. code evaluation.
Deﬁnition: CallPRdescent theory universetheory πR=def
PR /CGa+(π) strengthened by axiom scheme ( π) above of non-
inﬁnite descent.
52
5.2 PR code set
Themap code set —set of g¨ odel numbers—we want to evaluate
is PR /CG=⌈ /CG, /CG⌉ ⊂ /C6.It is p.r. deﬁned as follows:
•/rightanglenwba/rightanglene∈PR /CG—formal categorically:
PR /CG◦/rightanglenwba/rightanglene= true—this for basic map constant
ba∈bas ={˚0,˚s,˚ ıd,˚Π,˚∆,˚ℓ,˚r}:zero, successor, identity,
terminal map, diagonal, left and right projection. All of
theseinterpretedintoendomapMonoid PR /CG⊂PR( /C6, /C6)
of fundamental cartesian p.r. theory PR.
•foru,vin PR /CGin general ( u,vfree) add
–internally composed: /a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht=/rightanglenw(/rightanglenev/rightanglenw◦/rightangleneu/rightanglenw)/rightanglene∈PR /CG,
in particular so /rightanglenw(g◦f)/rightanglene=/a\}b∇ack⌉tl⌉{t/rightanglenwg/rightanglene⊙/rightanglenwf/rightanglene/a\}b∇ack⌉t∇i}ht;
–internally induced: /a\}b∇ack⌉tl⌉{tu;v/a\}b∇ack⌉t∇i}ht=/rightanglenw(/rightangleneu/rightanglenw,/rightanglenev/rightanglenw)/rightanglene,
in particular so /rightanglenw(f,g)/rightanglene=/a\}b∇ack⌉tl⌉{t/rightanglenwf/rightanglene;/rightanglenwg/rightanglene/a\}b∇ack⌉t∇i}ht;
–internalcartesian product: /a\}b∇ack⌉tl⌉{tu#v/a\}b∇ack⌉t∇i}ht ∈PR /CG,
in particular so /rightanglenw(f˙×g)/rightanglene=/a\}b∇ack⌉tl⌉{t/rightanglenwf/rightanglene#/rightanglenwg/rightanglene/a\}b∇ack⌉t∇i}ht;
–internally iterated:u$=u/rightanglenw˙§/rightanglene∈PR /CG,
in particular so /rightanglenwf˙§/rightanglene=/rightanglenwf/rightanglene$.
5.3 Iterative evaluation
For deﬁnition of evaluation ev we introduce an evaluation step
of form
e(u,x) = (emap(u,x),earg(u,x)) on PR /CG× /CG⊥
by primitive recursion. This within “outer” theory PR /CGa
which already has PRpredicates /CG, /CG⊥=bydef
/CG∪ {⊥}=/CG∪ {/rightanglenw⊥/rightanglene},and/a\}b∇ack⌉tl⌉{t /CG˙×ν /C6/a\}b∇ack⌉t∇i}ht: /C6→ /C6as objects.
53
Comment: earg(u,x)∈ /CG⊥meanshereone-step u-evaluated
argument, andemap(u,x) denotes the remaining part of map
codeustill to be evaluated after that evaluation step.
PRDeﬁnition of stepe,p.r. on depth( u)∈ /C6,now runs
as follows:
•depth(u) = 0,i.e.uof form/rightanglenwba/rightanglene,
ba∈bas = bydef{˚ ıd,˚0,˚s,˚Π,˚∆,˚ℓ,˚r}
one of the basic map constants of theory PR /CG⊂PR:
earg(/rightanglenwba/rightanglene,x) =defba(x)∈ /CG⊥,
emap(/rightanglenwba/rightanglene,x) =def/rightanglenwid/rightanglene∈PR /CG.
•cases of internal composition:
e(/a\}b∇ack⌉tl⌉{tv⊙/rightanglenwba/rightanglene/a\}b∇ack⌉t∇i}ht,x) =def(v,ba(x))∈PR /CG× /CG⊥
and foru/\⌉}atio\slash∈ {/rightanglenwba/rightanglene: ba∈bas}:
e(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x) =def(/a\}b∇ack⌉tl⌉{tv⊙emap(u,x)/a\}b∇ack⌉t∇i}ht,earg(u,x)) :
step-evaluate ﬁrst map code u,on argument x,and pre-
serve remainder of ufollowed by vas map code to be
step-evaluated on intermediate argument earg(u,x).
•cartesian cases:
e(/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;z/a\}b∇ack⌉t∇i}ht) =def(/rightanglenwid/rightanglene,/a\}b∇ack⌉tl⌉{ty;z/a\}b∇ack⌉t∇i}ht)∈PR /CG× /CG,
a terminating case.
For/a\}b∇ack⌉tl⌉{tu#v/a\}b∇ack⌉t∇i}ht /\⌉}atio\slash=/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht:
e(/a\}b∇ack⌉tl⌉{tu#v/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;z/a\}b∇ack⌉t∇i}ht)
=def(/a\}b∇ack⌉tl⌉{temap(u,y)#emap(v,z)/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{tearg(u,y);earg(v,z)/a\}b∇ack⌉t∇i}ht),
54
evaluateuandvin parallel.
Herefreevariable xon /CGlegitimatly runsonlyon /a\}b∇ack⌉tl⌉{t /CG˙× /CG/a\}b∇ack⌉t∇i}ht ⊂/CG,takes there the pair form /a\}b∇ack⌉tl⌉{ty;z/a\}b∇ack⌉t∇i}ht. x∈ /CG/integerdivide/a\}b∇ack⌉tl⌉{t /CG˙× /CG/a\}b∇ack⌉t∇i}htre-
sults in present evaluation case into ⊥.
•Cases of an induced (redundant via /rightanglenw∆/rightangleneand⊙):
e(/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene;/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht,z) =def(/rightanglenwid/rightanglene,/a\}b∇ack⌉tl⌉{tz;z/a\}b∇ack⌉t∇i}ht),
a terminating case.
For/a\}b∇ack⌉tl⌉{tu;v/a\}b∇ack⌉t∇i}ht /\⌉}atio\slash=/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene;/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht:
e(/a\}b∇ack⌉tl⌉{tu;v/a\}b∇ack⌉t∇i}ht,z)
=def(/a\}b∇ack⌉tl⌉{temap(u,z);emap(v,z)/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{tearg(u,z);earg(v,z)/a\}b∇ack⌉t∇i}ht),
evaluate both components uandv.
•iteration case, with $ := /rightanglenw§/rightanglenedesignating internal itera-
tion:
e(u$,/a\}b∇ack⌉tl⌉{ty;νn/a\}b∇ack⌉t∇i}ht) = (u[n],y) :
PR /CG× /CG⊃PR /CG×/a\}b∇ack⌉tl⌉{t /CG˙×ν /C6/a\}b∇ack⌉t∇i}ht →PR /CG× /CG.
Hereνn∈ν /C6free,n:=ν−1(νn)∈ /C6,andu[n]is given
bycode expansion as
u[0]=def/rightanglenwid/rightanglene, u[n+1]=def/a\}b∇ack⌉tl⌉{tu⊙u[n]/a\}b∇ack⌉t∇i}ht.
•trash casee(u,x) = (/rightanglenwid/rightanglene,⊥)∈PR /CG× /CG⊥if (u,x) in
none of the above—regular—cases.
For to convince ourselves on termination of iteration of ste p
e: PR /CG× /CG⊥→PR /CG× /CG⊥—on a pair of form ( /rightanglenwid/rightanglene,x)—we
introduce:
55
(Descending )complexity
cev(u,x) =c(u) : PR /CG× /CGℓ− →PR /CG→ /C6[ω]
deﬁned p.r. as
c(/rightanglenwid/rightanglene) =def0 = 0·ω∈ /C6[ω],
c(/rightanglenwba′/rightanglene) =def1∈ /C6[ω]
for ba′one of the other basic map constants in bas ,
c/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht=defc(u)+c(v)+1 =c(u)+c(v)+1·ω0∈ /C6[ω],
c/a\}b∇ack⌉tl⌉{tu#v/a\}b∇ack⌉t∇i}ht=defc(u)+c(v)+1,
c/a\}b∇ack⌉tl⌉{tu;v/a\}b∇ack⌉t∇i}ht=defc(u)+c(v)+1,
c(u$) =def(c(u)+1)·ω1∈ /C6[ω].
[()·ω1is to account for unknown iteration count nin argu-
ment/a\}b∇ack⌉tl⌉{tx;n/a\}b∇ack⌉t∇i}htbefore code expansion.]
Example: Complexity of addition + = bydefs§: /C6×/C6→ /C6:
c/rightanglenw+/rightanglene=c/rightanglenws§/rightanglene=c(/rightanglenws/rightanglene$)
= (c/rightanglenws/rightanglene+1)·ω1= 2·ω∈ /C6[ω] [≡0;2∈ /C6+]
Motivation for the above deﬁnition—in particular for this
latter iteration case—will become clear with the correspon ding
case in proof of Descent lemma below for evaluation
ev = ev(u,x) =defr/hatwide◦wh[cev>0, e] :
PR /CG× /CG⊥⇀PR /CG× /CG⊥r− → /CG⊥,
deﬁned by a while loop which reads
56
whilecev(u)>0do (u,x) :=e(u,x)od.
Evaluation stepandcomplexity above are in fact the right
ones to give
Basic descent lemma:
PR /CG⊢cev(u,x)>0 =⇒ceve(u,x)<cev(u,x) i.e.
PR /CG⊢c(u)>0 =⇒cemap(u,x)<c(u),as well as
PR /CG⊢c(u).= 0 [⇐⇒u≡/rightanglenwid/rightanglene]
=⇒ceve(u,x).= 0∧e(u,x).= (u,x),
This with respect to the canonical and—intuitively— ﬁnite-
descentorder of polynomial semiring /C6[ω].
Proof:The only non-trivial case ( v,b)∈PR /CG× /CGfor de-
scentceve(v,b)<cev(v,b) isiterationcase( v,b) = (u$,/a\}b∇ack⌉tl⌉{tx;n/a\}b∇ack⌉t∇i}ht).
In this “acute” iteration case we have
c(u[n]) =c(/a\}b∇ack⌉tl⌉{tu⊙/a\}b∇ack⌉tl⌉{tu...⊙u/a\}b∇ack⌉t∇i}ht.../a\}b∇ack⌉t∇i}ht)
=n·c(u)+(n.−1)<ω·(c(u)+1) =c(u$),
proved in detail by induction on nq.e.d.
5.4 Evaluation characterisation
Dominated characterisation theorem for evaluation:
ev = ev(u,a) : PR /CG× /CG⇀ /CGis characterised by
•PR /CGa⊢[ev(/rightanglenwba/rightanglene,x).= ba(x)]
as well as, again within PR /CGa,πRand strengthenings,
by:
57
•[mdeﬀev(v⊙u,x)] =⇒
ev(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x).= ev(v,ev(u,x));
this reads: if mdeﬁnesthelefthanditeration ev ,i.e. if it-
eration ev of stepeterminates on the left hand argument
after at most msteps, then ev terminates in at most m
steps on right hand side as well, and the two evaluations
have equal results.
•[mdeﬀev(/a\}b∇ack⌉tl⌉{tu#v/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht)] =⇒
ev(/a\}b∇ack⌉tl⌉{tu#v/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht).=/a\}b∇ack⌉tl⌉{tev(u,x);ev(v,y)/a\}b∇ack⌉t∇i}ht,
[mdeﬀev(/a\}b∇ack⌉tl⌉{tu;v/a\}b∇ack⌉t∇i}ht,z)] =⇒
ev(/a\}b∇ack⌉tl⌉{tu;v/a\}b∇ack⌉t∇i}ht,z).=/a\}b∇ack⌉tl⌉{tev(u,z);ev(v,z)/a\}b∇ack⌉t∇i}ht.
•ev(u$,/a\}b∇ack⌉tl⌉{tx;/rightanglenw0/rightanglene/a\}b∇ack⌉t∇i}ht).=x,
[mdeﬀev(u$,/a\}b∇ack⌉tl⌉{tx;ν(sn)/a\}b∇ack⌉t∇i}ht] =⇒:
[mdeﬀall ev below] ∧
ev(u$,/a\}b∇ack⌉tl⌉{tx;ν(sn)/a\}b∇ack⌉t∇i}ht).= ev(u,ev(u$,/a\}b∇ack⌉tl⌉{tx;νn/a\}b∇ack⌉t∇i}ht)).
•itterminates, with all properties above, when situated
in asettheoryT,since there complexity receiving ordi-
nal /C6[ω] has (only) ﬁnite descent, in terms of existential
quantiﬁcation.
Corollary: withinT,we have the double recursive equa-
tions
•ev(/rightanglenwba/rightanglene,x).= ba(x),
•ev(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x).= ev(v,ev(u,x)),
•ev(/a\}b∇ack⌉tl⌉{tu#v/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht).=/a\}b∇ack⌉tl⌉{tev(u,x);ev(v,y)/a\}b∇ack⌉t∇i}ht,
ev(/a\}b∇ack⌉tl⌉{tu;v/a\}b∇ack⌉t∇i}ht,z).=/a\}b∇ack⌉tl⌉{tev(u,z);ev(v,z)/a\}b∇ack⌉t∇i}ht,
•ev(u$,/a\}b∇ack⌉tl⌉{tx;/rightanglenw0/rightanglene/a\}b∇ack⌉t∇i}ht).=x,and
58
ev(u$,/a\}b∇ack⌉tl⌉{tx;ν(sn)/a\}b∇ack⌉t∇i}ht).= ev(u,ev(u$,/a\}b∇ack⌉tl⌉{tx;νn/a\}b∇ack⌉t∇i}ht)).
WithinT—as well as within partial p.r. theories P/hatwideR /CGa,π/hatwideR—
these equations can be taken as deﬁnition for PR /CGcode eval-
uation ev.WithinT,they deﬁne evaluation as a total map.
Proofof theorem by primitive recursion (Peano Induction)
onm∈ /C6free,via case distinction on codes w,and arguments
z∈ /CGappearing in the diﬀerent cases of the asserted conjunc-
tion (casewone of the basic map constants being trivial). All
of the following— induction step —is situated in PR /CGa,read:
PR /CGa⊢etc. If you are interested ﬁrst in the negative re-
sults for settheoriesT,you can read it “ T⊢...” butTstill
deriving properties just of PR /CGmap codes.
•case (w,z) = (/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x) of an (internally) composed,
subcaseu=/rightanglenwid/rightanglene: obvious.
Non-trivial subcase ( w,z) = (/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x), u/\⌉}atio\slash=/rightanglenwid/rightanglene:
m+1deﬀev(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x) =⇒:
ev(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x).=e§((/a\}b∇ack⌉tl⌉{tv⊙emap(u,x)/a\}b∇ack⌉t∇i}ht,earg(u,x)),m)
by iterative deﬁnition of ev in this case
.= ev(v,ev(emap(u,x),earg(u,x)))
by induction hypothesis on m
=⇒:
m+1deﬀev(v,ev(emap(u,x),earg(u,x)))
∧ev(v,ev(emap(u,x),earg(u,x))).= ev(v,ev(u,x)) :
The latter implication “holds” same way back.
59
•case (w,z) = (/a\}b∇ack⌉tl⌉{tu#v/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{tx;y/a\}b∇ack⌉t∇i}ht) of an (internal) cartesian
product: obvious by deﬁnition of ev on a cartesian prod-
uct of map codes. Pay attention to arguments out of/CG/integerdivide/a\}b∇ack⌉tl⌉{t /CG˙× /CG/a\}b∇ack⌉t∇i}htevaluated into ⊥in this case.
•case (w,z) = (u$,/a\}b∇ack⌉tl⌉{tx;/rightanglenw0/rightanglene/a\}b∇ack⌉t∇i}ht) of a null-fold (internally) iter-
ated: obvious.
•case (w,z) = (u$,/a\}b∇ack⌉tl⌉{tx;ν(sn)/a\}b∇ack⌉t∇i}ht) of a genuinely iterated:
m+1deﬀev(u$,/a\}b∇ack⌉tl⌉{tx;ν(sn)/a\}b∇ack⌉t∇i}ht) =⇒
m+1deﬀall instances of ev below, and:
ev(u$,/a\}b∇ack⌉tl⌉{tx;ν(sn)/a\}b∇ack⌉t∇i}ht)
.= ev(emap(u$,/a\}b∇ack⌉tl⌉{tx;ν(sn)/a\}b∇ack⌉t∇i}ht),earg(u$,/a\}b∇ack⌉tl⌉{tx;ν(sn)/a\}b∇ack⌉t∇i}ht))
.= ev(u[n+1],x).= ev(/a\}b∇ack⌉tl⌉{tu⊙u[n]/a\}b∇ack⌉t∇i}ht,x).= ev(u,ev(u[n],x))
the latter by induction hypothesis on m,
case of internal composed
.= ev(u,/a\}b∇ack⌉tl⌉{tev(u$,x);νn/a\}b∇ack⌉t∇i}ht) : same way back.
This shows the (remaining) predicative iteration equations
“anchor” and “step” for an (internally) iterated u$,and so
provesfullﬁllmentoftheabovedoublerecursivesystemofe qua-
tions for ev : PR /CGa× /CG⇀ /CGsubordinatedto globalevaluation
ev : PR /CG× /CG⇀ /CGq.e.d.
Characterisation corollary: Evaluation— P/hatwideR /CGamap—
ev = ev(u,x) : PR /CG× /CG⇀ /CG
deﬁnedas complexity controlled iteration —CCI—withcomplex-
ity values in ordinal /C6[ω],epi-terminates in theory π/hatwideR: has
epimorphic deﬁned arguments enumeration. This by deﬁnitio n
60
of this theory strengthening P/hatwideR /CGa.And it satisﬁes there the
characteristic double-recursive equations above for eval uation
ev.
Objectivity theorem: Evaluation ev is objective, i.e. for
eachsingle,(meta free) f:A→Bin theory PR /CGaitself, we
have
PR /CGa,πR⊢[mdeﬀev(/rightanglenwf/rightanglene,a)] =⇒
ev(/rightanglenwf/rightanglene,a) =f(a),symbolically:
πR⊢ev(/rightanglenwf/rightanglene,) =f:A⇀B.
For frame a settheoryT,there is no need for explicit domi-
nationmdeﬀetc.
Proofby substitution of codes of PR /CGamaps into code
variablesu,v,w∈PR /CG⊂ /C6in Evaluation Characterisation
above, in particular:
•[mdeﬀev(/rightanglenwg◦f/rightanglene,a)] =⇒
ev(/a\}b∇ack⌉tl⌉{t/rightanglenwg/rightanglene⊙/rightanglenwf/rightanglene/a\}b∇ack⌉t∇i}ht,a).= ev(/rightanglenwg/rightanglene,ev(/rightanglenwf/rightanglene,a)),
.=g(f(a)).= (g◦f)(a) recursively (on m) and
•[mdeﬀev(/rightanglenwf§/rightanglene,/a\}b∇ack⌉tl⌉{ta;ν(sn)/a\}b∇ack⌉t∇i}ht] =⇒:
[mdeﬀall ev below] ∧
ev(/rightanglenwf/rightanglene$,/a\}b∇ack⌉tl⌉{ta;ν(sn)/a\}b∇ack⌉t∇i}ht).= ev(/rightanglenwf/rightanglene,ev(/rightanglenwf/rightanglene$,/a\}b∇ack⌉tl⌉{ta;νn/a\}b∇ack⌉t∇i}ht))
.=f(f§(a,νn)) =f§(a,ν(sn)) recursively on m.
•itterminates, with this objectivity, within settheoryT.
6 PR Decidability by Set Theory
We embed evaluation ev( u,x) : PR /CG× /CG⇀ /CGof PR map
codes into settheory, theory T.
61
Notionf=PRgof p.r. maps is externally p.r. enumerated,
by complexity of (binary) deduction trees.
Internalising— formalising —gives internalnotionofp.r.equal-
ityuˇ =kv∈PR /CG×PR /CG,coming by internal deduction tree
dtreek,which can be canonically provided with arguments in/CG—top down from (suitable) argument xgiven to the root
equationuˇ =kvof dtree k.
We denote internal deduction tree argumented this way by
dtreek/x,rootof dtree k/xthen isu/xˇ =kv/x.
6.1 PR soundness framed by set theory
PR evaluation soundness theorem framed by set theory
T:
For p.r. theory PRwith its internal notion of equality ‘ˇ =’
we have:
(i) PR /CGtoTevaluation soundness:
T⊢uˇ =kv=⇒ev(u,x) = ev(v,x) (•)
Substituting in the above “concrete” PR /CGacodes intou
resp.v,we get, by objectivity of evaluation ev :
(ii)T-Framed Objective soundness of PR:
ForPR /CGamapsf,g: /CG⊃A→B⊂ /CG:
T⊢/rightanglenwf/rightangleneˇ =/rightanglenwg/rightanglene=⇒f(a) =g(a).
(iii) Specialising to case u:=/rightanglenwχ/rightanglene,χ: /CG→ /BEa p.r.predicate,
and tov:=/rightanglenwtrue/rightanglene,we get
62
T-framedLogical soundness of PR:
T⊢ ∃kProvPR(k,/rightanglenwχ/rightanglene) =⇒ ∀xχ(x) :
Ifa p.r. predicate is—within T—PR-internally prov-
able,then it holds in Tfor all of its arguments.
Proofof logically central assertion ( •) by primitive recur-
sion onk,dtreekthekth deduction tree of the theory. These
(argument-free) deduction trees are counted in lexicograp hical
order.
Remark: A detailed proofis given forframetheory PR /CGa
and termination-conditioned evaluation in next section. T his
proof logically includes present case of frame theory a setthe-
oryT: within such Tas frame, both evaluations, ev as well
asdeduction tree evaluation evd,terminate on all of their ar-
guments.
The proof for PR /CGaas frame explicitely accounts for for-
mal partiality (and not-inﬁnite descent) of evaluation of p .r.
map codes and boils down to present theorem and proof for
(extending) settheoryTtaken as frame. Because of the im-
portance of both frames we readily attack it the time being
directly for frame Tand take into account the overlap with
the later proof of termination conditioned soundness ofPR /CGa
in next section.
Super Case ofequational internal axioms:
•associativity of (internal) composition:
63
/a\}b∇ack⌉tl⌉{t/a\}b∇ack⌉tl⌉{tw⊙v/a\}b∇ack⌉t∇i}ht⊙u/a\}b∇ack⌉t∇i}htˇ =k/a\}b∇ack⌉tl⌉{tw⊙/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}ht=⇒
ev(/a\}b∇ack⌉tl⌉{tw⊙v/a\}b∇ack⌉t∇i}ht⊙u,x) = ev(/a\}b∇ack⌉tl⌉{tw⊙v/a\}b∇ack⌉t∇i}ht,ev(u,x))
= ev(w,ev(v,ev(u,x)))
= ev(w,ev(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x)) = ev(w⊙/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x).
This proves assertion ( •) in present associativity-of-com-
position case.
•analogousprooffortheotherﬂat,equationalcases, namely
reﬂexivity of equality, left and right neutrality ofid = bydef
id/CG,all substitution equations for the map constants,
Godement’s equations for the induced map as well as sur-
jective pairing and distributivity equation for composition
with an induced.
•proof of ( •) for the last equational case, the
iteration step, case ofgenuine iteration equation
dtreek=/a\}b∇ack⌉tl⌉{tu$⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}htˇ =ku⊙u$/a\}b∇ack⌉t∇i}ht:
T⊢ev(u$⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht) (1)
= ev(u$,ev(/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht))
= ev(u$,/a\}b∇ack⌉tl⌉{ty;ν(sn)/a\}b∇ack⌉t∇i}ht)
= ev(u,ev(u$,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht)
= ev(u⊙u$,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht). (2)
Proof of termination-conditioned inner soundness for the
remaining deep—genuine Horncases—for dtree k,Horntype
deduction ofroot:
Transitivity-of-equality case: withmapcodevariables u,v,w
we start here with argument-free deduction tree
64
uˇ =kw
dtreek=⇑
uˇ =iv∧vˇ =jw
Evaluate at argument xand get in fact
T⊢uˇ =kw
=⇒ev(u,x) = ev(v,x)∧ev(v,x) = ev(w,x)
(by hypothesis on i,j <k)
=⇒ev(u,x) = ev(w,x) :
transitivity export q.e.d. in this case.
Caseofsymmetryaxiomschemeforequalityisnowobvious.
Compatibility case of composition with equality
/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}htˇ =k/a\}b∇ack⌉tl⌉{tv⊙u′/a\}b∇ack⌉t∇i}ht
deduk=⇑
uˇ =iu′
By induction hypothesis on i<kwe have
/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}htˇ =k/a\}b∇ack⌉tl⌉{tv⊙u′/a\}b∇ack⌉t∇i}ht=⇒:
[ev(u,x) = ev(u′,x) =⇒
ev(v⊙u,x) = ev(v,ev(u,x)) = ev(v,ev(u′,x))
= ev(v⊙u′,x)]
by hypothesis on uˇ =iu′and by Leibniz’ substitutivity, q.e.d.
in this 1st compatibility case.
Case of composition with equality in second composition
factor:
65
/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}htˇ =k/a\}b∇ack⌉tl⌉{tv′⊙u/a\}b∇ack⌉t∇i}ht
deduk=⇑
vˇ =iv′
[Here dtree iis not (yet) provided with all of its arguments,
itiscompletly argumented during top down tree evaluation.]
/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}htˇ =k/a\}b∇ack⌉tl⌉{tv′⊙u/a\}b∇ack⌉t∇i}ht=⇒:
ev(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x) = ev(v,ev(u,x)) = ev(v′,ev(u,x)) (∗)
= ev(/a\}b∇ack⌉tl⌉{tv′⊙u/a\}b∇ack⌉t∇i}ht,x).
(∗) holdsbyvˇ =iv′,inductionhypothesison i<k,andLeibniz’
substitutivity: same argument into equal maps.
This proves soundness assertion ( •) in this 2nd compatibil-
ity case.
(Redundant) Case of compatibility of forming the induced
map, with equality is analogous to compatibilities above, e ven
easier, sincethetwomapcodesconcernedareindependentfr om
each other.
(Final) Case of Freyd’s (internal) uniqueness of the ini-
tialised iterated, is case
deduk//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht
w//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}htˇ =k/a\}b∇ack⌉tl⌉{tv$⊙/a\}b∇ack⌉tl⌉{tu#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}ht
=
root(ti) root( tj)
66
where
root(ti)
=/a\}b∇ack⌉tl⌉{tw⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene;/rightanglenw0/rightanglene⊙/rightanglenwΠ/rightanglene/a\}b∇ack⌉t∇i}ht/yˇ =iu/y/a\}b∇ack⌉t∇i}ht,
root(tj)
=/a\}b∇ack⌉tl⌉{tw⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}ht//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}htˇ =j/a\}b∇ack⌉tl⌉{tv⊙w/a\}b∇ack⌉t∇i}ht//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}ht.
Comment: wis here an internal comparison candidate
fullﬁlling the same internal p.r. equations as /a\}b∇ack⌉tl⌉{tv$⊙/a\}b∇ack⌉tl⌉{tu#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}ht.
It should be— is:soundness —evaluated equal to the latter, on
/a\}b∇ack⌉tl⌉{t /CG˙×ν /C6/a\}b∇ack⌉t∇i}ht ⊂ /CG.
Soundness assertion ( •) for the present Freyd’s uniqueness
case recurs on ˇ = i,ˇ =jturned into predicative equations ‘=’,
these being already deduced, by hypothesis on i,j <k.Further
ingredients are transitivity of ‘=’ and established proper ties of
basic evaluation ev of map terms.
So here is the remaining—inductive—proof, prepared by
T⊢ev(w,/a\}b∇ack⌉tl⌉{ty;ν(0)/a\}b∇ack⌉t∇i}ht) = ev(u;y) ( ¯0)
as well as
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(sn)/a\}b∇ack⌉t∇i}ht) = ev(w,/a\}b∇ack⌉tl⌉{ty;/rightanglenws/rightanglene⊙ν(n)/a\}b∇ack⌉t∇i}ht)
= ev(w⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht)
= ev(v⊙w,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht), (¯ s)
the same being true for w′:=v$⊙ /a\}b∇ack⌉tl⌉{tu#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}htin place of w,
once moreby (characteristic) doublerecursive equations f or ev,
this time with respect to the initialised internal iterated itself.
(¯0)and(¯ s)puttogether forboththenshow, byinductionon
iteration count n∈ /C6—all other free variables k,u,v,w,y to-
getherformthe passive parameter forthisinduction— truncated
67
soundness assertion( •)forthis Freyd’s uniqueness case, namely
T⊢ev(w,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht) = ev(v$⊙/a\}b∇ack⌉tl⌉{tu#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht).
Induction runs as follows:
anchorn= 0 :
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(0)/a\}b∇ack⌉t∇i}ht) = ev(u,y) = ev(w′,/a\}b∇ack⌉tl⌉{ty;ν(0)/a\}b∇ack⌉t∇i}ht),
step:
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht) = ev(w′,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht) =⇒:
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(sn)/a\}b∇ack⌉t∇i}ht) = ev(v,ev(w,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht))
= ev(v,ev(w′,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht)) = ev(w′,/a\}b∇ack⌉tl⌉{ty;ν(sn)/a\}b∇ack⌉t∇i}ht),
the latter since evaluation ev preserves predicative equal ity ‘=’
(Leibniz) q.e.d.
6.2 PR-predicate decision by set theory
We consider here PR /CGapredicates for decidability by setthe-
orie(s)T.Basic tool is T-framedsoundness ofPR /CGaabove,
namely
χ=χ(a) :A→ /BEPR /CGapredicate
T⊢ ∃kProvPR /CGa(k,/rightanglenwχ/rightanglene) =⇒ ∀aχ(a).
WithinTdeﬁne forχ:A→ /BEout ofPR /CGaa partially
deﬁned (alleged, individual) µ-recursive decision ∇χ: /BD⇀ /BE
by ﬁrst ﬁxing decision domain
D=Dχ:={k∈ /C6:¬χ(ctA(k))∨ProvPR /CGa(k,/rightanglenwχ/rightanglene)},
68
ctA: /C6→A(retractive) Cantor count of A; and then, with
(partial) recursive µD: /BD⇀D⊆ /C6withinT:
∇χ=def

false if¬χ(ctA(µD))
(counterexample ),
true if Prov PR /CGa(µD,/rightanglenwχ/rightanglene)
(internal proof ),
⊥(undeﬁned ) otherwise, i.e.
if∀aχ(a)∧ ∀k¬ProvPR /CGa(k,/rightanglenwχ/rightanglene).
[This (alleged) decision is apparently ( µ-)recursive within T,
even if apriori only partially deﬁned.]
There is a ﬁrst consistency problem with this deﬁnition:
are thedeﬁnedcasesdisjoint?
Yes, within frame theory Twhichsoundly frames theory
PR /CGa:T⊢(∃k∈ /C6)ProvPR /CGa(k,/rightanglenwχ/rightanglene) =⇒ ∀aχ(a).
This soundness makes ∇χinto a well-deﬁned partial µ-
recursive point ∇χ: /BD⇀ /BEin two-element set /BE={false,true}
of theory T.
For any such partial µ-recursive point γ: /BD⇀ /BEthe follow-
ing is acomplete alternative:
(a)T⊢γ= false
(b)T⊢γ= true
(c)T⊢γ=⊥(undeﬁned ).
Because of T-framed PR /CGa-soundness above we get in par-
ticular for recursive decision ∇χ: /BD⇀ /BEof p.r. predicate
χ=χ(a) :A→ /BEthe following
Complete T derivation alternative:
69
(a)T⊢ ∇χ= false⇐⇒ ∃a¬χ(a),
(b)T⊢ ∇χ= true⇐⇒ ∃kProvPR /CGa(k,/rightanglenwχ/rightanglene)
⇐⇒ ∃kProvPR /CGa(k,/rightanglenwχ/rightanglene)∧ ∀aχ(a),
the latter ‘ ⇐⇒’ in fact by T-framed soundness
ofPR /CGa.
(c)T⊢ ∇χ=⊥ ⇐⇒ ∀aχ(a)∧ ∀k¬ProvPR /CGa(k,/rightanglenwχ/rightanglene).
Remark:
•within quantiﬁed arithmetic Twe have the right to re-
placeχ(ctA(µD)) by∃a(χ(a)),and Prov PR /CGa(µD,/rightanglenwχ/rightanglene)
by∃kProvPR /CGa(k,/rightanglenwχ/rightanglene).
•for consistent T, χan arbitrary T-formula, and Proof
ProvTin place of Prov PR /CGa,soundness —and therefore
disjointness of (termination) cases(a) and (b) above—
does not work anymore: take for χG¨ odel’s undecidable
formulaϕwith its “characteristic” property
T⊢ ¬ϕ⇐⇒ ∃kProvT(k,/rightanglenwϕ/rightanglene).
Merging now the (right hand sides) of the latter two cases
intoT⊢ ∀aχ(a) gives the following complete alternative,
namely
Decidability of primitiverecursivefree-variable predicates
byquantiﬁed extension T(viaµ-recursive decision algorithm
∇χ: /BD⇀ /BE):
70
For (arbitrary) PR /CGapredicateχ=χ(a) :A→ /BEwe have
T⊢ ∀aχ(a)or
T⊢ ∃a¬χ(a)
or both. q.e.d.
Theorem or derivable existence of a counterexample.
Decision Remark: this does not mean a priori that de-
cision algorithm ∇χterminates for all such predicates χ.The
theorem says only that χisdecidable “by”,withintheoryT,
that it is not independent fromT.
6.3 G¨ odel’s incompleteness theorems
We visit §2. G¨ odel’s theorems, in Smorynski 1977.
First Incompleteness Theorem. LetTbe a formal
theory containing arithmetic. Then there is a sentence ϕwhich
asserts its own unprovability and such that:
(i)IfTis consistent, T/\⌉}atio\slash⊢ϕ.
(ii)IfTisω-consistent, T/\⌉}atio\slash⊢ ¬ϕ.
In§3.2.6 Smorynski discusses possible choices of arithmetic
(theory) S,namely
(a)PRA=(classical, free-variables)primitiverecursivearith-
metic, S. Feferman: “my PRA”, in contrast to PRa
above.
(b)PA= Peano Arithmetic.
Conjecture: PA∼=PR∃⊏PRa∃.
71
(c)ZF= Zermelo-Fraenkel set theory. “This is both a good
and a bad example. It is bad because the whole encod-
ing problem is more easily solved in a set theory than in
an arithmetical theory. By the same token, it is a good
example.”
Conjecture: PRA can categorically be viewed as carte-
sian theory with weak NNO in Lambek’s sense.
WetakeS:=PRa,embedding extension of categorical
theoryPR,formallystrongerthan PRAbecauseofuniqueness
of maps deﬁned by the full schema of primitive recursion, and
weaker than PA∼=PR∃.
By construction of arithmetic PRa,one can adequatly en-
code syntax in this S=PRa,since Smorynski’s conditions
(i)-(iii) for the representation of p.r. functions are fulﬁ lled.
We take for formal extension TofSone of the categori-
cal pendants to suitable settheories (subsystems of ZF,see
Osius1974), or the (ﬁrst order) elementary theory of two-
valued Topoi with NNO, cf.Freyd1972, or, minimal choice,
T:=PRa∃⊐PA.
Derivability theorem: OurSencoding, extended from
PRatoT,meets the following (quantiﬁer free categorically
72
expressed) Derivability Conditions in§2.1 of Smorynski:
D1Tk
⊢ϕinfersS⊢ProvT(num(k),/rightanglenwϕ/rightanglene).
D2S⊢ProvT(k,/rightanglenwϕ/rightanglene) =⇒ProvT(j2(k),ProvT(k,/rightanglenwϕ/rightanglene)),
j2=j2(k) : /C6→ /C6suitable.
D3S⊢ProvT(k,/rightanglenwϕ/rightanglene)∧ProvT(k′,/rightanglenwϕ⇒ψ/rightanglene)
=⇒ProvT(j3(k,k′),/rightanglenwψ/rightanglene),
j3=j3(k,k′) : /C62→ /C6suitable.
Smorynski’s proof gives the First G¨ odel’s incompleteness
theorem, and from that the
Second incompleteness theorem: LetTbe one of the
extensions above of PR∃,andTconsistent. Then
T/\⌉}atio\slash⊢ConT,
whereConT=∀k¬ProvT(k,/rightanglenwfalse/rightanglene)is the sentence asserting
the consistency of T.
FromthisG¨ odel’s theoremandour PR Decidability theorem
for quantiﬁed arithmetic PRa∃,Twe get
Inconsistency provability theorem for quantiﬁed arith-
metical (set) theories T:
IfTis consistent, then T⊢ ¬ConT.
[If not, then it derives everything, in particular ¬ConT.]
7 Consistency Decision within πR
This ﬁnal section is better read in overview than explained.
73
7.1 Termination conditioned soundness
Termination-conditioned soundness theorem:
For p.r. theory PR /CGa3and internal notion of equality
ˇ = = ˇ = k: /C6→PR /CG×PR /CG,dtreekthekth deduction tree
of universe theory PR /CG⊂PR( /C6, /C6),we have:
(i)Termination-Conditioned Innersoundness:
With r = r( u,x) =x: PR /CG× /CG→ /CGright projection:
PR /CGa⊢ /a\}b∇ack⌉tl⌉{tuˇ =kv/a\}b∇ack⌉t∇i}ht.= root (dtree k)
∧mdeﬀevd(dtreek/x)
=⇒ev(u,x).= ev(v,x).(•)
explicitly:
PR /CGa⊢uˇ =kv∧cdem
d(dtreek/x).= 0
=⇒ev(u,x).=em(u,x).=em(v,x)
.= ev(v,x), (•)
free map-code variables u,v,variablexfree in universal
set /CG.
[Argumentation dtreek/xof dtree kanddeﬁnitionof argu-
mented tree evaluation evdbased on its evaluation step ed
andcomplexity cdisbymergedrecursionondepth(dtree k),
within proof below]
In words, this “ m-Truncated”, “ m-Dominated” Inner
soundness says that theory PRaderives:
3a priori not directly for πRwith respect to its own internal equality, without
assumption of “ π-consistency,” in this regard RCF2 contains an error
74
Iffor an internal PR /CGequationuˇ =kvargumented de-
duction tree dtree k/xforuˇ =kv,argumented withx∈ /CG,
admits complete argumented-tree evaluation, i.e.
iftree-evaluation becomes completed after a ﬁnite number
mof evaluation steps,
thenboth sides of this internal(!) equation are completly
evaluated on xby (at most) mstepseofbasicevaluation
ev,into equal values.
Substitutingintheabove“concrete” codesinto uresp.v,
we get, by objectivity of evaluation ev ,formally “mutatis
mutandis”:
(ii)Termination-Conditioned Objective soundness for Map Equal -
ity:
ForPR /CGamapsf,g:A→B:
PR /CGa⊢[/rightanglenwf/rightangleneˇ =k/rightanglenwg/rightanglene∧mdeﬀevd(dtreek/a)]
=⇒f(a).=Brem(/rightanglenwg/rightanglene,a).=Bg(a),
a∈Afree :
Ifan internal p.r. deduction-tree for (internal) equality of
/rightanglenwf/rightangleneand/rightanglenwg/rightangleneis available, and ifon this tree—top down ar-
gumented with ainA—tree evaluation terminates, then
equalityf(a).=Bg(a)offandgat this argument is the
consequence.
(iii) Specialisingthistocaseof f:=χ:A→ /BEap.r.predicate
and tog:= true A:A→ /BEwe eventually get
Termination-Conditioned Objective Logical soundness:
PR /CGa⊢ProvPR /CG(k,/rightanglenwχ/rightanglene)∧mdeﬀevd(dtreek/a)
=⇒χ(a) :
75
Iftree-evaluation of an internal deduction tree for a free
variable p.r. predicate χ:A→ /BE—the tree argumented
witha∈A—terminates after a ﬁnite number mof evalu-
ation steps, thenχ(a).= trueis the consequence, within
PR /CGaas well as within its extensions πR—andsetthe-
oryT.
Remark to proof below: in present case of frame theory
PR /CGa(and stronger theory πR) we have to controlall eval-
uation step iterations, and we do that by control of iterativ e
evaluation ev dof whole argumented deduction trees, whose re-
cursive deﬁnition will be—merged—part of this proof.
Proofof—basic— termination-conditioned inner soundness,
i.e. of implication ( •) inES theorem is by induction on deduc-
tion tree counting index k∈ /C6counting family dtree k: /C6→
Bintree,starting with (ﬂat) dtree 0=/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightangleneˇ =0/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht. m∈ /C6is
to dominate argumented-deduction-tree evaluation ev dto be
recursively deﬁned below: condition
mdeﬀevd(dtreek/x),steped,complexity cd.
We argue by recursive case distinction on the form of the
top up-to-two layers—top (implicational) deduction—dedu k/x
of argumented deduction tree dtree k/xat hand.
Flatsuper case depth( dtreek) = 0,i.e. super case of un-
conditioned, axiomatic (internal) equationuˇ =kv:
The ﬁrst involved of these cases is associativity of (internal)
composition:
dtreek=/a\}b∇ack⌉tl⌉{t/a\}b∇ack⌉tl⌉{tw⊙v/a\}b∇ack⌉t∇i}ht⊙u/a\}b∇ack⌉t∇i}htˇ =k/a\}b∇ack⌉tl⌉{tw⊙/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}ht
76
In this case—no need of a recursion on k—
PR /CGa⊢mdeﬀevd(dtreek/x) =⇒
[mdeﬀev(/a\}b∇ack⌉tl⌉{tw⊙v/a\}b∇ack⌉t∇i}ht⊙u,x)]
∧[mdeﬀev(/a\}b∇ack⌉tl⌉{tw⊙v/a\}b∇ack⌉t∇i}ht,ev(u,x))
∧[mdeﬀev(w,ev(v,ev(u,x)))
∧[mdeﬀev(w,ev(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x))
∧[mdeﬀev(/a\}b∇ack⌉tl⌉{tw⊙/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}ht,x)]∧
ev(/a\}b∇ack⌉tl⌉{tw⊙v/a\}b∇ack⌉t∇i}ht⊙u,x).= ev(/a\}b∇ack⌉tl⌉{tw⊙v/a\}b∇ack⌉t∇i}ht,ev(u,x))
.= ev(w,ev(v,ev(u,x)))
.= ev(w,ev(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x)).= ev(w⊙/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x).
Thisprovesassertion( •)inpresent associativity-of-composition
case. [New in comparison to previous Inconsistency section is
here only the “preamble” mdeﬀetc.]
Analogous proof for the other ﬂat, equational cases, namely
reﬂexivity of equality, left and right neutrality of id = bydef
id/CG,all substitution equations for the map constants, Gode-
ment’s equations for the induced map as well as surjective pair-
inganddistributivity of composition over forming the induced
map.
Final equation: genuine iteration equation
77
dtreek=/a\}b∇ack⌉tl⌉{tu$⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}htˇ =ku⊙u$/a\}b∇ack⌉t∇i}ht:
PR /CGa⊢mdeﬀevd(dtreek//a\}b∇ack⌉tl⌉{ty;ν(n))/a\}b∇ack⌉t∇i}ht=⇒
mdeﬀ all instances of evbelow, and:
ev(u$⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht) (1)
.= ev(u$,ev(/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht))
.= ev(u$,/a\}b∇ack⌉tl⌉{ty;ν(sn)/a\}b∇ack⌉t∇i}ht)
.= ev(u[sn],y) (by deﬁnition of ev step e)
.= ev(u⊙u[n],y)
.= ev(u,ev(u$,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht)
.= ev(u⊙u$,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht). (2)
Proof of termination-conditioned inner soundness for the
remaining deep—genuine Horncases—for dtree k,Horntype
(at least) at deduction ofroot:
Transitivity-of-equality case: withmapcodevariables u,v,w
we start here with argument-free deduction tree
uˇ =kw
dtreek=
uˇ =iv
dtreeiidtreejivˇ =jw
dtreeijdtreejj
It is argumented with argument xsay, recursively spread
down:
78
dtreek/x
u/x w/x
=
u/x v/x
dtreeii/xiidtreeji/xjiv/x w/x
dtreeij/xijdtreejj/xjj
Spreading down arguments from upper level down to 2nd
level must/is given explicitly, further arguments spread d own
is then recursive by the type of deduction (sub)trees dtree i,
dtreej, i,j <k.
Now by induction hypothesis on i,jwe have for tree evalu-
ation ev d:
uˇ =kw∧mdeﬀevd(dtreek/x)
=⇒mdeﬀevd(dtreei/x),evd(dtreej/x)∧
evd(dtreei/x).=/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene/ev(u,x).=/rightanglenwid/rightanglene/ev(v,x)/a\}b∇ack⌉t∇i}ht
∧evd(dtreej/x).=/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene/ev(v,x).=/rightanglenwid/rightanglene/ev(w,x)/a\}b∇ack⌉t∇i}ht
=⇒ev(u,x).= ev(v,x)∧ev(v,x).= ev(w,x)
=⇒ev(u,x).= ev(w,x).
and this is what we wanted to show in present transitivity of
equality case.
[Transitivity axiom for equality is a main reason for ne-
cessity to consider (argumented) deduction trees: interme diate
map code equalities ‘ˇ =’ in a transitivity chain must be each
evaluated, and pertaining deduction trees may be of arbitra ry
high evaluation complexity]
79
Caseofsymmetryaxiomschemeforequalityisnowobvious.
Compatibility Case of composition with equality
/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht/xˇ =k/a\}b∇ack⌉tl⌉{tv⊙u′/a\}b∇ack⌉t∇i}ht/x
dtreek/x=
u/xˇ =ju′/x
dtreeij/xdtreejj/x
By induction hypothesis on j <k
mdeﬀevd(dtreek/x) =⇒
mdeﬀevd(dtreej/x) =⇒
ev(u,x).= ev(u′,x) =⇒
ev(v⊙u,x).= ev(v,ev(u,x)).= ev(v,ev(u′,x))
.= ev(v⊙u′,x)
by dominated characterisic equations for ev and Leibniz’ su b-
stitutivity, q.e.d. in this 1st compatibility case.
Spread down arguments is more involved in
Case of composition with equality in second composition
factor: argument spread down merged with tree evaluation ev d
and proof of result.
/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht/x/a\}b∇ack⌉tl⌉{tv′⊙u/a\}b∇ack⌉t∇i}ht/x
dtreek/x=
vˇ =iv′
dtreeiidtreeji
80
[Here dtree iis not (yet) provided with argument, it isar-
gumented during top down tree evaluation below]
mdeﬀevd(dtreek/x) =⇒
mdeﬀall instances of ev below, and:
ev(/a\}b∇ack⌉tl⌉{tv⊙u/a\}b∇ack⌉t∇i}ht,x).= ev(v,ev(u,x)).= ev(v′,ev(u,x)) (∗)
.= ev(/a\}b∇ack⌉tl⌉{tv′⊙u/a\}b∇ack⌉t∇i}ht,x).
(∗) holds by Leibniz’ substitutivity and
mdeﬀevd(dtreek/x) =⇒
mdeﬀevd(dtreei/ev(u,x))
[argumentation of dtreeiwith
ev(u,x)—calculated en cours de route ,
extra deﬁnition of ed]
=⇒
mdeﬀev(v,ev(u,x)).= ev(v′,ev(u,x)),
by induction hypothesis on i<k: The hypothesis is indepen-
dent of substituted argument, provided—and this is here the
case—that dtree iis evaluated on that argument, in m′< m
steps,m′suitable (minimal).
This proves assertion ( •) in this 2nd compatibility case.
(Redundant) case of compatibility of forming the induced
map with map equality is analogous to compatibilities above ,
even easier, because of almost independence of any two induc -
ing map codes from each other.
(Final) case, of Freyd’s (internal) uniqueness of the ini-
tialised iterated, is case
81
deduk//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht
w//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}htˇ =k/a\}b∇ack⌉tl⌉{tv$⊙/a\}b∇ack⌉tl⌉{tu#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}ht
=
root(ti) root( tj)
where
root(ti) =/a\}b∇ack⌉tl⌉{tw⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene;/rightanglenw0/rightanglene⊙/rightanglenwΠ/rightanglene/a\}b∇ack⌉t∇i}ht/yˇ =iu/y/a\}b∇ack⌉t∇i}ht,
root(tj) =/a\}b∇ack⌉tl⌉{tw⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}ht//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}htˇ =j/a\}b∇ack⌉tl⌉{tv⊙w/a\}b∇ack⌉t∇i}ht//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}ht
Comment: wis here an internal comparison candidate
fullﬁlling the same internal p.r. equations as /a\}b∇ack⌉tl⌉{tv$⊙/a\}b∇ack⌉tl⌉{tu#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht/a\}b∇ack⌉t∇i}ht.
It should be— is:soundness —evaluated equal to the latter, on
/a\}b∇ack⌉tl⌉{t /CG˙×ν /C6/a\}b∇ack⌉t∇i}ht ⊂ /CG.
The soundness assertion ( •) for the present Freyd’s unique-
nesscase recurs on ˇ = i,ˇ =jturned into predicative equations
‘.=’, these being already deduced, by hypothesis on i,j < k.
Furtheringredients aretransitivity of ‘.=’ andestablished prop-
erties of basic evaluation ev of map terms.
So here is the remaining—inductive—proof, prepared by
PR /CGa⊢mdeﬀdtreek//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht=⇒
mdeﬀall of the following ev-terms and
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(0)/a\}b∇ack⌉t∇i}ht).= ev(u;y) ( ¯0)
as well as
mdeﬀboth of the following ev-terms, and
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(sn)/a\}b∇ack⌉t∇i}ht).= ev(w,/a\}b∇ack⌉tl⌉{ty;/rightanglenws/rightanglene⊙ν(n)/a\}b∇ack⌉t∇i}ht)
.= ev(w⊙/a\}b∇ack⌉tl⌉{t/rightanglenwid/rightanglene#/rightanglenws/rightanglene/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht)
.= ev(v⊙w,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht), (¯ s)
82
the same being true for w′:=v$⊙ /a\}b∇ack⌉tl⌉{tu#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}htin place of w,
once moreby (characteristic) doublerecursive equations f or ev,
this time with respect to the initialised internal iterated itself.
(¯0)and(¯ s)puttogether forboththenshow, byinductionon
iteration count n∈ /C6—all other free variables k,u,v,w,y to-
getherformthe passive parameter forthisinduction— truncated
soundness assertion( •)forthis Freyd’s uniqueness case, namely
PR /CGa⊢mdeﬀdtreek//a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht=⇒
mdeﬀ all of the ev-terms concerned above, and
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht).= ev(v$⊙/a\}b∇ack⌉tl⌉{tu#/rightanglenwid/rightanglene/a\}b∇ack⌉t∇i}ht,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht).
Induction runs as follows:
Anchorn= 0 :
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(0)/a\}b∇ack⌉t∇i}ht).= ev(u,y).= ev(w′,/a\}b∇ack⌉tl⌉{ty;ν(0)/a\}b∇ack⌉t∇i}ht),
Step:mdeﬀetc. =⇒
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht).= ev(w′,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht) =⇒:
ev(w,/a\}b∇ack⌉tl⌉{ty;ν(sn)/a\}b∇ack⌉t∇i}ht).= ev(v,ev(w,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht))
.= ev(v,ev(w′,/a\}b∇ack⌉tl⌉{ty;ν(n)/a\}b∇ack⌉t∇i}ht)).= ev(w′,/a\}b∇ack⌉tl⌉{ty;ν(sn)/a\}b∇ack⌉t∇i}ht),
the latter since evaluation ev preserves predicative equal ity
‘.=’ (Leibniz) q.e.d.Termination Conditioned p.r. soundness
theorem.
Comment: Already for stating the evaluations, we needed
thecategorical, free-variablestheories PR,PRa,PR /CG,PR /CGa
of primitive recursion, as well as—for termination, even in clas-
sial frame T—PR complexities within /C6[ω].Since introduced
type of soundness is a corner stone in our approach, the above
83
complicated categorical combinatorics seem to be necessar y for
the constructive framework of descent theory πR.
7.2 Framed consistency
Fromtermination-conditionedsoundness—resp.from T-framed
PR soundness—we get
πR-framed internal p.r. consistency corollary: For
descenttheoryπR=PR /CGa+ (π),axiom (π) stating non-
inﬁnite iterative descent in ordinal /C6[ω],we have
πR⊢ConPR /CG,i.e. “necessarily” in free-variables form:
πR⊢ ¬ProvPR /CG(k,/rightanglenwfalse/rightanglene) : /C6→ /BE, k∈ /C6free,
T⊢ConPR /CG:
TheoryπR—as well as settheoriesTas extensions of πR—
derivethat nok∈ /C6is the internal PR /CG-Prooffor/rightanglenwfalse/rightanglene.
Proofforthiscorollaryfrom termination-conditioned sound-
ness:By assertion (iii) of that theorem, with χ=χ(a) :=
false(a) = false : /BD→ /BE,we get:
Evaluation-eﬀective internal inconsistency ofPR /CGimplies
false,i.e. availability of an evaluation-terminating internalde-
duction tree of/rightanglenwfalse/rightangleneimpliesfalse :
PR /CGa, πR⊢ProvPR /CG(k,/rightanglenwfalse/rightanglene)
∧cdem
d(dtreek//a\}b∇ack⌉tl⌉{t0/a\}b∇ack⌉t∇i}ht).= 0 =⇒false.
Contraposition to this, still with k,m∈ /C6free:
πR⊢true =⇒ ¬ProvPR /CG(k,/rightanglenwfalse/rightanglene)∨cdem
d(dtreek//a\}b∇ack⌉tl⌉{t0/a\}b∇ack⌉t∇i}ht)>0,
84
i.e. by free-variables (boolean) tautology:
πR⊢ProvPR /CG(k,/rightanglenwfalse/rightanglene) =⇒cdem
d(dtreek//a\}b∇ack⌉tl⌉{t0/a\}b∇ack⌉t∇i}ht)>0.
Fork“ﬁxed”, the conclusion of this implication— mfree—
means inﬁnite descent in /C6[ω] of iterative argumented deduct-
ion tree evaluation ev don dtree k/0,which is excluded intu-
itively. Formally it is excluded within our theory πRtaken as
frame:
We apply non-inﬁnite-descent scheme ( π) to ev d,which is
given by stepedand complexity cd—the latter descends (this
isargumented-tree evaluation descent ) with each application of
ed,as long as complexity 0 ∈ /C6[ω] is not (“yet”) reached. We
combine this with—choice of— overall “negative” condition
ψ=ψ(k) := Prov PR /CG(k,/rightanglenwfalse/rightanglene) : /C6→ /BE, k∈ /C6free
and get—by that scheme ( π)—overall negation of this (overall)
excluded predicateψ,namely
πR⊢ ¬ProvPR /CG(k,/rightanglenwfalse/rightanglene) : /C6→ /BE, k∈ /C6free, i.e.
πR⊢ConPR /CGq.e.d.
So “slightly” strengthened theory πR=PR /CGa+(π) derives
free variables Consistency Formula for theory PR /CGof primi-
tive recursion.
Scheme (π) holds in settheory, since there O:= /C6[ω] is an
ordinal, not quite to identify with set theoretical ordinal ωω,
because classical ordinal addition on that ordinal ωωdoes not
commute, e.g. classically ω+1/\⌉}atio\slash= 1+ω=ω.As linear orders
(with non-inﬁnite descent) the two are identical.
85
As is well known, consistency provability and soundness of
a theory are strongly tied together. We get in fact even
Theorem on πR-framed objective soundness of the-
ory PR /CGa:
•for aPR /CGapredicateχ=χ(a) :A→ /BEwe have
πR⊢ProvPR /CG(k,/rightanglenwχ/rightanglene) =⇒χ(a) : /C6×A→ /BE.
•more general, for PR /CGa-mapsf, g:A→Bwe have
πR⊢/rightanglenwf/rightangleneˇ =k/rightanglenwg/rightanglene=⇒f(a).=g(a).
[Same for settheoryTtaken as frame]
Proofof ﬁrst assertion is a slight generalisation of proof of
framed internal consistency above as follows—take predicate χ
instead of false :
Usetermination-conditioned soundness, assertion (iii) di-
rectly:
Evaluation-eﬀective internal provabiliity of/rightanglenwχ/rightanglenewithinPR /CGa—
i.e. availability of an evaluation-terminating internaldeduction
treeof/rightanglenwχ/rightanglene—impliesχ(a),a∈Afree :
PR /CGa, πR⊢ProvPR /CG(k,/rightanglenwχ/rightanglene)∧cdem
d(dtreek//a\}b∇ack⌉tl⌉{t0/a\}b∇ack⌉t∇i}ht).= 0
=⇒χ(a) : /C62×A→ /BE.
Boolean free-variables calculus, tautology
[α∧β⇒γ] = [¬[α⇒γ]⇒ ¬β]
(test withβ= 0 as well as with β= 1),
86
gives from this, still with k,m,afree:
πR⊢ ¬[ProvPR /CG(k,/rightanglenwχ/rightanglene)⇒χ(a)]
=⇒cdem
d(dtreek//a\}b∇ack⌉tl⌉{t0/a\}b∇ack⌉t∇i}ht)>0 : (A× /C6)× /C6→ /BE.
As before, we apply non-inﬁnit scheme ( π) to ev d,in com-
bination with—choice of— overall “negative” condition
ψ=ψ(k,a) :=¬[ProvPR /CG(k,/rightanglenwχ/rightanglene)⇒χ(a)] : /C6×A→ /BE,
and get—scheme ( π)—overall negation of this (overall) ex-
cludedpredicateψ,namely
πR⊢ProvPR /CG(k,/rightanglenwχ/rightanglene) =⇒χ(a) : /C6×A→ /BE.
q.e.d.for ﬁrst assertion.
For proof of second assertion, take in the above
χ=χ(a) := [f(a).=g(a)] :A→B2→ /BE
and get
πR⊢/rightanglenwf/rightangleneˇ =k/rightanglenwg/rightanglene
=⇒ProvPR /CG(j(k),/rightanglenwf.=g/rightanglene)
(substitutivity into.=)
=⇒[f(a).=g(a)] : /C6×A→ /BEq.e.d.
7.3πR decision
As the kernel of decision for p.r. predicate χ=χ(a) :A→ /BE
bytheoryπRwe introduce a (partially deﬁned) µ-recursive
decision algorithm ∇χ=∇PRχ: /BD⇀ /BEfor (individual) χ.
87
This decision algorithm is viewed as a map of theory π/hatwideR,of
partialπRmaps.
As apartialp.r. map it is given by three (PR) data:
•its index domain D=D∇χ,typically (and here): D⊆ /C6,
•its enumeration d=d∇χ:D→ /BDof itsdeﬁned argu-
ments,as well as
•itsrule/hatwide∇=/hatwide∇χ:D→ /BEmapping indices k,k′inD
pointing to the same argument d(k).=d(k′) in domain /BD,
to the same value/hatwide∇(k).=/hatwide∇(k′).
Now deﬁne alleged decision algorithm by ﬁxing its graph
∇χ=/a\}b∇ack⌉tl⌉{t(d,/hatwide∇) :D→ /BD× /BE/a\}b∇ack⌉t∇i}ht: /BD⇀ /BE
as follows:
Enumeration domain for deﬁned arguments is to be
D=D∇χ=def{k:¬χctA(k)∨ProvPR /CG(k,/rightanglenwχ/rightanglene)} ⊂ /C6,
with ct A: /C6→A(retractive) Cantor count, Aassumed
pointed.
Deﬁned arguments enumeration is here “simply”
d=defΠ :D⊆− → /C6Π− → /BD
—not a priori a retraction or empty—, and ruleis taken
/hatwide∇(k) =/hatwide∇χ(k) =def

false if¬χctA(k),
true if Prov PR /CG(k,/rightanglenwχ/rightanglene):D→ /BE.
88
/hatwide∇:D→ /BEis in fact a well deﬁned ruleforenumeration d:
D→ /C6→ /BDofdeﬁned argument(s) since by (earlier) framed
logical soundness theorem
πR⊢ProvPR /CG(k,/rightanglenwχ/rightanglene) =⇒χ(a) : /C6×A→ /BE,
whence disjointness of the alternative within D=D∇χ.
This taken together means intuitively within πR—and for-
mally within settheoryT:
∇(k) =∇χ(k) =

false if¬χctA(k),
true if Prov PR /CG(k,/rightanglenwχ/rightanglene),
undeﬁned otherwise.
Wehavethefollowing complete—metamathematical— case
distinction onD⊂ /C6:
•1stcase,termination: Dhas at least one (“total”) PR
point /BD→D⊆ /C6,and hence
t=t∇χ=bydefµD= minD: /BD→D
is a (total) p.r. point.
Subcases:
– 1.1st, negative (total) subcase:
¬χctA(t) = true.
[ThenπR⊢ ∇χ= false.]
– 1.2nd, positive (total) subcase:
ProvPR /CG(t,/rightanglenwχ/rightanglene) = true.
[ThenπR⊢ ∇χ= true,
byπR-framed objective soundness of PR /CG.]
89
These two subcases are disjoint, disjoint here by πR
framed soundness of theory PR /CGwhich reads
πR⊢ProvPR /CG(k,/rightanglenwχ/rightanglene) =⇒χ(a) :/C6×A→ /BE, k∈ /C6free,anda∈Afree,
here in particular—substitute t: /BD→ /C6intokfree:
πR⊢ProvPR /CG(t,/rightanglenwχ/rightanglene) =⇒χ(a) :A→ /BE,
afree.
So furthermore, by this framed soundness, in present
subcase:
πR⊢χ(a)∧ProvPR /CG(t,/rightanglenwχ/rightanglene) :A→ /BE.
•2ndcase,non-termination:
πR⊢D=∅/C6≡ { /C6: false/C6} ⊂ /C6
[then in particular πR⊢ ¬χ.= false A:A→ /BE,
soπR⊢χin this case], and
πR⊢ ¬ProvPR /CG(k,/rightanglenwχ/rightanglene) : /C6→ /BE, kfree;
•3rd,remaining, illcase is:
D(metamathematically) has no (total) points /BD→D,
but is nevertheless not empty.
Take in the above the (disjoint) union of 2nd subcase of
1st case and of 2nd case, last assertion. And formalise last,
remaining case frame πR.Arrive at the following
Quasi-Decidability Theorem: p.r.predicates χ:A→ /BE
give rise within theory πRto the following
complete (metamathematical) case distinction:
90
(a)πR⊢χ:A→ /BEor else
(b)πR⊢ ¬χctAt: /BD→D∇χ→ /BE
(deﬁned counterexample ), or else
(c)D=D∇χnon-empty, pointless, formally: in this case we
would have within πR:
[D/hatwide◦µD/hatwide= true : /BD⇀ /C6→ /BE]
and “nevertheless” for each p.r. point p: /BD→ /C6
¬D◦p= true : /BD→ /C6→ /BE.
Weruleoutthelatter—general—possibility ofa non-empty,
pointless predicate, for quantiﬁed arithmetical frame theory
Tby g¨ odelian assumption ofω-consistency which rules out
above instance of ω-inconsistency.
For frameπRwe rule it out by (corresponding) metamath-
ematical assumption of “µ-consistency,” as follows:
Intermission on two variants of ω-consistency:
G¨ odelian assumption of ω-consistency—non- ω-inconsistency—
for aquantiﬁed arithmetical theory Treads:
For no p.r. predicate ϕ: /C6→ /BE
T⊢(∃n∈ /C6)ϕ(n)
and(nevertheless)
T⊢ ¬ϕ(0),¬ϕ(1),¬ϕ(2), ...
Adaptation to (categorical) recursive theory πRis the fol-
lowingassumption ofµ-consistency, non- µ-inconsistency for
πR:
91
For no p.r. predicate ϕ: /C6→ /BE
πR⊢ϕ(µϕ) =bydefϕ/hatwide◦µϕ/hatwide= true : /BD⇀ /BE
and
πR⊢ ¬ϕ(0),¬ϕ(1), ... ,¬ϕ(num(n)), ...
For quantiﬁed Tﬁrst line reads: T⊢ ∃nϕ(n),and henceµ-
consistency is equivalent to g¨ odelian ω-consistency for such T.
Alternative to µ-consistency: π-consistency.
Byassertion (iii) of Structure Theorem insection 2— section
lemma—for theories /hatwideSof partial p.r. maps, ﬁrst factor µϕ:/BD⇀ /C6of (total) p.r. map true : /BD→ /BEabove is necessarily
itself a— totally deﬁned —PR map: Intuitively, a ﬁrst factor
of a total map cannot have undeﬁned arguments, since these
would be undeﬁned for the composition.
Now consider—here available—(external) point evaluation
into numerals4, externalisation of objective evaluation
ev :⌈ /BD, /C6⌉∼=− → ⌈ /BD, /C6⌉× /BDev⇀ /C6∼=− →ν /C6⊆ ⌈ /BD, /C6⌉
of point codes into (internal) numerals, ev( u) ˇ =u∈ ⌈ /BD, /C6⌉.
This externalised evaluation ev isassumed —meta-axiom
ofπ-consistency—to (correctly) terminate:
πR( /BD, /C6)⊃num /C6∋ev(p) =πp∈πR( /BD, /C6).
Comment: π-consistency means Semantical Completeness
of descent axiom ( π),this axiom is modeled into the external
world of p.r. Metamathematic. But π-consistency is somewhat
4Lassmann 1981
92
stronger: it assumes termination of ev instead of non-inﬁnite
descent.
Non-µ-inconsistency (ofπR) is then a consequence of
π-consistency of theory πRabove:
πR⊢true =ϕ(µϕ) =ϕ/hatwide◦µϕ=ϕ◦µϕ: /BD→ /C6→ /BE
entailsπR⊢ ¬(¬ϕ(num(n0))),with ev(µϕ) = num(n0).
End of Intermission.
Firstconsequence: TheoryπRadmitsnonon-emptypred-
icative subset {n∈ /C6:ϕ(n)} ⊆ /C6such that for each numeral
num(n)πR⊢ ¬ϕ◦num(n) : /BD→ /C6→ /BE.
This rules out—in quasi-decidability above—possibility (c)
for decision domain D=D∇χ⊆ /C6of decision operator ∇χfor
predicateχ:A→ /BE,and we get two unexpected results:
Decidability theorem: Each free-variable p.r. predicate
χ:A→ /BEgives rise to the following complete case distinc-
tionwithin, byπR:
•Under assumption of µ-consistency or π-consistency for
πR:
–πR⊢χ(a) :A→ /BE(theorem) or
–πR⊢ ¬χctAµD: /BD→D∇χ→ /BE
(deﬁned counterexample. )
•Under assumption of ω-consistency for settheoryT:
– T⊢χ(a) :A→ /BE(theorem) or
– T⊢ ¬χctAµD: /BD→D∇χ→ /BE,i.e.
T⊢(∃a∈A)¬χ(a).
93
7.4 Consistency provability
In the above take in case of settheoryTfor predicate χT’s
own free-variable p.r. consistency formula
ConT=¬ProvT(k,/rightanglenwfalse/rightanglene) : /C6→ /BE,
and get, under assumption of ω-consistency for Tconsistency
decidability forT.
This contradiction to (the postcedent) of G¨ odel’s 2nd In-
completeness theorem shows that the assumption ofω-con-
sistency for settheoriesT fails.
Now take in the theorem for χ πR’s own free variable p.r.
consistency formula
ConπR=¬ProvπR(k,/rightanglenwfalse/rightanglene) : /C6→ /BEand get
Consistency Decidability for descent theory πR:
Under assumption of π- orµ-consistency for theory πRwe
have
•πR⊢ConπR: /BD→ /BEor else
•πR⊢ ¬ConπR,will say
πR⊢ProvπR(µProvπR(k,/rightanglenwfalse/rightanglene),/rightanglenwfalse/rightanglene) = true
q.e.d.
Consistency provability theorem: πR⊢ConπR,under
assumption of π-consistency for theory πR.
Proof:Suppose we have 2nd alternative in consistency de-
cidability above,
πR⊢ProvπR(t,/rightanglenwfalse/rightanglene),
94
t=defµProvπR(k,/rightanglenwfalse/rightanglene) : /BD→ /C6,necessarily (”total”)
PR. Meta p.r. point evaluation ev would turn – π-consistency
–tinto a numeral num( k0) : /BD→ /C6, k0∈ /C6,num(k0) =πt,
hence
πR⊢ProvπR(num(k0),/rightanglenwfalse/rightanglene).
But by derivation-into- proofinternalisation we have
πR⊢ProvπR(num(k),/rightanglenwχ/rightanglene) (only) iﬀ πR⊢kχ,whence
we would get inconsistency πR⊢k0false,(and an inconsistent
theory derives everything.)
This rules out in fact 2nd alternative in consistency decid-
ability and so proves the theorem, here our main goal.
Problems:
(1) Is axiom scheme ( π) redundant, πR∼=PR /CGa? Certainly
not, since isotonic maps from canonically ordered /C6×/C6,..., /C6+≡ /C6[ω]≡ωωto /C6are not available.
(2) Can we get internal soundness for theory πRitself? Up
to now we get only objective soundness: this is the one
considered by mathematical logicians. Internal soundness
(ofevaluation versus the object language level) is a chal-
lenging open problem with present approach.
Discussion
•The claim for settheories Tis thatTderives¬ConT
which formally denies G¨ odel’s second incompleteness the-
orem: it denies its second postcedent and hence the as-
sumptionof ω-consistencyfor PM,ZF,andNGB.G¨ odel
95
himself has been said to be not completely convinced of
this assumption.
•TheoryPAisnotformally concerned by present incon-
sistency argument since descentscheme (π) needs for its
proof insettheorynestedinduction, available only(?) in
higher order framework, another germ of inconsistency,
cf. RCF3 in the references, built now (a postriori) on
currently proved properties of descent theory πR.
References
[1]J. Barwise ed. 1977: Handbook of Mathematical Logic.
North Holland.
[2]H.-B. Brinkmann, D. Puppe 1969:Abelsche und exakte
Kategorien, Korrespondenzen. LectureNotes in Math. 96.
[3]L. Budach, H.-J. Hoehnke 1975:Automaten und Funk-
toren.Akademie-Verlag Berlin.
[4]S. Eilenberg, C. C. Elgot 1970:Recursiveness. Aca-
demic Press.
[5]S. Eilenberg, G. M. Kelly 1966: Closed Categories.
Proc. Conf. on Categorical Algebra , La Jolla 1965, pp.
421-562. Springer.
[6]S. Eilenberg, S. Mac Lane 1945: General Theory of
natural Equivalences. Trans. AMS 58,231-294.
[7]G. Frege 1879:Begriﬀsschrift. Reprint in “Begriﬀss-
chrift und andere Aufs¨ atze”, 2te Auﬂage 1971, I. Angelelli
editor. Georg Olms Verlag Hildesheim, New York.
96
[8]P. J. Freyd 1972: Aspects of Topoi. Bull. Australian
Math. Soc. 7,1-76.
[9]K. G¨odel1931:¨Uber formal unentscheidbare S¨ atze
der Principia Mathematica und verwandter Systeme I.
Monatsh. der Mathematik und Physik 38,173-198.
[10]R. L. Goodstein 1957/64: Recursive Number Theory.
A Development of Recursive Arithmetic in a Logic-Free
Equation Calculus. North-Holland.
[11]R. L. Goodstein 1971:Development of Mathematical
Logic,ch. 7: Free-Variable Arithmetics. Logos Press.
[12]A. Joyal 1973: Arithmetical Universes. Talk at Oberwol-
fach.
[13]J. Lambek, P. J. Scott 1986:Introduction to higher
order categorical logic. Cambridge University Press.
[14]M. Lassmann 1981: G¨ odel’s Nichtableitbarkeitstheoreme
und Arithmetische Universen. Diploma Thesis. Techn.
Univ. Berlin.
[15]F. W. Lawvere 1964: An Elementary Theory of the
category of Sets. Proc. Nat. Acad. Sc. USA 51,1506-1510.
[16]F. W. Lawvere 1970: Quantiﬁers and Sheaves. Actes du
Congr` es International des Math´ ematiciens, Nice, Tome I,
329-334.
[17]F. W. Lawvere, S. H. Schanuel 1997, 2000: Concep-
tual Mathematics. Cambridge University Press.
[18]S. Mac Lane 1972:Categories for the working mathe-
matician. Springer.
97
[19]M. E. Maietti 2010: Joyal’s arithmetic universe as list-
arithmetic pretopos. Theory and Applications of Cate-
gories24(3), 39-83.
[20]G. Osius 1974: Categorical set theory: a characterisation
of the category of sets. J. Pure and Appl. Alg. 4, 79-119.
[21]B. Pareigis 1969:Kategorien und Funktoren . Teubner.
[22]B. Pareigis 2004:Category Theory. pdf Script, author’s
Home page LMU M¨ unchen.
[23]R. P´eter1967:Recursive Functions . Academic Press.
[24]M. Pfender 1974: Universal Algebra in S-Monoidal Cat-
egories.Algebra-Berichte Nr. 20, Mathematisches Institut
der Universit¨ at M¨ unchen. Verlag Uni-Druck M¨ unchen.
[25]M. Pfender 2008b: RCF2: Evaluation and Consistency.
arXiv:0809.3881v2 [math.CT]. Has a gap.
[26]M. Pfender 2008c: RCF3: Map-CodeInterpretation via
Closure. arXiv:0809.4970v1 [math.CT].
[27]M. Pfender 2012:Arithmetical Foundations, αversion,
http://www3.math.tu-berlin.de/preprints/ﬁles/Prepri nt-
38-2012.pdf
[28]M. Pfender 2014:Arithmetical Foundations, γversion,
scratch book pp.237, KIEPERT Berlin.
[29]M. Pfender, M. Kr ¨oplin, D. Pape 1994: Primitive
Recursion, Equality, and a Universal Set. Math. Struct. in
Comp. Sc. 4,295-313.
[30]M. Pfender, R. Reiter, M. Sartorius 1982: Con-
structive Arithmetics. Lecture Notes in Math. 962,228-
236.
98
[31]R. Reiter 1980: Mengentheoretische Konstruktionen in
arithmetischen Universen. Diploma Thesis. Techn. Univ.
Berlin.
[32]R. Reiter 1982: Ein algebraisch-konstruktiver Abbil-
dungskalk¨ ul zur Fundierung der elementaren Arithmetik.
Dissertation, rejected by Math. dpt. of TU Berlin.
[33]L. Rom`an1989: Cartesian categories with natural num-
bers object. J. Pure and Appl. Alg. 58,267-278.
[34]C. Smorynski 1977: TheIncompleteness Theorems. Part
D.1 inBarwise ed. 1977.
[35]A. Yashuhara 1971:Recursive function theory and logic.
Academic Press.
99

Transforming while/do/for/foreach -Loops
into Recursive Methods?
David Insa and Josep Silva
Departamento de Sistemas Inform aticos y Computaci on
Universitat Polit ecnica de Val encia
Camino de Vera s/n
E-46022 Valencia, Spain
fdinsa,jsilvag@dsic.upv.es
Abstract.
In software engineering, taking a good election between recursion and iteration is essential because
their eciency and maintenance are dierent. In fact, developers often need to transform iteration into
recursion (e.g., in debugging, to decompose the call graph into iterations); thus, it is quite surprising
that there does not exist a public transformation from loops to recursion that handles all kinds of
loops. This article describes a transformation able to transform iterative loops into equivalent recursive
methods. The transformation is described for the programming language Java, but it is general enough
as to be adapted to many other languages that allow iteration and recursion. We describe the changes
needed to transform loops of types while/do/for/foreach into recursion. Each kind of loop requires a
particular treatment that is described and exemplied.
Keywords: Program transformation, Iteration, Recursion
1 Introduction
Iteration and recursion are two dierent ways to reach the same objective. In some paradigms, such as the
functional or logic, iteration does not even exist. In other paradigms, e.g., the imperative or the object-oriented
paradigm, the programmer can decide which of them to use. However, they are not totally equivalent, and
sometimes it is desirable to use recursion, while other times iteration is preferable. In particular, one of the
most important dierences is the performance achieved by both of them. In general, compilers have produced
more ecient code for iteration, and this is the reason why several transformations from recursion to iteration
exist (see, e.g., [6,9,10]). Recursion in contrast is known to be more intuitive, reusable and debuggable. In fact,
other researchers have obtained both theoretical and experimental results showing signicant performance
benets of recursive algorithms on both uniprocessor hierarchies and on shared-memory systems [12]. In
particular, Gustavson and Elmroth [4,3] have demonstrated signicant performance benets from recursive
versions of Cholesky and QR factorization, and Gaussian elimination with pivoting.
Transforming loops to recursion is also useful in debugging, as demonstrated by the technique presented
in [8]. In this paper, transforming all iterative loops into recursive methods before starting an algorithmic de-
bugging session can improve the interaction between the debugger and the programmer, and it can also reduce
the granularity of the errors found. In particular, algorithmic debuggers only report buggy methods. Thus,
a bug inside a loop is reported as a bug in the whole method that contains the loop, which is sometimes too
imprecise. Transforming a loop into a recursive method allows the debugger to identify the recursive method
(and thus the loop) as buggy. Hence, we wanted to implement this transformation and integrate it in the
Declarative Debugger for Java (DDJ) [7], but, surprisingly, we did not nd any available transformation from
iterative loops into recursive methods for Java (neither for any other object-oriented language). Therefore,
we had to implement it by ourselves and decided to automatize and generalize the transformation to make
?This work has been partially supported by the EU (FEDER) and the Spanish Ministerio de Econom a y Com-
petitividad (Secretar a de Estado de Investigaci on, Desarrollo e Innovaci on) under grant TIN2013-44742-C4-1-R
and by the Generalitat Valenciana under grant PROMETEO/2011/052. David Insa was partially supported by the
Spanish Ministerio de Educaci on under FPU grant AP2010-4415.arXiv:1410.4956v2  [cs.PL]  21 Oct 2014
II
it publicly available. From the best of our knowledge this is the rst transformation for all kinds of iterative
loops.
One important property of our transformation is that it always produces tail recursive methods [2]. This
means that they can be compiled to ecient code because the compiler only needs to keep two activation
records in the stack to execute the whole loop [5,1]. Another important property is that each iteration is always
represented with one recursive call. This means that a loop that performs 100 iterations is transformed into
a recursive method that performs 100 recursive calls. This equivalence between iterations and recursive calls
is very important for some applications such as debugging, and it produces code that is more maintainable.
In the rest of the paper we describe our transformation for all kinds of loops in Java (i.e., while/do/for/foreach ).
The transformation of each particular kind of loop is explained with an example. We start with an illustrative
example that provides the reader with a general view of how the transformation works.
Example 1. Consider the Java code in Algorithm 1 that computes the square root of the input argument.
Algorithm 1 Sqrt (iterative version)
1:public double sqrt(double x)f
2: if(x<0)
3: return Double.NaN;
4: double b = x;
5: while (Math.abs(b * b - x) >1e-12)
6: b = ((x / b) + b) / 2;
7: return b;
8:g
This algorithm implements a while -loop where each iteration obtains a more accurate approximation of
the square root of variable x. The transformed code is depicted in Algorithm 2 that implements the same
functionality but replacing the while -loop with a new recursive method sqrtloop.
Algorithm 2 Sqrt (recursive version)
1:public double sqrt(double x)f
2: if(x<0)
3: return Double.NaN;
4: double b = x;
5: if(Math.abs(b * b - x) >1e-12)
6: b = this.sqrt loop(x, b);
7: return b;
8:g
9:private double sqrtloop(double x,double b)f
10: b = ((x / b) + b) / 2;
11: if(Math.abs(b * b - x) >1e-12)
12: return this.sqrt loop(x, b);
13: return b;
14:g
Essentially, the transformation performs two steps:
1. Substitute the original loop by new code (lines 5-6 in Algorithm 2).
2. Create a new recursive method (lines 9-14 in Algorithm 2).
In Algorithm 2, the new code in method sqrt includes a call (line 6) to the recursive method sqrtloop
that implements the loop (lines 9-14). This new recursive method contains the body of the original loop
(line 10). Therefore, each time the method is invoked, an iteration of the loop is performed. The rest of the
III
code added during the transformation (lines 5, 11-13) is the code needed to simulate the same eects of a
while -loop. Therefore, this is the only code that we should change to adapt the transformation to the other
kinds of loops ( do/for/foreach ).
2 Transforming loops into recursive methods
Our program transformations are summarized in Table 1. This table has a dierent row for each kind of loop.
For each loop, we have two columns. One for the iterative version of the loop, and one for the transformed
recursive version. Observe that the code is presented in an abstract way, so that it is formed by a parameterized
skeleton of the code that can be instantiated with any particular loop of each kind.
In the recursive version, the code inside the ellipses is code inserted by the programmer (it comes from
the iterative version). The rest of the code is automatically generated by the transformation. Here, result
andloop are fresh names (not present in the iterative version) for a variable and a method respectively; type
is a data type that corresponds to the data type declared by the user (it is associated to a variable already
declared in the iterative version). The code inside the squares has the following meaning:
1contains the sequence formed by all variables declared in Code1 (and in iniinfor-loops) that are used
inCode2 andcond (and in updinfor-loops).
1'contains the previous sequence but including types (because it is used as the parameters of the method,
and the previous sequence is used as the arguments of the call to the method).
2contains for each object in the array result (which contains the same variables as 1and 1'), a
casting of the object to assign the corresponding type. For instance, if the array contains two variables
[x,y] whose types are respectively double andint; then 2contains:
x = (Double) result[0];
y = (Integer) result[1] ;
Observe that, even though these steps are based on Java, the same steps (with small modications) can
be used to transform loops in many other imperative or object-oriented languages. The code in Table 1 is
generic. In some specic cases, this code can be optimized. For instance, observe that the recursive method
always returns an array of objects ( return new Object[] f...g) with all variables that changed in the
loop. This array is unnecessary and inecient if the recursive method only needs to return one variable
(or if it does not need to return any variable). Therefore, the creation of the array should be replaced by
a single variable or null (i.e., return null ). In the rest of the paper, we always apply optimizations when
possible, so that the code does not perform any unnecessary operations. This allows us to present a generic
transformation as the one in Table 1, and also to provide specic ecient transformations for each kind of
loop. The optimizations are not needed to understand the transformation, but they should be considered
when implementing it. In the rest of this section we explain the transformation of all kinds of loop. The four
kinds of loops ( while/do/for/foreach ) present in the Java language behave nearly in the same way. Therefore,
the modications needed to transform each kind of loop into a recursive method are very similar. We start
by describing the transformation for while -loops, and then we describe the variations needed to adapt the
transformation for do/for/foreach -loops.
2.1 Transformation of while -loops
In Table 2 we show a general overview of the steps needed to transform a Java iterative while -loop into an
equivalent recursive method. Each step is described in the following.
Substitute the loop by a call to the recursive method The rst step is to remove the original loop and
substitute it with a call to the new recursive method. We can see this substitution in Figure 1(b). Observe
that some parts of the transformation have been labeled to ease later references to the code. The tasks
performed during the substitution are explained in the following:
{ Perform the rst iteration
In the while -loop, rst of all we check whether the loop condition holds. If it does not hold, then the
IV
Table 1. Loops transformation taxonomy
V
(a) Original method
 (b) Transformed method
(c) Recursive method
Fig. 1. while -loop transformation
Step Correspondence with Figure 1
Figure 1(b)
1) Substitute the loop by a call to the recursive method Caller
1.1) If the loop condition is satised Loop condition
1.1.1) Perform the rst iteration First iteration
1.2) Catch the variables modied during the recursion Modied variables
1.3) Update the modied variables Updated variables
Figure 1(c)
2) Create the recursive method Recursive method
2.1) Dene the method's parameters Parameters
2.2) Dene the code of the recursive method
2.2.1) Include the code of the original loop Loop code
2.2.2) If the loop condition is satised Loop condition
2.2.2.1) Perform the next iteration Next iteration
2.2.3) Otherwise return the modied variables Modied variables
Table 2. Steps of the while -loop transformation
loop is not executed. Otherwise, the rst iteration is performed by calling the recursive method with the
variables used inside the loop as arguments of the method call. Hence, we need an analysis to know what
variables are used inside the loop. The recursive method is in charge of executing as many iterations of
the loop as needed.
{ Catch the variables modied during the recursion
The variables modied during the recursion cannot be automatically updated in Java because all param-
eters are passed by value. Therefore, if we modify an argument inside a method we are only modifying
a copy of the original variable. This also happens with objects. Hence, in order to output those modied
VI
variables that are needed outside the loop, we use an array of objects. Because the modied variables can
be of any data type1, we use an array of objects of class Object .
In presence of call-by-reference, this step should be omitted.
{ Update the modied variables
After the execution of the loop, the modied variables are returned inside an Object array. Each variable
in this array must be cast to its respective type before being assigned to the corresponding variable
declared before the loop.
In presence of call-by-reference, this step should be omitted.
Create the recursive method Once we have substituted the loop, we create a new method that implements
the loop in a recursive way. This recursive method is shown in Figure 1(c).
The code of the recursive method is explained in the following:
{ Dene the method's parameters
There are variables declared inside a method but declared outside the loop and used by this loop. When
the loop is transformed into a recursive method , these variables are not accessible from inside the recursive
method . Therefore, they must be passed as arguments in the calls to it. Hence, the parameters of the
recursive method are the intersection between the variables declared before the loop and the variables
used inside it.
{ Dene the code of the recursive method
Each iteration of the original iterative loop is emulated with a call to the new recursive method. Therefore
in the code of the recursive method we have to execute the current iteration and control whether the next
iteration must be executed or not.
Include the code of the original loop
When the recursive method is invoked it means that we want to execute one iteration of the loop.
Therefore, we place the original code of the loop at the beginning of the recursive method . This code
is supposed to update the variables that control the loop condition . Otherwise, the original loop is in
fact an innite loop and the recursive method created will be invoked innitely.
Perform the next iteration
Once the iteration is executed, we check the loop condition again to know whether another iteration
must still be executed. In such a case, we perform the next iteration with the same arguments. Note
that the values of the arguments can be modied during the execution of the iteration, therefore,
each iteration has dierent arguments values, but the names and the number of arguments remain
always the same.
Otherwise return the modied variables
If the loop condition does not hold, the loop ends and thus we must nish the sequence of recursive
method calls and return to the original method in order to continue executing the rest of the code.
Because the arguments have been updated in each recursive call, at this point we have the last values
of the variables involved in the loop. Hence these variables must be returned in order to update them
in the original method. Observe that these variables are passed from iteration to iteration during the
execution of the recursive method until it is nally returned to the recursive method caller.
In presence of call-by-reference, this step should be omitted.
Figure 2 shows an example of transformation of a while -loop.
2.2 Transformation of do-loops
do-loops behave exactly in the same way as while -loops except in one detail: The rst iteration of the do-loop
is always performed. In Figure 3 we can see an example of a do-loop.
This code obtains the square root value of variable xas the code in Algorithm 1. The dierence is that,
if variable xis either 0 or 1, then the method directly returns variable x, otherwise the loop is performed in
1In the case that the returned values are primitive types, then they are naturally encapsulated by the compiler in
their associated primitive wrapper classes.
VII
(a) Original method
 (b) Transformed method
(c) Recursive method
Fig. 2. while -loop transformation
Fig. 3. do-loop
order to calculate the square root. In order to transform the do-loop into a recursive method, we can follow
the same steps used in Table 2 with only one change: in step 1.1 the loop condition is not evaluated; instead,
we only need to add a new code block to ensure that those variables created during the transformation are
not available outside the transformed code.
Figure 4 illustrates the only change needed to transform the do-loop into a recursive method. Observe
that in this example there is no need to introduce a new block, because the transformed code does not create
new variables, but in the general case the block could be needed.
{ Add a new code block
Observe in Table 1, in column Caller , that, contrarily to while -loops, do-loops need to introduce a new
block (i.e., a new scope). The reason is that there could exist variables with the same name as the variables
created during the transformation (e.g., result ). Hence, the new block avoids variable clashes and limits
the scope of the variables created by the transformation.
VIII
(a) Recursive method caller
 (b) Recursive method
Fig. 4. do-loop transformation
2.3 Transformation of for-loops
One of the most frequently used loops in Java is the for-loop. This loop behaves exactly in the same way as
thewhile -loop except in one detail: for-loops provide the programmer with a mechanism to declare, initialize
and update variables that will be accessible inside the loop.
In Figure 5(a) we can see an example of a for-loop. This code obtains the square root value of variable x
exactly as the code in Algorithm 1, but it also prints the approximation obtained in every iteration. We can
see in Figure 5(b) and 5(c) the additional changes needed to transform the for-loop into a recursive method.
(a) For loop
(b) Recursive method caller
 (c) Recursive method
Fig. 5. for-loop transformation
As shown in Figure 5, in order to transform the for-loop into a recursive method, we can follow the same
steps used in Table 2, but we have to make three changes:
{ Add a new code block
Exactly in the same way and with the same purpose as in do-loops.
IX
{ Add the declarations/initializations at the beginning of the block
In the original method, those variables created during the declaration and initialization of the loop are
only available inside it (and not in the code that follows the loop). We must ensure that these variables
keep the same scope in the transformed code. This can be easily achieved with the new block . In the
transformed code, those variables are declared and initialized at the beginning of the new block , and they
are passed as arguments to the recursive method in every iteration to make them accessible inside it.
{ Add the updates between the loop code and the loop condition
Infor-loops there exists the possibility of executing code between iterations. This code is usually a
collection of updates of the variables declared at the beginning of the loop (e.g., in Figure 5(a) this code
isiter++ ). However, this code could be formed by a series of expressions separated by commas that
could include method invocations, assignments, etc. Because this update code is always executed before
the condition of the loop, it must be placed in the recursive method between the loop code and the loop
condition .
2.4 Transformation of foreach -loops
foreach -loops are specially useful to traverse collections of elements. In particular, this kind of loops traverses
a given collection and it executes a block of code for each element. The transformation of a foreach -loop
into a recursive method is dierent depending on the kind of collection that is traversed. In Java we can use
foreach -loops either with arrays oriterable objects. We explain each transformation separately.
foreach -loop used to traverse arrays An array is a composite data structure where elements have been
sequentialized, and thus, they can be traversed linearly. We can see an example of a foreach -loop that traverses
an array in Algorithm 3.
Algorithm 3 foreach -loop that traverses an array (iterative version)
1:public void foreachArray()f
2: double[] numbers = new double[]f4.0, 9.0g;
3: for(double number : numbers) f
4: double sqrt = this.sqrt(number);
5: System.out.println(\sqrt(" + number + \) = " + sqrt);
6:g
7:g
This code computes and prints the square root of all elements in the array [4.0, 9.0]. Each individual
square root is computed with Algorithm 1. The foreach -loop traverses the array sequentially starting in
position numbers[0] until the last element in the array. The transformation of this loop into an equivalent
recursive method is very similar to the transformation of a for-loop. However there are dierences. For
instance, foreach -loops lack of a counter. This can be observed in Figure 6 that implements a recursive
method equivalent to the loop in Algorithm 3.
In Figure 6 we can see the symmetry with respect to the for-loop transformation. The only dierence is
the creation of a fresh variable that is passed as argument in the recursive method calls (in the example this
variable is called index ). This variable is used for:
{ Controlling whether there are more elements to be treated
Aforeach -loop is only executed if the array contains elements. Therefore we need a loop condition in the
recursive method caller and another in the recursive method to know when there are no more elements
in the array and thus nish the traversal. The later is controlled with a variable ( index in the example)
acting as a counter.
{ Obtaining the next element to be treated
During each iteration of the foreach -loop a variable called number is instantiated with one element of the
array (line 3 of Algorithm 3). In the transformation this behavior is emulated by declaring and initializing
this variable at the beginning of the recursive method. It is initialized to the corresponding element of
the array by using variable index .
X
(a) Recursive method caller
(b) Recursive method
Fig. 6. foreach -loop transformation (Array version)
foreach -loop used to traverse iterable objects Aforeach -loop can be used to traverse objects that
implement the interface Iterable . Algorithm 4 shows an example of a foreach -loop using one of these objects.
Algorithm 4 foreach -loop used to traverse an iterable object (iterative version)
1:public void foreachIterable() f
2: List<Double >numbers = Arrays.asList(4.0, 9.0);
3: for(double number : numbers) f
4: double sqrt = this.sqrt(number);
5: System.out.println(\sqrt(" + number + \) = " + sqrt);
6:g
7:g
This code behaves exactly in the same way as Algorithm 3 but using an iterable object instead of an array
(numbers is an iterable object because it is an instance of class List that in turn implements the interface
Iterable ). The interface Iterable only has one method, called iterator , that returns an object that implements
theIterator interface. With regard to the interface Iterator , it forces the programmer to implement the
next,hasNext and remove methods; and these methods allow the programmer to freely implement how the
collection is traversed (e.g., the order, whether repetitions are taken into account or not, etc.). Therefore,
the transformed code should use these methods to traverse the collection. We can see in Figure 7 a recursive
method equivalent to Algorithm 4.
Observe that the transformed code in Figure 7 is very similar to the one in Figure 6. The only dierence
is the use of an iterator variable (instead of an integer variable) that controls the element of the collection
to be treated. Note that method next of variable iterator allows us to know what is the next element to be
treated, and method hasNext tell us whether there exist more elements to be processed yet.
3 Correctness
In this section we provide a formal semantics-based specication of our transformation in order to prove
its correctness. For this, we provide a BNF syntax specication and an operational semantics of Java. We
consider the subset of Java that is needed to implement the transformation ( if-then-else ,while , method calls,
XI
(a) Recursive method caller
(b) Recursive method
Fig. 7. foreach-loop transformation (Iterable version)
return , etc.), and we ignore the rest of syntax elements for the sake of simplicity (they do not have any
inuence because any command inside the body of the loop remains unchanged in the transformed code).
Moreover, in this section, we center the discussion on while -loops and prove properties for this kind of loop.
The proof for the other kinds of loops is omitted, but it would be in all cases analogous or slightly incremental.
We start with a BNF syntax of Java:
P::=M1; : : : ; M n; Sp (program) Domains
x; y; z : : :2V(variables)
a; b; c : : :2C(constants)
M::=m(x1; : : : ; x n)fSp;Srg (method denition) where x1; : : : ; x n2V
andmis the name
of the method
Sp::=x:=E (assignment)
jx:=m(E0; : : : ; E n) (method invocation)
jifEbthen Sp (if-then)
jifEbthen SpelseSp (if-then-else)
jwhile EbdoSp (while)
jSp;S0
p (sequence)
Sr::= return E (return)
E::=EajEb (expresion)
Ea::=Ea+EajEa EajV (arithmetic expresion)
Eb::=Eb!=EbjEb==EbjV (boolean expresion)
V::=xja (variables or constants)
A program is a set of method denitions and at least one initial statement (usually a method invocation).
Each method denition is composed of a set of statements followed by a return statement. For simplicity, the
arguments of a method invocation can only be expressions (not statements). This is not a restriction, because
any statement can be assigned to a variable and then be passed as argument of the method invocation.
However, this simplication allows us to ease the semantics of method invocations and, thus, it increases
readability.
In the following we consider two versions of the same program shown in Algorithms 5 and 6. We assume
that in Algorithm 5 there exists a variable xalready dened before the loop and, for the sake of simplicity,
XII
it is the only variable modied inside S. Therefore, Algorithm 6 is the recursive version of the while -loop
in Algorithm 5 according to our transformation, and hence, p0; : : : ; p nrepresent all variables dened before
the loop and used in S(the loop statements) and cond (the loop condition). In the case that more than
one variable are modied, then the output would be an array with all variables modied. We avoid this case
because it is not necessary for the proof.
Algorithm 5 While version
1:while cond do
2: S;
Algorithm 6 Recursive version
1:m(p0; : : : ; p n)f
2: S;
3: ifcond then
4: x:=m(a0; : : : ; a n);
5: return x;
6:g
7:ifcond then
8: x:=m(a0; : : : ; a n);
In order to provide an operational semantics for this Java subset, which allows recursion, we need a stack
to push and pop dierent frames that represent individual method activations. Frames, f0; f1; : : :2F, are
sequences of pairs variable-value. States, s0; s1; : : :2S, are sequences of frames ( S:Fx: : :xF). We make
the program explicitly accessible to the semantics through the use of an environment, e2E, represented with
a sequence of functions from method names Mto pairs of parameters Pand statements I(E: (M!(Px
I)) x: : :x (M!(PxI))). Our semantics is based on the Java semantics described in [11] with some minor
modications. It uses a set of functions to update the state, the environment, etc.
Function Upd vis used to update a variable ( var) in the current frame of the state ( s) with a value ( value ).
The current frame in the state is always the last frame introduced (i.e., the last element in the sequence of
frames that represent the state). We use the standard notation f[var!value ] to denote that variable var
in frame fis updated to value value .
Upd v(s; var!value ) =error ifs= []
[f0; : : : ; f n[var!value ]]ifs= [f0; : : : ; f n]
Function Upd rrecords the returned value ( value ) of the current frame of the state ( s) inside a fresh
variable<of this frame, so that other frames can consult the value returned by the current frame.
Upd r(s; value ) =error ifs= []
[f0; : : : ; f n[<! value ]]ifs= [f0; : : : ; f n]
Function Upd vris used to update a variable ( var) in the penultimate frame of the state ( s) taking the
value returned by the last frame in the state (which must be previously stored in <). This happens when a
method calls another method and the latter nishes returning a value. In this situation, the last frame in the
state should be removed and the value returned should be updated in the penultimate frame. We use the
notation fn(<) to consult the value of variable <in frame fn.
Upd vr(s; var ) =error ifs= [] ors= [f]
[f0; : : : ; f n 1[var!fn(<)]; fn]ifs= [f0; : : : ; f n 1; fn]
Function Upd eis used to update the environment ( env) with a new method denition ( m!(P; I)). The
environment is used in method invocations to know the method that should be executed.
Upd e(env; m!(P; I)) =env[m!(P; I)]
XIII
Function AddFrame adds a new frame to the state ( s). This frame is a sequence of mappings from
parameters ( p0; : : : ; p n) to the evaluation of arguments ( a0; : : : ; a n). To evaluate an expression we use function
Eval: a variable is consulted in the state, a constant is just returned, and a mathematical or boolean expression
is evaluated with the standard semantics. We use this notation because the evaluation of expressions does
not have inuence in our proofs, but it signicantly reduces the size of derivations, thus, improving clarity
of presentation.
AddFrame (s;[p0; : : : ; p n];[a0; : : : ; a n]) =[[p0!Eval (a0); : : : ; p n!Eval (an)]] if s= []
[f0; : : : ; f m;[p0!Eval (a0); : : : ; p n!Eval (an)]]if s= [f0; : : : ; f m]
Analogously, function RemFrame removes the last frame inserted into the state ( s).
RemFrame (s) =8
<
:error if s = []
[] if s= [f]
[f0; : : : ; f n]if s= [f0; : : : ; f n; fn+1]
We are now in a position ready to introduce our Java operational semantics. Essentially, the semantics is
a big-step semantics composed of a set of rules of the form:p1:::pn
env`<st; s>+s0that should be read as \The
execution of statement stin state sunder the environment envcan be reduced to state s0provided that
premises p1: : : p nhold". The rules of the semantics are shown in Figure 8.
We can now prove our main result.
Theorem 1 (Correctness). Algorithm 6 is semantically equivalent to Algorithm 5.
Proof. We prove this claim by showing that the nal state of Algorithm 5 is always the same as the nal
state of Algorithm 6. The semantics of a program Pis:
S(P) =si []`< P; []>+s
Therefore, we say that two programs P1andP2are equivalent if they have the same semantics:
S(P1) =S(P2) i []`< P 1;[]>+s1^[]`< P 2;[]>+s2^s1=s2
For the sake of generality, in the following we consider that the loops can appear inside any other code.
Therefore, the environment and the state are not necessarily empty. Thus, we will assume an initial environ-
ment env0and an initial state s:env0`< P; s > . We proof this semantic equivalence analyzing two possible
cases depending on whether the loop is executed or not.
1) Zero iterations
This situation can only happen when the condition cond is not satised the rst time it is evaluated.
Hence, we have the following semantics derivation for each program:
Iterative version
< cond; s > )false
env0`< while cond do S; s > +s
Recursive version
env=Upde(env0; m!(P; S;I))< cond; s > )false env`<p; s > +s
env`< if cond then t elsep; s > +s
env`< if cond then t; s > +s
env0`< m(P)fS;Igif cond then t; s > +s
Clearly, the state is never modied neither in the iterative version nor in the recursive version. Therefore,
both versions are semantically equivalent.
XIV
New method
env0=Upde(env; m 0!(P; I ))^env0`<m1i; s> +s0
env`<m0(P)fIgm1i; s> +s0
Empty statement
env`<p; s>+s
Asignment
s0=Updv(s; x!Eval (op; s ))
env`<x:=op; s> +s0
Method invocation
(P; I ) = env(m)^s0=AddFrame (s;P;A )^env`<I; s0>+s00^s000=Updvr(s00;x)^s0000=RemFrame (s000)
env`<x:=m(A); s>+s0000
If
env`<if cond then i 0elsep; s>+s0
env`<if cond then i 0; s>+s0
<cond; s> )true^env`<i0; s>+s0
env`<if cond then i 0else i 1; s>+s0
<cond; s> )false ^env`<i1; s>+s0
env`<if cond then i 0else i 1; s>+s0
While
<cond; s> )false
env`<while cond do i; s> +s
<cond; s> )true^env`<i; s> +s0^env`<while cond do i; s0>+s00
env`<while cond do i; s> +s00
Sequence
env`<i0; s>+s0^env`<i1; s0>+s00
env`<i0;i1; s>+s00
Return
s0=Updr(s;Eval (op; s ))
env`<return op; s> +s0
Fig. 8. Java Semantics
2) One or more iterations
This means that the condition cond is satised at least once. Let us consider that cond is satised ntimes,
producing niterations. We proof that the nal state of the program in Algorithm 6 is equal to the nal state
of the program in Algorithm 5 by induction over the number of iterations performed.
(Base Case) In the base case, only one iteration is executed. Hence, we have the following derivations:
Iterative version
< cond; s > )true env0`< S; s > +s1< cond; s > )false
env0`< while cond do S; s1>+s1
env0`< while cond do S; s > +s1
Recursive version
env`< S; s1>+s2< cond; s2>)false env`<p; s2>+s2
env`< if cond then t elsep; s2>+s2
env`< if cond then t; s2>+s2s3=Updr(s2;Eval(x; s2))
env`< return x; s2>+s3
env`< if cond then t ;return x; s2>+s3
env`< S;I; s1>+s3
4
XV
< cond; s > )true(P; I) =env(m)s1=AddFrame (s; P;[a0; : : : ; a n])4s4=Updvr(s3; x)s5=RemFrame (s4)
env`< x:=m(a0; : : : ; a n); s > +s5
env`< if cond then t elsep; s > +s5
env`< if cond then t; s > +s5
We can assume that variable xhas an initial value z0, which must be the same in both versions of the
algorithm. Then, states are modied during the iteration as follows:
Iterative version Recursive version
s= [f0])f0=fx!z0g s= [f0])f0=fx!z0g
s1= [f0])f0=fx!z1g s1= [f0; f1])f0=fx!z0g^f1=fx!z0g
s2= [f0; f1])f0=fx!z0g^f1=fx!z1g
s3= [f0; f1])f0=fx!z0g^f1=fx!z1;<! z1g
s4= [f0; f1])f0=fx!z1g^f1=fx!z1;<! z1g
s5= [f0])f0=fx!z1g
Clearly, with the same initial states, both algorithms produce the same nal state.
(Induction Hypothesis) We assume as the induction hypothesis that executing iiterations in both versions
with an initial value z0forxthen, if the iterative version obtains a nal value znforxthen the recursive
version correctly obtains and stores the same nal value znfor variable xin the top frame.
Iterative version Recursive version
s= [f0])f0=fx!z0g s= [f0])f0=fx!z0g
: : : : : :
s0= [f0])f0=fx!zng s0= [f0])f0=fx!zng
(Inductive Case) We now prove that executing i+ 1 iterations in both versions with an initial value z0for
xthen, if the iterative version obtains a nal value znforxthen the recursive version correctly obtains and
stores the same nal value znfor variable xin the top frame.
The derivation obtained for each version is the following:
Iterative version
< cond; s > )true env0`< S; s > +s1Induction hypotesis
env0`< while cond do S; s1>+s2
env0`< while cond do S; s > +s2
Recursive version
env`< S; s1>+s2Induction hypotesis
env`< if cond then t; s2>+s3s4=Updr(s3;Eval(x; s3))
env`< return x; s3>+s4
env`< if cond then t ;return x; s2>+s4
env`< S;I; s1>+s4
4
< cond; s > )true(P; I) =env(m)s1=AddFrame (s; P;[a0; : : : ; a n])4s5=Updvr(s4; x)s6=RemFrame (s5)
env`< x:=m(a0; : : : ; a n); s > +s6
env`< if cond then t elsep; s > +s6
env`< if cond then t; s > +s6
Because both algorithms have the same initial value z0forxthen the states during the iteration are modied
as follows (the * state is obtained by the induction hypothesis):
Iterative version Recursive version
s= [f0])f0=fx!z0g s= [f0])f0=fx!z0g
s1= [f0])f0=fx!z1g s1= [f0; f1])f0=fx!z0g^f1=fx!z0g
s2= [f0])f0=fx!zng s2= [f0; f1])f0=fx!z0g^f1=fx!z1g
s3= [f0; f1])f0=fx!z0g^f1=fx!zng
s4= [f0; f1])f0=fx!z0g^f1=fx!zn;<! zng
s5= [f0; f1])f0=fx!zng^f1=fx!zn;<! zng
s6= [f0])f0=fx!zng
Hence, Algorithm 6 and Algorithm 5 obtain the same nal state, and thus, they are semantically equivalent.
XVI
4 Conclusions
Transforming loops to recursion is useful in many situations such as, e.g., debugging, verication or memory
hierarchies optimization. It is therefore surprising that there did not exist an automatic transformation from
loops to recursion, but it is even more surprising that no public report exists that describes how to implement
this transformation.
In this article the transformation of each kind of Java loop has been described independently with a
specic treatment for it that is illustrated with an example of use. Moreover, the transformation has been
described in such a way that it can be easily adapted to any other language where recursion and iteration
exist.
References
1. H. G. Baker. Garbage collection, tail recursion and rst-class continuations in stack-oriented languages. Patent,
dec 1996. US 5590332.
2. W. Clinger. Proper tail recursion and space eciency. ACM SIGPLAN Notices , 33(5):174{185, may 1998.
3. E. Elmroth and F. G. Gustavson. Applying recursion to serial and parallel QR factorization leads to better
performance. IBM Journal of Research and Development , 44(4):605{624, 2000.
4. F. G. Gustavson. Recursion leads to automatic variable blocking for dense linear-algebra algorithms. IBM Journal
of Research and Development , 41(6):737{756, 1997.
5. C. Hanson. Ecient stack allocation for tail-recursive languages. In Proceedings of the 1990 ACM conference on
LISP and Functional Programming (LFP'90) , pages 106{118, New York, NY, USA, 1990. ACM.
6. P. G. Harrison and H. Khoshnevisan. A new approach to recursion removal. Electronic Notes in Theoretical
Computer Science , 93(1):91{113, feb 1992.
7. D. Insa and J. Silva. An Algorithmic Debugger for Java. In Proceedings of the 26th IEEE International Conference
on Software Maintenance (ICSM 2010) , pages 1{6, 2010.
8. D. Insa, J. Silva, and C. Tom as. Enhancing Declarative Debugging with Loop Expansion and Tree Compression.
In E. Albert, editor, Proceedings of the 22th International Symposium on Logic-based Program Synthesis and
Transformation (LOPSTR 2012) , volume 7844 of Lecture Notes in Computer Science (LNCS) , pages 71{88.
Springer, sep 2012.
9. Y. A. Liu and S. D. Stoller. From recursion to iteration: what are the optimizations? In Proceedings of the 2000
ACM-SIGPLAN Workshop on Partial Evaluation and semantics-based Program Manipulation (PEPM'00) , pages
73{82, New York, NY, USA, 2000. ACM.
10. J. McCarthy. Towards a Mathematical Science of Computation. In Proceedings of the 2nd International Federation
for Information Processing Congress (IFIP'62) , pages 21{28. North-Holland, 1962.
11. H. R. Nielson and F. Nielson. Semantics with Applications: A Formal Introduction . John Wiley & Sons, Inc.,
New York, NY, USA, 1992.
12. Q. Yi, V. Adve, and K. Kennedy. Transforming Loops to Recursion for Multi-level Memory Hierarchies. In Proceed-
ings of the 21st ACM-SIGPLAN Conference on Programming Language Design and Implementation (PLDI'00) ,
pages 169{181, 2000.

15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 1
OVERVIEW – THEGENOME SEQUENCING PROBLEM
MAJOR THEMES
Deﬁning precise problem and data abstractions,
Designing and programming
/trianglerightsldcorrect and efﬁcient algorithms and data structures
/trianglerightsldfor given problems and data abstractions
Abstraction Implementation
Functions Problem Algorithm
Data Abstract Data Type Data Structure
OVERVIEW – THEGENOME SEQUENCING PROBLEM 2/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROBLEM VS . ALGORITHM
Sorting, string matching, ﬁnding shortest paths
in graphs,. . . , are problems
/trianglerightsldInput : A sequence [a1,a2,···,an]
/trianglerightsldOutput : A permutation of the sequence
[ai1,ai2,···,ain]such that∀j,1≤j<n,aij≤aij+1
Quicksort, Mergesort, Insertion Sort , . . . , are
algorithms for sorting.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 3/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ABSTRACT DATATYPES VS .
DATASTRUCTURES
A set is an abstract data type (ADT)
/trianglerightsldTest membership, intersect, union, difference, . . .
Sequences, trees, hash-tables are examples of
data structures.
ADT’s determine functionality, data structures
determine costs.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 4/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TECHNOLOGY – M OORE ’SLAW
Source: Wikipedia
OVERVIEW – THEGENOME SEQUENCING PROBLEM 5/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROCESSOR TECHNOLOGY
OVERVIEW – THEGENOME SEQUENCING PROBLEM 6/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MULTI-CORE CHIPS
OVERVIEW – THEGENOME SEQUENCING PROBLEM 7/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MULTI-CORE CHIPS
OVERVIEW – THEGENOME SEQUENCING PROBLEM 8/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLEL ALGORITHMS
Serial Parallel
1-core 8-core 32h-core
Sorting 10M strings 2.90 2.90 0.40 .095 (30.5)
Remove dupl. 10M strings 0.66 1.00 0.14 .038 (17.4)
Min. span. tree 10M edges 1.60 2.50 0.42 .140 (11.4)
BFS 10M edges 0.82 1.20 0.20 .046 (17.8)
Running times in seconds
OVERVIEW – THEGENOME SEQUENCING PROBLEM 9/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210 VS.ATRADITIONAL COURSE
Emphasis on parallel thinking at a high level
/trianglerightsldParallel algorithms and parallel data structures
Purely functional model of computation
/trianglerightsldSafe for parallelism
/trianglerightsldHigher level of abstraction
Ideas still relevant for imperative computation
/trianglerightsldLot of overlap, but covered differently!
OVERVIEW – THEGENOME SEQUENCING PROBLEM 10/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
A real world problem: Gene sequencing.
The computational problem.
Algorithms
OVERVIEW – THEGENOME SEQUENCING PROBLEM 11/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SEQUENCING THE GENOME
The human DNA molecule encodes the
complete set of genetic information using 4
bases
/trianglerightsldAdenine (A), Cytosine (C), Guanine (G) and
Thymine (T)
A sequence of about
/trianglerightsld3 billion base pairs
/trianglerightsldarranged into 46 chromosomes
makes up the human genome.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 12/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SEQUENCING THE GENOME
A chromosome is a sequence of genes
A gene is a sequence of the base pairs
/trianglerightsldBut there seem to be a lot of base-pairs with no
apparent functions.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 13/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SEQUENCING THE GENOME
Source: Wikipedia
OVERVIEW – THEGENOME SEQUENCING PROBLEM 14/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SEQUENCING THE GENOME
Source: Wikipedia
OVERVIEW – THEGENOME SEQUENCING PROBLEM 15/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SEQUENCING THE GENOME
Source: Wikipedia
OVERVIEW – THEGENOME SEQUENCING PROBLEM 16/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SEQUENCING THE GENOME
Determining the complete DNA sequence is a
grand challenge.
Very hard to do in one go with wet lab
techniques.
The Shotgun Technique has been found work
quite well.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 17/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SHOTGUN SEQUENCING
Break up multiple DNA strands into short
segments
/trianglerightsldChemistry!
Short segments are sequenced.
/trianglerightsldChemistry!
Stitch short sequences computationally.
/trianglerightsldThis is where CS comes in.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 18/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SHOTGUN SEQUENCING
Source: Wikipedia
OVERVIEW – THEGENOME SEQUENCING PROBLEM 19/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SHOTGUN SEQUENCING
Suppose you have three strands sequenced
catt ag gagtat
cat tagg ag tat
ca tta gga gtat
But they really come in a messy way, e.g.,
catt ag tta cat tagg ag gagtat
tat ca gga gtat
So how do we stitch them?
/trianglerightsldGiven a set of overlapping genome subsequences,
construct the “best” sequence that includes them all.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 20/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
A real world problem: Gene sequencing.
The computational problem.
Algorithms
OVERVIEW – THEGENOME SEQUENCING PROBLEM 21/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEABSTRACT PROBLEM
THESHORTEST SUPERSTRING PROBLEM
Given
an alphabet of symbols Σ, and
a set of ﬁnite strings S⊆Σ+,
return
a shortest string rthat contains every s∈Sas a
substring of r.
Σ,Σ+
Σ ={A,C,G,T}
OVERVIEW – THEGENOME SEQUENCING PROBLEM 22/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SOME OBSERVATIONS
Ignore strings that are already in other strings.
Why?
{catt,ag,gagtat ,cat,tagg ,ag,tat,ca,tta,gga,gtat}
⇓
{catt,gagtat ,tagg ,tta,gga,}
Each string must start at a distinct position in
the result. Why?
OVERVIEW – THEGENOME SEQUENCING PROBLEM 23/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
A real world problem: Gene sequencing.
The computational problem.
Algorithms:
/trianglerightsldThe Brute Force Algorithm
OVERVIEW – THEGENOME SEQUENCING PROBLEM 24/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEBRUTE FORCE ALGORITHM
THEBRUTE FORCE TECHNIQUE
Enumerate all possible candidate solutions for a
problem
score each solution, and/or
check each satisﬁes the problem constraints
Return the best solution.
How does this apply to the SS Problem?
/trianglerightsldGenerate permutations
/trianglerightsldRemove overlaps
/trianglerightsldStitch strings
/trianglerightsldSelect the shortest resulting string
OVERVIEW – THEGENOME SEQUENCING PROBLEM 25/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEBRUTE FORCE ALGORITHM
catt tta tagg gga gagtat
catt tta tagg gga gagtat
cattaggagtat
LEMMA
Given a ﬁnite set of strings S⊆Σ+, the brute force
technique ﬁnds the shortest superstring.
See handout.
So what is the problem with this technique?
OVERVIEW – THEGENOME SEQUENCING PROBLEM 26/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEBRUTE FORCE ALGORITHM
There are just too many permutations!
So,n=100→100!≈10158permutations.
Testing at 1010permutations/sec, you need
/trianglerightsld≈10148seconds
/trianglerightsld≈10143days (≈105seconds/day)
/trianglerightsld≈2.7×10140years
/trianglerightsld≈2.7×10138centuries
Not bloody likely you will test each permutation
before hell freezes over!
/trianglerightsldEven if every subatomic particle in the universe was
a processor
OVERVIEW – THEGENOME SEQUENCING PROBLEM 27/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROSPECTS FOR A FASTER
ALGORITHM ?
SS belongs to very important class of problems
called NP (for Nondeterministic Polynomial).
For such problems, no algorithm with polynomial
work is known.
But solutions can be veriﬁed in polynomial work!
Wait for 15-451 and 15-453 for the gory details!
But usually there are approximation algorithms
/trianglerightsldwith bounds on the quality of results, and
/trianglerightsldperform better in practice.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 28/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
A real world problem: Gene sequencing.
The computational problem.
Algorithms:
/trianglerightsldThe Brute Force Algorithm
/trianglerightsldReducing SS to TSP
OVERVIEW – THEGENOME SEQUENCING PROBLEM 29/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROBLEM REDUCTION
A reduction is a mapping from one problem ( A)
to another problem ( B), so that the solution B
problem can be used to solve A.
/trianglerightsldSolving a set of linear equations, reduces to
inverting a matrix.
Map the instance of problem Ato an instance of
B,
Solve using algorithms for B
Map the resulting solution back.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 30/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REDUCING SSTOTSP
THE(ASYMMETRIC ) TRAVELING
SALESPERSON PROBLEM (TSP)
Given a weighted directed graph
ﬁnd the shortest path that starts at vertex s, and
visits each vertex once, and
returns to s.
≡Hamiltonian path with the lowest total sum of
weights
So, how is this related to SS?
OVERVIEW – THEGENOME SEQUENCING PROBLEM 31/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REDUCING SSTOTSP
Ifsiis followed by sjin how much will the SS
length increase?
/trianglerightsldsi=tagg followed by sj=gga→tagga
General case?
/trianglerightsldwi,j=|sj|−overlap (si,sj)
/trianglerightsldoverlap (”tagg ”,”gga”) = 2
/trianglerightsld|”gga”|−2=1
OVERVIEW – THEGENOME SEQUENCING PROBLEM 32/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REDUCING SSTOTSP
Build a graph D= (V,A)
One vertex for each siand one for special “null”
node, Λ
A directed edge from sitosjhas weight
wi,j=|sj|−overlap (si,sj)
wΛ,i=|si|→no overlap, maximal increase
wi,Λ=0→, no overlap, no increase
OVERVIEW – THEGENOME SEQUENCING PROBLEM 33/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REDUCING SSTOTSP
S={catt ,tta,acat}
⋀catt
ttaacat40
41230403
44
OVERVIEW – THEGENOME SEQUENCING PROBLEM 34/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REDUCING SSTOTSP
⋀catt
ttaacat40
41230403
44
This tour≡cattacat ta
Length 10
⋀catt
ttaacat40
41230403
44
This tour≡catt acat
Length 8
OVERVIEW – THEGENOME SEQUENCING PROBLEM 35/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REDUCING SSTOTSP
TSP considers all Hamiltonian paths (hence is
brute force)
TSP ﬁnds the minimum cost Hamiltonian path.
/trianglerightsldTotal cost is the length of the SS
TSP is also NP-hard.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 36/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
A real world problem: Gene sequencing.
The computational problem.
Algorithms:
/trianglerightsldThe Brute Force Algorithm
/trianglerightsldReducing SS to TSP
/trianglerightsldThe Greedy Algorithm
OVERVIEW – THEGENOME SEQUENCING PROBLEM 37/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY TECHNIQUE
THEGREEDY TECHNIQUE
Given a sequence of steps to be made, at each
decision point
make a locally optimal decision
without ever backtracking on previous decisions.
Greedy is a quite general algorithmic paradigm.
In general, it does not get the best solution.
/trianglerightsldBut it does work for some other problems (e.g.,
Huffman Encoding, MST)
OVERVIEW – THEGENOME SEQUENCING PROBLEM 38/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY APPROXIMATION TO SS
Start with a pair of strings with maximal overlap
(Why?)
Continue with strings that adds the least
extension every time.
/trianglerightsldThis is the locally optimal decision!
/trianglerightsldWe already deﬁned overlap (si,sj)
/trianglerightsldjoin(si,sj)≡concatenate sjtosiand remove
overlap.
⋆join(”tagg ”,”gga”) = ” tagga ”
OVERVIEW – THEGENOME SEQUENCING PROBLEM 39/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY APPROXIMATION TO SS
GREEDY APPROX SS
1fungreedyApproxSS (S) =
2 if|S|=1then s0
3 else let
4 valO={(overlap (si,sj),si,sj) :si∈S,sj∈S,si/negationslash=sj}
5 val (o,si,sj) =maxval <#1O
6 valsk=join (si,sj)
7 valS/prime= ({sk}∪S)\{si,sj}
8 in
9 greedyApproxSS (S/prime)
10 end
S’gets smaller by one string after each
recursion.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 40/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY APPROXIMATION TO SS
GreedyApproxSS returns a string with length
within 3.5 times the shortest string.
Conjectured to return within a factor of 2.
Does much better in practice.
OVERVIEW – THEGENOME SEQUENCING PROBLEM 41/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY APPROXIMATION TO SS
Let’s do an example.
S={catt,gagtat ,tagg ,tta,gga,}
OVERVIEW – THEGENOME SEQUENCING PROBLEM 42/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Interfaces vs Implementations
/trianglerightsldPrecise interfaces are key.
The Shortest Superstring Problem
/trianglerightsldThe brute-force approach
/trianglerightsldReduction to TSP
/trianglerightsldApproximate solution using greedy paradigm
OVERVIEW – THEGENOME SEQUENCING PROBLEM 43/43
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 2
ALGORITHMIC COSTMODELS
SYNOPSIS
Cost Models
Parallelism
Scheduling
Cost Analysis for the Shortest Super String
Problem
/trianglerightsldThe Brute Force Algorithm
/trianglerightsldThe Greedy Algorithm
ALGORITHMIC COSTMODELS 2/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTMODELS
Sequential: the Random Access Machine
(RAM) model
Parallel: the Parallel RAM model
Parallel: the 15-210 model
/trianglerightsldTied to high-level programming constructs –
operational semantics
/trianglerightsldThink parallel!
ALGORITHMIC COSTMODELS 3/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210 C OSTMODEL
W(e): Work needed to evaluate e
S(e): Span of the evaluation of e
Parameterized with relevant problem size
measures.
Asymptotic Models
/trianglerightsldHow do algorithms scale to large problems!
ALGORITHMIC COSTMODELS 4/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARAMETERIZATION
We measure the size of representation of the
input.
Sorting: Number of items to sort
Map, Reduce: Number of items in the sequence
Graph Problems: Number of Nodes, Edges
Searching: Number of items in the database
Matrix operations: Number of rows and columns
Prime number testing: Size – number of bits to represent
the number (not the value!)
Computing nthFibonacci number: Size – number of bits
to represent the number (not the value!)
ALGORITHMIC COSTMODELS 5/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RULES OF COMPOSITION
(e1,e2): Sequential Composition
/trianglerightsldAdd work and span
e1||e2: Parallel Composition
/trianglerightsldAdd work but take the maximum span
e1e2WorkSpane1e2WorkSpan
ALGORITHMIC COSTMODELS 6/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RULES OF COMPOSITION
e W (e) S(e)
c 1 1
ope 1 1
(e1,e2) 1+W(e1) +W(e2) 1+S(e1) +S(e2)
(e1||e2) 1+W(e1) +W(e2)1+max(S(e1),S(e2))
let val x=e1 1+W(e1)+ 1+S(e1)+
ine2end W(e2[Eval(e1)/x]) S(e2[Eval(e1)/x])
{f(x)|x∈A} 1+/summationtext
x∈AW(f(x)) 1+max x∈AS(f(x))
ALGORITHMIC COSTMODELS 7/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RULES OF COMPOSITION
{f(x)|x∈A}≡mapf A
•W(mapf/angbracketlefts0,..., sn−1/angbracketright) =1+n−1/summationdisplay
i=0W(f(si))
•S(mapf/angbracketlefts0,..., sn−1/angbracketright) =1+n−1max
i=0S(f(si))
ALGORITHMIC COSTMODELS 8/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
UPPER AND LOWER BOUNDS
Upper bound: The maximum asymptotic work
(and span) that a given algorithm needs for all
inputs of size n.
Lower bound: The minimum asymptotic work
(and span) that any algorithm for a problem
needs for all inputs of size n.
ALGORITHMIC COSTMODELS 9/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
Cost Models
Parallelism
Scheduling
Cost Analysis for the Shortest Super String
Problem
/trianglerightsldThe Brute Force Algorithm
/trianglerightsldThe Greedy Algorithm
ALGORITHMIC COSTMODELS 10/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLELISM
For a given WandS, what is the maximum
number of processors you can utilize?
•P=W
S
Why?
Mergesort has W=θ(nlogn)andS=θ(log2n)
P=θ(n
logn)
/trianglerightsldThe larger the problem is, the higher the parallelism
ALGORITHMIC COSTMODELS 11/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DESIGNING PARALLEL ALGORITHMS
Keep work as low as possible
/trianglerightsldNo unnecessary computation
Keep span as low as possible
/trianglerightsldHence get high-parallelism
ALGORITHMIC COSTMODELS 12/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
Cost Models
Parallelism
Scheduling
Cost Analysis for the Shortest Super String
Problem
/trianglerightsldThe Brute Force Algorithm
/trianglerightsldThe Greedy Algorithm
ALGORITHMIC COSTMODELS 13/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
UNDER THE HOOD:
TASKSCHEDULING
Mapping from a computation graph to
processors
ALGORITHMIC COSTMODELS 14/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GREEDY SCHEDULING
A greedy scheduler will schedule a ready task
on an available processor.
ALGORITHMIC COSTMODELS 15/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
A L OWER BOUND
LetTpbe the “time” needed when using p
processors,
max(W
p,S)≤Tp
Why?
ALGORITHMIC COSTMODELS 16/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANUPPER BOUND
With pprocessors
Tp<W
p+S
Why?
ALGORITHMIC COSTMODELS 17/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TYING THINGS TOGETHER
Speed-up isW
Tp
/trianglerightsldMaximum possible speed-up is p.
Tp<W
p+S
=W
p+W
P
=W
p/parenleftBig
1+p
P/parenrightBig
P/greatermuchp→near perfect parallelism
ALGORITHMIC COSTMODELS 18/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
Cost Models
Parallelism
Scheduling
Cost Analysis for the Shortest Super String
Problem
/trianglerightsldThe Brute Force Algorithm
/trianglerightsldThe Greedy Algorithm
ALGORITHMIC COSTMODELS 19/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTS FOR THE BRUTE FORCE SS
ALGORITHM
The brute-force algorithm
/trianglerightsldFor each permutation
⋆Remove overlaps
⋆Stitch strings
/trianglerightsldOutput (one of) the shortest string(s)
overlap (si,sj)will be needed many times.
/trianglerightsldPreprocess Sonce and store overlaps as a table
⋆What preﬁx to remove
⋆Increase in length
ALGORITHMIC COSTMODELS 20/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PREPROCESSING – INPUTS
A set Sisnstrings, s1,s2,···,sn
Deﬁne
m=n/summationdisplay
i=1|si|
and observe n≤m.
ALGORITHMIC COSTMODELS 21/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PREPROCESSING A PAIR
sjsisi….Overlap?si          sjOverlap?sj….si          sjOverlap?
Work and span for preprocessing one pair, si
andsj?
/trianglerightsldW=O(|si|·|sj|)Why?
/trianglerightsldS=O(log(|si|+|sj|))Why?
ALGORITHMIC COSTMODELS 22/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PREPROCESSING – W ORK
Wov≤n/summationdisplay
i=1n/summationdisplay
j=1W(overlap (si,sj))
=n/summationdisplay
i=1n/summationdisplay
j=1O(|si||sj|)
≤n/summationdisplay
i=1n/summationdisplay
j=1(k1+k2|si||sj|)
=n/summationdisplay
i=1n/summationdisplay
j=1k1+n/summationdisplay
i=1n/summationdisplay
j=1(k2|si||sj|)
=k1n2+k2n/summationdisplay
j=1|sj|(n/summationdisplay
i=1|si|) =k1n2+k2m2∈O(m2)
ALGORITHMIC COSTMODELS 23/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PREPROCESSING – SPAN
Allsi,sjpairs can be processed in parallel.
Sov≤nmax
i=1nmax
j=1S(overlap (si,sj)))
∈O(logm)
ALGORITHMIC COSTMODELS 24/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BRUTE FORCE SS A LGORITHM
Work:
/trianglerightsldO(n)lookups each with O(1)work. Why?
/trianglerightsldn!permutations
/trianglerightsldO(n·n!) =O((n+1)!)
/trianglerightsldWovcan be ignored!
Span:
/trianglerightsldAll permutations can be done in parallel, but!
func permutations S =
if |S| = 1 then {S}
else
{append([s], p) :
s in S, p in permutations(S\s)}
/trianglerightsldThis has span O(n). Why?
/trianglerightsldSovcan be ignored.
ALGORITHMIC COSTMODELS 25/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
Cost Models
Parallelism
Scheduling
Cost Analysis for the Shortest Super String
Problem
/trianglerightsldThe Brute Force Algorithm
/trianglerightsldThe Greedy Algorithm
ALGORITHMIC COSTMODELS 26/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY SS A LGORITHM
1fungreedyApproxSS (S) =
2 if|S|=1then s0
3 else let
4 valO={(overlap (si,sj),si,sj) :si∈S,sj∈S,si/negationslash=sj}
5 val(o,si,sj) =maxval<#1O
6 valsk=join (si,sj)
7 valS/prime= ({sk}∪S)\{si,sj}
8 in
9 greedyApproxSS (S/prime)
10 end
ALGORITHMIC COSTMODELS 27/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY SS A LGORITHM
1fungreedyApproxSS (S) =
2 if|S|=1then s0
3 else let
4 valO={(overlap (si,sj),si,sj) :si∈S,sj∈S,si/negationslash=sj}
5 val(o,si,sj) =maxval<#1O
6 valsk=join (si,sj)
7 valS/prime= ({sk}∪S)\{si,sj}
8 in
9 greedyApproxSS (S/prime)
10 end
Wov=O(m2),Sov=O(logm)
ALGORITHMIC COSTMODELS 28/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY SS A LGORITHM
1fungreedyApproxSS (S) =
2 if|S|=1then s0
3 else let
4 valO={(overlap (si,sj),si,sj) :si∈S,sj∈S,si/negationslash=sj}
5 val(o,si,sj) =maxval<#1O
6 valsk=join (si,sj)
7 valS/prime= ({sk}∪S)\{si,sj}
8 in
9 greedyApproxSS (S/prime)
10 end
Wmaxval =O(m2),Smaxval =O(logm)
Why?
ALGORITHMIC COSTMODELS 29/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY SS A LGORITHM
1fungreedyApproxSS (S) =
2 if|S|=1then s0
3 else let
4 valO={(overlap (si,sj),si,sj) :si∈S,sj∈S,si/negationslash=sj}
5 val(o,si,sj) =maxval<#1O
6 valsk=join (si,sj)
7 valS/prime= ({sk}∪S)\{si,sj}
8 in
9 greedyApproxSS (S/prime)
10 end
No more than W=O(m2),S=O(logm)
Why?
ALGORITHMIC COSTMODELS 30/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEGREEDY SS A LGORITHM
1fungreedyApproxSS (S) =
2 if|S|=1then s0
3 else let
4 valO={(overlap (si,sj),si,sj) :si∈S,sj∈S,si/negationslash=sj}
5 val(o,si,sj) =maxval<#1O
6 valsk=join (si,sj)
7 valS/prime= ({sk}∪S)\{si,sj}
8 in
9 greedyApproxSS (S/prime)
10 end
At most n(sequential) calls to greedyApproxSS
/trianglerightsldEach with W=O(m2),S=O(logm)
Wgreedy =O(nm2)andSgreedy =O(nlogm)
Why?
ALGORITHMIC COSTMODELS 31/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Cost Models: Rules of Composition
Parallelism and Scheduling
Cost Analysis for the Shortest Super String
Problem
/trianglerightsldPreprocessing for overlaps
/trianglerightsldThe Brute Force Algorithm
/trianglerightsldThe Greedy Algorithm
ALGORITHMIC COSTMODELS 32/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 3
ALGORITHMIC TECHNIQUES AND DIVIDE -AND -CONQUER
SYNOPSIS
Algorithmic Techniques
Divide-and-Conquer
/trianglerightsldAnalysis of Costs
The Maximum Contiguous Subsequence Sum
Problem
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 2/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ALGORITHMIC TECHNIQUES
Brute Force
/trianglerightsldTry all possibilities
/trianglerightsldAlmost always intractable
/trianglerightsldUseful for testing small cases
/trianglerightsldCode usually easy to write
Reducing one problem to another
/trianglerightsldTransform the structure or the instance of a problem.
/trianglerightsldShortest Superstring →Traveling Salesperson
Problem
/trianglerightsldApply algorithms for the new problem
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 3/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
INDUCTIVE TECHNIQUES
Solve one or more smaller problems to solve the
large problem.
Techniques differ on
/trianglerightsldThe number of subproblems
/trianglerightsldHow subproblem solutions are used
Divide-and-Conquer
Greedy
Contraction
Dynamic Programming
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 4/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER
Divide a problem of size nintok>1 problems
/trianglerightsldSizes n1,n2,..., nk
Solve each problem recursively.
Combine the subproblem solutions.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 5/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GREEDY
Given a problem of size n
Remove one (or more) elements using a greedy
approach
/trianglerightsldSmallest, two smallest, nearest, lowest, etc.
Solve the remaining smaller problem
/trianglerightsldUsually smaller by 1 or 2 items.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 6/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CONTRACTION
Given a problem of size n
Generate a signiﬁcantly smaller (contracted)
instance
/trianglerightslde.g., of size n/2
Solve the smaller instance
Use the result to solve the original problem.
One recursive call instead of multiple!
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 7/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DYNAMIC PROGRAMMING
Like Divide-and-Conquer
Solutions to subproblems used multiple times!
Compute once and store, and then reuse.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 8/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ADT S AND DATA STRUCTURES
Techniques rely on Abstract Data Types (for
functionality)
/trianglerightsldand on data structures that implement them (for
costs)
Sequences, Sets, Tables, Priority Queues,
Graphs, Trees, . . .
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 9/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RANDOMIZATION
Introduce randomness at a choice point
/trianglerightsldQuicksort: choose a pivot randomly
Testing for primality
/trianglerightsldMiller-Rabin primality test
/trianglerightsld3/4 of numbers <nare “witnesses” to n’s
compositeness.
/trianglerightsldRandomly choose 100 numbers <n
/trianglerightsldP(Failing to ﬁnd a witness ) =1−(1
4)100
/trianglerightsldP(nis prime ) =1−(1
4)100=0.9999...9327...
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 10/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
Algorithmic Techniques
Divide-and-Conquer
/trianglerightsldAnalysis of Costs
The Maximum Contiguous Subsequence Sum
Problem
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 11/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER
Very versatile.
Easy to implement.
Parallelizable
Code follows the structure of a proof.
Cost reasoning follows code structure.
/trianglerightsldRecurrences
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 12/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
STRENGTENING THE PROBLEM
Compute more than “superﬁcially” needed.
No increase to work or span.
More efﬁcient combine step.
At the end, this extra information can be
discarded.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 13/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GENERAL STRUCTURE
Base case(s)
/trianglerightsldWhen problem small enough, use a different
technique.
/trianglerightsldFor example, in quicksort, switch to insertion sort to
sort<30 elements.
Inductive Step
/trianglerightsldDivide into parts
⋆Sometimes quite simple: e.g., mergesort
⋆Sometimes a bit tricky: e.g., quicksort
/trianglerightsldSolve subproblems (in parallel)
/trianglerightsldCombine results
⋆Sometimes quite simple: e.g., quicksort
⋆Sometimes a bit tricky: e.g., mergesort
Costs can be in the divide or combine steps or
in both.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 14/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GENERAL STRUCTURE
foo(n1)foo(n2)foo(nk)DIVIDECOMBINEfoo(n)
W(n)=Wdivide (n) +k/summationdisplay
i=1W(ni) + Wcombine (n)
S(n)=Sdivide (n) +kmax
i=1S(ni) + Scombine (n)
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 15/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SOLVING RECURRENCES
Tree method (Brick method)
Substitution method
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 16/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THETREE METHOD
Expand recurrence into a tree structure.
Cost of level 0Cost of level 1Cost of level 2
Add/Max costs at levels.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 17/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THETREE METHOD
Solve W(n) =2W(n/2) +O(n)
In general, solve
W(n) =2W(n/2) +g(n)
where g(n)∈O(f(n))
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 18/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THETREE METHOD
g(n)∈O(f(n))⇒g(n)≤c·f(n)
/trianglerightsldFor some c>0,N0>0 and n≥N0
g(n)≤k1·f(n) +k2for some k1,k2andn≥1
/trianglerightslde.g., k1=candk2=/summationtextN0
i=1|g(i)|(Why?)
Solve W(n)≤2W(n/2) +k1·n+k2
/trianglerightsldf(n) =nin our case.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 19/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THETREE METHOD
Solving W(n)≤2W(n/2) +k1·n+k2
k1 n + k2k1 (n/2) + k2k1 (n/2) + k2k1 (n/4) + k2k1 (n/4) + k2k1 (n/4) + k2k1 (n/4) + k2k1 n + k2k1 n + 2 k2k1 n + 4 k2
Questions:
/trianglerightsldNumber of levels in the tree?
/trianglerightsldProblem size at level i?
/trianglerightsldCost for each node at level i?
/trianglerightsldNumber of nodes at level i?
/trianglerightsldTotal cost at level i?
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 20/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THETREE METHOD
Total cost at level iis at most
2i·/parenleftBig
k1n
2i+k2/parenrightBig
=k1·n+2i·k2
Total cost over all levels is
W(n)≤log2n/summationdisplay
i=0/parenleftbig
k1·n+2i·k2/parenrightbig
=k1n(1+log2n) +k2(20+21+···+2log2n)
≤k1n(1+log2n) +2k2n(Why ?)
∈O(nlogn)
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 21/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEBRICK METHOD
Look at the cost structure at the levels of the
cost tree
/trianglerightsldLeaves dominated
/trianglerightsldBalanced
/trianglerightsldRoot dominated
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 22/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LEAVES -DOMINATED COST TREES
For someρ>1, for all levels i
cost i+1≥ρ·cost i
++
++++
++++++
++++++++
Overall cost is O(cost d)where dis the depth.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 23/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BALANCED COST TREES
All levels have about the same cost
++++++++
++++++++
++++++++
++++++++
Overall cost is O(d·max icost i)where dis the
depth.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 24/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ROOT-DOMINATED COST TREES
For someρ<1, for all levels i
cost i+1≤ρ·cost i
++++++++
++++++
++++
++
Overall cost is O(cost 0)where dis the depth.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 25/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEBRICK METHOD
What type of a cost tree is this?
k1 n + k2k1 (n/2) + k2k1 (n/2) + k2k1 (n/4) + k2k1 (n/4) + k2k1 (n/4) + k2k1 (n/4) + k2k1 n + k2k1 n + 2 k2k1 n + 4 k2
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 26/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
Algorithmic Techniques
Divide-and-Conquer
/trianglerightsldAnalysis of Costs
The Maximum Contiguous Subsequence Sum
Problem
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 27/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEMCSS P ROBLEM
THEMAXIMUM CONTIGUOUS SUBSEQUENCE SUMPROBLEM
Given a sequence of numbers S=/angbracketlefts1,..., sn/angbracketright,
Find
mcss (S) = max
1≤i≤j≤n/braceleftBiggj/summationdisplay
k=isk/bracerightBigg
S=/angbracketleft0,−1,2,−1,4,−1,0/angbracketright,mcss (S) =5
How many possible subsequences are there?
All positive numbers?
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 28/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BRUTE FORCE ALGORITHM
Compute the sum of all O(n2)possible
subsequences (in parallel)
/trianglerightsldUse plus reduce
Subsequence (i,j)needs
/trianglerightsldO(j−i)work (Why?)
/trianglerightsldO(log(j−i))span (Why?)
W(n)=1+/summationdisplay
1≤i≤j≤nWreduce (j−i)≤1+n2·Wreduce (n)
=1+n2·O(n)∈O(n3)
S(n)=1+max
1≤i≤j≤nSreduce (j−i)≤1+Sreduce (n)∈O(logn)
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 29/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BRUTE FORCE ALGORITHM
Compute maximum over all O(n2)sums
/trianglerightsldUse max reduce
/trianglerightsldNeeds O(n2)work and O(logn)span
/trianglerightsldCan be ignored (Why?)
Total costs for brute force are:
/trianglerightsldO(n3)work
/trianglerightsldO(logn)span
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 30/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – I
/angbracketleft—— L——/bardbl—— R——/angbracketright
⇓
L=/angbracketleft ··· /angbracketright/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
mcss =56R=/angbracketleft.../angbracketright/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
mcss =17
Let’s solve S=/angbracketleft−2,−1,2,3,2,−2/angbracketright
Is this right?
How do we combine subproblem results?
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 31/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – I
Recursion handles
/trianglerightsldWhen mcss (S)subsequence is in the left.
/trianglerightsldWhen mcss (S)subsequence is in the right.
What happens when mcss (S)spans across the
divide point?
LRLargest Sum SufﬁxLargest Sum PreﬁxMaximum sum across the divide
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 32/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – I
1funmcss (s) =
2 case (showt s)
3 ofEMPTY =−∞
4|ELT (x) =x
5|NODE (L,R) =
6 let val (mL,mR) = (mcss (L)/bardblmcss (R) )
7 valmA=bestAcross (L,R)
8 inmax{mL,mR,mA}
9 end
W(n) =2W(n/2) +O(n)(Why?)→W(n)∈O(nlogn)
S(n) =S(n/2) +O(logn)(Why?)→S(n)∈O(log2n)
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 33/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – II
IMPORTANT QUESTIONS
Can we do better than O(nlogn)work?
What part of the divide-and-conquer is the
bottleneck?
/trianglerightsldCombine takes linear work? (Why?)
How can we improve?
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 34/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – II
The answers lie here
LRLargest Sum SufﬁxLargest Sum PreﬁxMaximum sum across the divide
Strengthen the subproblems
/trianglerightsldCompute additional information
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 35/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – II
TotalLmpsLmssLmcssLLeft Subproblem
mps = maximum preﬁx summss = maximum sufﬁx sum
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 36/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – II
TotalLmpsLmssLmcssLLeft SubproblemTotalRmpsRmssRmcssRRight Subproblem
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 37/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – II
TotalLmpsLmssLmcssLLeft SubproblemTotalRmpsRmssRmcssRRight Subproblem
Totalmcssmpsmss
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 38/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – II
TotalLmpsLmssLmcssLLeft SubproblemTotalRmpsRmssRmcssRRight Subproblem
TotalmcssmpsmssTotalTotalLTotalR= + mcss= max ( mcssLmcssR, , mssLmpsR+ ) mps= max ( mpsL, TotalL+ mpsR) mss= max ( mssL+ TotalR, mssR) 
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 39/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE -AND -CONQUER – II
1funmcss (a) =
2let
3 funmcss’ (a)
4 case (showt a)
5 ofEMPTY = (−∞ ,−∞ ,−∞ ,0)
6|ELT (x) = ( x,x,x,x)
7|NODE (L,R) =
8 let
9 val ((m1,p1,s1,t1),(m2,p2,s2,t2)) = ( mcss (L)/bardblmcss (R) )
10 in
11 (max (s1+p2,m1,m2),max (p1,t1+p2),max (s1+t2,s2),t1+t2)
12 end
13 val (m,p,s,t) =mcss/prime(a)
14 inmend
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 40/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COST ANALYSIS
1funmcss (a) =
2let
3 funmcss’ (a)
4 case (showt a)
5 ofEMPTY = (−∞ ,−∞ ,−∞ ,0)
6|ELT (x) = ( x,x,x,x)
7|NODE (L,R) =
8 let
9 val ((m1,p1,s1,t1),(m2,p2,s2,t2)) = ( mcss (L)/bardblmcss (R) )
10 in
11 (max (s1+p2,m1,m2),max (p1,t1+p2),max (s1+t2,s2),t1+t2)
12 end
13 val (m,p,s,t) =mcss/prime(a)
14 inmend
Assuming showt hasO(logn)work and span.
/trianglerightsldW(n) =2W(n/2) +O(logn)
/trianglerightsldS(n) =S(n/2) +O(logn)
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 41/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COST ANALYSIS
W(n) =2W(n/2) +O(logn)
k1 log nk1 log (n/2)k1 log (n/2)k1 log (n/4)k1 log (n/4)k1 log (n/4)k1 log (n/4)k1 log nk1 2 log (n/2)k1 4 log (n/4)
W(n)≤/summationtextlogn
i=0k12ilog(n/2i)
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 42/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUBSTITUTION METHOD
Solve W(n)≤2W(n/2) +k·logn
/trianglerightsldk>0
/trianglerightsldW(n)≤kforn≤1
Guess W(n)≤κ1n−κ2logn−κ3
/trianglerightsldNeed to ﬁnd κ1,κ2, andκ3.
Base case: W(1)≤k⇒κ1−κ3≤k
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 43/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUBSTITUTION METHOD
Inductive Step
W(n)≤2W(n
2) +k·logn
≤2(κ1n
2−κ2log(n
2)−κ3) +k·logn
=κ1n−2κ2(logn−1)−2κ3+k·logn
= (κ1n−κ2logn−κ3) + (klogn−κ2logn+2κ2−κ3)
≤κ1n−κ2logn−κ3
Chooseκ2=kand 2κ2−κ3≤0 (Why?)
For example, κ2=k,κ1=3k,κ3=2ksatisﬁes the
constraints.
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 44/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Algorithmic Paradigms
Divide-and-Conquer
/trianglerightsldGeneral Form
/trianglerightsldCost Analysis
/trianglerightsldTree and Brick Methods
/trianglerightsldSubstitution Method
Maximum Contiguous Subsequence Problem
/trianglerightsldBrute Force
/trianglerightsldDivide-and-Conquer
/trianglerightsldDivide-and-Conquer with Subproblem Strengthening
ALGORITHMIC TECHNIQUES AND DIVIDE -AND-CONQUER 45/45
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 4
DIVIDE -AND-CONQUER CONTINUED
SYNOPSIS
The Euclidian Travelling Salesperson Problem
Divide-and-Conquer Heuristic Algorithm
Analysis of Costs
DIVIDE -AND-CONQUER CONTINUED 2/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEEUCLIDIAN TSP
Given a set of points in a n-dimensional
Euclidian space.
/trianglerightsldWhat is a Euclidian space?
Find the shortest Hamiltonian cycle.
/trianglerightsldWhat is a Hamiltonian cycle?
We get a planar Euclidian Traveling Salesperson
Problem when the points are in 2-dimensional
space.
DIVIDE -AND-CONQUER CONTINUED 3/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEPLANAR TSP
DIVIDE -AND-CONQUER CONTINUED 4/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
The Euclidian Travelling Salesperson Problem
Divide-and-Conquer Heuristic Algorithm
Analysis of Costs
DIVIDE -AND-CONQUER CONTINUED 5/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
A D IVIDE -AND-CONQUER HEURISTIC
What is a heuristic?
Approximation algorithm
/trianglerightsldResulting tour length is guaranteed to be close to the
actual minimum tour length
/trianglerightsldIf you spend enough work (but polynomial).
The Divide-and-Conquer does work both before
and after the recursive calls.
DIVIDE -AND-CONQUER CONTINUED 6/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
A D IVIDE -AND-CONQUER HEURISTIC
+PlPr?
Assume P/lscriptandPrhave tour lengths T/lscriptandTr.
Tour length for the combination?
DIVIDE -AND-CONQUER CONTINUED 7/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
A D IVIDE -AND-CONQUER HEURISTIC
+PlPrelerulurvlvr
T/lscript+Tr+/bardblu/lscript−ur/bardbl+/bardblv/lscript−vr/bardbl/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Add these−/bardblu/lscript−v/lscript/bardbl−/bardbl ur−vr/bardbl/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Subtract these
DIVIDE -AND-CONQUER CONTINUED 8/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
A D IVIDE -AND-CONQUER HEURISTIC
+PlPrelerulurvlvr
T/lscript+Tr+/bardblu/lscript−vr/bardbl+/bardblv/lscript−ur/bardbl/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Add these−/bardblu/lscript−v/lscript/bardbl−/bardbl ur−vr/bardbl/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
Subtract these
DIVIDE -AND-CONQUER CONTINUED 9/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
A D IVIDE -AND-CONQUER HEURISTIC
Try all pairs of edges e/lscriptfrom P/lscriptanderfrom Pr
/trianglerightsldHow many pairs are there?
For each pair of edges, ﬁnd the smallest
increase.
Then combine the small tours into a large tour.
DIVIDE -AND-CONQUER CONTINUED 10/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
A D IVIDE -AND-CONQUER HEURISTIC
1funeTSP (P) =
2 case (|P|)
3 of0,1⇒raise TooSmall
4|2⇒ { (P[0],P[1]),(P[1],P[0])}
5|n⇒let
6 val(P/lscript,Pr) =splitLongestDim (P)
7 val(L,R) = (eTSP (P/lscript)/bardbleTSP (Pr) )
8 val(c,(e/prime
/lscript,e/prime
r)) =
9 minval <#1{(swapCost (e/lscript,er),(e/lscript,er)) :
10 e/lscript∈L,er∈R}
11 in
12 swapEdges (append (L,R),e/prime
/lscript,e/prime
r)
13 end
DIVIDE -AND-CONQUER CONTINUED 11/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
A D IVIDE -AND-CONQUER HEURISTIC
1funeTSP (P) =
2 case (|P|)
3 of0,1⇒raise TooSmall
4|2⇒ { (P[0],P[1]),(P[1],P[0])}
5|n⇒let
6 val(P/lscript,Pr) =splitLongestDim (P)
7 val(L,R) = (eTSP (P/lscript)/bardbleTSP (Pr) )
8 val(c,(e/prime
/lscript,e/prime
r)) =
9 minval <#1{(swapCost (e/lscript,er),(e/lscript,er)) :
10 e/lscript∈L,er∈R}
11 in
12 swapEdges (append (L,R),e/prime
/lscript,e/prime
r)
13 end
DIVIDE -AND-CONQUER CONTINUED 12/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SPLITTING THE POINTS
x-spready-spread
Split at the median along the longer spread
dimension.
DIVIDE -AND-CONQUER CONTINUED 13/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SWAP COST
Given e/lscript= (u/lscript,v/lscript)∈Lander= (ur,vr)∈R
swapCost ((u/lscript,v/lscript),(ur,vr)) =Cost Added−Cost Removed
Cost Added =min(/bardblu/lscript−ur/bardbl+/bardblv/lscript−vr/bardbl,
/bardblu/lscript−vr/bardbl+/bardblv/lscript−ur/bardbl)
Cost Removed =/bardblu/lscript−v/lscript/bardbl+/bardblur−vr/bardbl
+PlPrelerulurvlvr
+PlPrelerulurvlvr
DIVIDE -AND-CONQUER CONTINUED 14/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SWAPPING EDGES
swapEdges(append(L,R),e’ /lscript,e’ r)
Appends the Tour edge lists from subproblems
Then removes and adds appropriate edges.
DIVIDE -AND-CONQUER CONTINUED 15/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SYNOPSIS
The Euclidian Travelling Salesperson Problem
Divide-and-Conquer Heuristic Algorithm
Analysis of Costs
DIVIDE -AND-CONQUER CONTINUED 16/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTANALYSIS
1funeTSP (P) =
2 case (|P|)
3 of0,1⇒raise TooSmall
4|2⇒ { (P[0],P[1]),(P[1],P[0])}
5|n⇒let
6 val(P/lscript,Pr) =splitLongestDim (P)O(n)work O(logn)span (Why?)
7 val(L,R) = ( eTSP (P/lscript)/bardbleTSP (Pr) )2W(n/2)work S(n/2)span
8 val(c,(e/prime
/lscript,e/prime
r)) =
9 minval <#1{(swapCost (e/lscript,er),(e/lscript,er)) :
10 e/lscript∈L,er∈R}O(n2)work O(logn)span (Why?)
11 in
12 swapEdges (append (L,R),e/prime
/lscript,e/prime
r)O(logn)span (Why?)
13 end
DIVIDE -AND-CONQUER CONTINUED 17/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTANALYSIS
W(n)=2W(n/2) +O(n2)
S(n)=S(n/2) +O(logn)
S(n)∈O(log2n)
DIVIDE -AND-CONQUER CONTINUED 18/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTANALYSIS
Solve (directly)
W(n) =2W(n/2) +k·n1+ε
for constant ε >0.
/trianglerightsldDepth is log2n(Is this technically right?)
/trianglerightsldAt level i, we have 2inodes each costing k·(n/2i)1+ε
W(n)=logn/summationdisplay
i=0k·2i·/parenleftBign
2i/parenrightBig1+ε
= k·n1+ε·logn/summationdisplay
i=02−i·ε
≤ k·n1+ε·∞/summationdisplay
i=02−i·ε
W(n)∈O(n1+ε)(Why?)
DIVIDE -AND-CONQUER CONTINUED 19/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Euclidian Traveling Salesperson Problem
/trianglerightsldDivide-and-Conquer Heuristic
/trianglerightsldProcessing before and after the subproblem
solutions.
Cost Analysis
DIVIDE -AND-CONQUER CONTINUED 20/20
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 5
DATAABSTRACTION AND SEQUENCES
SYNOPSIS
Abstractions and Implementations
/trianglerightsldMeldable Priority Queues
The Sequence ADT
Thescan operation
Introduction to contraction
DATAABSTRACTION AND SEQUENCES 2/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ABSTRACTIONS AND
IMPLEMENTATIONS
Abstraction Implementation
Functions Problem Algorithm
Data Abstract Data Type Data Structure
DATAABSTRACTION AND SEQUENCES 3/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MELDABLE PRIORITY QUEUES
Priority Queues
/trianglerightsldInsert an item – insert
/trianglerightsldReturn and delete the item with the minimum priority
–deleteMin
Meldable Priority Queue
/trianglerightsldJoin two priority queues into one – meld
DATAABSTRACTION AND SEQUENCES 4/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MELDABLE PRIORITY QUEUES
Sis a totally ordered set (integers, strings, reals,
. . . ).
Tis a type representing subsets ofS.
empty :T ={}
insert (S,e) :T×S→T =S∪{e}
deleteMin (S):T→T×
(S∪{⊥} )=/braceleftbigg
(S,⊥) S={}
(S\{minS},minS)otherwise
meld (S1,S2) :T×T→T =S1∪S2
DATAABSTRACTION AND SEQUENCES 5/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MPQ D EFINITION IN SML
signature MPQ
sig
struct S : ORD
type t
val empty : t
val insert : t *S.t -> t
val deleteMin : t -> t *S.t option
val meld : t *t -> t
end
No semantics, only the types.
DATAABSTRACTION AND SEQUENCES 6/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MPQ: C OSTSPECIFICATIONS
Implementation 1:
Operation Work
insert (S,e) O(|S|)
deleteMin (S) O(1)
meld (S1,S2) O(|S1|+|S2|)
What is the underlying data structure? Sorted
Array
meld is actually an array merge.
DATAABSTRACTION AND SEQUENCES 7/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MPQ: C OSTSPECIFICATIONS
Implementation 2:
Operation Work
insert (S,e) O(log|S|)
deleteMin (S)O(log|S|)
meld (S1,S2) O(|S1|+|S2|)
What is the underlying data structure? Heaps
DATAABSTRACTION AND SEQUENCES 8/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MPQ: C OSTSPECIFICATIONS
Implementation 3:
Operation Work
insert (S,e) O(log|S|)
deleteMin (S) O(log|S|)
meld (S1,S2) O(log(|S1|+|S2|))
Later!
DATAABSTRACTION AND SEQUENCES 9/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ABSTRACTIONS AND
IMPLEMENTATIONS
The Abstract Data Type
/trianglerightsldFunctionality
/trianglerightsldCorrectness
The Cost Speciﬁcation
/trianglerightsldMultiple Cost Speciﬁcations
/trianglerightsldWe only need these to do cost analysis.
Underlying Data Structure
/trianglerightsldMultiple Data Structures
DATAABSTRACTION AND SEQUENCES 10/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESEQUENCE ADT - S OME BASICS
Arelation is a set of ordered pairs.
/trianglerightsldFirst from set A, second from set B
A relationρ⊆A×B.
Afunction is a relation ρ, where for every a∈A
there is only one bsuch that (a,b)∈ρ.
Asequence is a function where
A={0,..., n−1}for some n∈N.
DATAABSTRACTION AND SEQUENCES 11/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESEQUENCE ADT –
FUNCTIONALITY
Asequence is a type Sαrepresenting functions
from{0,..., n−1}toα.
empty :Sα ={}
length (A) :Sα→N =|A|
singleton (v):α→Sα ={(0,v)}
nth (A,i) :Sα→α =A(i)
map (f,A) : (α→β)×Sα→Sβ={(i,f(v)) : (i,v)∈A}
tabulate (f,n): (N→α)×N→Sα ={(i,f(i)) :i∈{0,..., n−1}}
take (A,n) :Sα×N→Sα ={(i,v)∈A|i<n}
drop (A,n) :Sα×N→Sα ={(i−n,v) : (i,v)∈A|i≥n}
append (A,B) :Sα×Sα→Sα =A∪{(i+|A|,v) : (i,v)∈B}
DATAABSTRACTION AND SEQUENCES 12/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESEQUENCE ADT – C OSTSPECS
ArraySequence
Work Span
length (T) O(1) O(1)
nth(T) O(1) O(1)
append (S1,S2)O(|S1|+|S2|)O(1)
DATAABSTRACTION AND SEQUENCES 13/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESEQUENCE ADT – C OSTSPECS
ArraySequence
Work Span
tabulate f n O/parenleftBiggn/summationdisplay
i=0W(f(i))/parenrightBigg
O/parenleftbigg
nmax
i=0S(f(i))/parenrightbigg
mapf S O/parenleftBigg/summationdisplay
s∈SW(f(s))/parenrightBigg
O/parenleftbigg
max
s∈SS(f(s))/parenrightbigg
DATAABSTRACTION AND SEQUENCES 14/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESEQUENCE ADT – C OST
SPECIFICATIONS
TreeSequence
Work Span
length (T) O(1) O(1)
nth(T) O(logn) O(logn)
append (S1,S2)O(log(|S1|+|S2|))O(log(|S1|+|S2|))
DATAABSTRACTION AND SEQUENCES 15/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESEQUENCE ADT – C OST
SPECIFICATIONS
TreeSequence
Work Span
tabulate f n O/parenleftBiggn/summationdisplay
i=0W(f(i))/parenrightBigg
O/parenleftbigg
logn+nmax
i=0S(f(i))/parenrightbigg
mapf S O/parenleftBigg/summationdisplay
s∈SW(f(s))/parenrightBigg
O/parenleftbigg
log|S|+max
s∈SS(f(s))/parenrightbigg
DATAABSTRACTION AND SEQUENCES 16/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SOME NOTATIONAL CONVENTIONS
Si Theithelement of sequence S
|S| The length of sequence S
/angbracketleft/angbracketright The empty sequence
/angbracketleftv/angbracketright A sequence with a single element v
/angbracketlefti,..., j/angbracketright A sequence of integers starting at iand
ending at j≥i.
/angbracketlefte:p∈S/angbracketrightMap the expression eto each element pof
sequence S.
The same as “ map (fnp⇒e)S” in ML.
/angbracketleftp∈S|e/angbracketrightFilter out the elements pinSthat satisfy the
predicate e.
The same as “ filter (fnp⇒e)S” in ML.
More examples are given in the “Syntax and
Costs” document.
DATAABSTRACTION AND SEQUENCES 17/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESCAN OPERATION
Related to reduce .
scan f I S : (α×α→α)→α→αseq
→(αseq×α)
Iis the identity value
fis an (associative) function
Sis a sequence
Produces/angbracketleftI,f(I,S0),f(f(I,S0),S1),.../angbracketrightand
reduce f I S
/trianglerightsldscan +0/angbracketleft2,1,4,6/angbracketright= (/angbracketleft0,2,3,7/angbracketright,13)
DATAABSTRACTION AND SEQUENCES 18/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESCAN OPERATION
scan computes preﬁx sums.
1funscan f I S =
2 (/angbracketleftreduce f I (take (S,i)) :i∈/angbracketleft0,...n−1/angbracketright/angbracketright,
3 reduce f I S )
Shasnelements
Apply reduce to each preﬁx of Sofielements,
0≤i≤n−1
/trianglerightsldGives you the αseqpart
Apply reduce toS
/trianglerightsldGives you the αpart
So you get (αseq→α)
DATAABSTRACTION AND SEQUENCES 19/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESCAN OPERATION
scan +0/angbracketleft2,1,3/angbracketright= (/angbracketleftreduce +0/angbracketleft/angbracketright,
reduce +0/angbracketleft2/angbracketright,
reduce +0/angbracketleft2,1/angbracketright/angbracketright
reduce +0/angbracketleft2,1,3/angbracketright)
= (/angbracketleft0,2,3/angbracketright,6)
This is obviously not efﬁcient!
We will see how to do this with
W(scan f I S ) = O(|S|)
S(scan f I S ) = O(log|S|)
DATAABSTRACTION AND SEQUENCES 20/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEINCLUSIVE SCAN OPERATION
reduce all preﬁxes ending at position i,
0≤i≤n−1
scanI +0/angbracketleft2,1,3/angbracketright=/angbracketleft2,3,6/angbracketright
DATAABSTRACTION AND SEQUENCES 21/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
USING SCAN IN THE MCSS P ROB.
THEMAXIMUM CONTIGUOUS SUBSEQUENCE SUMPROBLEM
Given a sequence of numbers S=/angbracketlefts1,..., sn/angbracketright,
Find
mcss (S) = max
1≤i≤j≤n/braceleftBiggj/summationdisplay
k=isk/bracerightBigg
S=/angbracketleft0,−1,2,−1,4,−1,0/angbracketright,mcss (S) =5
DATAABSTRACTION AND SEQUENCES 22/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
USING SCAN IN THE MCSS P ROB.
Consider S=/angbracketleft1,−2,3,−1,2,−3/angbracketright
LetX=scanI +0S=/angbracketleft1,−1,2,1,3,0/angbracketright
What is Xj−Xiforj>i?
/trianglerightsld/summationtextj
k=i+1Sk
/trianglerightsldX4−X0=3−1=2
DATAABSTRACTION AND SEQUENCES 23/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
USING SCAN IN THE MCSS P ROB.
Deﬁne Rjas the maximum sum that starts at
some iand ends at j>i.
Rj=jmax
i=0j/summationdisplay
k=iSk
=jmax
i=0(Xj−Xi−1)
=Xj+jmax
i=0(−Xi−1)
=Xj+j−1max
i=0(−Xi)=Xj−j−1
min
i=0Xi(Why ?)
DATAABSTRACTION AND SEQUENCES 24/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
USING SCAN IN THE MCSS P ROB.
Rj=Xj−j−1
min
i=0Xi
Y ou need Xjand the minimum previous Xi,i<j
/trianglerightsldcan be done by a minimum scan
(M,) =scan min 0 X= (/angbracketleft0,0,−1,−1,−1,−1/angbracketright,−1)
R=/angbracketleftXj−Mj:0≤j<|S|/angbracketright=/angbracketleft1,−1,3,2,4,1/angbracketright
DATAABSTRACTION AND SEQUENCES 25/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LET’SRECAP
Given S=/angbracketleft1,−2,3,−1,2,−3/angbracketright
We computed Xwith a +scanI .
/trianglerightsldX=/angbracketleft1,−1,2,1,3,0/angbracketright
We computed Mwith a minscan
/trianglerightsldM=/angbracketleft0,0,−1,−1,−1,−1/angbracketright
We computed R=/angbracketleftXj−Mj:0≤j<|S|/angbracketright
/trianglerightsldR=/angbracketleft1,−1,3,2,4,1/angbracketright
A ﬁnal max reduce onRgives us the MCSS, 4.
DATAABSTRACTION AND SEQUENCES 26/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
USING SCAN IN THE MCSS P ROB.
1funMCSS (S) =
2let
3 valX=scanI +0S
4 val (M,) =scan min 0 X
5in
6 max/angbracketleftXj−Mj:0≤j<|S|/angbracketright
7end
Work? O(n)
Span? O(logn)
DATAABSTRACTION AND SEQUENCES 27/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COPYSCAN
Scan can also be used to pass information
along a sequence.
/angbracketleftNONE,SOME (7),NONE,NONE,SOME (3),NONE/angbracketright
↓
/angbracketleftNONE,NONE,SOME (7),SOME (7),SOME (7),SOME (3)/angbracketright
Each element receives the nearest previous
SOME() value.
Easy to do sequentially with iter .
DATAABSTRACTION AND SEQUENCES 28/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COPYSCAN
Can we do this with scan ?
f:αoption×αoption→αoption
1funcopy (a,b) =
2 case bof
3 SOME ()⇒b
4|NONE⇒a
Passes its right argument if it is SOME , else
passes its left argument.
How do you show copy is associative.
DATAABSTRACTION AND SEQUENCES 29/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING SCAN –
CONTRACTION
scan looks inherently sequential.
/trianglerightsldNaive implementation needs O(n2)work.
/trianglerightsldSlightly clever sequential implementation needs
O(n)work.
/trianglerightsldDivide an Conquer approaches do not break the
sequentiality. (Why?)
Contraction
1Construct a much smaller instance of the problem
2Solve the smaller instance recursively
3Construct solution to the original instance.
DATAABSTRACTION AND SEQUENCES 30/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING REDUCE WITH
CONTRACTION
Given/angbracketleft2,1,3,2,2,5,4,1/angbracketright
Apply +pairwise and (in parallel) to get
/angbracketleft3,5,7,5/angbracketright
/trianglerightsldThis is the contracted instance!
Apply +pairwise to get/angbracketleft8,12/angbracketright
Finally apply +pairwise to get/angbracketleft20/angbracketright
The 3rdstep of the contraction does nothing in
this case.
DATAABSTRACTION AND SEQUENCES 31/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING SCAN WITH
CONTRACTION
Given S=/angbracketleft2,1,3,2,2,5,4,1/angbracketright
/trianglerightsldscan +0S= (/angbracketleft0,2,3,6,8,10,15,19/angbracketright,20)
First do pairwise +onSto get/angbracketleft3,5,7,5/angbracketright
Now (recursively) do scan on this to get
(/angbracketleft0,3,8,15/angbracketright,20)
/trianglerightsldWhat is the relation to the ﬁnal scan?
We have every other element of the ﬁnal scan!
How do we ﬁll in the rest?
DATAABSTRACTION AND SEQUENCES 32/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING SCAN WITH
CONTRACTION
Input =h2,1,3,2,2,5,4,1iPartial Output =(h0,3,8,15i,20)Desired Output =(h0,2,3,6,8,10,15,19i,20)++++
DATAABSTRACTION AND SEQUENCES 33/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING SCAN WITH
CONTRACTION
1% implements: the Scan problem on sequences that have a power of 2 length
2funscanPow2 f i s =
3 case|s|of
4 0⇒(/angbracketleft/angbracketright,i)
5|1⇒(/angbracketlefti/angbracketright,s[0])
6|n⇒
7 let
8 vals/prime=/angbracketleftf(s[2i],s[2i+1]) :0≤i<n/2/angbracketright
9 val(r,t) =scanPow2 f i s/prime
10 in
11 (/angbracketleftpi:0≤i<n/angbracketright,t),where pi=/braceleftBigg
r[i/2] ifeven (i)
f(r[i/2],s[i−1])otherwise .
12 end
General case is in the course notes.
DATAABSTRACTION AND SEQUENCES 34/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Abstractions and Implementations
/trianglerightsldMeldable Priority Queues
The Sequence ADT
Thescan operation
Introduction to contraction
Implementing scan with contraction.
DATAABSTRACTION AND SEQUENCES 35/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 6
SEQUENCES - II
SYNOPSIS
Thereduce operation
Implementing divide and conquer with reduce
/trianglerightsldImplementing MCSS with reduce
Analyzing cost of higher order functions
/trianglerightsldCost analysis for reduce
SEQUENCES - II 2/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEREDUCE OPERATION
reduce f I S : (α×α→α)→α
→αseq→α
When fis associative, reduce returns sum with
respect to f.
Same result as iter f I S
/trianglerightslditer is sequential and allows more general f(e.g.,
β×α→β
Iffis not associative, reduce anditer results
may differ.
SEQUENCES - II 3/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEREDUCE OPERATION
Speciﬁc combination based on a perfect binary
tree.
x0x1x2x3x4x5        = combine    = "dummy" elements
x0x1x2x3x4x5
SEQUENCES - II 4/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE AND CONQUER WITH
REDUCE
Many divide and conquer have the following
structure
1funmyDandC (S) =
2 case showt (S)of
3 EMPTY⇒emptyVal
4|ELT (v)⇒base (v)
5|NODE(L, R)⇒let
6 val (L/prime,R/prime) = ( myDandC (L)/bardblmyDandC (R) )
7 in
8 someMessyCombine (L/prime,R/prime)
9 end
This corresponds to a binary tree combination
scheme.
SEQUENCES - II 5/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE AND CONQUER WITH
REDUCE
someMessyCombinebasebasebasebasesomeMessyCombinesomeMessyCombine
SEQUENCES - II 6/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIVIDE AND CONQUER WITH
REDUCE
1funmyDandC (S) =
2 case showt (S)of
3 EMPTY⇒emptyVal
4|ELT (v)⇒base (v)
5|NODE(L, R)⇒let
6 val (L/prime,R/prime) = ( myDandC (L)/bardblmyDandC (R) )
7 in
8 someMessyCombine (L/prime,R/prime)
9 end
↓
reduce someMessyCombine emptyVal (map base S)
SEQUENCES - II 7/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MCSS USING REDUCE
mcss (s) = max
1≤i≤j≤n/braceleftBiggj/summationdisplay
k=isk/bracerightBigg
TotalLmpsLmssLmcssLLeft SubproblemTotalRmpsRmssRmcssRRight Subproblem
TotalmcssmpsmssTotalTotalLTotalR= + mcss= max ( mcssLmcssR, , mssLmpsR+ ) mps= max ( mpsL, TotalL+ mpsR) mss= max ( mssL+ TotalR, mssR) 
SEQUENCES - II 8/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MCSS USING REDUCE
mcss (s) = max
1≤i≤j≤n/braceleftBiggj/summationdisplay
k=isk/bracerightBigg
funcombine ((ML,PL,SL,TL),(MR,PR,SR,TR)) =
(max (SL+PR,ML,MR),max (PL,TL+PR),
max (SR,SL+TR),TL+TR)
funbase (v) = ( v,v,v,v)
valemptyVal = (−∞,−∞,−∞,0)
funmcss (S) =
reduce combine emptyVal (map base S)
SEQUENCES - II 9/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SOME OBSERVATIONS
Which code to use is a matter of taste
/trianglerightsldreduce version is shorter
/trianglerightsldDivide and Conquer version exposes the inductive
structure.
reduce version not applicable when split is
complicated
/trianglerightslde.g., in Quicksort
SEQUENCES - II 10/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SCAN VIA REDUCE
How do we implement the divide and conquer
scan with this template?
/trianglerightsldscan f I S≡
reduce combine emptyVal (map base S)
emptyVal =?(/angbracketleft/angbracketright,I)
funbase (v) =?(/angbracketleftI/angbracketright,f(I,v))
funcombine =?
funcombine ((S1,T1),(S2,T2)) =
append (S1,(map (fn x⇒f(x,T1))S2),f(T1,T2))
/trianglerightsldIs this right?
funcombine ((S1,T1),(S2,T2)) =
(append (S1,(map (fn x⇒f(T1,x))S2),f(T1,T2))
SEQUENCES - II 11/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COST OF HIGHER ORDER
FUNCTIONS
We assume that fruns in O(1)work and span.
/trianglerightsld⇒reduce hasO(n)work and O(logn)span
Easy for mapandtabulate
W(map f S) = 1+/summationdisplay
s∈SW(f(s))
S(map f S) = 1+max
s∈SS(f(s))
How about reduce ?
SEQUENCES - II 12/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MERGESORT VIA REDUCE
Remember the reduce template for divide and
conquer?
reduce combine emptyVal (map base S)
valcombine =merge <
valbase =singleton
valemptyVal =empty ()
funreduceSort (S) =
reduce combine emptyVal (map base S)
SEQUENCES - II 13/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COST OF REDUCESORT
merge <is an associative function with costs:
W(merge <(S1,S2)) = O(n1+n2)
S(merge <(S1,S2)) = O(log(n1+n2))
fhas no longer O(1)work and span.
What is the cost of reduceSort .
SEQUENCES - II 14/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COST OF REDUCESORT
For costs, reduction sequence matters!
Sequential order
On input x=/angbracketleftx0,x1,..., xn−1/angbracketright, we get
merge <(...merge <(merge <(merge <(I,/angbracketleftx0/angbracketright),/angbracketleftx1/angbracketright),/angbracketleftx2/angbracketright),... )
Left arg. has length 0 through n−1
Right arg. always has length 1.
Work:
W(reduceSort S)≤n−1/summationdisplay
i=0c·(1+i)∈O(n2)Why ?
SEQUENCES - II 15/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MERGESORT WITH REDUCE
Equivalent to iter version
funreduceSort’ (S) =
iter merge <(emptyVal (map base S)
Works really as an Insertion Sort.
/trianglerightsldInserts each element into a sorted preﬁx!
No parallelism except in merge
S(reduceSort S)≤n−1/summationdisplay
i=0c·log(1+i)∈O(nlogn)
SEQUENCES - II 16/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MERGESORT WITH REDUCE
The reduction tree is very unbalanced!
Suppose n=2kand merge tree is as follows
x0x1x2x3x4x5x6x7= merge
SEQUENCES - II 17/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MERGESORT WITH REDUCE
x0x1x2x3x4x5x6x7= merge
nnodes at constant cost at each leaf ( i=log2n)
n/2 nodes one level up, each costing c(1+1)
(i=log2n
2)(Why?)
In general, for level i, we have 2inodes merging
two sequences each lengthn
2i+1
SEQUENCES - II 18/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MERGESORT WITH REDUCE
x0x1x2x3x4x5x6x7= merge
For level i, we have 2inodes merging two
sequences each lengthn
2i+1
W(reduceSort x)≤logn/summationdisplay
i=02i·c/parenleftBign
2i+1+n
2i+1/parenrightBig
=logn/summationdisplay
i=02i·c/parenleftBign
2i/parenrightBig
∈O(nlogn)
SEQUENCES - II 19/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MERGESORT WITH REDUCE
x0x1x2x3x4x5x6x7= merge
W(reduceSort S)∈O(nlogn)⇒
mergeSort .
mergeSort andinsertionSort are special
cases of reduceSort using different reduction
orders.
SEQUENCES - II 20/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REDUCE ORDER
Result of reduce depends on the order when f
is not associative
When fis associative, different orders result in
different costs.
SEQUENCES - II 21/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 7
COLLECT , SETS AND TABLES
SYNOPSIS
The collect operation
The map-collect-reduce paradigm
Sets
Tables
COLLECT , SETS AND TABLES 2/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THECOLLECT OPERATION
Group items that share a common key.
valData =/angbracketleft(“jack sprat” ,“15-210” ),
(“jack sprat” ,“15-213” ),
(“mary contrary” ,“15-210” ),
(“mary contrary” ,“15-251” ),
(“mary contrary” ,“15-213” ),
(“peter piper” ,“15-150” ),
(“peter piper” ,“15-251” ),
. . ./angbracketright
↓
valrosters =/angbracketleft(“15-150” ,/angbracketleft“peter piper” , . . ./angbracketright)
(“15-210” ,/angbracketleft“jack sprat”, “mary contrary” , . . ./angbracketright)
(“15-213” ,/angbracketleft“jack sprat” , . . ./angbracketright)
(“15-251” ,/angbracketleft“mary contrary”, “peter piper” /angbracketright)
. . ./angbracketright
COLLECT , SETS AND TABLES 3/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THECOLLECT OPERATION
Very common operation in Relational Databases
Usually called the Group by operation.
valrosters =/angbracketleft(“15-150” ,/angbracketleft“peter piper” , . . ./angbracketright)
(“15-210” ,/angbracketleft“jack sprat”, “mary contrary” , . . ./angbracketright)
(“15-213” ,/angbracketleft“jack sprat” , . . ./angbracketright)
(“15-251” ,/angbracketleft“mary contrary”, “peter piper” /angbracketright)
. . ./angbracketright
Students are grouped by Course Numbers.
COLLECT , SETS AND TABLES 4/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THECOLLECT OPERATION
collect : (α×α→order )→(α×β)seq
→(α×βseq)seq
1α×α→order is a function for comparing keys
of typeα
2(α×β)seqis a sequence of key-value pairs
3(α×βseq)seqis the resulting sequence:
/trianglerightsldeach unique αvalue is paired with a sequence of all
βvalues it appears with
COLLECT , SETS AND TABLES 5/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THECOLLECT OPERATION
valcollectStrings =collect String.compare
valrosters =collectStrings (/angbracketleft(n,c) : (c,n)∈Data/angbracketright)
valrosters =/angbracketleft(“15-150”,/angbracketleft“peter piper” ,.../angbracketright)
(“15-210”,/angbracketleft“jack sprat”, “mary contrary” ,.../angbracketright)
(“15-213”,/angbracketleft“jack sprat” ,.../angbracketright)
(“15-251”,/angbracketleft“mary contrary”, “peter piper” /angbracketright)
.../angbracketright
/angbracketleft(n,c) : (c,n)∈Data/angbracketrightarranges the data
appropriately.
COLLECT , SETS AND TABLES 6/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THECOLLECT OPERATION
How would you implement collect ?
/trianglerightsldSort the items on their keys
/trianglerightsldPartition the resulting sequence
/trianglerightsldPull out pairs between each key change
COLLECT , SETS AND TABLES 7/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THECOLLECT OPERATION
The dominant cost of collect is in sorting.
Work is O(Wcnlogn), Span is O(Sclog2n)
/trianglerightsldWcwork bound for the comparison function
/trianglerightsldScspan bound for the comparison function
AO(n)work can be implemented with hashing.
/trianglerightsldNeed a separate hash function
/trianglerightsldOutput not in sorted order
COLLECT , SETS AND TABLES 8/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
USING COLLECT IN MAP-REDUCE
The map-reduce paradigm is used to process
very large collection of documents.
/trianglerightsldA document is a collection of words/strings.
/trianglerightsldNot the mapReduce of 15-150!
map-reduce paradigm ≡map-collect-reduce(s).
COLLECT , SETS AND TABLES 9/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
USING COLLECT IN MAP-REDUCE
fmmaps each document to a sequence of
key-value pairs.
/trianglerightsldfm: (document→(key×α)seq)
All key-value pairs in a document are collected.
fris applied to the keys to get a single value for
a key.
/trianglerightsldfr:key×αseq→β
COLLECT , SETS AND TABLES 10/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANEXAMPLE
docs =/angbracketleft“this is a document” ,
“this is is another document” ,
“a last document”/angbracketright
↓
/angbracketleft(“this”,1),(“is”,1),(“a”,1),(“document” ,1),
(“this”,1),(“is”,1),(“is”,1)(“a”,1),(“another”,1),
(“document” ,1),(“a”,1),(“last”,1),(“document” ,1)/angbracketright
↓
/angbracketleft(“a”,2),(“another”,1),(“document” ,3),(“is”,3),(“last”,1),
(“this”,2)/angbracketright
COLLECT , SETS AND TABLES 11/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MAPREDUCE IN SML
1funmapCollectReduce fmfrdocs =
2 let
3 valpairs =flatten/angbracketleftfm(s) :s∈docs/angbracketright
4 valgroups =collect String.compare pairs
5 in
6/angbracketleftfr(g) :g∈groups/angbracketright
7 end
flatten/angbracketleft/angbracketlefta,b,c/angbracketright,/angbracketleftd,e/angbracketright/angbracketright⇒/angbracketleft a,b,c,d,e/angbracketright
COLLECT , SETS AND TABLES 12/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MAPREDUCE IN SML
1funmapCollectReduce fmfrdocs =
2 let
3 valpairs =flatten/angbracketleftfm(s) :s∈docs/angbracketright
4 valgroups =collect String.compare pairs
5 in
6/angbracketleftfr(g) :g∈groups/angbracketright
7 end
fun fm(doc ) =/angbracketleft(w,1) :tokens doc/angbracketright
fun fr(w,s) = (w,reduce +0s)
COLLECT , SETS AND TABLES 13/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MAPREDUCE EXAMPLE IN SML
fun fm(doc ) =/angbracketleft(w,1) :tokens doc/angbracketright
fun fr(w,s) = (w,reduce +0s)
valcountWords =mapCollectReduce fmfr
countWords/angbracketleft“this is a document” ,
“this is is another document” ,
“a last document”/angbracketright
⇒/angbracketleft(“a”,2),(“another”,1),(“document” ,3),(“is”,3),
(“last”,1),(“this”,2)/angbracketright
COLLECT , SETS AND TABLES 14/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SETS
Sets play a very important role in math.
Often needed in many algorithms.
Many languages either support sets directly or
have libraries for sets.
In 15-210 we use a purely functional deﬁnition
for sets:
/trianglerightsldWhen updates are done, a new set is returned.
COLLECT , SETS AND TABLES 15/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SETS AS AN ADT
Uis a universe of elements.
The SET ADT is a type Sthat represents the
power set of U.
empty :S =∅
size (S) : S→Z≥0 =|S|
singleton (e) :U→S ={e}
filter (f,S) : (( U→{T,F}) ={s∈S|f(s)}
×S)→S
COLLECT , SETS AND TABLES 16/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SETS AS AN ADT
find (S,e) :S×U =|{s∈S|s=e}|=1
→{T,F}
insert (S,e) : S×U→S=S∪{e}
delete (S,e) : S×U→S=S\{e}
intersection (S1,S2):S×S→S=S1∩S2
union (S1,S2) : S×S→S=S1∪S2
difference (S1,S2) : S×S→S=S1\S2
What is the relationship between these two
groups?
COLLECT , SETS AND TABLES 17/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SETS AS AN ADT
We do not really need find ,insert ,delete !
find (S,e)=size (intersection (S,singleton (e))) = 1
insert (S,e)=union (S,singleton (e))
delete (S,e)=difference (S,singleton (e))
intersection ,union , and difference
/trianglerightsldcan operate on multiple elements, and
/trianglerightsldare suitable for parallelism
COLLECT , SETS AND TABLES 18/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTMODEL FOR SETS
Underlying data structure can be
/trianglerightsldhash-tables
/trianglerightsldbalanced trees
We will assume a balanced-tree implementation.
We will assume comparison of two set elements
take
/trianglerightsldWcwork and Scspan.
COLLECT , SETS AND TABLES 19/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTMODEL FOR SETS
Work Span
size (S)O(1) O(1)singleton (e)
filter (f,S)O/parenleftBigg/summationdisplay
e∈SW(f(e))/parenrightBigg
O/parenleftbigg
log|S|+max
e∈SS(f(e))/parenrightbigg
find (S,e)
O(Wc·log|S|) O(Sc·log|S|) insert (S,e)
delete (S,e)
COLLECT , SETS AND TABLES 20/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTMODELS FOR SETS
intersection (S1,S2) Work =O/parenleftbig
Wc·m·log(1+n
m)/parenrightbig
union (S1,S2)⇒
difference (S1,S2) Span =O(Sc·log(n+m))
n=max (|S1|,|S2|) m=min(|S1|,|S2|)
Sets are equal size (n=m)
/trianglerightsldWork =O(Wc·m·log(1+1)) =O(Wc·n)
/trianglerightsldSpan =O(Sc·logn)
One of the sets is a singleton (m=1)
/trianglerightsldWork =O(Wc·log(1+n)) =O(Wc·logn)
/trianglerightsldSpan =O(Sc·log(n+1)) =O(Sc·logn)
COLLECT , SETS AND TABLES 21/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TABLES
Table is an ADT for sets of key-value pairs.
{(k1/mapsto→v1),(k2/mapsto→v2),..., (kn/mapsto→vn)}
{(k1,v1),(k2,v2),..., (kn,vn)}
Each key appears only once
Many languages provide either built-in support
or libraries.
COLLECT , SETS AND TABLES 22/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TABLES
Kis the universe of keys.
Vis the universe of values.
Tis a type that represents the power set of
K×V
/trianglerightsldrestricted so that each key appears at most once.
/trianglerightsldSis the power set of K.
/trianglerightsldZ≥0denotes the positive integers.
COLLECT , SETS AND TABLES 23/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TABLE FUNCTIONS
empty :T =∅
size (T) : T→Z≥0 =|T|
singleton (k,v) : K×V→T ={(k,v)}
filter (f,T) : ((V→{T,F})×T)
→T ={(k,v)∈T|f(v)}
map (f,T) : ((K×V→V)×T)
→T ={(k,f(k,v))|((k,v)∈T)}
insert (f,T,(k,v)) : (V×V→V)×T
×(K×V)→T =
(T\{(k,v)})∪
{(k,f(v,v/prime))}
if(k,v/prime)∈T
T∪{(k,v)}
otherwise
delete (T,k)) : T×K→T ={(k/prime,v)∈T|k/negationslash=k/prime}
COLLECT , SETS AND TABLES 24/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TABLE FUNCTIONS
find (T,k) :T×K→(V∪⊥) =/braceleftbiggv (k,v)∈T
⊥otherwise
merge (f,T1,T2): (V×V→V)×T×T→T=
/uniondisplay
k∈K

{(k,f(v1,v2))}(k,v1)∈T1∧(k,v2)∈T2
{(k,v1)} (k,v1)∈T1∧(k,v2)/∈T2
{(k,v2)} (k,v2)∈T2∧(k,v1)/∈T1
extract (T,S):T×S→T ={(k,v)∈T|k∈S}
erase (T,S) :T×S→T ={(k,v)∈T|k/∈S}
COLLECT , SETS AND TABLES 25/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TABLE EXAMPLES
Suppose we have the two tables:
/trianglerightsldSummer ={tree/mapsto→green,sky/mapsto→blue,cmuq/mapsto→
tan}
/trianglerightsldFall ={grass/mapsto→gray,tree/mapsto→brown}
merge (fn(a,b)⇒b)Summer Fall
/trianglerightsld{grass/mapsto→gray,tree/mapsto→brown,sky/mapsto→
blue,cmuq/mapsto→tan}
COLLECT , SETS AND TABLES 26/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TABLE EXAMPLES
Suppose we have the two tables:
/trianglerightsldSummer ={tree/mapsto→green,sky/mapsto→blue,cmuq/mapsto→
tan}
/trianglerightsldFall ={grass/mapsto→gray,tree/mapsto→brown}
extract (Summer,{sky,grass})
/trianglerightsld{sky/mapsto→blue}
COLLECT , SETS AND TABLES 27/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TABLE EXAMPLES
Suppose we have the two tables:
/trianglerightsldSummer ={tree/mapsto→green,sky/mapsto→blue,cmuq/mapsto→
tan}
/trianglerightsldFall ={grass/mapsto→gray,tree/mapsto→brown}
erase (Summer,{sky,grass})
/trianglerightsld{tree/mapsto→green,cmuq/mapsto→tan}
COLLECT , SETS AND TABLES 28/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TABLE EXAMPLES
Other useful functions from the library
collect :(key×α)seq→(αseq)table
fromSeq :(key×α)seq→αtable
/trianglerightsldfromSeq (A) ={k/mapsto→s0: (k/mapsto→S)∈collect (A)}
COLLECT , SETS AND TABLES 29/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TABLE FUNCTIONS
Major differences from sets:
/trianglerightsldfind returns the value if key is in the table else
returns⊥(NONE ).
/trianglerightsldinsert/merge need a function to combine if the
key is already in the/both table(s).
Just as with sets, there is symmetry between
/trianglerightsldextract andfind
/trianglerightsldmerge andinsert
/trianglerightslderase anddelete
COLLECT , SETS AND TABLES 30/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTMODELS FOR TABLES
Work Span
size (T)O(1) O(1)singleton (k,v)
filter (f,T) O
/summationdisplay
(k,v)∈TW(f(v))
 O/parenleftbigg
log|T|+max
(k,v)∈TS(f(v))/parenrightbigg
map (f,T) O
/summationdisplay
(k,v)∈TW(f(k,v))
 O/parenleftbigg
max
(k,v)∈TS(f(k,v))/parenrightbigg
COLLECT , SETS AND TABLES 31/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTMODELS FOR TABLES
Work Span
find (S,k)
O(Wclog|T|) O(Sclog|T|) insert (T,(k,v))
delete (T,k)
extract (T1,T2)
O/parenleftbig
Wcmlog(1+n
m)/parenrightbig
O(Sclog(n+m)) merge (T1,T2)
erase (T1,T2)
n=max (|T1|,|T2|)m=min(|T1|,|T2|)
COLLECT , SETS AND TABLES 32/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Collect
Map-Collect-Reduce
Sets
Tables
COLLECT , SETS AND TABLES 33/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 8
SETS AND TABLES –II
SYNOPSIS
How search engines work
Single-threaded sequences
SETS AND TABLES –II 2/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BUILDING A SEARCH ENGINE
How do search engines work?
What are the inputs?
/trianglerightsld(Billions and billions of) documents consisting of
“words”.
How do we interact with the search engine
/trianglerightsld(Boolean) Keyword queries
/trianglerightsldList of matching documents (URLS)
SETS AND TABLES –II 3/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
HOW DOES THE SEARCH REALLY
WORK ?
User inputs a query (say a couple of words)
SE starts searching for the words in each
document one-by-one
Returns documents when they match.
Not really!
/trianglerightsldNot scalable (even for one user)
Preprocessing
SETS AND TABLES –II 4/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PREPROCESSING
Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc1Doc75Preprocessing
IndexQuery ProcessingQueryResultCrawlers
SETS AND TABLES –II 5/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PLAN
What kinds of queries we want to have.
What is the interface we want to have.
How do we implement all these
SETS AND TABLES –II 6/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
QUERIES
Bingle (:-) supports logical queries on words
involving
/trianglerightsldAnd: “15210” And“course” And“slides”
/trianglerightsldOr: “15210” Or“15150”
/trianglerightsldAndNot : “15210” AndNot “Pittsburgh”
“CMU” And“fun” And(“courses Or“clubs”)
Result would be a list of webpages/documents
that match.
SETS AND TABLES –II 7/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEINTERFACE
signature INDEX = sig
type word = string
type docId = string
type ’a seq
type index
type docList
val makeIndex : (docId *string) seq -> index
val find : index -> word -> docList
val And : docList *docList -> docList
val AndNot : docList *docList -> docList
val Or : docList *docList -> docList
val size : docList -> int
val toSeq : docList -> docId seq
endSETS AND TABLES –II 8/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DOCUMENTS
Indexing a tweet database.
T=/angbracketleft(“jack”, “chess club was fun”),
(“mary”, “I had a fun time in 210 class today”),
(“nick”, “food at the cafeteria sucks”),
(“sue”, “In 217 class today I had fun reading my email”),
(“peter”, “I had fun at nick’s party”),
(“john”, “tiddlywinks club was no fun, but more fun than 218”),
/angbracketright
“jack” is a document id
“chess club was fun” is a document
SETS AND TABLES –II 9/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
USING THE INTERFACE
T=/angbracketleft(“jack”, “chess club was fun”),
(“mary”, “I had a fun time in 210 class today”),
(“nick”, “food at the cafeteria sucks”),
(“sue”, “In 217 class today I had fun reading my email”),
(“peter”, “I had fun at nick’s party”),
(“john”, “tiddlywinks club was no fun, but more fun than 218”),
/angbracketright
valf= (find (makeIndex (T))) :word→doclist
toSeq (And (f"fun",Or(f"class",f"club" )))
⇒/angbracketleft"jack", "mary", "sue", "john" /angbracketright
SETS AND TABLES –II 10/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
USING THE INTERFACE
T=/angbracketleft(“jack”, “chess club was fun”),
(“mary”, “I had a fun time in 210 class today”),
(“nick”, “food at the cafeteria sucks”),
(“sue”, “In 217 class today I had fun reading my email”),
(“peter”, “I had fun at nick’s party”),
(“john”, “tiddlywinks club was no fun, but more fun than 218”),
/angbracketright
size (AndNot (f"fun",f"tiddlywinks" ))
⇒4
SETS AND TABLES –II 11/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THE MAKE INDEX FUNCTION
1funmakeIndex (docs ) =
2let
3 funtagWords (id,str ) =/angbracketleft(w,id) :w∈tokens (str )/angbracketright
4 valPairs =flatten/angbracketlefttagWords (d) :d∈docs/angbracketright
5 valWords =Table.collect(Pairs)
6in
7{w/mapsto→Set.fromSeq (d) : (w/mapsto→d)∈Words}
8end
What does tagWords do?
tagWords (“jack” ,“chess club was fun” )
⇒/angbracketleft(“chess”,“jack”),(“club”, “jack”), (“was”, “jack”), (“fun”, “jack”) /angbracketright
SETS AND TABLES –II 12/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THE PAIRS FUNCTION
1funmakeIndex (docs ) =
2let
3 funtagWords (id,str ) =/angbracketleft(w,id) :w∈tokens (str )/angbracketright
4 valPairs =flatten/angbracketlefttagWords (d) :d∈docs/angbracketright
5 valWords =Table.collect(Pairs)
6in
7{w/mapsto→Set.fromSeq (d) : (w/mapsto→d)∈Words}
8end
What does Pairs do?
Pairs =/angbracketleft(“chess”,“jack”),(“club”, “jack”), (“was”, “jack”) ,
(“fun”, “jack”), (“I”, “mary”), (“had”, “mary”),
(“fun”, “mary”), . . ./angbracketright
SETS AND TABLES –II 13/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THE COLLECT FUNCTION
1funmakeIndex (docs ) =
2let
3 funtagWords (id,str ) =/angbracketleft(w,id) :w∈tokens (str )/angbracketright
4 valPairs =flatten/angbracketlefttagWords (d) :d∈docs/angbracketright
5 valWords =Table.collect(Pairs)
6in
7{w/mapsto→Set.fromSeq (d) : (w/mapsto→d)∈Words}
8end
What does collect do?
Words ={(“a”/mapsto→/angbracketleft“mary”/angbracketright),
(“at”/mapsto→/angbracketleft“mary”, “peter”/angbracketright),
. . .
(“fun”/mapsto→/angbracketleft“jack”, “mary”, “sue”, “peter”, “john” /angbracketright),
. . .
SETS AND TABLES –II 14/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINAL TOUCHES
1funmakeIndex (docs ) =
2let
3 funtagWords (id,str ) =/angbracketleft(w,id) :w∈tokens (str )/angbracketright
4 valPairs =flatten/angbracketlefttagWords (d) :d∈docs/angbracketright
5 valWords =Table.collect(Pairs)
6in
7{w/mapsto→Set.fromSeq (d) : (w/mapsto→d)∈Words}
8end
What is happening here?
Sequences are converted to tables.
SETS AND TABLES –II 15/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MAKE INDEX COSTS
1funmakeIndex (docs ) =
2let
3 funtagWords (id,str ) =/angbracketleft(w,id) :w∈tokens (str )/angbracketright
4 valPairs =flatten/angbracketlefttagWords (d) :d∈docs/angbracketright
5 valWords =Table.collect(Pairs)
6in
7{w/mapsto→Set.fromSeq (d) : (w/mapsto→d)∈Words}
8end
Assuming tokens have a upper bound on length
/trianglerightsldWmakeIndex (n)∈O(nlogn),SmakeIndex∈O(log2n)
/trianglerightsldWhat does nrepresent?
SETS AND TABLES –II 16/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REST OF THE INTERFACE
funfind T v =Table.find T v
funAnd (s1,s2) =s1∩s2
funOr(s1,s2) =s1∪s2
funAndNot (s1,s2) =s1\s2
funsize (s) =|s|
funtoSeq (s) =Set.toSeq (s)
SETS AND TABLES –II 17/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SINGLE -THREADED ARRAY
SEQUENCES
Updating an array sequence in an imperative
language takes O(1)work.
In a functional setting, everything is persistent.
An update to a sequence of nelements needs
/trianglerightsldO(n)work for arraySequence implementation to
copy and update.
/trianglerightsldO(logn)work for treeSequence implementation
(because of substructure sharing)
Interfaces do not provide functions for updating
a single position.
/trianglerightsldto discourage sequential (and expensive)
computation.
SETS AND TABLES –II 18/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SINGLE -THREADED ARRAY
SEQUENCES
Amapcan be implemented as follows
funmap f S =
iter (fn((i,S/prime),v)⇒(i+1,update (i,f(v))S/prime))
(0,S)
S
Iterates ntimes ( i=0,...n−1)
and updates the value Siwith f(Si).
arraySequence : Each update will do O(n)
work for a total O(n2)work
treeSequence : Each update will do O(logn)
work for a total O(nlogn)work.
SETS AND TABLES –II 19/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SINGLE -THREADED SEQUENCES
A new ADT: Single Threaded Sequence: stseq
Useful when a bunch of items need to be
updated.
Straigthforward interface
Cost speciﬁcation imply non-functional stuff
under the hood!
SETS AND TABLES –II 20/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
STSEQ INTERFACE AND COSTS
Work Span
fromSeq(S) : αseq→αstseq O(|S|)O(1)
Converts from a regular sequence to a stseq.
toSeq(ST) : αstseq→αseq O(|S|)O(1)
Converts from a stseq to a regular sequence.
nth ST i:αstseq→int→α O(1) O(1)
Returns the ithelement of ST. Same as for seq.
update ( i,v)S: (int×α)→ O(1) O(1)
αstseq→αstseq
Replaces the ithelement of Swith v.
inject I S: (int×α) seq O(|I|) O(1)
→αstseq→αstseq
For each (i,v)∈Ireplaces the ithelement of Swith v.
Cost bounds for nthandupdate only valid for
the “current” version of the sequence.
SETS AND TABLES –II 21/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MAP WITH STSEQ
1funmap f S =let
2 valS/prime=StSeq.fromSeq (S)
3 valR=iter
4 (fn((i,S/prime/prime),v)⇒(i+1,StSeq.update (i,f(v))S/prime/prime))
5 (0,S/prime)
6 S/prime
7in
8 StSeq.toSeq (R)
9end
Overall work and span is O(n)(Why?)
Multiple updates can be done in O(n)time.
SETS AND TABLES –II 22/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING STSEQ
Keep two full copies of the sequence
/trianglerightsldOriginal and Current
/trianglerightsldWe keep a change log: updates to the original to get
Current.
When Current is updated
/trianglerightsldWe create a “new” Current with the update, and
update change log.
/trianglerightsldMark the previous version as old, remove its Current
and but keep the old change log.
Any item from the current version is accessible
in constant work.
Any item from the any previous version is
accessible but needs more work.
SETS AND TABLES –II 23/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING STSEQ
OriginalCurrent( )Change Log
SETS AND TABLES –II 24/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING STSEQ
OriginalCurrent( )update(3, 5)Original( )Original((3,5) )Current5Change Log
Old Version1
There really is only one copy of the Original.
All point to that copy.
SETS AND TABLES –II 25/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING STSEQ
Original( )Original((3,5) )Current5update(6, 7)Old Version1
Original( )Original((3,5) )Original((6, 7)(3,5) )Current57Old Version1Old Version2
SETS AND TABLES –II 26/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING STSEQ
Original( )Original((3,5) )Original((6, 7)(3,5) )Current57Old Version1Old Version2updateOldversion2(4, 5)Original( )Original((4, 5)(3,5) )Original((6, 7)(3,5) )Current57Old Version1Old Version3Original((3,5) )Old Version2
SETS AND TABLES –II 27/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
How search engines work
Single-threaded sequences
SETS AND TABLES –II 28/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 9
GRAPHS
SYNOPSIS
Graphs
Graph terminology/deﬁnitions
Graph representations/costs.
Graph search
GRAPHS 2/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPHS
Most versatile ADT in the study of algorithms
Captures relationships between pairs of items
A graph consists of
/trianglerightslda set of Vvertices/nodes
/trianglerightslda set edges E⊆V×V
Edges represent relationships between nodes.
/trianglerightslddirected edges (asymmetric relationships)
/trianglerightsldundirected edges (symmetric relationships)
Nodes or edges can have additional weights or
values associated.
GRAPHS 3/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SOCIAL NETWORKS
GRAPHS 4/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SOCIAL NETWORKS - QUESTIONS
Who is popular?
What is the largest “clique”?
Do I know somebody who knows X?
What is the “diameter”?
GRAPHS 5/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TRANSPORTATION NETWORKS
GRAPHS 6/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TRANSPORTATION NETWORKS -
QUESTIONS
What is the shortest route from NYC to Los
Angeles?
/trianglerightsldwithout Toll Roads?
/trianglerightsldwithout any state roads?
What is the expected driving time from Boston
to Atlanta?
/trianglerightsldconsidering trafﬁc congestion?
GRAPHS 7/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FLOW NETWORKS
GRAPHS 8/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FLOW NETWORKS - QUESTIONS
Is it possible to send 1 Mcubic meters of gas to
Paris daily?
What is the maximum gas that can be pumped
from Azerbaijan to Italy?
GRAPHS 9/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
OTHER EXAMPLES OF GRAPHS
Course prerequisite relation graphs
(directed-acyclic)
Web-page linkage graph
Protein-protein interaction graph
Neural networks
Semantic networks
GRAPHS 10/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIRECTED GRAPHS
A directed graph (digraph) is G= (V,E)
/trianglerightsldVis a set of vertices (or nodes), and
/trianglerightsldE⊆V×Vis a set of directed edges (or arcs).
Each arc is an ordered pair e= (u,v)
/trianglerightsldArcs represent asymmetric relationships
/trianglerightsldA graph can have self loops (u,u)
BA
DC
GRAPHS 11/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
UNDIRECTED GRAPHS
An undirected graph is G= (V,E)
/trianglerightsldVis a set of vertices (or nodes), and
/trianglerightsldE⊆V×Vis a set of edges
Each edge is an unordered pair e={u,v}
/trianglerightsldEdges represent symmetric relationships
/trianglerightsldA undirected graphs do not have self-loops.
1234
GRAPHS 12/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
NEIGHBORS
In an undirected graph, G= (V,E), a vertex vis
a neighbor of uif{u,v}∈E.
In an undirected graph,
NG(v) ={u|{u,v}∈E}is the neighborhood of
v
IfUis a set of nodes,
/trianglerightsldNG(U) =∪v∈UNG(v)is the neighborhood of U
GRAPHS 13/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
NEIGHBORS
In a directed graph, G= (V,E),
/trianglerightslduis an in-neighbor of vif(u,v)∈E
/trianglerightslduis an out-neighbor of vif(v,u)∈E
In a directed graph
/trianglerightsldN−
G(u)is the set of in-neighbors of u.
/trianglerightsldN+
G(u)is the set of out-neighbors of u.
/trianglerightsldWhen we use NG(v), we mean out-neighbors.
IfUis a set of nodes,
/trianglerightsldN+
G(U) =∪u∈UN+
G(u)is the out-neighborhood of U.
GRAPHS 14/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
NODEDEGREES
Undirected graphs: degree dG(v)of a vertex v
is|NG(v)|
Directed graphs:
/trianglerightsldin-degree of a vertex visd−
G(v) =|N−
G(v)|
/trianglerightsldout-degree of a vertex visd+
G(v) =|N+
G(v)|
We will remove subscript Gif it is clear from
context.
GRAPHS 15/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PATHS
A path is a sequence of adjacent vertices.
For a graph G= (V,E)
Paths (G) =/braceleftbig
P∈V+|1≤i<|P|,(Pi,Pi+1)∈E/bracerightbig
/trianglerightsldV+is denotes of sequence of length 1 or more.
/trianglerightsldRepeats are allowed.
The length of a path is the number of edges.
A path may have an inﬁnite length.
A simple path has no repeated vertices.
/trianglerightsldOften “simple” will be dropped.
GRAPHS 16/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REACHABILITY
A vertex vis reachable from a vertex uinGif
there is a path starting at uand ending at vinG.
RG(u)is the set of vertices reachable from u.
An undirected graph is connected if all vertices
are reachable from all other vertices.
A directed graph is strongly connected if all
vertices are reachable from all other vertices.
GRAPHS 17/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CYCLES
A cycle is a path that starts and ends at the
same vertex.
In a directed graph a cycle can have length 1
(i.e. a self loop).
In an undirected graph we require that a cycle
must have length at least three.
/trianglerightsldGoing from utovand back to udoes not count.
A simple cycle is a cycle that has no repeated
vertices other than the start vertex being the
same as the end.
GRAPHS 18/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TREES, FORESTS AND DAG S
An undirected graph with no cyles is a forest.
If it is connected then it is a tree.
A directed graph is a forest or tree if it becomes
a forest or tree when all arcs are made
undirected.
In a rooted tree one node is the root.
For a directed graph, all edges are either
towards the root or away from the root.
A directed graph with no cycles is a directed
acyclic graph (DAG)
GRAPHS 19/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DISTANCE AND DIAMETER
The distance δG(u,v)from a vertex uto a vertex
vin a graph Gis the shortest path (minimum
number of edges) from utov.
The diameter of a graph is the maximum
shortest path length over all pairs of vertices:
diam (G) =max{δG(u,v) :u,v∈V}.
GRAPHS 20/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MULTI-GRAPHS
Multi-graphs allow multiple edges between
same pair of vertices.
GRAPHS 21/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SPARSE AND DENSE GRAPHS
Letn=|V|andm=|E|.
A directed graph can have at most n2edges.
An undirected graph can have at mostn(n−1)
2edges.
A graph is sparse if m/lessmuchn2. Otherwise it is
called dense.
In most applications, that graphs are sparse.
/trianglerightsldNobody on Twitter has 109followers
/trianglerightsldThough some have very large number– but still small
when compared to n.
GRAPHS 22/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
OPERATIONS ON GRAPHS
1Map over the vertices v∈V.
2Map over the edges (u,v)∈E.
3Map over the neighbors of a vertex v∈V, or in a directed
graph the in-neighbors or out-neighbors.
4Return the degree of a vertex v∈V.
5Determine if an edge (u,v)is inE.
6Insert or delete vertices.
7Insert or delete edges.
GRAPHS 23/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ADJACENCY MATRIX
REPRESENTATION
Assume vertices are numbered 1 ,2,..., n(or
0,1,..., n−1).
Graph is represented by an n×nmatrix of
binary values in which location (i,j)is 1 if
(i,j)∈Eand 0 otherwise.
/trianglerightsldFor undirected graphs, matrix is symmetric and has
0’s along the diagonal.
1234
0 0 1 1
0 0 0 1
1 0 0 1
1 1 1 0

GRAPHS 24/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ADJACENCY LISTREPRESENTATION
Graph is represented by an array Aof length n
where each entry A[i]contains a pointer to a
linked list of all the out-neighbors of vertex i.
/trianglerightsldIn an undirected graph edge {u,v}will appear in the
adjacency list for both uandv(not always
necessary!)
1234
123434414123
GRAPHS 25/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
OTHER REPRESENTATIONS
Adjacency Array
1234
344141230235
Edge List
((1,3),(1,4),(2,4),(3,1),(3,4),(4,1),(4,2),(4,3))
GRAPHS 26/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MOREABSTRACT REPRESENTATIONS
Edge Sets
/trianglerightsldDirected graphs: Set items are pairs (u,v)
representing arcs.
/trianglerightsldUndirected graphs: Set items are sets {u,v}
representing edges.
Edge Tables
/trianglerightsldDirected graphs: Table items are pairs
((u,v)/mapsto→wu,v)representing arcs and associated
values.
/trianglerightsldUndirected graphs: Set items are pairs
({u,v}/mapsto→wu,v)representing edges and associated
values.
GRAPHS 27/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EDGESETS AND TABLES
Similar to edge lists but abstracts from
underlying representation.
Search for an edge needs O(logm)work.
Searching for neighbors is not efﬁcient: O(m)
work but O(logm)span. (Why?)
GRAPHS 28/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ADJACENCY TABLES
Table items are (key,value )pairs.
Keys are vertex/node labels.
Values are either sets or tables
/trianglerightsldSets: All neighbors node labels or out-neighbor node
labels.
/trianglerightsldTables: All pairs of neighbors node labels and
associated edge values.
Accessing neighbors needs O(logn)work and
span.
(Constant work) Map over neighbors needs
O(dG(u))work and O(logdG(u))span.
Looking up an edge needs O(logn)work and
span.
GRAPHS 29/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTSUMMARY
edge set adj table
work span work span
isEdge (G,(u,v))O(logm)O(logm)O(logn)O(logn)
map over O(m)O(logm) O(m) O(logn)
all edges
map over O(m)O(logm)O(logn O (logn)
neighbors of v +dG(v))
dG(v) O(m)O(logm)O(logn)O(logn)
GRAPHS 30/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH
Fundamental operation of graphs
/trianglerightsldStart at some (set of) node(s)
/trianglerightsldSystematically visit all reachable nodes (only once)
Used for determining properties of
graphs/nodes
/trianglerightsldConnected?
/trianglerightsldBipartite?
/trianglerightsldNode vreachable from node u?
/trianglerightsldShortest path from utov?
GRAPHS 31/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH
For all graph search methods vertices can be
partitioned into three sets at any time during the
search:
1vertices already visited (X),
2the unvisited neighbors of the visited vertices,
called the frontier (F),
3and the rest.
GRAPHS 32/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH METHODS
Breadth-ﬁrst Search (BFS)
/trianglerightsldParallelizable but for shallow graphs!
Depth-ﬁrst Search (DFS)
/trianglerightsldInherently sequential!
Priority-ﬁrst Search (PFS)
All reachable nodes from a source are visited,
but in different orders.
GRAPHS 33/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH TREES
Each search starting from a source node
creates a search tree.
We refer to the source node as the root.
s s s 
s s 1 1 2 2 2 3 s 1 1 2 2 2 3 
Which search schemes do these correspond to?
GRAPHS 34/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Graphs
Graph terminology/deﬁnitions
Graph representations/costs.
Graph search
GRAPHS 35/35
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 10
BREADTH -FIRST SEARCH
SYNOPSIS
Breadth-ﬁrst search
BFS Extensions
BFS Costs
BFS with Single-threaded Sequences
BREADTH -FIRST SEARCH 2/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH
Fundamental operation of graphs
/trianglerightsldStart at some (set of) vertex(s)
/trianglerightsldSystematically visit all reachable vertices (only once)
Used for determining properties of
graphs/vertices
/trianglerightsldConnected?
/trianglerightsldBipartite?
/trianglerightsldVertex vreachable from vertex u?
/trianglerightsldShortest path from utov?
BREADTH -FIRST SEARCH 3/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH METHODS
Breadth-ﬁrst Search (BFS)
/trianglerightsldParallelizable but for shallow graphs!
Depth-ﬁrst Search (DFS)
/trianglerightsldInherently sequential!
Priority-ﬁrst Search (PFS)
All reachable vertices from a source are visited,
but in different orders.
BREADTH -FIRST SEARCH 4/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BREADTH -FIRST SEARCH
Applicable to a variety of problems
/trianglerightsldConnectedness
/trianglerightsldReachability
/trianglerightsldShortest path
/trianglerightsldDiameter
/trianglerightsldBipartiteness
Applicable to both directed and undirected
graphs
/trianglerightsldFor digraphs, we only consider outgoing arcs.
BREADTH -FIRST SEARCH 5/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH
For all graph search methods vertices can be
partitioned into three sets at any time during the
search:
1vertices already visited (X⊆V),
2the unvisited neighbors of the visited vertices, called
thefrontier (F),
3the rest; unseen vertices.
The search essential goes as follows:
while vertices remain
-visit some unvisited neighbors
of the visited set
Web navigation analogy.
BREADTH -FIRST SEARCH 6/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BREADTH -FIRST SEARCH
Starting from a source vertex s
/trianglerightsldVisit all vertices that are (out-)neighbors of s(at
distance 1)
/trianglerightsldVisit all vertices at distance 2 from s
/trianglerightsldVisit all vertices at distance 3 from s, etc.
A vertex at distance i+1 must have a
(in-)neighbor at distance i.
BREADTH -FIRST SEARCH 7/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BREADTH -FIRST SEARCH
BFS needs to keep track of vertices already
visited
Xi: all vertices visited at start of level i
/trianglerightsldVertices in Xihave distance less than i.
Fi: all unvisited neighbors of vertices in Xi
/trianglerightsldVertices in Fihave distance exactly i.
“Visit”⇒Do something with the vertices (e.g.,
print it)
Xi+1=Xi∪Fi
Fi+1=NG(Fi)\Xi+1(NG(Fi) =/uniontext
v∈FiN(v))
BREADTH -FIRST SEARCH 8/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BREADTH -FIRST SEARCH
1funBFS (G= (V,E),s) =
2let
3 funBFS/prime(X,F,i) =
4 if|F|=0then (X,i)
5 else let
6 valX/prime=X∪F % Visit the Frontier
7 valN=NG(F) % Determine the neighbors
8 % of the frontier
9 valF/prime=N\X/prime% Remove vertices that have
10 % been visited
11 inBFS/prime(X/prime,F/prime,i+1)% Next level
12 end
13 inBFS/prime({},{s},0)
14 end
BREADTH -FIRST SEARCH 9/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SOME DETAILS
Adjacency table representation
/trianglerightsldEntries of the sort (Vertex,{Neighbors}).
Remember NG(F) =/uniontext
v∈FN(v)
fun NG(F) =Table.reduce Set.Union {}
Table.extract (G,F)
X2 F1 F2 NG(F1) X1 
BREADTH -FIRST SEARCH 10/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROVING BFS C ORRECT
State and prove an invariant.
All reachable vertices are returned.
Algorithm terminates.
BREADTH -FIRST SEARCH 11/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROVING BFS C ORRECT
LEMMA
In algorithm BFS when calling BFS/prime(X,F,i), we have
X={v∈VG|δG(s,v)<i}, and
F={v∈VG|δG(s,v) =i}
By induction on levels i
For base case (i=0)X0={},F0={s}
/trianglerightsldOnly shas distance 0 from s
/trianglerightsldNo vertex has distance <0 from s.
So base case is true!
BREADTH -FIRST SEARCH 12/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROVING BFS C ORRECT
Assume claims are true for i, show for i+1.
Xi+1is the union of
/trianglerightsldXi: all vertices at distance <i
/trianglerightsldFi: all vertices at distance =i
Hence Xi+1must have all vertices at distance
<i+1
Fi+1=NG(Fi)\Xi+1
/trianglerightsldVertices in Fihave distance exactly i
/trianglerightsldVertices in NG(Fi)have distance no more than i+1
/trianglerightsldVertices in NG(Fi)are reachable from a vertex at
distance i
/trianglerightsldWhen we remove Xi+1from NG(Fi)only unvisited
vertices at distance exactly i+1 remain.
BREADTH -FIRST SEARCH 13/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ADDITIONAL OBSERVATIONS
Ifvis reachable from sand has distance d,
there must be a vertex uat distance d−1.
/trianglerightsldBSF will not terminate without ﬁnding v.
For any vertex δ(s,v)<|V|, so algorithm will
terminate in at most |V|rounds/levels.
BREADTH -FIRST SEARCH 14/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXTENSIONS TO BFS
Finding shortest distances
What do we need to keep?
1funBFS (G,s) =let
2 funBFS/prime(X,F,i) =
3 if|F|=0then X
4 else let
5 valX/prime=X∪{v/mapsto→i:v∈F}
6 valF/prime=NG(F)\domain (X/prime)
7 inBFS/prime(X/prime,F/prime,i+1)end
8inBFS/prime({},{s},0)end
BREADTH -FIRST SEARCH 15/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXTENSIONS TO BFS
Finding BFS trees.
s s s 
s s 1 1 2 2 2 3 s 1 1 2 2 2 3 
There could be multiple BFS trees.
BREADTH -FIRST SEARCH 16/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINDING BFS T REES
What do we need to keep for each vertex?
Record a parent
/trianglerightsldIfvis in a frontier, then there should be one or more
visited vertices usuch that (u,v)∈E.
/trianglerightsldAny of those could be the parent of v.
s s s 
s s 1 1 2 2 2 3 s 1 1 2 2 2 3 
BREADTH -FIRST SEARCH 17/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IDENTIFYING PARENTS
Post-process the BFS distance table
Identify one (in-)neighbor vertex in N−(v)whose
distance is one less.
Another way is to keep a table of vertices
mapping to parents.
/trianglerightsldFor each v∈F, generate a table {u/mapsto→v:u∈N(v)}
/trianglerightsldMaps each neighbor of vback to v.
Merge these tables to X
/trianglerightsldChoose one if you have multiple parents.
BREADTH -FIRST SEARCH 18/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTANALYSIS FOR BFS
Most graph algorithms do NOT use divide and
conquer.
/trianglerightsldSo no natural way to develop recurrences and solve
them.
Instead, we just count steps
BREADTH -FIRST SEARCH 19/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTANALYSIS FOR BFS
BFS works in a sequence rounds (one per level)
We can add up work and span in each round.
/trianglerightsldBut work at a level depends on number of outgoing
edges from the frontier!
Take a more global view
/trianglerightsldEach vertex appears exactly once in some frontier.
/trianglerightsldAll their (out-)edges are processed once.
WBFS(n,m) =Wvn+Wem
/trianglerightsldn=|V|andm=|E|
BREADTH -FIRST SEARCH 20/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTS ANALYSIS FOR BFS
Span is a bit more tricky!
SBFS(n,m,d) =Sldwhere dis the maximum
distance ( d=max v∈Vδ(s,v))
Assuming Wv=O(logn)andWe=O(logn)
and span/level Sl=O(log2n)
WBFS(n,m)=O(nlogn+mlogn)
=O(mlogn) (Why ?)
SBFS(n,m,d)=O(dlog2n)
BREADTH -FIRST SEARCH 21/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTS PER VERTEX AND EDGE
Nontrivial operations are
1X/prime=X∪F
2N=NG(F)
3F/prime=N\X/prime.
These all depend on size of Fand number of
outgoing edges from F.
Let||F||=/summationtext
v∈F(1+d+
G(v))
/trianglerightsldVertices and outgoing edges in f.
BREADTH -FIRST SEARCH 22/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTS PER VERTEX AND EDGE
Work Span
X∪FO(|F|logn)O(logn)
N\X/primeO(|F|logn)O(logn)
These come from set cost specs.
Work =O(Wc·|F|log(1+n
|F|)) =O(|F|logn)
Span =O(Sc·log(n+|F|)) =O(logn)
BREADTH -FIRST SEARCH 23/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTS PER VERTEX AND EDGE
Work Span
NG(F)O(||F||logn)O(log2n)
Graph is represented as a table mapping
vertices to a set of their outneigbors.
fun NG(F) =Table.reduce Set.Union {}
(Table.extract (G,F))
Extract vertices from table: Work is O(|F|logn)
BREADTH -FIRST SEARCH 24/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIGRESSION – BACK TO REDUCE !
fun NG(F) =Table.reduce Set.Union {}
(Table.extract (G,F))
R(reduce fIS) =/braceleftBig
all function applications f(a,b)in the reduction tree/bracerightBig
.
W(reduce fIS)=O
n+/summationdisplay
f(a,b)∈R(fIS)W(f(a,b))

S(reduce fIS)=O/parenleftbigg
logn max
f(a,b)∈R(fIS)S(f(a,b))/parenrightbigg
BREADTH -FIRST SEARCH 25/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIGRESSION – BACK TO REDUCE !
LEMMA
For any combine function f:α×α→αand a
monotone size measure s:α→R +, if for any x,y,
1s(f(x,y))≤s(x) +s(y)and
2W(f(x,y))≤cf(s(x) +s(y))for some universal
constant cfdepending on the function f,
then
W(reduce fIS) =O/parenleftBigg
log|S|/summationdisplay
x∈S(1+s(x))/parenrightBigg
BREADTH -FIRST SEARCH 26/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BACK TO COSTS
In our case αis the set type, fisSet.union ,s
the size of a set.
1Size of the union ≤sum of the sizes.
2Work of a union≤is at most proportional to size of
the sets!
SoSet.union satisﬁes the conditions of the
lemma.
Fngh=Table.extract (G,F)
/trianglerightsldFnghis a set of neighbor sets.
W(reduce union{}Fngh)=O
log|Fngh|/summationdisplay
ngh∈Fngh(1+|ngh|)

=O(logn·||F||)
BREADTH -FIRST SEARCH 27/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BACK TO COSTS
S(reduce union{}Fngh) =O(log2n)
Each union has span O(logn)
The reduction tree is bounded by log ndepth.
So at level i,W=O(||Fi||·logn)and each edge
is processed once, ⇒
/trianglerightsldwork per edge is O(logn).
Span depends on d
(SBFS(n,m,d) =O(dlog2n))
/trianglerightsldIn worst d∈O(n)⇒BFS is sequential.
BREADTH -FIRST SEARCH 28/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BFS WITH ST S EQUENCES
BFS Costs revisited
WBFS(n,m) = O(mlogn)
SBFS(n,m,d) = O(dlog2n)
Using single-threaded sequences reduces costs
to
WBFS(n,m) =O(m)
SBFS(n,m,d)=O(dlogn)
BREADTH -FIRST SEARCH 29/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BFS WITH ST S EQUENCES
Vertices are labeled with integers:
/trianglerightsldV={0,1,..., n−1}
/trianglerightsldInteger labeled (IL) graphs.
We use (array) sequences to represent graphs.
/trianglerightsldConstant work access to vertices.
/trianglerightsldNeighbors also stored as integer indices
IL graphs are implemented with type
(int seq) seq
BREADTH -FIRST SEARCH 30/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BFS WITH ST S EQUENCES
BFS returns a mapping from each vertex to its
parent in the BFS tree.
Visited vertices are maintained as
(int option) stseq
/trianglerightsldNONE : Vertex has not been visited.
/trianglerightsldSOME(v) : Vertex visited from parent v.
BREADTH -FIRST SEARCH 31/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BFS WITH ST S EQUENCES
1funBFS(G:(int seq) seq ,s:int) =
2let
3 funBFS/prime(XF:int option stseq ,F:int seq ) =
4 if|F|=0then stSeq.toSeq XF
5 else let
6 % compute neighbors of the frontier
7 valN=flatten/angbracketleft/angbracketleft(u,SOME (v)) :u∈G[v]&XF[u] =NONE/angbracketright:v∈F/angbracketright
8 % add new parents
9 valXF/prime=stSeq.inject (N,XF)
10 % remove duplicates
11 valF/prime=/angbracketleftu: (u,v)∈N|XF/prime[u] =v/angbracketright
12 inBFS/prime(XF/prime,F/prime)end
13 valX0=stSeq.toSTSeq (/angbracketleftNONE :v∈/angbracketleft0, . . . ,|G|−1/angbracketright/angbracketright)
14 in
15 BFS/prime(stSeq.update (s,SOME (s),X0),/angbracketlefts/angbracketright)
16 end
BREADTH -FIRST SEARCH 32/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTS
XF:stseq
line work span
flatten O(||Fi||)O(logn)
inject O(||Fi||) O(1)
remove dup .O(||Fi||)O(logn)
total across
alldroundsO(m)O(dlogn)
BREADTH -FIRST SEARCH 33/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Breadth-ﬁrst search
BFS Extensions
BFS Costs
BFS with Single-threaded Sequences
BREADTH -FIRST SEARCH 34/34
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 11
DEPTH -FIRST SEARCH
SYNOPSIS
Depth-ﬁrst search
Cycle-detection in directed and undirected
graphs
Topological Sorting
Generalizing DFS
DFS with Single-threaded Sequences
DEPTH -FIRST SEARCH 2/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH
Fundamental operation of graphs
/trianglerightsldStart at some (set of) node(s)
/trianglerightsldSystematically visit all reachable nodes (only once)
Used for determining properties of
graphs/nodes
/trianglerightsldConnected?
/trianglerightsldBipartite?
/trianglerightsldNode vreachable from node u?
/trianglerightsldShortest path from utov?
DEPTH -FIRST SEARCH 3/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH METHODS
Breadth-ﬁrst Search (BFS)
/trianglerightsldParallelizable but for shallow graphs!
Depth-ﬁrst Search (DFS)
/trianglerightsldInherently sequential!
Priority-ﬁrst Search (PFS)
All reachable nodes from a source are visited,
but in different orders.
DEPTH -FIRST SEARCH 4/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BREADTH -FIRST SEARCH
Applicable to a variety of problems
/trianglerightsldConnectedness
/trianglerightsldReachability
/trianglerightsldShortest path
/trianglerightsldDiameter
/trianglerightsldBipartedness
Applicable to both directed and undirected
graphs
/trianglerightsldFor digraphs, we only consider outgoing arcs.
DEPTH -FIRST SEARCH 5/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH SEARCH
For all graph search methods vertices can be
partitioned into three sets at any time during the
search:
1vertices already visited (X⊆V),
2the unvisited neighbors of the visited vertices, called
thefrontier (F),
3the rest; unseen vertices.
The search essentially goes as follows:
while vertices remain
-visit some invisited neighbors
of the visited set
Web navigation analogy.
DEPTH -FIRST SEARCH 6/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TAKING CS C OURSES
Take the following courses – but one per
semester
122150210213251451410359412859712750
What are some possible orders?
DEPTH -FIRST SEARCH 7/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TOPOLOGICAL SORTING
This problem is known as topological sorting.
/trianglerightsldPut vertices in a linear order that respects the graph
precedence relationships.
How can we know if a schedule is even
possible?
/trianglerightsldThere should be no cycles!
Both these problems can be solved by
depth-ﬁrst search (DFS)
/trianglerightsldDFS looks at any edge at most twice.
DEPTH -FIRST SEARCH 8/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DFS VSBFS
BFS
Explores vertices one
level at a time.
/trianglerightsldIncreases breadth
/trianglerightsldNo backtracking
Can solve/generate
/trianglerightsldreachability
/trianglerightsldconnectedness
/trianglerightsldspanning tree
Not suitable for
topological sortDFS
Explores vertices one
vertex at a time.
/trianglerightsldIncreases depth
/trianglerightsldBacktracking when it
can’t go deeper
Can solve/generate
/trianglerightsldreachability
/trianglerightsldconnectedness
/trianglerightsldspanning tree
Not suitable for
shortest unweighted
path
DEPTH -FIRST SEARCH 9/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DFS VS. BFS
s123456s123456s12BFSDFS3
DEPTH -FIRST SEARCH 10/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DFS VS. BFS
s123456s123456s12BFSDFS3
DEPTH -FIRST SEARCH 11/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEDFS A LGORITHM
:funDFS (G,s) =let
: funDFS/prime(X,v) =
: if(v∈X)
TOUCH v: then X
: else let
ENTER v: valX/prime=X∪{v}
: valX/prime/prime=iter DFS/primeX/prime(NG(v))
EXIT v: inX/prime/primeend
:inDFS/prime({},s)end
Each iter does a mapping of the sort f:α×β→α
S=s0
foreach a∈A:
S=f(S,a)
return SDEPTH -FIRST SEARCH 12/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SOME OBSERVATIONS
iter goes sequentially
/trianglerightsldSets are unordered, ordering depends on
implementation!
When a vertex vis entered (ENTER v) in code
/trianglerightsldit picks the “ﬁrst” outgoing edge ( v,w1)
/trianglerightsldthrough iter calls DFS/prime(X∪{v},w1)
When DFS/prime(X∪{v},w1)returns
/trianglerightsldAll vertices reachable from w1are explored
/trianglerightsldVertex set returned is
X1=X∪{v}∪{ All vertices reachable from w1}
iter picks next edge (v,w2)and continues
When iter is done
X/prime/prime=X∪{v}∪{ All vertices reachable from v}
DEPTH -FIRST SEARCH 13/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TOUCHING , ENTERING AND EXITING
:funDFS (G,s) =let
: funDFS/prime(X,v) =
: if(v∈X)
TOUCH v: then X
: else let
ENTER v: valX/prime=X∪{v}
: valX/prime/prime=iter DFS/primeX/prime(NG(v))
EXIT v: inX/prime/primeend
:inDFS/prime({},s)end
We try to visit a vertex v
We process vand its outgoing edges.
We are done with v.
DEPTH -FIRST SEARCH 14/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DFS WITH PARALLELISM
Can we do all outgoing edges in parallel?
/trianglerightsldY es - if parallel searches never meet up (then we
really have a tree!)
/trianglerightsldNo - otherwise.
abcdesabcdes
DEPTH -FIRST SEARCH 15/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COST OF DFS
LEMMA
For a graph G= (V,E)withmout edges and n
vertices:
DFS’ will be called at most mtimes
There will be at most min (n,m)“enters”.
v∈Xcan fail at most mtimes.
we make call to DFS’, when we have an edge
(total mtimes)
/trianglerightsldBut we can enter a vertex a most once per DFS’
So number of enters ≤min(n,m)
DEPTH -FIRST SEARCH 16/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COST OF DFS
COROLLARY
TheDFSalgorithm on a graph with mout edges, and
nvertices, and using the tree-based cost
speciﬁcation for sets, runs in O(mlogn)work and
span.
Using ST sequences reduces work and span to
O(m)
DEPTH -FIRST SEARCH 17/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CYCLE DETECTION IN UNDIRECTED
GRAPHS
DFS’ arrives at va second time and this time
from u. What can we conclude?
/trianglerightsldThere must be two paths between uandv! (Why?)
Not really! In undirected graphs cycles should
have length at least 3.
U
VDFSxy
U
VDFS
DEPTH -FIRST SEARCH 18/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CYCLE DETECTION IN UNDIRECTED
GRAPHS
:funundirectedCycle (G,s) =let
:funDFS/primep((X,C),v) =
: if(v∈X)
TOUCH v: then (X,true )
: else let
ENTER v: valX/prime=X∪{v}
: val (X/prime/prime,C/prime) =iter (DFS/primev) (X/prime,C) (NG(v)\{p})
EXIT v: in(X/prime/prime,C/prime)end
:inDFS/primes({},false ),s)end
Ckeeps tracks of cycles.
pis the parent – removed from neighbors and curried!
DEPTH -FIRST SEARCH 19/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TOPOLOGICAL SORTING
Order the vertices so that the ordering respects
reachability.
/trianglerightsldIfuis reachable from v,vmust come earlier in the
ordering.
122150210213251451410359412859712750
DEPTH -FIRST SEARCH 20/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARTIAL ORDERS
A DAG deﬁnes a partial order on the vertices.
For vertices a,b∈V,a≤pbif and only if there
is a directed path from atob
Partial order is a relation ≤pthat obeys
1reﬂexivity — a≤pa,
2antisymmetry — if a≤pbandb≤pa, then b=a,
and
3transitivity — if a≤pbandb≤pcthen a≤pc.
DEPTH -FIRST SEARCH 21/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TOPOLOGICAL SORT
If≤tis the total ordering then
a≤pb⇒a≤tb
but not reverse is not necessarily true!
abcdefgh
a≤tb≤tc≤td≤te≤tf≤tg≤th
DEPTH -FIRST SEARCH 22/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TOPOLOGICAL SORT WITH DFS
Augment with a new source vertex s
G= (V,E)→G/prime= (V∪{s},E∪{(s,v) :v∈V})
Why do we need to do this?
:funtopSort (G= (V,E)) = let
: vals=a new vertex
: valG/prime= (V∪{s},E∪{(s,v) :v∈V})
: funDFS/prime((X,L),v) =
: if(v∈X)
TOUCH v: then (X,L)
: else let
ENTER v: valX/prime=X∪{v}
: val (X/prime/prime,L/prime) =iter DFS/prime(X/prime,L) (NG/prime(v))
EXIT v: in(X/prime/prime,v::L/prime)end
:inDFS/prime(({},[]),s)end
DEPTH -FIRST SEARCH 23/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TOPOLOGICAL SORT WITH DFS
THEOREM
On a DAG when exiting a vertex vinDFSall vertices
reachable from vhave already exited.
Assume uis reachable from v.
/trianglerightslduis entered before v.umust exit before vis entered
(otherwise there is a cycle!)
/trianglerightsldvis entered before u.uwill exit ﬁrst.
vux
vux
DEPTH -FIRST SEARCH 24/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CYCLE DETECTION IN DIRECTED
GRAPHS
Important preprocessing step in Topological
Sort
/trianglerightsldTopological sort will return garbage when graph has
cycles.
We augment the graph with a node swith an
edge to every other vertex the graph.
/trianglerightsldThis can not add cycles. Nothing comes into s.
DEPTH -FIRST SEARCH 25/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CYCLE DETECTION IN DIRECTED
GRAPHS
:fundirectedCycle (G= (V,E)) = let
: vals=a new vertex
: valG/prime= (V∪{s},E∪{(s,v) :v∈V})
: funDFS/prime((X,Y,C),v) =
: if(v∈X)
TOUCH v: then (X,Y,v∈?Y)
: else let
ENTER v: valX/prime=X∪{v}
: valY/prime=Y∪{v}
: val (X/prime/prime,Y/prime/prime,C/prime) =iter DFS/prime(X/prime,Y/prime,C) (NG/prime(v))
EXIT v: in(X/prime/prime,Y/prime/prime\{v},C/prime)end
: val (,,C) =DFS/prime(({},{},false ),s)
:inCend
DEPTH -FIRST SEARCH 26/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BACK EDGES IN A DFS S EARCH
A back edge goes from a vertex vto an
ancestor uin the DFS tree.
THEOREM
A directed graph G= (V,E)has a cycle if and only if
forG/prime= (V∪{s},E∪{(s,v) :v∈V})aDFSfrom s
has a back edge.
DEPTH -FIRST SEARCH 27/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GENERALIZING DFS
All DFS code seem very much alike.
They do work on
/trianglerightsldTouching,
/trianglerightsldEntering, and
/trianglerightsldExiting
We need to keep some state σaround
/trianglerightsldand update it appropriately!
Σ0 :α
touch :α×vertex×vertex→α
enter :α×vertex×vertex→α
exit :α×vertex×vertex→α
Each function takes a state, the current vertex v
and the parent vertex p.DEPTH -FIRST SEARCH 28/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GENERIC DFS A LGORITHM
1funDFS (G,Σ0,s) =let
2 funDFS/primep((X,Σ),v) =
3 if(v∈X)then (X,touch (Σ,v,p))
4 else let
5 val Σ/prime=enter (Σ,v,p)
6 val (X/prime,Σ/prime/prime) =iter (DFS/primep) (X∪{v},Σ/prime)N+
G(v)
7 val Σ/prime/prime/prime=exit (Σ,Σ/prime/prime,v,p)
8 in(X/prime,Σ/prime/prime/prime)end
9inDFS/primes(({},Σ0),s)end
DEPTH -FIRST SEARCH 29/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
UNDIRECTED CYCLE DETECTION
1funDFS (G,Σ0,s) =let
2 funDFS/primep((X,Σ),v) =
3 if(v∈X)then (X,touch (Σ,v,p))
4 else let
5 val Σ/prime=enter (Σ,v,p)
6 val (X/prime,Σ/prime/prime) =iter (DFS/primep) (X∪{v},Σ/prime)N+
G(v)
7 val Σ/prime/prime/prime=exit (Σ,Σ/prime/prime,v,p)
8 in(X/prime,Σ/prime/prime/prime)end
9inDFS/primes(({},Σ0),s)end
Σ0= ([s],false ) :vertex list×bool
funtouch ((Lash::T,fl),v,p) = (L,h/negationslash=p)
funenter ((L,fl),v,p) = (v::L,fl)
funexit ((Lash::T,fl),v,p) = (T,fl)
DEPTH -FIRST SEARCH 30/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TOPOLOGICAL SORT
1funDFS (G,Σ0,s) =let
2 funDFS/primep((X,Σ),v) =
3 if(v∈X)then (X,touch (Σ,v,p))
4 else let
5 val Σ/prime=enter (Σ,v,p)
6 val (X/prime,Σ/prime/prime) =iter (DFS/primep) (X∪{v},Σ/prime)N+
G(v)
7 val Σ/prime/prime/prime=exit (Σ,Σ/prime/prime,v,p)
8 in(X/prime,Σ/prime/prime/prime)end
9inDFS/primes(({},Σ0),s)end
Σ0= [] : vertex list
funtouch (L,v,p) =L
funenter (L,v,p) =L
funexit (L,v,p) =v::L
DEPTH -FIRST SEARCH 31/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIRECTED CYCLE DETECTION
1funDFS (G,Σ0,s) =let
2 funDFS/primep((X,Σ),v) =
3 if(v∈X)then (X,touch (Σ,v,p))
4 else let
5 val Σ/prime=enter (Σ,v,p)
6 val (X/prime,Σ/prime/prime) =iter (DFS/primep) (X∪{v},Σ/prime)N+
G(v)
7 val Σ/prime/prime/prime=exit (Σ,Σ/prime/prime,v,p)
8 in(X/prime,Σ/prime/prime/prime)end
9inDFS/primes(({},Σ0),s)end
Σ0= ({},false ) :Set×bool
funtouch ((S,fl),v,p) = (S,v∈?S)
funenter ((S,fl),v,p) = (S∪{v},fl)
funexit ((S,fl),v,p) = (S\{v},fl)
DEPTH -FIRST SEARCH 32/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DFS W ITHST S EQUENCES
1funDFS (G:(int seq) seq ,s:int ) =
2let
3 funDFS/primep((X:bool stseq ,Σ),v:int ) =
4 if(X[v])then (X,touch (Σ,v,p)
5 else let
6 valX/prime=update (v,true,X)
7 val Σ/prime=enter (Σ,v,p)
8 val (X/prime/prime,Σ/prime/prime) =iter (DFS/primev) (X/prime,Σ/prime) (G[v])
9 in(X/prime/prime,exit (Σ/prime/prime,v,p))
10 valXinit=stSeq.fromSeq (/angbracketleftfalse :v∈/angbracketleft0,...,|G|−1/angbracketright/angbracketright)
11 in
12 stSeq.toSeq (DFS/prime((Xinit,Σ0),s))
13 end
O(m)work and span.
DEPTH -FIRST SEARCH 33/33
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 13
SHORTEST WEIGHTED PATHS
SYNOPSIS
Representing weighted graphs.
Priority-ﬁrst Search
Shortest weighted paths
Dijkstra’s Algorithm
SHORTEST WEIGHTED PATHS 2/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
WEIGHTED GRAPH
REPRESENTATION
G= (V,E,w)where w:E→eVal
eVal is a set (type) of possible values
/trianglerightsldTypically real numbers, but could be anything!
Table of (edge/mapsto→weight ).
W={(0,1)/mapsto→0.7,(1,2)/mapsto→−2.0,(0,2)/mapsto→1.5}
We could use find W e to ﬁnd w(e).
SHORTEST WEIGHTED PATHS 3/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
WEIGHTED GRAPH
REPRESENTATION
Table of (vertex/mapsto→table of (vertex/mapsto→weight ))
G={0/mapsto→{1/mapsto→0.7,2/mapsto→1.5},1/mapsto→{2/mapsto→−2.0},2/mapsto→{}}.
With one lookup, we can get to the neighbors
and weights.
We will mostly use this representation.
SHORTEST WEIGHTED PATHS 4/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PRIORITY -FIRST SEARCH
Generalization of BFS and DFS – also called
best-ﬁrst search
Visits vertices in some priority order
/trianglerightsldStatic - decided ahead of time
/trianglerightsldDynamic – decided on the ﬂy– while things change
during the search
SHORTEST WEIGHTED PATHS 5/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PRIORITY -FIRST SEARCH
1funpfs (G,s) =let
2 funpfs’ (X,F) =
3 if(F={})then X
4 else let
5 valM=highest priority vertices in F
6 valX/prime=X∪M
7 valF/prime= (F∪N(M))\X/prime
8 inpfs’ (X/prime,F/prime)end
9inpfs’ ({},{s})end
SHORTEST WEIGHTED PATHS 6/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PRIORITY -FIRST SEARCH
Several famous graph algorithms are instances
of priority-ﬁrst search.
/trianglerightsldDijkstra’s Algorithm for single-source shortest paths
(SSSP).
/trianglerightsldPrim’s Algorithm for minimum spanning trees (MST).
PFS is a greedy algorithm.
/trianglerightsldIt greedily adds vertices from the frontier based on a
cost function.
/trianglerightsldIt never backs up!
SHORTEST WEIGHTED PATHS 7/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SHORTEST WEIGHTED PATHS
G= (V,E,w)withw:E→R
/trianglerightsldw(u,v) =∞if(u,v)/negationslash∈E
The weight of a path is the sum of the weights of
the edges on it.
THESSSP P ROBLEM
Given a graph Gand a source vertex s, ﬁnd the
shortest weighted path to every other vertex.
/trianglerightsldδG(u,v)is the weight of the shortest path from utov
SHORTEST WEIGHTED PATHS 8/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SALGORITHM
Dijkstra’s Algorithm solves SSSP when the
weights are non-negative ( w:E→R +∪{0}).
/trianglerightsldGreedy
/trianglerightsldFinds optimal solutions to a nontrivial task.
Why do we need a new algorithm? Why not use
BFS?
113sab
sabcd11122
SHORTEST WEIGHTED PATHS 9/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
OBSERVATIONS
Which vertices can we deﬁnitely claim we know
the shortest path from s?
/trianglerightsldsitself (Why?)
/trianglerightsldThe vertex vnearest to s(Why?)
In general
/trianglerightsldif we know the shortest path distances to a set of
vertices
/trianglerightsldhow can we determine the shortest path to another
vertex?
SHORTEST WEIGHTED PATHS 10/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SALGORITHM
At any point in time we know the exact shortest
path weight from s
/trianglerightsldto vertices in X⊂V(s∈X), and
/trianglerightsldto some vertex v∈T(=V\X)that is closest to
some vertex in X
based on paths going through only vertices in X.
Thus, expand Xby considering only the nearest
neighbors of the vertices visited!
DeﬁneδG,X(s,v)to be the shortest path length
from stovinGthat only goes through
vertices in X(except for v)
SHORTEST WEIGHTED PATHS 11/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SPROPERTY
Consider a graph
/trianglerightsldG= (V,E),w:E→R +∪{0},
/trianglerightslda source vertex s∈V
For any partitioning of the vertices VintoXand
T=V\Xwiths∈X,
min
t∈TδG,X(s,t) =min
t∈TδG(s,t).
What is this assertion saying?
/trianglerightsldThe actual shortest distance to the vertex in Tthat is
closest to s, has to go through vertices in X!
SHORTEST WEIGHTED PATHS 12/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SPROPERTY
16  XvX vT !vm T 
Consider
/trianglerightslda vertex vm∈Tsuch thatδG(s,vm) =min t∈TδG(s,t),
and
/trianglerightslda shortest path from stovminG.
The path must cross from XtoTat some point
using some edge (vX,vT).
/trianglerightsld(Preﬁx) Subpaths of shortest paths are also shortest
paths! (Why?)
SHORTEST WEIGHTED PATHS 13/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SPROPERTY
16  XvX vT !vm T 
The subpath from stovTis the shortest path to
vT
Since edges weights are ≥0
δG(s,vT)≤δG(s,vm)
/trianglerightsldIt could be that vT=vm.
Also,δG,X(s,vT) =δG(s,vT)(Why?)
SHORTEST WEIGHTED PATHS 14/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SPROPERTY
16  XvX vT !vm T 
min
t∈TδG,X(s,t)≤δG,X(s,vT)=δG(s,vT)≤δG(s,vm)=min
t∈TδG(s,t).
But
min
t∈TδG,X(s,t)≥min
t∈TδG(s,t)Why ?
⇒min
t∈TδG,X(s,t) =min
t∈TδG(s,t).
SHORTEST WEIGHTED PATHS 15/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SPROPERTY
We can ﬁnd the shortest path to a node in
T=V\X, by just considering neighbors of X.
Pick a vertex t∈Tthat minimizes priority
δG,X(s,t).
At that point
/trianglerightsldδG,X(s,t)becomesδG(s,t)- we now know the exact
shortest path length to t.
/trianglerightsldX=X∪{t}, that is, tis now visited.
/trianglerightsldT=T\{t}
/trianglerightsldδG,X(s,u)where u∈Tand (t,u)∈Emust be
updated. (Why?/How?)
SHORTEST WEIGHTED PATHS 16/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SPROPERTY – UPDATES
XstvTuxw(t, u)∂G,X(s, u)BeforeXstvTuxw(t, u)new ∂G,X(s, u)= minx∈X(∂G(s, x)+w(x,u)) After∂G,X(s, t)
∂G(s, t)
SHORTEST WEIGHTED PATHS 17/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SALGORITHM
Given a weighted graph G= (V,E,w)and a
source s, Dijkstra’s algorithm is priority ﬁrst
search on G
/trianglerightsldstarting at s, with d(s) =0 (and d(v) =∞,v/negationslash=s)
/trianglerightsldusing priority P(v) =min t∈V(d(t) +w(t,v))(to be
minimized)
/trianglerightsldsetting d(v) =P(v)when vis visited.
SHORTEST WEIGHTED PATHS 18/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SALGORITHM
LEMMA
When Dijkstra’s Algorithm returns d(v) =δG(s,v)
for all reachable v,
Base case is true: d(s) =0.
Assume true for|X|=i, then add vertex that
minimizes P(v) =δG,X(s,v).
By Dijkstra’s Property we know
min v∈TδG,X(s,v) =min v∈TδG(s,v)
Sod(v) =δG(s,v)for|X|=i+1.
SHORTEST WEIGHTED PATHS 19/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SALGORITHM
1fundijkstra (G,s) =
2let
3 fundijkstra/prime(X,Q) =
4 case PQ.deleteMin (Q)of
5 (NONE ,)⇒X
6|(SOME (d,v),Q/prime)⇒
7 if( (v,)∈X)then dijkstra/prime(X,Q/prime)
8 else let
9 valX/prime=X∪{(v,d)}
10 funrelax (Q,(u,w)) = PQ.insert (d+w,u)Q
11 valQ/prime/prime=iter relax Q/primeNG(v)
12 indijkstra/prime(X/prime,Q/prime/prime)end
13 in
14 dijkstra/prime({},PQ.insert (0,s){})
15 end
SHORTEST WEIGHTED PATHS 20/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA VARIANTS
Update the neighbors in the priority queue
instead of adding duplicates.
/trianglerightsldPQ needs to support decreaseKey function.
Visit all equally closest vertices in parallel (like
BFS)
/trianglerightsldPotentially not much parallelism!
/trianglerightsldPQ needs to return all such vertices.
SHORTEST WEIGHTED PATHS 21/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA IN ACTION
Dijkstras algorithm 
s f a b d e c g 4 2 5 1 1 1 5 2 0 
1    X            Q         0↦s 
SHORTEST WEIGHTED PATHS 22/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA IN ACTION
Dijkstras algorithm 
3 s f a b d e c g 4 2 5 1 1 1 5 2 0    X         s↦0    Q         2↦b 4↦c 5↦a 
SHORTEST WEIGHTED PATHS 23/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA IN ACTION
Dijkstras algorithm 
5 s f a b d e c g 4 2 5 1 1 1 5 2 0    X         s↦0 b↦2    Q         3↦d 4↦c 5↦a 7↦e 
SHORTEST WEIGHTED PATHS 24/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA IN ACTION
Dijkstras algorithm 
7 s f a b d e c g 4 2 5 1 1 1 5 2 0    X         s↦0 b↦2 d↦3    Q         4↦c 4↦a 5↦a 7↦e 
SHORTEST WEIGHTED PATHS 25/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA IN ACTION
Dijkstras algorithm 
9 s f a b d e c g 4 2 5 1 1 1 5 2 0    X         s↦0 b↦2 d↦3 c↦4    Q         4↦a 5↦a 5↦b 6↦f 7↦e 
SHORTEST WEIGHTED PATHS 26/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA IN ACTION
Dijkstras algorithm 
11  s f a b d e c g 4 2 5 1 1 1 5 2 0    X         s↦0 b↦2 d↦3 c↦4 a↦4    Q         5↦a 5↦b 6↦f 7↦e 
SHORTEST WEIGHTED PATHS 27/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA IN ACTION
Dijkstras algorithm 
13  s f a b d e c g 4 2 5 1 1 1 5 2 0    X         s↦0 b↦2 d↦3 c↦4 a↦4 f↦6    Q         6↦e 7↦e 
SHORTEST WEIGHTED PATHS 28/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA IN ACTION
Dijkstras algorithm 
15     X         s↦0 b↦2 d↦3 c↦4 a↦4 f ↦6 e↦6 s f a b d e c g 4 2 5 1 1 1 5 2 0    Q         
SHORTEST WEIGHTED PATHS 29/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DIJKSTRA ’SALGORITHM
1fundijkstra (G,s) =
2let
3 fundijkstra/prime(X,Q) =
4 case PQ.deleteMin (Q)of
5 (NONE ,)⇒X
6|(SOME (d,v),Q/prime)⇒
7 if( (v,)∈X)then dijkstra/prime(X,Q/prime)
8 else let
9 valX/prime=X∪{(v,d)}
10 funrelax (Q,(u,w)) = PQ.insert (d+w,u)Q
11 valQ/prime/prime=iter relax Q/primeNG(v)
12 indijkstra/prime(X/prime,Q/prime/prime)end
13 in
14 dijkstra/prime({},PQ.insert (0,s){})
15 end
SHORTEST WEIGHTED PATHS 30/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTANALYSIS
Priority Queue with O(logn)work and span.
Graphs: tree-based table or arrays
Table of distances: tree-based table, array
sequence, or ST sequence.
Operation Line # of calls PQ Tree Table Array ST Array
deleteMin 4 O(m) O(logm) - - -
insert 10 O(m) O(logm) - - -
Priority Q total O(mlogm) - - -
find 7 O(m) - O(logn) O(1) O(1)
insert 9 O(n) - O(logn) O(n) O(1)
Distances total - O(mlogn) O(n2) O(m)
NG(v) 11 O(n) - O(logn) O(1) -
iter 11 O(m) - O(1) O(1) -
Graph access total - O(m+nlogn)O(m) -
Using a tree table work is O(mlogn).
SHORTEST WEIGHTED PATHS 31/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Representing weighted graphs.
Priority-ﬁrst Search
Shortest weighted paths
Dijkstra’s Algorithm
SHORTEST WEIGHTED PATHS 32/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 14
SHORTEST WEIGHTED PATHS –II
SYNOPSIS
Graphs with negative edge weights.
Bellman Ford Algorithm
Cost Analysis
SHORTEST WEIGHTED PATHS –II 2/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPHS WITH NEGATIVE WEIGHTS
sa
b32-2
What is a problem with this graph?
SHORTEST WEIGHTED PATHS –II 3/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPHS WITH NEGATIVE WEIGHTS
0 s b a 3 2 - 2 ∞ 0 s b a 3 2 - 2 3 2 0 s b a 3 2 - 2 0 2 ∞ 
Dijkstra fails! (Why?)
SHORTEST WEIGHTED PATHS –II 4/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPHS WITH NEGATIVE WEIGHTS
V3V1V6V4V2V7V5425213-1061162
What is a problem with this graph?
SHORTEST WEIGHTED PATHS –II 5/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPHS WITH NEGATIVE WEIGHTS
V3V1V6V4V2V7V5425213-1061162
Negative cost cycle!
There is no shortest path from v3tov5
We need to detect such cycles!SHORTEST WEIGHTED PATHS –II 6/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPHS WITH NEGATIVE WEIGHTS
Currency Exchange Arbitrage
USDQARTL
EURBPYEN3.650.50.01…….
0.871420.451.3
100 USD→365 QAR→177.5 TL→80.68 EUR
→104.9 USD
/trianglerightsldY ou just made 5 USD out of thin air!
SHORTEST WEIGHTED PATHS –II 7/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPHS WITH NEGATIVE WEIGHTS
I have USDs but I want to buy BPs.
/trianglerightsldI can buy directly, or
/trianglerightsldI can buy through some intermediate currencies!
USDBP1.51
Which way will get me more BPs?
I need to do this fast!
SHORTEST WEIGHTED PATHS –II 8/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SHORTEST PATHS
How does this problem relate to the shortest
problem?
/trianglerightsldWhere are the negative weights?
USDQARTL
EURBPYEN-1.290.694.60…….
0.14-4.950.80-0.26
Weights are−log of the exchange rates!
SHORTEST WEIGHTED PATHS –II 9/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SHORTEST PATHS WITH NEGATIVE
WEIGHTS
Deﬁneδl
G(s,t)the shortest weighted path from s
totusing at most ledges.
/trianglerightsldso the unweighted path length is l!
Base cases:
/trianglerightsldδ0
G(s,s) =0
/trianglerightsldδ0
G(s,v) =∞for all v/negationslash=s.
Induction
δk+1(v) = min
x∈N−(v)(δk(x) +w(x,v)).
Minimum of δk(x) +w(x,v)over the
in-neighbors.
SHORTEST WEIGHTED PATHS –II 10/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEBELLMAN FORD ALGORITHM
1funBellmanFord (G= (V,E),s) =
2let
3 fun BF(D,k) =
4 let
5 valD/prime={v/mapsto→minu∈N−
G(v)(Du+w(u,v)) :v∈V}
6 in
7 if(k=|V|)then⊥
8 else if (all{Dv=D/prime
v:v∈V})then D
9 else BF(D/prime,k+1)
10 end
11 valD={v/mapsto→ifv=sthen 0else∞:v∈V}
12 inBF(D,0)end
SHORTEST WEIGHTED PATHS –II 11/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
HOWBELLMAN FORD ALGORITHM
WORKS
s c b d a 3 2 - 2 1 1 1 0 ∞ ∞ ∞ ∞ path lengths = 0!
SHORTEST WEIGHTED PATHS –II 12/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
HOWBELLMAN FORD ALGORITHM
WORKS
0 s c b d a 3 2 - 2 1 1 1 ∞ ∞ 3 2 path lengths ≤ 1"
SHORTEST WEIGHTED PATHS –II 13/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
HOWBELLMAN FORD ALGORITHM
WORKS
0 s c b d a 3 2 - 2 1 1 1 0 2 path lengths ≤ 2"4 3 
SHORTEST WEIGHTED PATHS –II 14/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
HOWBELLMAN FORD ALGORITHM
WORKS
0 s c b d a 3 2 - 2 1 1 1 0 2 path lengths ≤ 3"1 3 
SHORTEST WEIGHTED PATHS –II 15/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
HOWBELLMAN FORD ALGORITHM
WORKS
0 s c b d a 3 2 - 2 1 1 1 0 2 path lengths ≤ 4"1 2 
SHORTEST WEIGHTED PATHS –II 16/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BELLMAN FORD CORRECTNESS
THEOREM
Given a directed weighted graph G= (V,E,w),
w:E→R, and a source s, theBellmanFord
algorithm returns the shortest path length from s
to every vertex orindicates that there is a
negative weight cycle in Greachable from s.
SHORTEST WEIGHTED PATHS –II 17/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BELLMAN FORD CORRECTNESS
Use induction on the the number of edges kin
the path.
Base case is correct, Ds=0.
On each step, for all v∈V\{s}, a shortest path
with at most k+1 edges
/trianglerightsldmust consist of a path of at most kedges for vertex u
/trianglerightsldfollowed by a single edge (u,v).
Taking the minimum combination, gives us the
shortest path with at most k+1 edges.
SHORTEST WEIGHTED PATHS –II 18/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
NEGATIVE COSTCYCLES
This can go at most for n=|V|−1 rounds
If we reach round n, there must be reachable
negative cost cycle.
Otherwise, Bellman Ford will stop earlier with all
simple shortest paths.
SHORTEST WEIGHTED PATHS –II 19/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTANALYSIS
Graph represented as a table.
/trianglerightsld(RvtxTable )vtxTable , where ﬁrst vtxTable
maps vertices to their in-neighbors
G={0/mapsto→{1/mapsto→0.7,2/mapsto→1.5},1/mapsto→{2/mapsto→−2.0},2/mapsto→{}} .
Graph represented as a sequence of sequences.
/trianglerightsld((int×eVal )seq )seq
SHORTEST WEIGHTED PATHS –II 20/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BELLMAN - FORD ALGORITHM
(AGAIN )
1funBellmanFord (G= (V,E),s) =
2let
3 fun BF(D,k) =
4 let
5 valD/prime={v/mapsto→minu∈N−
G(v)(Du+w(u,v)) :v∈V}
6 in
7 if(k=|V|)then⊥
8 else if (all{Dv=D/prime
v:v∈V})then D
9 else BF(D/prime,k+1)
10 end
11 valD={v/mapsto→ifv=sthen 0else∞:v∈V}
12 inBF(D,0)end
Line 5 is tabulate over the vertices
Line 8 is tabulate with a reduction over the vertices
SHORTEST WEIGHTED PATHS –II 21/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTANALYSIS
valD/prime={v/mapsto→minu∈N−
G(v)(Du+w(u,v)) :v∈V}
Sum work and max span over vertices.
n=|V|andm=|E|
For each vertex we have the following costs:
/trianglerightsldFind the neighbors find G v :O(logn)work and
span.
/trianglerightsldMap over neighbors – ﬁnd distance Duand add:
O(logn)work and span for each uin the
in-neigborhood.
/trianglerightsldMin reduce: O(1+NG(v)|)work and O(log|NG(v)|)
span.
SHORTEST WEIGHTED PATHS –II 22/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
WORK PER STAGE -1
valD/prime={v/mapsto→minu∈N−
G(v)(Du+w(u,v)) :v∈V}
Operation Over one vertex v Over graph G
Find O(logn) O(nlogn)
Map O(1+|N−
G(v)|logn)O(n+mlogn)
Min Reduce O(1+|N−
G(v)|) O(n+m)
Total work is O((n+m)logn)and assuming
m>n,O(mlogn)
SHORTEST WEIGHTED PATHS –II 23/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SPAN PER STAGE -1
valD/prime={v/mapsto→minu∈N−
G(v)(Du+w(u,v)) :v∈V}
Operation Over one vertex v Over graph G
Find O(logn) O(logn)
Map O(1+logn) O(1+logn)
Min Reduce O(log|N−
G(v)|) O(logn)
Total span is O(logn)
SHORTEST WEIGHTED PATHS –II 24/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
WORK / SPAN PER STAGE - 2 –
TOTAL COST
else if (all{Dv=D/prime
v:v∈V})then D
This involves a tabulate and an and-reduction.
Work =O(nlogn), Span =O(logn)
nsequential calls to BF, so total costs are:
W(n,m)=O(n·mlogn)
S(n,m)=O(nlogn)
SHORTEST WEIGHTED PATHS –II 25/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTS WITH ST S EQUENCES
We use IL (integer labeled) graphs.
find→nth:O(1)work.
Similar improvements for looking up neighbors
and distance table.
W(n,m) = O(nm)
S(n,m) = O(n)
SHORTEST WEIGHTED PATHS –II 26/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY
Graphs with negative edge weights.
Bellman Ford Algorithm
Analysis
SHORTEST WEIGHTED PATHS –II 27/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 15
PROBABILITY AND RANDOMIZED ALGORITHMS
SYNOPSIS
Overview of Discrete Probability
Finding the two largest elements
Find the kthsmallest element.
PROBABILITY AND RANDOMIZED ALGORITHMS 2/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RANDOMIZED ALGORITHMS
Exploit randomness during computation
/trianglerightsldPivot selection in Quicksort
/trianglerightsldAverage case analysis
/trianglerightsldPrimality testing
Question: How many comparisons are needed to
ﬁnd the second largest number on a sequence of
nnumbers?
/trianglerightsldNaive algorithm: 2 n−3 comparisons
/trianglerightsldDivide and Conquer algorithm: 3 n/2 comparisons
/trianglerightsldSimple randomized algorithm: n−1+2 log n
comparisons on the average .
PROBABILITY AND RANDOMIZED ALGORITHMS 3/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
OVERVIEW OF DISCRETE
PROBABILITY
Probabilistic Experiment: outcome is
probabilistic.
Sample Space ( Ω): arbitrary and possibly
countably inﬁnite set of possible outcomes.
/trianglerightsldTossing a coin
/trianglerightsldThrowing a die/pair of dice.
Primitive Event: Any one of the elements of Ω.
Event: Any subset of Ω
/trianglerightsldFirst die is a 5
/trianglerightsldDice sum to 7
/trianglerightsldAny die is even.
PROBABILITY AND RANDOMIZED ALGORITHMS 4/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROBABILITY FUNCTION
Probability Function: Ω→[0,1]
/summationdisplay
e∈ΩPr[e] =1
Probability of an event A:
/summationdisplay
e∈APr[e]
/trianglerightsldProbability of “ﬁrst die is 4”?
/trianglerightsldProbability of “dice sum to to 4”?
PROBABILITY AND RANDOMIZED ALGORITHMS 5/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RANDOM VARIABLES
Random Variable: X: Ω→/Rfractur
/trianglerightsldXis the sum of the two die rolls
Indicator Random Variable: Y: Ω→{0,1}
/trianglerightsldYis 1 if the dice are the same, 0 otherwise
/trianglerightsldYis 1 if the total is larger than 7, 0 otherwise
Fora∈/Rfractur, the event “ X=a” is the set
{ω∈Ω|X(ω) =a}
PROBABILITY AND RANDOMIZED ALGORITHMS 6/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTATION
The expectation of a random variable
E
Ω,Pr[][X] =/summationdisplay
e∈ΩX(e)·Pr[e].
The expectation of an indicator random variable:
E[Y] =/summationdisplay
e∈Ω,p(e)=truePr[e] =/summationdisplay
e∈ΩPr[{e∈Ω|p(e)}].
/trianglerightsldp: Ω→bool
PROBABILITY AND RANDOMIZED ALGORITHMS 7/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
INDEPENDENCE
Events AandBare independent if the occurence
of one does not affect the probability of the other
Pr[A∩B] =Pr[A]·Pr[B]
/trianglerightsldA={(d1,d2)∈Ω|d1=1}and
B={(d1,d2)∈Ω|d2=1}are independent.
/trianglerightsldC={(d1,d2)∈Ω|d1+d2=4}is NOT independent
ofA(Why?)
PROBABILITY AND RANDOMIZED ALGORITHMS 8/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
INDEPENDENCE
Events A1,..., Akaremutually independent if
and only if for any non-empty subset
I⊆{1,..., k},
Pr[/intersectiondisplay
i∈IAi] =/productdisplay
i∈IPr[Ai].
Random variable XandYare independent if
ﬁxing one does NOT affect the probability
distribution of the other.
/trianglerightsldX=“value of the ﬁrst die” is independent of Y=
“value of the second die”.
/trianglerightsldXis NOT independent of Z=“sum of the dice”
PROBABILITY AND RANDOMIZED ALGORITHMS 9/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LINEARITY OF EXPECTATIONS
Important Theorem: given two random variables
XandY
E[X] +E[Y] =E[X+Y]
Easy to show!
/summationdisplay
e∈ΩPr[e]X(e) +/summationdisplay
e∈ΩPr[e]Y(e) =/summationdisplay
e∈ΩPr[e](X(e) +Y(e))
Expected sum of two dice
/trianglerightsldConsider 36 outcomes and take average
/trianglerightsldSum expectations for each dice (3 .5+3.5=7)
PROBABILITY AND RANDOMIZED ALGORITHMS 10/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LINEARITY OF EXPECTATIONS
In general, for a binary function fthe equality
f(E[X],E[Y]) =E[f(X,Y)]
isnottrue in general.
/trianglerightsldmax (E[X],E[Y])/negationslash=E[max (X,Y)]
/trianglerightsldWhat is E[max (X,Y)]?
E[X]×E[Y] =E[X×Y]is true if XandYare
independent.
PROBABILITY AND RANDOMIZED ALGORITHMS 11/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXAMPLES
Toss ncoins with probability of heads, p. What is
the expected value of X, the number of heads?
E[X]=n/summationdisplay
k=0k·Pr[X=k]
=n/summationdisplay
k=1k·pk(1−p)n−k/parenleftbiggn
k/parenrightbigg
(Why ?)
=n/summationdisplay
k=1k·n
k/parenleftbiggn−1
k−1/parenrightbigg
pk(1−p)n−k[because/parenleftbiggn
k/parenrightbigg
=n
k/parenleftbiggn−1
k−1/parenrightbigg
]
=nn/summationdisplay
k=1/parenleftbiggn−1
k−1/parenrightbigg
pk(1−p)n−k
PROBABILITY AND RANDOMIZED ALGORITHMS 12/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXAMPLES
Toss ncoins with probability of heads, p. What is
the expected value of X, the number of heads?
E[X]=n/summationdisplay
k=0k·Pr[X=k]
. . .
=nn−1/summationdisplay
j=0/parenleftbiggn−1
j/parenrightbigg
pj+1(1−p)n−(j+1)[ because k=j+1 ]
=n·pn−1/summationdisplay
j=0/parenleftbiggn−1
j/parenrightbigg
pj(1−p)(n−1)−j)
=n·p·(p+ (1−p))n−1[ Binomial Theorem ]
=n·p
PROBABILITY AND RANDOMIZED ALGORITHMS 13/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXAMPLES
Toss ncoins with probability of heads, p. What is
the expected value of X, the number of heads?
Using linearity of expectations.
/trianglerightsldXi=I{i-th coin turns up heads }
/trianglerightsldX=/summationtextn
i=1Xi
E[X]=E/bracketleftBiggn/summationdisplay
i=1Xi/bracketrightBigg
=n/summationdisplay
i=1E[Xi] =n/summationdisplay
i=1p=n·p
/trianglerightsldbecause E[Xi] =p.
PROBABILITY AND RANDOMIZED ALGORITHMS 14/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXAMPLES
A coin has a probability pof coming up heads.
What is the expected value of Yrepresenting the
number of ﬂips until we see a head?
Write a recurrence!
/trianglerightsldWith probability p, we’ll get a head and we are done,
/trianglerightsldWith probability 1 −p, we’ll get a tail and we’ll go
back to square one
E[Y]=p·1+ (1−p)/parenleftBig
1+E[Y]/parenrightBig
=1+ (1−p)E[Y] =⇒E[Y] =1/p.
PROBABILITY AND RANDOMIZED ALGORITHMS 15/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINDING THE TOPTWOELEMENTS
1funmax2 (S) =let
2 funreplace ((m1,m2),v) =
3 ifv≤m2then (m1,m2)
4 else if v≤m1then (m1,v)
5 else (v,m1)
6 valstart =ifS1≥S2then (S1,S2)else (S2,S1)
7initer replace start S/angbracketleft3,..., n/angbracketright
8end
We will do exact analysis.
1+2(n−2) =2n−3 comparisons in the worst case.
(Why?)
A Divide and Conquer algorithm gives 3 n/2−2
comparison. (How?)PROBABILITY AND RANDOMIZED ALGORITHMS 16/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
WORST CASEANALYSIS
1funmax2 (S) =let
2 funreplace ((m1,m2),v) =
3 ifv≤m2then (m1,m2)
4 else if v≤m1then (m1,v)
5 else (v,m1)
6 valstart =ifS1≥S2then (S1,S2)else (S2,S1)
7initer replace start S/angbracketleft3,..., n/angbracketright
8end
An already sorted sequence (e.g., /angbracketleft1,2,3,..., n/angbracketright)
will need exactly 2 n−3 comparisons.
But this happens with 1 /n!chance!
PROBABILITY AND RANDOMIZED ALGORITHMS 17/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
A R ANDOMIZED ALGORITHM
The worst-case analysis is overly pessimistic.
Consider the following variant:
On input of a sequence Sofnelements:
1LetT=permute (S,π), whereπis a random
permutation (i.e., we choose one of the n!
permutations).
2Run the na ¨ıve algorithm on T.
No need to really generate the permutation!
/trianglerightsldJust pick an unprocessed element randomly until all
elements are processed.
/trianglerightsldIt is convenient to model this by one initial
permutation!
PROBABILITY AND RANDOMIZED ALGORITHMS 18/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS
1funmax2 (S) =let
2 funreplace ((m1,m2),v) =
3 ifv≤m2then (m1,m2)
4 else if v≤m1then (m1,v)
5 else (v,m1)
6 valstart =ifS1≥S2then (S1,S2)else (S2,S1)
7initer replace start S/angbracketleft3, . . . , n/angbracketright
8end
Xi=1 ifTiis compared in Line 4, 0 otherwise.
Yis the number of comparisons
Y=1/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Line 6+n−2/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Line 3+n/summationdisplay
i=3Xi;
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
Line 4
PROBABILITY AND RANDOMIZED ALGORITHMS 19/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS
This expression in true regardless of the random
choice we’re making.
We’re interested in computing the expected value
ofY.
By linearity of expectation,
E[Y] = E/bracketleftBigg
1+ (n−2) +n/summationdisplay
i=3Xi/bracketrightBigg
=1+ (n−2) +n/summationdisplay
i=3E[Xi].
PROBABILITY AND RANDOMIZED ALGORITHMS 20/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS
Problem boils down to computing E[Xi], for
i=3,..., n!
What is the probability that Ti>m2?
/trianglerightsldTi>m2holds when Tiis either the largest or the
second largest in {T1,..., Ti}
So, what is the probability that Tiis one of the
two largest elements in a randomly permuted
sequence of length i?
/trianglerightsld1
i+1
i=2
i
E[Xi] =1·2
i=2/i
PROBABILITY AND RANDOMIZED ALGORITHMS 21/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS
E[Y] =1+ (n−2) +n/summationdisplay
i=3E[Xi]
=1+ (n−2) +n/summationdisplay
i=32
i
=1+ (n−2) +2/parenleftBig
1
3+1
4+...1
n/parenrightBig
=n−4+2/parenleftBig
1+1
2+1
3+1
4+...1
n/parenrightBig
=n−4+2Hn
Hnis the nthHarmonic number
Hn≤1+log2n
E[Y]≤n−2+2 log2n
PROBABILITY AND RANDOMIZED ALGORITHMS 22/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINDING THE kthSMALLEST
ELEMENT
Input: a sequence of nnumbers (not necessarily
sorted)
Output: thekthsmallest value in S(i.e.,(nth
(sort S) k) ).
Requirement: O (n)expected work and O(log2n)
span.
We can’t really sort the sequence!
PROBABILITY AND RANDOMIZED ALGORITHMS 23/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINDING THE kthSMALLEST
ELEMENT
1funkthSmallest (k,S) =let
2 valp=a value from Spicked uniformly at random
3 valL=/angbracketleftx∈S|x<p/angbracketright
4 valR=/angbracketleftx∈S|x>p/angbracketright
5 in if (k<|L|)then kthSmallest (k,L)
6 else if (k<|S|−|R|)then p
7 else kthSmallest (k−(|S|−|R|),R)
LetXn=max{|L|,|R|}
W(n)=W(Xn) +O(n)
S(n)=S(Xn) +O(logn)
PROBABILITY AND RANDOMIZED ALGORITHMS 24/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINDING THE kthSMALLEST
ELEMENT
We want to ﬁnd E[Xn]?
RLmax(L, R)
E[Xn] =n−1/summationdisplay
i=1max{i,n−i}·1
n≤n−1/summationdisplay
j=n/22
n·j≤3n
4
PROBABILITY AND RANDOMIZED ALGORITHMS 25/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINDING THE kthSMALLEST
ELEMENT
E[Xn]≤3n
4⇒geometrically decreasing sum
⇒O(n)work.
What is Pr[Xn≤3
4n]?
Since|R|<n−|L|,
Xn≤3
4n⇔n/4<|L|≤3n/4
and the probability is
3n/4−n/4
n=n/2
n=1
2
PROBABILITY AND RANDOMIZED ALGORITHMS 26/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINDING THE kthSMALLEST
ELEMENT
W(n)=/summationdisplay
iPr[Xn=i]·W(i) +c·n
Using stepwise approximation
≤Pr[Xn≤3n
4]W(3n/4) +Pr[Xn>3n
4]W(n) +c·n
=1
2W(3n/4) +1
2W(n) +c·n
=⇒(1−1
2)W(n) =1
2W(3n/4) +c·n
=⇒W(n)≤W(3n/4) +2c·n
Root Dominated hence solves to O(n).
PROBABILITY AND RANDOMIZED ALGORITHMS 27/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINDING THE kthSMALLEST
ELEMENT
S(n) =S(Xn) +O(logn)
S(n)=≤/summationdisplay
iPr[Xn=i]·S(i) +clogn
≤Pr[Xn≤3n
4]S(3n/4) +Pr[Xn>3n
4]S(n) +c·logn
≤1
2S(3n/4) +1
2S(n) +c·logn
=⇒(1−1
2)S(n)≤1
2S(3n/4) +clogn
=⇒S(n)≤S(3n/4) +2clogn
This solves to O(log2n).
PROBABILITY AND RANDOMIZED ALGORITHMS 28/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 16
GRAPH CONTRACTION
SYNOPSIS
Graph Contraction
Finding Connected Components
Edge Contraction
Star Contraction
GRAPH CONTRACTION 2/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MOTIVATION
Most graph search algorithms were either
/trianglerightsldsequential, or
/trianglerightsldhad span dependent on the diameter.
Can we make these algorithms more parallel?
/trianglerightsldPolylogarithmic span: span is bounded by a
polynomial in log n
We will look at contraction as a way to build
parallel algorithms for some graph problems:
/trianglerightsldGraph Connectivity
/trianglerightsldSpanning Trees
GRAPH CONTRACTION 3/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH CONNECTIVITY
Two vertices in an undirected graph are
connected if there is a path between them.
A graph is connected if all pairs of vertices are
connected.
The graph connectivity problems partitions a
graph into its maximal connected subgraphs.
abcdef
has two connected subgraphs: {a,b,c,d}and{e,f}
GRAPH CONTRACTION 4/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH CONNECTIVITY
BFS or DFS
/trianglerightsldIdentify vertices of a connected component
/trianglerightsldIdentify allconnected components!
BFS could be parallel but has span ∝diameter d
Each connected component needs to be done
sequentially!
GRAPH CONTRACTION 5/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH CONTRACTION
Problem→Smaller Problem
Shrink the size of the graph and solve the
connectivity problem on the small graph.
/trianglerightsldDifferent components can be handled in parallel!
Applicable to other problems
/trianglerightsldSpanning Trees
/trianglerightsldMinimum Spanning Trees
GRAPH CONTRACTION 6/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH CONTRACTION
contract :graph→partition
Takes a graph G(V,E)and returns a partitioning
ofVinto connected subgraphs.
/trianglerightsldNot necessarily maximally connected subgraphs (yet)
/trianglerightsldBut vertices in a partition are connected.
abcdef
abcdef
{{a,b,c},{d},{e,f}}
GRAPH CONTRACTION 7/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
GRAPH CONTRACTION
abcdefa,b,cde,fa,b,cde,fcomponents identifiedcontractedparallel edges removed
If the graph contracts on each round, eventually
each maximal connected component will shrink
down to a single vertex!
a,b,c,de,fa,b,cde,fa,b,c,d,e,f
GRAPH CONTRACTION 8/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REPRESENTING PARTITIONS
abcdef
↓
{{a,b,c},{d},{e,f}}
↓
({a,d,e},{a/mapsto→a,b/mapsto→a,c/mapsto→a,d/mapsto→d,e/mapsto→e,f/mapsto→e}).
GRAPH CONTRACTION 9/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COUNTING COMPONENTS
1funnumComponents ((V,E),i) =
2if|E|=0then|V|
3else let
4 val(V/prime,P) =contract ((V,E),i)
5 valE/prime={(P[u],P[v]) : (u,v)∈E|P[u]/negationslash=P[v]}
6in
7 numComponents ((V/prime,E/prime),i+1)
8end
Ignore ifor the time being!
V/primeis the set of representative vertices
Pmaps every v∈Vto av/prime∈V/prime.
E/primeis the set of edges in the contracted graph.
/trianglerightsldSelf-loops are removed!
GRAPH CONTRACTION 10/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COMPUTING COMPONENTS
1funcomponents ((V,E),i) =
2if|E|=0then{v/mapsto→v:v∈V}
3else let
4 val(V/prime,P) =contract ((V,E),i)
5 valE/prime={(P[u],P[v]) : (u,v)∈E|P[u]/negationslash=P[v]}
6 valP/prime=components ((V/prime,E/prime),i+1)
7in
8{v/mapsto→P/prime[P[v]] :v∈V}
9end
GRAPH CONTRACTION 11/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COMPUTING COMPONENTS
1funcomponents ((V,E),i) =
2if|E|=0then{v/mapsto→v:v∈V}
3else let
4 val(V/prime,P) =contract ((V,E),i)
5 valE/prime={(P[u],P[v]) : (u,v)∈E|P[u]/negationslash=P[v]}
6 valP/prime=components ((V/prime,E/prime),i+1)
7in
8{v/mapsto→P/prime[P[v]] :v∈V}
9end
Base case: Every vertex maps to itself!
GRAPH CONTRACTION 12/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COMPUTING COMPONENTS
1funcomponents ((V,E),i) =
2if|E|=0then{v/mapsto→v:v∈V}
3else let
4 val(V/prime,P) =contract ((V,E),i)
5 valE/prime={(P[u],P[v]) : (u,v)∈E|P[u]/negationslash=P[v]}
6 valP/prime=components ((V/prime,E/prime),i+1)
7in
8{v/mapsto→P/prime[P[v]] :v∈V}
9end
(Recursively) ﬁnd components of the contracted
graph
GRAPH CONTRACTION 13/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COMPUTING COMPONENTS
1funcomponents ((V,E),i) =
2if|E|=0then{v/mapsto→v:v∈V}
3else let
4 val(V/prime,P) =contract ((V,E),i)
5 valE/prime={(P[u],P[v]) : (u,v)∈E|P[u]/negationslash=P[v]}
6 valP/prime=components ((V/prime,E/prime),i+1)
7in
8{v/mapsto→P/prime[P[v]] :v∈V}
9end
Map each vertex to the representative vertex of
its partition!
GRAPH CONTRACTION 14/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COMPUTING COMPONENTS
1funcomponents ((V,E),i) =
2if|E|=0then{v/mapsto→v:v∈V}
3else let
4 val(V/prime,P) =contract ((V,E),i)
5 valE/prime={(P[u],P[v]) : (u,v)∈E|P[u]/negationslash=P[v]}
6 valP/prime=components ((V/prime,E/prime),i+1)
7in
8{v/mapsto→P/prime[P[v]] :v∈V}
9end
abcdef
After 4:V/prime={a,d,e}
P={a/mapsto→a,b/mapsto→a,c/mapsto→a,d/mapsto→d,e/mapsto→e,f/mapsto→e}
After 6: P/prime={a/mapsto→a,d/mapsto→a,e/mapsto→a}
8 returns:{a/mapsto→a,b/mapsto→a,c/mapsto→a,d/mapsto→a,e/mapsto→a,f/mapsto→a}
GRAPH CONTRACTION 15/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING CONTRACT
Edge Contraction:Only pairs of vertices
connected by an edge are contracted.
Star Contraction: Vertices around a “center star”
collapse to the “star”
Tree Contraction: disjoint trees within the graph
are identiﬁed and vetices in a tree are collapsed
to the root.
Parallel
Reduce graph size (vertices/edges?) by a
constant factor every round.
/trianglerightsldWill lead to O(logn)rounds!.
GRAPH CONTRACTION 16/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EDGECONTRACTION
Find disjoint edges – edges can not share
vertices.
abcdef
Vertex matching problem
Can be done in parallel
/trianglerightsldEach edge picks a random priority in [0,1]
/trianglerightsldAny edge which has highest priority for both vertices
gets selected.
It turns out this has some problems!
GRAPH CONTRACTION 17/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EDGECONTRACTION
Consider a graph like
v
How many edges can be contracted each round?
How many rounds are needed to contract to 1
node?
Not very parallel!
GRAPH CONTRACTION 18/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
STARCONTRACTION
Star subgraphs can be contracted in parallel!
How do we ﬁnd disjoint stars?
GRAPH CONTRACTION 19/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
FINDING DISJOINT STARS
Each vertex throws a coin
/trianglerightsldHeads→vertex is a star-center
/trianglerightsldTails→vertex is a potential satellite (Why potential ?)
Each satellite then selects a center.
GRAPH CONTRACTION 20/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RANDOM COINTOSSES
Pretend each vertex has a potentially inﬁnite
sequence of random coin ﬂips
heads (v,i) :vertex×int→bool provides
access to these coin tosses.
This can be implemented with a pseudorandom
number generator.
GRAPH CONTRACTION 21/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
STARCONTRACTION
1funstarContract (G= (V,E),i) =
2let
3 % select edges that go from a tail to a head
4 valTH={(u,v)∈E|¬heads (u,i)∧heads (v,i)}
5 % make mapping from tails to heads, removing duplicates
6 valP=∪(u,v)∈TH{u/mapsto→v}
7 % remove vertices that have been remapped
8 valV/prime=V\domain (P)
9 % Map remaining vertices to themselves
10 valP/prime={u/mapsto→u:u∈V/prime}∪P
11 in(V/prime,P/prime)end
abcdeHHTTTabcdeHHTTTabcdeHHTTTcoin ﬂips (heads(v,i))ﬁnd potential centers (TH)compute "hook" edges (P)
GRAPH CONTRACTION 22/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
STARCONTRACTION
abcdeHHTTTabcdeHHTTTabcdeHHTTTcoin ﬂips (heads(v,i))ﬁnd potential centers (TH)compute "hook" edges (P)
valTH={(u,v)∈E|¬heads (u,i)∧heads (v,i)}
TH={(c,a),(c,b),(e,b)}.
GRAPH CONTRACTION 23/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
STARCONTRACTION
abcdeHHTTTabcdeHHTTTabcdeHHTTTcoin ﬂips (heads(v,i))ﬁnd potential centers (TH)compute "hook" edges (P)
valP=∪(u,v)∈TH{u/mapsto→v}
TH={(c,a),(c,b),(e,b)}
P={c/mapsto→b,e/mapsto→b}
GRAPH CONTRACTION 24/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
STARCONTRACTION
abcdeHHTTTabcdeHHTTTabcdeHHTTTcoin ﬂips (heads(v,i))ﬁnd potential centers (TH)compute "hook" edges (P)
valV/prime=V\domain (P)
P={c/mapsto→b,e/mapsto→b}
domain (P) ={c,e}
V/prime={a,b,d}
GRAPH CONTRACTION 25/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
STARCONTRACTION
abcdeHHTTTabcdeHHTTTabcdeHHTTTcoin ﬂips (heads(v,i))ﬁnd potential centers (TH)compute "hook" edges (P)
valP/prime={u/mapsto→u:u∈V/prime}∪P
P={c/mapsto→b,e/mapsto→b},V/prime={a,b,d}
P/prime={a/mapsto→a,b/mapsto→b,c/mapsto→b,d/mapsto→d,e/mapsto→b}
GRAPH CONTRACTION 26/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF STARCONTRACTION
LEMMA
For a graph Gwithnnon-isolated vertices, let Xnbe the random
variable indicating the number of vertices removed by
starContract (G,). Then, E[Xn]≥n/4.
Hv: vertex vcomes up heads, Tv: vertex vcomes up tails
Rv: vertex vis removed in contraction
vhas at least one neighbor u.
Tv∧Huimplies Rv
/trianglerightsldIfvis a tail, join u/primesstar or some other star.
Pr[Rv]≥Pr[Tv]Pr[Hu] =1/4
Expected total≥n/4
GRAPH CONTRACTION 27/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF STARCONTRACTION
1funstarContract (G= (V,E),i) =
2let
3 % select edges that go from a tail to a head – O(m)work, O(1)span
4 valTH={(u,v)∈E|¬heads (u,i)∧heads (v,i)}
5 % make mapping from tails to heads, removing duplicates
6 %O(n)work, O(logn)span
7 valP=∪(u,v)∈TH{u/mapsto→v}
8 % remove vertices that have been remapped
9 %O(n)work, O(logn)span
10 valV/prime=V\domain (P)
11 % Map remaining vertices to themselves - O(n)work, O(lognspan
12 valP/prime={u/mapsto→u:u∈V/prime}∪P
13 in(V/prime,P/prime)end
nnodes, medges
O(n+m)work, O(logn)span.
GRAPH CONTRACTION 28/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF CONNECTIVITY
1funnumComponents ((V,E),i) =
2if|E|=0then|V|
3else let
4 val(V/prime,P) =starContract ((V,E),i)
5 valE/prime={(P[u],P[v]) : (u,v)∈E|P[u]/negationslash=P[v]}
6in
7 numComponents ((V/prime,E/prime),i+1)
8end
S(n) =S(n/prime) +O(logn)
n/prime=n−XnandE[Xn] =n/4, so E[n/prime] =3n/4
S(n)∈O(log2n)
GRAPH CONTRACTION 29/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF CONNECTIVITY
We can remove a constant fraction of vertices
every round.
For each vertex removed, we remove at least
one edge.
Consider a hypothetical contraction
round vertices edges
1 n m
2 n/2 m−n/2
3 n/4 m−3n/4
4 n/8 m−7n/8
Number of edges does not go below m−n.
GRAPH CONTRACTION 30/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF CONNECTIVITY
W(n,m)≤W(n/prime,m) +O(n+m),
As before, E[n/prime] =3n/4, so
E[W(n,m)]∈O(n+mlogn)
GRAPH CONTRACTION 31/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TREECONTRACTION
Identify disjoint trees and contract them.
For every tree of tvertices contracted, t−1
edges are removed.
Number of edges also go down geometrically at
every round.
Leads to O(m)work and O(log2n)span.
GRAPH CONTRACTION 32/32
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 17
NOLECTURE
SYNOPSIS
There is no lecture 17
NOLECTURE 2/2
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 18
MINIMUM SPANNING TREES
SYNOPSIS
Minimum Spanning Trees
Kruskal’s and Prim’s Algorithms
Using Star Contraction for MST
MINIMUM SPANNING TREES 2/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINIMUM SPANNING TREES
MINIMUM SPANNING TREES 3/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINIMUM SPANNING TREES
MINIMUM SPANNING TREES 4/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINIMUM SPANNING TREES
Given a connected undirected graph G= (V,E)
/trianglerightsldEach edge ehaswe≥0
Find a spanning tree, Tthat minimizes
w(T) =/summationdisplay
e∈E(T)we.
Sequential algorithms:
/trianglerightsldKruskal’s Algorithm
/trianglerightsldPrim’s Algorithm
MINIMUM SPANNING TREES 5/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINIMUM SPANNING TREES
MINIMUM SPANNING TREES 6/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LIGHT EDGERULE
Given G= (V,E),U/subsetnoteqlVpartitions the graph
into two parts with vertices UandV\U.
The edges between UandV\Uare called the
cut edges E(U,U).
MINIMUM SPANNING TREES 7/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LIGHT EDGERULE
THEOREM
LetG= (V,E,w)be a connected undirected weighted graph
with distinct edge weights.
For any nonempty U/subsetnoteqlV
the minimum weight edge ebetween UandV\Uis in the
minimum spanning tree MST( G) ofG.
MINIMUM SPANNING TREES 8/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LIGHT EDGERULE
uvUV\U
Assume e= (u,v)is the minimum edge in the cut but not
in the MST.
MST should have at least another edge in the cut.
Adding eto the path between uandvcreates a cycle.
Removing the max edge from path (blue line) and adding e
should give a ST with less weight.
Original (claimed) MST (through blue line) can not be a
MST!MINIMUM SPANNING TREES 9/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
KRUSKAL ’SALGORITHM
Greedy
Each vertex is a subtree by itself initially
Combine the two sub-trees on both sides of the
next smallest edge (if they are different)
Uses the union-ﬁnd data structure.
O(mlogn)work and span!
MINIMUM SPANNING TREES 10/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
KRUSKAL ’SALGORITHM
MINIMUM SPANNING TREES 11/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PRIM’SALGORITHM
Greedy
Based on Priority-based Search – Variant of
Dijsktra’s Algorithm
Maintain visited Xand frontier Fvertices.
Visit the nearest unvisited vertex in the frontier.
O(mlogn)work and span!
MINIMUM SPANNING TREES 12/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PRIM’SALGORITHM
1funprim (G) =
2let
3 funenqueue v(Q,(u,w)) =PQ.insert (w,(v,u))Q
4 funproper (X,Q,T) =
5 case PQ.deleteMin (Q)of
6 (NONE,)⇒T
7|(SOME (d,(u,v)),Q/prime)⇒
8 if(v∈?X)then proper (X,Q/prime,T)
9 else let
10 valX/prime=X∪{v}
11 valT/prime=T∪{(u,v)}
12 valQ/prime/prime=iter (enqueue v)Q/primeNG(v)
13 inproper (X/prime,Q/prime/prime,T/prime)end
14 vals=an arbitrary vertex from G
15 valQ=iter (enqueue s){}NG(s)
16 in
17 proper ({s},Q,{})
18 end
MINIMUM SPANNING TREES 13/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PRIM’SALGORITHM
MINIMUM SPANNING TREES 14/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLEL MST A LGORITHMS
OBSERVATION
The minimum weight edge out of every vertex of
a weighted graph Gbelongs to its MST.
Why should this be the case?
12354671235467
MST can contain other edges!
MINIMUM SPANNING TREES 15/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLEL MST - I DEA#1
Throw all minimum weight edges into MST
Tree contract the vertices for all these edges
Repeat until no edges remain!
12354671235467
Each rounds removes at least 1/2 of the vertices
(Why?)
MINIMUM SPANNING TREES 16/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLEL MST - I DEA#2
LetminE be the set of minimum weight edges.
LetH= (V,minE )be a subgraph of G
We apply (modiﬁed) star contraction to H
/trianglerightsldThe tails hook up through the minimum weight edge!
1funminStarContract (G= (V,E),i) =
2let
3 valminE =minEdges (G)
4 valP={u/mapsto→(v,w)∈minE|¬heads (u,i)∧heads(v,i)}
5 valV/prime=V\domain (P)
6in(V/prime,P)end
MINIMUM SPANNING TREES 17/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLEL MST - I DEA#2
Even though we are working with a subgraph, the
star contract lemma still applies.
LEMMA
For a graph Gwithnnon-isolated vertices, let Xnbe
the random variable indicating the number of vertices
removed by minStarContract (G,r). Then,
E(Xn)≥n/4.
MST will take expected O(logn)rounds.
MINIMUM SPANNING TREES 18/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BOOKKEEPING
As the graph contracts, the end point of each
edge changes!
At the end, the edges may not have the original
end points.
Associate a unique label to each edge initially:
/trianglerightsld(vertex×vertex×weight×label )
/trianglerightsldThe end points change but the label does not!
MINIMUM SPANNING TREES 19/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MODIFIED STARCONTRACT
1funminStarContract (G= (V,E),i) =
2let
3 valminE =minEdges (G)
4 valP={(u/mapsto→(v,w,/lscript))∈minE|¬heads (u,i)∧heads (v,i)}
5 valV/prime=V\domain (P)
6in(V/prime,P)end
Line 3: Finds min edge for each vertex.
/trianglerightsldAll these belong to the MST
Line 4: Picks tails and heads, and the creates
mapping from tails to heads.
Line 5: Removes all tail vertices from the vertex
set.
MINIMUM SPANNING TREES 20/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEMST A LGORITHM
1funMST((V,E),T,i) =
2if|E|=0then T
3else let
4 val(V/prime,PT) =minStarContract ((V,E),i)
5 valP={u/mapsto→v:u/mapsto→(v,w, /lscript)∈PT}∪{ v/mapsto→v:v∈V/prime}
6 valT/prime={/lscript:u/mapsto→(v,w, /lscript)∈PT}
7 valE/prime={(P[u],P[v],w,l) : (u,v,w,l)∈E|P[u]/negationslash=P[v]}
8in
9 MST((V/prime,E/prime),T∪T/prime,i+1)
10 end
Invoked by MST(G,{},1).
MINIMUM SPANNING TREES 21/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTING M I NED G E S (G)
funjoinEdges ((v1,w1,l1),(v2,w2,l2)) =
if(w1≤w2)then (v1,w1,l1)else (v2,w2,l2)
funminEdges (E) =
let
valET={u/mapsto→(v,w,l) : (u,v,w,l)∈E}
in
(merge joinEdges ){} ET
end
MINIMUM SPANNING TREES 22/22
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 19
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS
SYNOPSIS
Quicksort
Work and Span Analysis of Randomized
Quicksort
Lower Bound for Comparison-based Sorting
Lower Bound for Merging
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 2/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
QUICKSORT
Originally invented and analyzed by Hoare in
1960’s.
I strongly urge to watch Jon Bentley on “Three
beautiful Quicksorts” at
/trianglerightsldwww.youtube.com/watch?v=QvgYAQzg1z8 .
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 3/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SEQUENTIAL QUICKSORT
int i, j;
for( i = low, j = high - 1; ; )
{
while( a[ ++i ] < pivot );
while( pivot < a[ --j ] );
if( i >= j )
break;
swap( a, i, j );
}
// Restore pivot
swap( a, i, high - 1 );
quicksort( a, low, i - 1 ); // Sort small elements
quicksort( a, i + 1, high ); // Sort large elements
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 4/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
QUICKSORT
1funquicksort (S) =
2 if|S|=0then S
3 else let
4 valp=pick a pivot from S
5 valS1=/angbracketlefts∈S|s<p/angbracketright
6 valS2=/angbracketlefts∈S|s=p/angbracketright
7 valS3=/angbracketlefts∈S|s>p/angbracketright
8 val (R1,R3) = (quicksort (S1)/bardblquicksort (S3) )
9 in
10 append (R1,append (S2,R3))
11 end
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 5/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
QUICKSORT
Each call to Quicksort either makes
/trianglerightsldNo recursive calls (base case), or
/trianglerightsldTwo recursive calls
Call tree is a binary
Depth the call tree determines the span of the
algorithm.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 6/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PICKING THE PIVOT
Always pick the ﬁrst element
/trianglerightsldWorst case O(n2)work.
/trianglerightsldIn practice, almost sorted inputs are not uncommon.
Pick the median of 3 elements (e.g., ﬁrst, middle
and last elements)
/trianglerightsldcould possible divide evenly
/trianglerightsldworst case is still bad
Pick an element at random
/trianglerightsldwe hope this divides evenly in expectation
/trianglerightsldleading to expected O(nlogn)work and O(log2n)
span.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 7/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PICKING THE PIVOT
Pick ﬁrst element
/trianglerightsldWorst case O(n2)work.
/trianglerightsldExpected O(nlogn)work
⋆Averaged over all possible orderings.
/trianglerightsldWork well on the average
/trianglerightsldSlow on some, possibly common, cases.
Pick a random element
/trianglerightsldExpected worst-case O(nlogn)work.
⋆For input in anyorder, the expected work is O(nlogn)
/trianglerightsldNo input has expected O(n2)work.
/trianglerightsldWith a small probability, we could be unlucky and
have O(n2)work.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 8/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RANDOMIZED QUICKSORT
Assign a uniformly random priority to each
number in [0,1].
1funquicksort (S) =
2 if|S|=0then S
3 else let
4 valp=pick as pivot the highest priority element from S
5 valS1=/angbracketlefts∈S|s<p/angbracketright
6 valS2=/angbracketlefts∈S|s=p/angbracketright
7 valS3=/angbracketlefts∈S|s>p/angbracketright
8 val (R1,R3) = ( quicksort (S1)/bardblquicksort (S3) )
9 in
10 append (R1,append (S2,R3))
11 end
Once the priorities are assigned, the algorithm is
deterministic.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 9/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RANDOMIZED QUICKSORT
Count comparisons made!
/trianglerightsldAlmost all the work is comparisons.
Xn= # of comparisons quicksort
makes on input of size n
Find E[Xn]for any input sequence S
Notation:
/trianglerightsldLetT=sort (S)
/trianglerightsldTiandTjrefer to elements in the ﬁnal sorted order
andi<jandTi≤Tj.
/trianglerightsldpirefers to priority chosen for Ti.
/trianglerightsldAi,j=1 ifTiandTjwere ever compared during the
sort.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 10/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYZING QUICKSORT
Crucial point is how to model Ai,j.
In any one call to quicksort , there are three
cases:
/trianglerightsldPivot pis either TiorTj⇒Ai,j=1
/trianglerightsldTi<p<Tj⇒Ti∈S1,Tj∈S3,Ai,j=0
/trianglerightsldEither p<Tiorp>Tj⇒Ti,Tj∈S1orTi,Tj∈S3
If two elements are compared in a quicksort
call, they will never be compared again in any
other call!
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 11/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYZING QUICKSORT
Xn≤3n/summationdisplay
i=1n/summationdisplay
j=i+1Aij
The non-optimized code compares each element
to pivot 3 times.
1. . .
2 valS1=/angbracketlefts∈S|s<p/angbracketright
3 valS2=/angbracketlefts∈S|s=p/angbracketright
4 valS3=/angbracketlefts∈S|s>p/angbracketright
5. . .
By linearity of expectation
E[Xn]≤3n/summationdisplay
i=1n/summationdisplay
j=i+1E[Aij] =3n/summationdisplay
i=1n/summationdisplay
j=i+1Pr[Aij=1]
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 12/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYZING QUICKSORT
Consider ﬁrst when the pivot is one of
Ti,Ti+1, ...,Tj
TiandTjare compared⇔piorpjis the highest
priority among{pi,pi+1, . . . , pj}.
/trianglerightsldAssume Tk,i<k<jhas higher priority.
/trianglerightsldFor any subdivision ···,Ti,···,Tk,···,Tjwill
become a pivot and separate TiandTj
/trianglerightsldTiandTjwill never be compared!
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 13/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYZING QUICKSORT
E[Aij]=Pr[Aij=1]
=Pr[piorpjis the maximum in {pi, . . . , pj}]
=2
j−i+1(Why ?)
j−i+1 elements between piandpjand each is
equally likely to be the maximum.
We want either piorpj, hence2
j−i+1
Tiis compared to Ti+1with probability 1.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 14/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYZING QUICKSORT
E[Xn]≤3n/summationdisplay
i=1n/summationdisplay
j=i+1E[Aij]
=3n/summationdisplay
i=1n/summationdisplay
j=i+12
j−i+1
=3n/summationdisplay
i=1n−i+1/summationdisplay
k=22
k(change variables)
≤6n/summationdisplay
i=1Hn
≤6·n·Hn∈O(nlogn)
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 15/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYZING QUICKSORT
Indirectly, average work for basic deterministic
quicksort is O(nlogn).
/trianglerightsldJust shufﬂe data randomly and apply the basic
algorithm
/trianglerightsld≡to picking random priorities
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 16/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ALTERNATIVE ANALYSIS
Write a recurrence for the number of
comparisons:
X(n) =X(Yn) +X(n−Yn−1) +n−1
Random variable Ynis the size of S1.
E[X(n)] = E[X(Yn) +X(n−Yn−1) +n−1]
=E[X(Yn)] +E[X(n−Yn−1)] +n−1
=1
nn−1/summationdisplay
i=0(E[X(i)] +E[X(n−i−1)]) + n−1
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 17/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ALTERNATIVE ANALYSIS
E[X(n)] =1
nn−1/summationdisplay
i=0(E[X(i)] +E[X(n−i−1)]) + n−1
=2
nn−1/summationdisplay
i=0E[X(i)] +n−1
With telescoping, this also solves as O(nlogn)
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 18/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED SPAN
Sis split into L(ess),E(qual )and (g)R(eater ).
LetXn=max{|L|,|R|},
We use filter to partition.
S(n) =S(Xn) +O(logn)
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 19/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED SPAN
LetS(n)denote E[S(n)]
We bound S(n)by considering Pr[Xn≤3n/4]
andPr[Xn>3n/4].
Pr[Xn≤3n/4] =1/2
/trianglerightsldAs with SmallestK , 1/2 of the randomly chosen
pivots results in larger partition of at most size 3 n/4
elements.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 20/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED SPAN
S(n)=/summationdisplay
iPr[Xn=i]·S(i) +clogn
≤Pr[Xn≤3n
4]S(3n
4) +Pr[Xn>3n
4]S(n) +c·logn
≤1
2S(3n
4) +1
2S(n) +c·logn
=⇒(1−1
2)S(n)≤1
2S(3n
4) +clogn
=⇒S(n)≤S(3n
4) +2clogn
=⇒S(n)∈O(log2n)
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 21/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LOWER BOUND FOR SORTING
What is asymptotically the minimum number
comparisons any sorting algorithm has to make?
Lower-bounds apply to problems not to
algorithms.
/trianglerightsldAlgorithms provide upper bounds!
We say sorting is Ω(nlogn)
No (comparison-based) sorting algorithm has
work asymptotically lower than nlogn.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 22/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DECISION TREES
Does it live in the water?Does it have ﬁns?More than 4 legs?Can it ﬂy?Can it ﬂy?ﬁshfrogﬂyspiderparrotbisonYYYYYNNNNN
If there are Noutcomes, the number of questions
is at least log2N.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 23/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SORTING AS A DECISION PROBLEM
Fornitems, how many possible outcomes can there be?
/trianglerightsldn!⇒we need at least log2(n!)“questions”.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 24/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SORTING AS A DECISION PROBLEM
log(n!)=logn+log(n−1) +···+log(n/2) +···+log 1
≥logn+log(n−1) +···+log(n/2)
≥n
2·log(n/2)∈Ω(nlogn)
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 25/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LOWER BOUND FOR MERGING
We have sorted sequences A,|A|=nand
B,|B|=mandm≤n.
/trianglerightsldAssume all elements are unique.
All interleavings are possible
We need to choose mpositions out of n+mto
place the elements of Bamongst elements of A.
This can be done in log2/parenleftbign+m
m/parenrightbig
ways.
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 26/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LOWER BOUND FOR MERGING
/parenleftbign
r/parenrightbig
≥/parenleftbign
r/parenrightbigr
/trianglerightsldSee Lemma in the notes.
log2/parenleftbiggn+m
m/parenrightbigg
≥log2(n+m
m)m=mlog2(1+n
m)
QUICKSORT ANALYSIS AND SORTING LOWER BOUNDS 27/27
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 20
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION
SYNOPSIS
Binary Search Trees
Basic Structural Operations on BSTs
Basic Operations on BSTs
Concrete Implementations
Cost Analysis
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 2/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BINARY TREES
Trees where each node has at most 2 children
each of which is a binary tree.
/trianglerightsldLeft child / Left subtree
/trianglerightsldRight child / Right subtree
kkLkR
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 3/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BINARY SEARCH TREES
Binary trees with the “search” property
For each node vwith key k
/trianglerightsldThe key of the left child kL<k
/trianglerightsldThe key of the right child kR>k
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 4/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BALANCED TREES
We try to keep binary search trees balanced.
/trianglerightsldBoth children are about the same height
/trianglerightsldBoth subtrees are about the same size
AVL Trees
/trianglerightsldLeft and right subtree heights differ by at most 1.
/trianglerightsldO(logn)root height maintained after each insertion
and deletion.
Splay Trees
/trianglerightsldBalanced in the amortized sense
/trianglerightsldA sequence of nfind ,insert , ordelete
operations take O(nlogn)work.
/trianglerightsldSo average is O(logn)work.
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 5/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS
Data type is deﬁned by structural induction
/trianglerightsldLeaf
/trianglerightsldNode with a left child, a right child, a key, optional
additional data.
datatype BST = Leaf |
Node of (BST *BST *key *data)
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 6/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS
split (T,k) :BST×key→
BST×(data option )×BST
split divides Tinto two BSTs,
/trianglerightsldone consisting of all the keys from Tless than k
/trianglerightsldthe other all the keys greater than k
Ifkappears in the tree with associated data d
thensplit returns SOME (d)
Otherwise it returns NONE .
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 7/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS
join (L,m,R) :BST×(key×data )option×
BST→BST
Takes a left subtree ( L) an optional key-data pair
mand a right subtree ( R)
/trianglerightsldAssumes all keys in Lare less than all keys in R.
/trianglerightsldIf present, the optional key is also larger than all keys
inLand smaller than all keys in R.
Creates a new BST that is the union of LandR
andm.
We also assume both split and join maintain
balance.
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 8/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS
expose (T) :BST→
(BST×BST×key×data )option
Returns the components if BST Tis not empty.
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 9/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS - SEARCH
1funsearch T k =
2let val (,v,) =split (T,k)
3inv
4end
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 10/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS - INSERT
1funinsert T(k,v) =
2let val (L,v/prime,R) =split (T,k)
3injoin (L,SOME (k,v),R)
4end
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 11/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS - DELETE
1fundelete T k =
2let val (L,,R) =split (T,k)
3injoin (L,NONE ,R)
4end
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 12/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CONCRETE IMPLEMENTATIONS :
SPLIT
datatype BST = Leaf |
Node of (BST *BST *key *data)
1funsplit (T,k) =
2 case Tof
3 Leaf⇒(Leaf ,NONE ,Leaf )
4|Node (L,R,k/prime,v)⇒
5 case compare (k,k/prime)of
6 LESS⇒
7 let val (L/prime,r,R/prime) =split (L,k)
8 in(L/prime,r,Node (R/prime,R,k/prime,v))end
9 EQUAL⇒(L,SOME (v),R)
10 GREATER⇒
11 let val (L/prime,r,R/prime) =split (R,k)
12 in(Node (L,L/prime,k/prime,v),r,R/prime)end
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 13/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CONCRETE IMPLEMENTATIONS : JOIN
1funjoin (T1,m,T2) =
2 case mof
3 SOME (k,v)⇒Node (T1,T2,k,v)
4|NONE⇒
5 case T1of
6 Leaf⇒T2
7|Node (L,R,k,v)⇒Node (L,join (R,NONE ,T2),k,v))
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 14/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CONCRETE IMPLEMENTATIONS :
UNION
k1L1R1T1T2L2R2< k1> k1k1union(L1,L2)union(R1,R2)
ForT1with key k1and children L1andR1at the root, use
k1to split T2intoL2andR2.
Recursively ﬁnd Lu=union (L1,L2)and
Ru=union (R1,R2).
Now join (Lu,k1,Ru).
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 15/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
CONCRETE IMPLEMENTATIONS :
UNION
1fununion (T1,T2) =
2 case expose (T1)of
3 NONE⇒T2
4|SOME (L1,R1,k1,v1)⇒
5 let val (L2,v2,R2) =split (T2,k1)
6 val (L,R) =union (L1,L2)||union (R1,R2)
7 injoin (L,SOME (k1,v1),R)
8 end
Returns the value from T1if a key appears in
both trees.
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 16/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF UNION
1fununion (T1,T2) =
2 case expose (T1)of
3 NONE⇒T2
4|SOME (L1,R1,k1,v1)⇒
5 let val (L2,v2,R2) =split (T2,k1)
6 val (L,R) =union (L1,L2)||union (R1,R2)
7 injoin (L,SOME (k1,v1),R)
8 end
split costs O(log|T2|).
Two recursive calls to union
join costs O(log(|T1|+|T2|)
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 17/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF UNION -
ASSUMPTIONS
T1is perfectly balanced.
/trianglerightsldexpose return subtrees of size |T1|/2
/trianglerightsldEach a key from T1splits T2, it splits exactly in half.
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 18/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF UNION
W(|T1|,|T2|) =2W(|T1|/2,|T2|/2)/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
recursive union calls+O(log(|T1|+|T2|))/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
split and join,
and
W(1,|T2|) =O(log(1+|T2|)).
When|T1|=1,expose give us two empty
subtrees L1andR1
union (L1,L2)returns L2,union (R1,R2)returns
R2immediately!
Joining these costs at most
O(log(|T1|+|T2|)) = O(log(1+|T2|)
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 19/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF UNION
Letm=|T1|andn=|T2|
c log nc log (n/2)c log (n/2)c log (n/4)c log (n/4)c log (n/4)c log (n/4)c log nc 2 log (n/2)c 4 log (n/4)
Bottom level: Each box costs log (n/m)
Leaf dominated (Why?)
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 20/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ANALYSIS OF UNION
How many leaves are there in this recursion
tree?
/trianglerightsldT2has no impact.
/trianglerightsldWe get m=|T1|leaves.
How deep is the tree?
/trianglerightsld1+log2m
What is the size of T2at the leaves?
/trianglerightsldn/2log2m=n
m
Total cost at the leaves =O(mlog(1+n
m))
Union cost =O(mlog(1+n
m))
SEARCH TREES I: BST SSPLIT, JOIN,AND UNION 21/21
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 21
SEARCH TREES II: T REAPS
SYNOPSIS
Overview of Binary Search Trees
Relationship between Quicksort and BSTs
Treaps
Expected Depth of a Treap
SEARCH TREES II: T REAPS 2/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BST O VERVIEW
There are many options for keeping trees
balanced.
split andjoin are the main structural
operations to implement find ,insert ,
delete ,union , etc.
Cost of split andjoin are logarithmic in the
size of the input and output trees.
Union needs O(mlog(1+n
m))work ( m≤n).
SEARCH TREES II: T REAPS 3/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
QUICKSORT AND BST S
Write out the recursion tree for quicksort.
/trianglerightsldAssume distinct keys.
Annotate each node with the pivot picked at that
stage.
Y ou get a BST.
SEARCH TREES II: T REAPS 4/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SEQUENCE TO BST
1funqs_tree (S) =
2 if|S|=0then LEAF
3 else let
4 valp=pick a pivot from S
5 valS1=/angbracketlefts∈S|s<p/angbracketright
6 valS3=/angbracketlefts∈S|s>p/angbracketright
7 val(TL,TR) = (qs_tree (S1)/bardblqs_tree (S3) )
8 in
9 NODE (TL,p,TR)
10 end
Unlike Quicksort, we do not know what elements
will be in the tree, when we start.
/trianglerightsldWe can not select a (n) (future?) element to be the
root.
SEARCH TREES II: T REAPS 5/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TREAPS
Treap = TRee + hEAP
A treap is a randomized BST that maintains
balance in a probabilistic way.
Each element/key gets a unique random priority
The nodes in the treap satisfy BST property.
/trianglerightsldKeys are stored in-order in the tree.
The associated priorities satify the (max) heap
property.
SEARCH TREES II: T REAPS 6/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEMAX-HEAP PROPERTY
Priority at each node is greater than the priorities
of the children.
Suppose we have
S= (a,3),(b,9),(c,2),(e,6),(f,5)
(b,9)
(a,3) (e,6)
(c,2) (f,5)
SEARCH TREES II: T REAPS 7/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LET’S DO AN EXAMPLE
Draw the treap for the following ( key,priority )
sequence.
(G,50),(C,35),(E,33),(H,29),(I,25),(B,24),(A,21),(L,16),(J,13),
(K,9),(D,8)
SEARCH TREES II: T REAPS 8/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
TREAPS
THEOREM
For any set Sof unique key-priority pairs, there is
exactly one treap Tcontaining the key-priority pairs in
Swhich satisﬁes the treap properties.
Keykwith highest priority must be at the root.
All keys <kmust be in the left subtree
All keys >kmust be in the right subtree
Subtrees of kare constructed inductively in the
same manner.
SEARCH TREES II: T REAPS 9/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS - SEARCH
1funsearch T k =
2let val (,v,) =split (T,k)
3inv
4end
SEARCH TREES II: T REAPS 10/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS - INSERT
1funinsert T(k,v) =
2let val (L,v/prime,R) =split (T,k)
3injoin (L,SOME (k,v),R)
4end
SEARCH TREES II: T REAPS 11/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BASIC BST O PERATIONS - DELETE
1fundelete T k =
2let val (L,,R) =split (T,k)
3injoin (L,NONE ,R)
4end
So if split andjoin are implemented the
other more useful operations are covered.
SEARCH TREES II: T REAPS 12/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
JOIN AND SPLIT
split (T,k) :BST×key→
BST×(data option )×BST
split divides Tinto two BSTs,
/trianglerightsldone consisting of all the keys from Tless than k
/trianglerightsldthe other all the keys greater than k
Ifkappears in the tree with associated data d
thensplit returns SOME (d)
Otherwise it returns NONE .
SEARCH TREES II: T REAPS 13/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
JOIN AND SPLIT
join (L,m,R) :BST×(key×data )option×
BST→BST
Takes a left subtree ( L) an optional key-data pair
mand a right subtree ( R)
/trianglerightsldAssumes all keys in Lare less than all keys in R.
/trianglerightsldIf present, the optional key is also larger than all keys
inLand smaller than all keys in R.
Creates a new BST that is the union of LandR
andm.
SEARCH TREES II: T REAPS 14/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SPLIT ON TREAPS
Split code does not have to change.
Priority orders do not change.
Split does not put a larger priority below a
smaller priority.
SEARCH TREES II: T REAPS 15/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SPLIT ON TREAPS
datatype BST = Leaf |
Node of (BST *BST *key *data)
1funsplit (T,k) =
2 case Tof
3 Leaf⇒(Leaf ,NONE ,Leaf )
4|Node (L,R,k/prime,v)⇒
5 case compare (k,k/prime)of
6 LESS⇒
7 let val (L/prime,r,R/prime) =split (L,k)
8 in(L/prime,r,Node (R/prime,R,k/prime,v))end
9 EQUAL⇒(L,SOME (v),R)
10 GREATER⇒
11 let val (L/prime,r,R/prime) =split (R,k)
12 in(Node (L,L/prime,k/prime,v),r,R/prime)end
SEARCH TREES II: T REAPS 16/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
JOIN ON TREAPS
Join needs to change!
/trianglerightsldThe priorities of the roots of two trees need to be
compared.
/trianglerightsldThe root with the larger priority becomes the new
root.
Basic join took the root of the ﬁrst tree or the new
node as the root.
1funjoin (T1,m,T2) =
2 case mof
3 SOME (k,v)⇒Node (T1,T2,k,v)
4|NONE⇒
5 case T1of
6 Leaf⇒T2
7|Node (L,R,k,v)⇒Node (L,join (R,NONE ,T2),k,v)
SEARCH TREES II: T REAPS 17/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
JOIN ON TREAPS
1funjoin (T1,m,T2) =
2let
3 funsingleton (k,v) =Node (Leaf ,Leaf ,k,v)
4 funjoin/prime(T1,T2) =
5 case (T1,T2)of
6 (Leaf ,)⇒T2
7|(,Leaf )⇒T1
8|(Node (L1,R1,k1,v1),Node (L2,R2,k2,v2))⇒
9 if(priority (k1)>priority (k2))then
10 Node (L1,join/prime(R1,T2),k1,v1)
11 else
12 Node (join/prime(T1,L2),R2,k2,v2)
13 in
14 case mof
15 NONE⇒join/prime(T1,T2))
16|SOME (k,v)⇒join/prime(T1,join/prime(singleton (k,v),T2))
17 end
SEARCH TREES II: T REAPS 18/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED DEPTH OF A KEY
Cost of split and join depend on the expected
depth of a key.
Given a set of keys Kand priorities p:key→int
/trianglerightsldPriorities are unique!
Consider the elements of the tree laid out in
order
/trianglerightsldkey i<key j⇒··· ,key i,···,key j,···
/trianglerightsldkey j<key i⇒··· ,key j,···,key i,···
Aj
iis an indicator variable:
/trianglerightsldAj
i=1 ifkey jis an ancestor of key iin the treap.
/trianglerightsldAj
i=0 otherwise.
SEARCH TREES II: T REAPS 19/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED DEPTH OF A KEY
···,key i,···,key j,···
keyjkeyi < keyjpi= max(pi, …., pj)keyikeyjkeyikeykpk= max(pi, …., pj)i<k<jpj= max(pi, …., pj)
keyikeyjAij = 0Aij = 0Aij = 1
SEARCH TREES II: T REAPS 20/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED DEPTH OF A KEY
···,key j,···,key i,···
keyjkeyi > keyjpi= max(pj, …., pi)keyikeyikeyjkeykpk= max(pj, …., pi)i<k<jpj= max(pj, …., pi)
keyikeyjAij = 0Aij = 0Aij = 1
SEARCH TREES II: T REAPS 21/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED DEPTH OF A KEY
E[depth of iinT] =E
n/summationdisplay
j=1,j/negationslash=iAj
i
=n/summationdisplay
j=1,j/negationslash=iE/bracketleftBig
Aj
i/bracketrightBig
.
E/bracketleftBig
Aj
i/bracketrightBig
=1
|j−i|+1(Why?)
SEARCH TREES II: T REAPS 22/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED DEPTH OF A KEY
E[depth of iinT] =n/summationdisplay
j=1,j/negationslash=i1
|j−i|+1
(Split||⇒ )=i−1/summationdisplay
j=11
i−j+1+n/summationdisplay
j=i+11
j−i+1
(Change variables⇒)=i/summationdisplay
k=21
k+n−i+1/summationdisplay
k=21
k
=Hi−1+Hn−i+1−1
(lnn<Hn<lnn+1⇒)<lni+ln(n−i+1)
=O(logn)
Relative (sorted) position of a key determines expected
depth in treap.
SEARCH TREES II: T REAPS 23/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COST OF SPLIT AND JOIN
THEOREM
For treaps
join (T1,m,T2)returning T
split (T,(k,v))
have O(log|T|)expected work and span.
See notes for short proofs.
SEARCH TREES II: T REAPS 24/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED MAXDEPTH OF A TREAP
Expected depth of treap node is O(logn)
/trianglerightsldFind takes on the average O(logn)work and span.
What is the expected maximum depth of a treap?
/trianglerightsldWhy is this important?
/trianglerightsldExpected worst-case cost!
ButE[max i{Ai}]/negationslash=max i{E[Ai]}!
It turns out this is almost the same problem as
the expected span of the quicksort.
SEARCH TREES II: T REAPS 25/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXPECTED MAXDEPTH OF A TREAP
Ynn-1-YnD(n)D(Yn)1
Ynis the size of the larger partition.
D(n) =D(Yn) +1⇒D(n)∈O(logn)
SEARCH TREES II: T REAPS 26/26
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 23
MORE WITH TREES
SYNOPSIS
Ordered Sets and Tables
Bingle Revisited
Augmenting Balanced Trees
Ordered Tables with Reduced Values
Application Examples
MORE WITH TREES 2/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ORDERED SETS AND TABLES
So far, we did not worry about the ordering of the
values/keys in sets and tables.
/trianglerightsldFind, union, intersect, merge, etc.
For many applications, exploiting any order is
very important!
/trianglerightsldFind all elements between 3 and 17.
/trianglerightsldFind all customers who bought more that 5 of one
item.
/trianglerightsldFind all emails in the week of March 31st.
Ordered sets and tables.
MORE WITH TREES 3/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ORDERED SETADT
We have a totally ordered universe U, and S
represents the set of all subsets of U.
With the following operations
all operations supported by the Set ADT, and
last (S) : S→U =max S
first (S) : S→U =minS
split (S,k) : S×U→S = ({k/prime∈S|k/prime<k},k?∈S,
×bool×S{k/prime∈S|k/prime>k})
join (S1,S2) : S×S→S =S1∪S2,assuming
maxS1<minS2
getRange (S,k1,k2) : S×U×U→S={k∈S|k1≤k≤k2}
MORE WITH TREES 4/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ORDERED SETADT
Underlying implementation uses trees.
first andlast are easy
/trianglerightsldfirst traverses down the left spine to the minimum
value.
/trianglerightsldlast traverses down the right spine to the maximum
value.
getRange involves two splits.
MORE WITH TREES 5/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPROVISING BINGLE
signature INDEX = sig
type word = string
type docId = string
type ’a seq
type index
type docList
val makeIndex : (docId *string) seq -> index
val find : index -> word -> docList
val And : docList *docList -> docList
val AndNot : docList *docList -> docList
val Or : docList *docList -> docList
val size : docList -> int
val toSeq : docList -> docId seq
end
docList is a set.
index is a table.
MORE WITH TREES 6/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPROVISING BINGLE
We want to limit the search to certain domains
(e.g., cmu.edu )
/trianglerightsldor docs with a certain name.
We want to add
val inDomain : domain *docList -> docList
For example
inDomain("cs.cmu.edu",
and(find idx "cool", find idx "TAs"))
MORE WITH TREES 7/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPROVISING BINGLE
Assume doc ids are URLs.
Assume they are “reverse” lexicographically
ordered.
/trianglerightsldThe last character is the most important!
1funinDomain (domain,L) =
2 getRange (L,domain,string.prepend (domain,"$" ))
$is a character that is greater than any character.
MORE WITH TREES 8/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
AUGMENTING BALANCED TREES
Sets (and underlying trees) hold the key and any
associated values.
We can add other additional values to help with
other search operations.
/trianglerightsldTrack key positions and certain subset sizes.
rank(S, k) : How many elements in Sare less
than k?
select(S, i) : Which element in Shas rank i?
splitIdx(S,i) : Split Sinto two sets: ﬁrst i
keys and the remaining n−ikeys.
MORE WITH TREES 9/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
AUGMENTING BALANCED TREES
rank (S,k) : S×U→int =|{k/prime∈S|k/prime<k}|
select (S,i) : S×int→U=ksuch that|{k/prime∈S|k/prime<k}|=i
splitIdx (S,i) : S×int→ = ({k∈S|k<select (S,i)},
S×S{k∈S|k≥select (S,i)})
Without additional information stored with the
keys, these operations would take θ(|S|)work.
MORE WITH TREES 10/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
AUGMENTING BALANCED TREES
LetS={1,2,3,4,5,6}
rank(S, 4) =|{1,2,3}|=3
select(S, 3) =4 since rank(S, 4) =3
splitIdx(S, 3) = ({1,2,3},{4,5,6})
MORE WITH TREES 11/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
AUGMENTING BALANCED TREES
At each node keep the size of the subtree.
This allows size and the three other operations
inO(d)work with das the depth of the tree.
Size can be computed on the ﬂy by adding 1 to
the sum of the subtree sizes!
MORE WITH TREES 12/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SELECT WITH AUGMENTED TREES
1funselect (T,i) =
2 case expose (T)of
3 NONE⇒raise Range
4|SOME (L,R,k)⇒
5 case compare (i,|L|)of
6 LESS⇒select (L,i)
7|EQUAL⇒k
8|GREATER⇒select (R,i−|L|−1)
MORE WITH TREES 13/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RANK AND SPLIT IDX
rank is easy: just split and return the size of the
left tree!
splitIdx is just like split (or you navigate using
sizes (as opposed to key values))
MORE WITH TREES 14/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ORDERED TABLES WITH REDUCED
VALUES
Maintain at each node a “sum” based on an
associative operator f.
/trianglerightsldUpdated during insert/delete, merge, extract, etc.
Given f:v×v→v, and If
/trianglerightsldAll operations on ordered tables are supported, and
/trianglerightsld
reduceVal (A) :T→v=reduce f IfA
/trianglerightsldWe want to be able to do reduceVal inO(1)work
(assuming fneeds O(1)work).
/trianglerightsldfis known beforehand!
MORE WITH TREES 15/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ORDERED TABLES WITH REDUCED
VALUES
(e,2,13)(c, 1, 6)(a, 3, 3)(d, 2, 2)(g,5,5)
fis+
(e,2,5)(c, 1, 3)(a, 3, 3)(d, 2, 2)(g,5,5) fis max
MORE WITH TREES 16/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTATION
1datatype Treap =Leaf|Node of(Treap×Treap
2 ×key×data×data )
3funreduceVal (T) =
4 case Tof
5 Leaf⇒Reduce.I
6|Node (,,,,r)⇒r
7funmakeNode (L,R,k,v) =
8 Node (L,R,k,v,Reduce.f(reduceVal (L),
9 Reduce.f(v,reduceVal (R))))
MORE WITH TREES 17/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
IMPLEMENTATION
1funjoin/prime(T1,T2) =
2 case (T1,T2)of
3 (Leaf,)⇒T2
4|(,Leaf )⇒T1
5|(Node (L1,R1,k1,v1,s1),Node (L2,R2,k2,v2,s2))⇒
6 if(priority (k1)>priority (k2))then
7 makeNode (L1,join (R1,T2),k1,v1)
8 else
9 makeNode (join (T1,L2),R2,k2,v2)
MORE WITH TREES 18/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXAMPLE APPLICATION - SALES
DATA
Sales information are kept by the time stamp in
an ordered table.
/trianglerightsld(2/3/2013−12:30,$120)
Find the total sales between t1andt2
fis+
reduceVal (getRange (T,t1,t2))takes O(logn )
work
MORE WITH TREES 19/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXAMPLE APPLICATION - STOCK
DATA
Stock prices information are kept by the time
stamp in an ordered table.
/trianglerightsld(2/3/2013−12:30,$120/share )
Find the maximum price between t1andt2
fis max
reduceVal (getRange (T,t1,t2))takes O(logn )
work
MORE WITH TREES 20/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
EXAMPLE APPLICATION - INTERVAL
TREES
An interval is a region on the real number line
starting at xland ending at xr
an interval table supports the following
operations on intervals:
insert (A,I) : T×(real×real )→T insert interval Iinto table A
delete (A,I) : T×(real×real )→T delete interval Ifrom table A
count (A,x) : T×real→int return the number of
intervals crossing xinA
MORE WITH TREES 21/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
INTERVAL TREES
Organize intervals as a BST based on
lower-boundary as key
Use the max upper boundary in the subtree as
additional information.
[16,21]30[8,9]23[25,30]30[5,8]10[0,3]3[6,10]10[15,23]23[17,19]20[19,20]20[26,26]26
MORE WITH TREES 22/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COUNTING INTERVALS
1datatype intTree =Leaf|Node of(intTree×intTree
2 ×real×real×real )
3funoverlap (x,low,high ) =
4 if(x≥low &x≤high )then 1else 0
5funcountInt (T,x) =
6 case Tof
7 Leaf⇒0
8|Node (L,R,low,high,max )⇒
9 if(x>max )then 0
10 else countInt (L,x)+
11 overlap (x,low,high )+
12 if(x>low)then countInt (R,x)else 0
MORE WITH TREES 23/23
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 24
DYNAMIC PROGRAMMING
SYNOPSIS
Dynamic Programming
Subset Sum Problem
Minimum Edit Distance Problem
Additional example applications
DYNAMIC PROGRAMMING 2/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
ALGORITHMIC PARADIGMS
CONTRASTED
Inductive Paradigms combine solutions to
smaller subproblem(s).
Paradigm Subproblems Reuse of
Solutions
Divide and Conquer >1 NO
Contraction =1 NO
Greedy =1 NO
Dynamic Programming >1 YES
DYNAMIC PROGRAMMING 3/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REUSING SOLUTIONS
foo(A) foo(C) foo(B) foo(D) size k size j < k 
Y ou can save some work if you semember the
solutions to the smaller subproblems.
DYNAMIC PROGRAMMING 4/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
RESUSING SOLUTIONS
How much work does this code need?
1funfib (n) =
2 if(n≤1)then 1
3 else fib (n−1) +fib (n−2)
It turns out Wﬁb(n) =O(cn)(Why?)
DYNAMIC PROGRAMMING 5/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
REUSING SOLUTIONS
It also turns out that fib (n)can be computed
withO(n)work.
/trianglerightsldNote that nis not the right measure for modeling work
here (Why? ) but it is convenient!
ﬁb(5) ﬁb(3) ﬁb(4) ﬁb(2) ﬁb(1) ﬁb(0) 
DYNAMIC PROGRAMMING 6/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SOLUTION COMPOSITION GRAPH
ﬁb(5) ﬁb(3) ﬁb(4) ﬁb(2) ﬁb(1) ﬁb(0) 
DAG
Each node is a subproblem
instance
Edges model dependences
Edges go from smaller to larger
subproblems
Vertices with no in-edges are
base cases
Vertices with no out edges are
the instance we are trying to
solve.
DYNAMIC PROGRAMMING 7/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DYNAMIC PROGRAMMING
Dynamic programming can be seen as evaluating
a DAG by navigating from the leaves to the root.
/trianglerightsldComputing the subsolutions at each node as needed
and when possible.
Work and span fall out of the DGA structure!
/trianglerightsldWork: sum over nodes
/trianglerightsldSpan: Find the longest path!
Many DP solutions have signiﬁcant parallelism,
but some do not.
DYNAMIC PROGRAMMING 8/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DYNAMIC PROGRAMMING
The challenge is to ﬁnd the appropriate DAG
structure for a given problem.
DP is most suitable for optimization problems.
/trianglerightsldSolution optimizes (minimizes/maximizes) some
criteria.
DP is also suitable for decision problems.
/trianglerightsldIs there a solution to this instance?
DYNAMIC PROGRAMMING 9/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
DYNAMIC PROGRAMMING
Top-down approach
/trianglerightsldStarts at the root
/trianglerightsldUses recursion to solve the subproblems
/trianglerightsldBut remembers the solutions – memoization.
/trianglerightsldUsually elegant and evaluates only the needed
subproblems.
Bottom-up approach
/trianglerightsldStarts at the leaves
/trianglerightsldTraverses the DAG in some fashion.
/trianglerightsldAll subproblems may need to be computed.
/trianglerightsldMore parallelizable.
Coming up with the abstract inductive structure is
important.
/trianglerightsldSharing and coding comes later.
DYNAMIC PROGRAMMING 10/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESUBSET SUMPROBLEM
THESUBSET SUM(SS) P ROBLEM
Given a multiset of positive integers Sand a positive
integer value k, determine if there is any X⊆Ssuch
that/summationtext
x∈Xx=k.
Given S={1,4,2,9,9}
/trianglerightsldNo solution for k=8
/trianglerightsldFork=7{1,4,2}is a solution.
NP−hard if kis unconstrained.
We will include kin the work bounds.
kis polynomial in|S|, work is polynomial in |S|.
Pseudo-polynomial work solution.
DYNAMIC PROGRAMMING 11/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESUBSET SUMPROBLEM
Brute force: Consider all 2nsubset for a total
work of O(n2n).
Divide and Conquer: also ends up being
exponential work.
Sharing solutions however works.
DYNAMIC PROGRAMMING 12/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESUBSET SUMPROBLEM
To solve SS(S,k), pick some element a∈S
Solve (recursively) SS(S\{a},k−a)
/trianglerightsldIf there is a solution, we are done.
If not, solve SS(S\{a},k).
1funSS(S,k) =
2 case (showl (S),k)of
3 (,0)⇒true
4|(NIL,)⇒false
5|(CONS (a,R),)⇒
6 if(a>k)then SS(R,k)
7 else (SS(R,k−a)orelse SS(R,k))
DYNAMIC PROGRAMMING 13/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESUBSET SUMPROBLEM DAG
SS({1,1,1}, 3) SS({1,1}, 2) SS({1}, 1) SS(ϕ, 0) SS(ϕ, 1) SS(ϕ, 1) SS(ϕ, 2) SS(ϕ, 1) SS(ϕ, 2) SS(ϕ, 2) SS(ϕ, 3) SS({1}, 2) SS({1,1}, 3) SS({1}, 2) SS({1}, 3) 
DYNAMIC PROGRAMMING 14/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESUBSET SUMPROBLEM DAG
SS({1,1,1}, 3) SS({1,1}, 2) SS({1}, 1) SS(ϕ, 1) SS(ϕ, 0) SS(ϕ, 2) SS(ϕ, 3) SS({1,1}, 3) SS({1}, 2) SS({1}, 3) 
How many distinct subproblems do we need to
solve?
DYNAMIC PROGRAMMING 15/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESUBSET SUMPROBLEM
ForSS(S,k), there are only|S|distinct lists ever
used.
The second argument decreases down to 0, so
has at most k+1 values.
So we have at most |S|(k+1) =O(k|S|)
instances.
Each instance has constant work ⇒total O(k|S|)
work.
Longest path in DAG is |S|⇒span is O(|S|)
/trianglerightsldO(k)parallelism.
DYNAMIC PROGRAMMING 16/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THESUBSET SUMPROBLEM
Why pseudo-polynomial ?
Fork, the input size is log k, but the work is
O(2logk|S|)
/trianglerightsldExponential in input size!
Ifk≤|S|cfor some constant c, then work is
O(k|S|) =O(|S|c+1)on input of size
clog|S|+|S|
DYNAMIC PROGRAMMING 17/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINIMUM EDITDISTANCE
MINIMUM EDITDISTANCE (MED)
Given a character set Σand two sequences of
characters S= Σ∗andT= Σ∗, determine the
minimum number of insertions and deletions of single
characters required to transform StoT.
Start with S=/angbracketleftA,B,C,A,D,A/angbracketright
/trianglerightsldDelete C
/trianglerightsldDelete last A
/trianglerightsldInsert a C
Y ou get T=/angbracketleftA,B,A,D,C/angbracketright
SoMED (S,T) =3
DYNAMIC PROGRAMMING 18/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
APPLICATIONS OF MED
Spelling correction
/trianglerightsldWhat is an English word close to Ynglisd ?
Storing multiple versions of ﬁles efﬁciently.
Approximate matching of genome sequences
DYNAMIC PROGRAMMING 19/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINIMUM EDITDISTANCE
Given S=s::S/primeandT=t::T/prime
Ifs=t,MED (S,T)is determined by S/primeandT/prime
Otherwise we have two subproblems:
/trianglerightsldFind MED (S,T/prime)– consider a deletion from Tto get
T/prime
/trianglerightsldFind MED (S/prime,T)– consider an deletion to Sto get S/prime
Find the minimum and add 1.
DYNAMIC PROGRAMMING 20/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINIMUM EDITDISTANCE
1funMED (S,T) =
2 case (showl (S),showl (T))of
3 (,NIL )⇒|S|
4|(NIL,)⇒|T|
5|(CONS (s,S/prime),CONS (t,T/prime))⇒
6 if(s=t)then MED (S/prime,T/prime)
7 else 1+min(MED (S,T/prime),MED (S/prime,T))
If run recursively, this would take exponential
work.
/trianglerightsldBinary tree with linear depth!
But there is signiﬁcant sharing!
DYNAMIC PROGRAMMING 21/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINIMUM EDITDISTANCE
MED(ABC, DBC) MED(BC, DBC) MED(C, DBC) MED(C , BC) MED(ϕ, DBC) MED(BC, C) MED(ABC, ϕ) MED(ABC, BC) MED(BC, BC) MED(ABC, C) MED(C, C) MED(ϕ, BC) MED(BC, ϕ) MED(ϕ, ϕ) ABC = <A,B,C> 
DYNAMIC PROGRAMMING 22/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINIMUM EDITDISTANCE
There are at most |S|+1 possible values for the
ﬁrst argument.
There are at most |T|+1 possible values for the
second argument.
So we have (|S|+1)×(|T|+1) =O(|S||T|)
possible subproblems, each of constant work.
/trianglerightsldTotal work is O(|S||T|).
Total span is O(|S|+|T|)(Why?)
DYNAMIC PROGRAMMING 23/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THELONGEST COMMON
SUBSEQUENCE (LENGTH )
A longest common subsequence of strings S1
andS2is a longest subsequence shared by both.
LCS (ABCDEF ,EBCEG ) =BCE
May be empty or not necessarily unique.
LLCS (S1,S2)computes the length of the LCS.
Subproblem structure is very similar to MED.
(Work it out!)
DYNAMIC PROGRAMMING 24/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
OPTIMAL CHANGE
For a currency with coins C1,C2, . . .Cn=1
(cents), what is the minimum number of coins
needed to make Kcents of change.
US Currency has 25, 10, 5, 1 cent coins.
To give back 63 cents, you need to give
25+25+10+1+1+1, a total of 6 coins.
/trianglerightsldGreedy works in this case, but not always
/trianglerightsldIf you had a 21 cent coin (for some strange reason),
greedy would not work.
DP solutions solves two subproblems K1=iand
K2=K−ifor all i=1, . . .⌊K/2⌋
Then chooses ithat minimizes the sum of the
solutions
DYNAMIC PROGRAMMING 25/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
0-1 K NAPSACK
Items with “beneﬁt” piand cost wi
/trianglerightsldxi=1 or 0 – take item ior not.
Maximize/summationtextn
j=1pj·xj
Subject to/summationtextn
j=1wj·xj≤c
Optimal Exam Strategy Problem (:-)
/trianglerightsldQuestions 1 through n,worth p1, . . .pnpoints.
/trianglerightsldTime estimate for solving question jiswj
/trianglerightsldY ou have Tunits of time.
/trianglerightsldWhich questions do you solve to maximize your
grade?
/trianglerightsldSubproblem structure is resembles the thinking for
subset sum problem
DYNAMIC PROGRAMMING 26/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
OPTIMAL MATRIX MULTIPLICATION
We need to multiply nmatrices A1×A2×··· An
/trianglerightsldAihas sizes pi−1×piandAi+1has sizes pi×pi+1
/trianglerightsldMultiplying AiandAi+1needs O(pi−1·pi·pi+1)work
What is the best way to “parenthesize” the
sequence to minimize the number of scalar
mutiplications?
m[i,j]is the minimum number of scalar
multiplications for multiplying Ai×···× Aj
/trianglerightsldA subproblem
DYNAMIC PROGRAMMING 27/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
OPTIMAL MATRIX MULTIPLICATION
m[i,j] =/braceleftbigg
0 i=j
min i≤k<j{m[i,k] +m[k+1,j] +pi−1·pk·pj}i<j
Find that kthat minimizes the cost of multiplying
Ai×···× Aj
We need to compute m[1,n]and how we got that
(the choice of k’s when we are minimizing
subproblems)
DYNAMIC PROGRAMMING 28/28
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 25
DYNAMIC PROGRAMMING – II
SYNOPSIS
Top-down Dynamic Programming
Bottom-up Dynamic Programming
Optimal Binary Search Trees
DYNAMIC PROGRAMMING – II 2/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
TOP-DOWN DP
Run the recursive code as is:
/trianglerightsldStart with the root
/trianglerightsldWork down to the leaves
Memoization: We need to avoid redundant
computation.
/trianglerightsldIf we encounter the same arguments, we just look up
the solution
/trianglerightsldIf not, we compute once and store in a memo table.
Checking for equal arguments could be costly.
/trianglerightsldWe use simple surrogates for actual arguments (e.g.,
integers)
DYNAMIC PROGRAMMING – II 3/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
TOP-DOWN DP FORMED
MED takes two sequences and on each recursive
call, uses sufﬁxes of the original sequences.
/trianglerightsldThere is a one-to-one mapping from non-negative
integers to sufﬁxes (rather to sufﬁx lengths!)
/trianglerightsldCould also use preﬁxes!
/trianglerightsldThis makes indexing a bit easier.
DYNAMIC PROGRAMMING – II 4/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ORIGINAL MED CODE
1funMED(S,T) =
2 case (showl (S),showl (T))of
3 (,NIL)⇒|S|
4|(NIL ,)⇒|T|
5|(CONS (s,S/prime),CONS (t,T/prime))⇒
6 if(s=t)then MED(S/prime,T/prime)
7 else 1+min(MED(S,T/prime),MED(S/prime,T))
DYNAMIC PROGRAMMING – II 5/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
MED WITH SURROGATES
1funMED(S,T) =let
2 funMED/prime(i,0) =i
3|MED/prime(0,j) =j
4|MED/prime(i,j) =case (Si=Tj)of
5 true⇒MED/prime(i−1,j−1)
6 |false⇒1+min(MED/prime(i,j−1),
7 MED/prime(i−1,j))
8in
9 MED/prime(|S|,|T|)
10 end
MED’ hasiandj, instead of SandT
/trianglerightsldirepresents S/angbracketleft0, . . . , i−1/angbracketright
/trianglerightsldjrepresents T/angbracketleft0, . . . , j−1/angbracketright
No memo table yet!
DYNAMIC PROGRAMMING – II 6/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
MEMO TABLE
We can now add a memo table, accessed with
(i,j)
/trianglerightsldWe can also use a two dimensional array!
1funmemo f(M,a) =
2 case find (M,a)of
3 SOME (v)⇒(M,v)
4|NONE⇒let
5 val(M/prime,v) =f(M,a)
6 in
7 (update (M/prime,a,v),v)
8 end
DYNAMIC PROGRAMMING – II 7/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
MEMOIZED MED
1funMED (S,T) =let
2 funMED/prime(M,(i,0)) = ( M,i)
3|MED/prime(M,(0,j)) = ( M,j)
4|MED/prime(M,(i,j)) =case (Si=Tj)of
5 true⇒MED/prime/prime(M,(i−1,j−1))
6 |false⇒let
7 val (M/prime,v1) =MED/prime/prime(M,(i,j−1))
8 val (M/prime/prime,v2) =MED/prime/prime(M/prime,(i−1,j))
9 in(M/prime/prime,1+min(v1,v2))end
10 and MED/prime/prime(M,(i,j)) =memo MED/prime(M,(i,j))
11 in
12 MED/prime({} ,(|S|,|T|))
13 end
Purely functional
but highly sequential
DYNAMIC PROGRAMMING – II 8/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
BOTTOM -UPDP
Start with the leaves
Works through the subproblems consistent with
the DAG
/trianglerightsldif(u,v)is a dependency edge in the DAG, compute u
before v, for all such u.
/trianglerightsldAll values will be available for vwhen they are
needed!
Uses a memo table.
Understanding the DAG structure is important
DYNAMIC PROGRAMMING – II 9/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
BOTTOM -UPDP FORMED
t 1 c 2 a 3 t 4 a  1 t  2 c  3 i
j0    0 
Dag for MED (”tcat”,”atc”)
DYNAMIC PROGRAMMING – II 10/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
BOTTOM -UPDP FORMED
3          2          1          0          t 1 c 2 a 3 t 4 a  1 t  2 c  3 4          5          6          7          0    0 i
k
We can go by diagonals.
DYNAMIC PROGRAMMING – II 11/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
BOTTOM -UPDP FORMED
t 1 c 2 a 3 t 4 a  1 t  2 c  3 i
j0    0 
We can go by rows.
DYNAMIC PROGRAMMING – II 12/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
BOTTOM -UPDP FORMED
t 1 c 2 a 3 t 4 a  1 t  2 c  3 i
j0    0 
We can go by columns.
DYNAMIC PROGRAMMING – II 13/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
BOTTOM -UPDP FORMED
1funMED(S,T) =let
2 funMED/prime(M,(i,0)) = i
3|MED/prime(M,(0,j)) = j
4|MED/prime(M,(i,j)) =case (Si=Tj)of
5 true⇒Mi−1,j−16 |false⇒1+min(Mi,j−1,Mi−1,j)
7 fundiagonals (M,k) =
8 if(k>|S|+|T|)then M
9 else let
10 vals=max (0,k−|T|)
11 vale=min(k,|S|)
12 valM/prime=M∪{(i,k−i)/mapsto→MED/prime(M,(i,k−i)) :i∈{s, . . . , e}}
13 in
14 diagonals (M/prime,k+1)
15 end
16 in
17 diagonals ({},0)
18 end
DYNAMIC PROGRAMMING – II 14/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
BOTTOM -UPDP FORMED
In Round 0, we compute M0,0
In Round 1, we compute M0,1andM1,0
In Round 2, we compute M0,2,M1,1,M2,0
In Round 3, we compute M0,3,M1,2,M2,1,M3,0
. . .
How about parallelism?
DYNAMIC PROGRAMMING – II 15/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPTIMAL BINARY SEARCH TREES
Let’s revisit BSTs
/trianglerightsldThe cost of ﬁnding a key is proportional to the depth
of the key in the tree.
/trianglerightsldFully balanced BST with nnodes⇒average depth is
logn
Suppose you have a (ﬁxed/static) dictionary and
you know the probability that a given key will be
accessed
What is the BST structure with the lowest overall
cost?
DYNAMIC PROGRAMMING – II 16/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPTIMAL BINARY SEARCH TREES
OPTIMAL BST
Theoptimal binary search tree (OBST) problem is
given an ordered set of keys Sand a probability
function p:S→[0:1], to ﬁnd ˆT
ˆT=arg min T∈Trees (S)/parenleftBigg/summationdisplay
s∈Sd(s,T)·p(s)/parenrightBigg
where Trees (S)is the set of all BSTs on S, and
d(s,T)is the depth of the key sin the tree T
(Assume the root has depth 1).
DYNAMIC PROGRAMMING – II 17/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPTIMAL BINARY SEARCH TREES
key k1k2 k3 k4k5k6
p(key)1/8 1 /32 1 /16 1 /32 1 /4 1 /2
k5k1k6k3k2k4
Cost =1
8×2+1
32×4+1
16×3+1
32×4+1
4×1+1
2×2=31
16
DYNAMIC PROGRAMMING – II 18/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPTIMAL BINARY SEARCH TREES
How many binary search trees of ndistinct keys
are there?
/trianglerightsldHint: Think of matrix chain multiplication!
in DP , an optimal solution should be based on
optimal subproblem solutions.
One of the keys ( Sr) must be at the root of the
optimal tree.
/trianglerightsldBoth subtrees must be optimal .
How do we select Sr?
/trianglerightsldPick the key with highest probability and put it at the
root, and recurse?
/trianglerightsldDoes not really work!
DYNAMIC PROGRAMMING – II 19/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPTIMAL BINARY SEARCH TREES
Try all elements as a potential root
For each, recursively ﬁnd their optimal solutions
Pick the best among the |S|possibilities.
All elements under a root are contiguous in the
sorted sequence.
k5k1k6k3k2k4
DYNAMIC PROGRAMMING – II 20/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPTIMAL BINARY SEARCH TREES
Use (i,j)as a surrogate for the tree spanning
Si, . . . , Sj.
LetTbe the tree covering Si, . . . , Sjwith root
Sr,i≤r≤j, with TLTRas the subtrees.
Cost (T) =/summationdisplay
s∈Td(s,T)·p(s)
=p(Sr) +/summationdisplay
s∈TL(d(s,TL) +1)·p(s) +/summationdisplay
s∈TR(d(s,TR) +1)·p(s)
=/summationdisplay
s∈Tp(s) +/summationdisplay
s∈TLd(s,TL)·p(s) +/summationdisplay
s∈TRd(s,TR)·p(s)
=/summationdisplay
s∈Tp(s) +Cost (TL) +Cost (TR)
Find the r,i≤r≤jthat minimizes this cost.
DYNAMIC PROGRAMMING – II 21/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPTIMAL BINARY SEARCH TREES
1funOBST (S) =
2 if|S|=0then 0
3 else/parenleftbig/summationtext
s∈Sp(s)/parenrightbig
+min i∈/angbracketleft1...|S|/angbracketright(OBST (S1,i−1)+
4 OBST (Si+1,|S|))
How many possible subproblems are there?
/trianglerightsldA subsequence can end at ndifferent positions
/trianglerightsldFor the ithend position there are ipossible start
positions.
/summationtextn
i=1i=n(n+1)/2∈O(n2)possible subproblems.
Longest path of dependences in the DAG is O(n)since
recursion can go down for nlevels (Why?)
DYNAMIC PROGRAMMING – II 22/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
WORK AND SPAN
Cost of each subproblem is not uniform! (Why?)
Each subproblem has O(n)work and O(logn)
span (Why?)
We get total O(n3)work and O(nlogn)span.
(Why?)
DYNAMIC PROGRAMMING – II 23/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
CODE FOR OPTIMAL BST
1funOBST (S) =let
2 funOBST’ (i,l) =
3 ifl=0then 0
4 else/summationtextl−1
k=0p(Si+k) +minl−1
k=0(OBST’ (i,k)+
5 OBST’ (i+k+1,l−k−1))
6in
7 OBST’ (1,|S|)
8end
DYNAMIC PROGRAMMING – II 24/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
BOTTOM -UPOPTIMAL BST
For a bottom up version, a triangular table is
sufﬁcient
C1C2C3C4C5C12C23C34C45C13C24C35C14C25C15
cij = optimal cost of the tree covering Sij
DYNAMIC PROGRAMMING – II 25/25
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 26
HASH TABLES
SYNOPSIS
Hashing and Hash Tables
Handling Collisions
/trianglerightsldLinear Probing
/trianglerightsldQuadratic Probing
HASHTABLES 2/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
HASH TABLES – BASIC IDEAS
Data structure that allows you to quickly insert,
delete, and retrieve items with expected O(1)
work.
Relies on
/trianglerightslda ﬁxed size array data structure (of some size m), and
/trianglerightslda hash function that can map from a potentially
inﬁnite space of keys to integer indexes [0,..., m−1]
Disadvantages
/trianglerightsldCollisions
/trianglerightsldIncreased memory use to avoid collisions
/trianglerightsldNot work efﬁcient for ﬁndmin ,ﬁndmax , orextracting
keys in sorted order
HASHTABLES 3/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
HASH TABLE - BASIC IDEAS
HASHTABLES 4/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
HASH FUNCTIONS
There is a deep theory behind hash functions.
We will be interested in some simple functions.
We will assume hash functions have the
idealized property of simple uniform hashing :
/trianglerightsldThe hash function uniformly distributes keys in range
[0,..., m−1]
/trianglerightsldHash value for one key is independent of the hash
value for another key.
HASHTABLES 5/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
HASH FUNCTIONS
For integers key we can use a linear congruential
hash function
h(x) = (ax+b)mod m
where a∈[1,..., m−1],b∈[0,..., m−1], and
mis prime.
HASHTABLES 6/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
HASH FUNCTIONS
For strings, we can use a polynomial like
h(S) =
|S|/summationdisplay
i=1siai
mod m
HASHTABLES 7/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
HASH TABLES
Support insert ,find anddelete .
Can implement abstract data types Setand
Table .
Do not require total ordering on the universe of
keys.
Collision is the main issue
/trianglerightsldTwo keys hash to the same location.
/trianglerightsldImpossible to avoid if we do not know the keys in
advance
⋆Size of key universe >>size of table.
HASHTABLES 8/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
COLLISIONS
For a table size of 365, one needs 23 keys for a
50% chance of collision and 66 for a 99% chance
of collision (Why?)
/trianglerightsldBirthday paradox
HASHTABLES 9/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
HANDLING COLLISIONS
Separate chaining
/trianglerightsldStore elements not in a table, but in linked lists
(containers,bins) hanging off the table.
Open addressing :
/trianglerightsldPut everything into the table, but not necessarily into
cellh(k).
The perfect hash :
/trianglerightsldWhen you know the keys in advance, construct hash
functions that avoids collisions entirely.
Multiple-choice hashing/Cuckoo hashing :
/trianglerightsldConsider exactly two locations h1(k)andh2(k)only.
HASHTABLES 10/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
HANDLING COLLISIONS
We will only consider the ﬁrst two.
We will assume we have a set nkeys Kand a
hash function h:key→[0,..., m−1]for some
m.
HASHTABLES 11/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
SEPARATE CHAINING
Maintain an array of linked lists (buckets).
Keys that hash to the same value live in the
same list at location h(k)
Insertion : Insert at the beginning
/trianglerightsldMultiple inserts for the same key ⇒traverse the list
/trianglerightsldMay as well insert at the end.
Find : hash to h(k)and search in the list.
Delete : remove from the list.
HASHTABLES 12/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
SEPARATE CHAINING
Costs depend on the load factor λ=n/mwhich
is also the average length of a list.
HASHTABLES 13/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
SEPARATE CHAINING
Assume h(k)takes O(1)work and we have
simple uniform hashing
Unsuccessful search takes expected Θ(1+λ)
work.
/trianglerightsldO(1)forh(k)andλfor traversing the list.
HASHTABLES 14/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
SEPARATE CHAINING
Successful search takes expected Θ(1+λ)work.
Cost of Successful search = Cost of unsuccessful
search at the time of insertion (Why?)
With ikeys, the unsuccesssful search would take
(1+i/m)work.
Averaging over iwe get
1
nn−1/summationdisplay
i=0(1+i/m) =1+(n−1)/2m=1+λ/2−λ/2m= Θ( 1+λ)
Considering constant factors, successful search
looks at 1/2 the list on the average.
HASHTABLES 15/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPENADDRESSING
No lists – everything is stored in the array directly
The arrays is some constant factor larger than
the maximum number of keys we want to store.
HASHTABLES 16/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 17/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 18/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 19/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 20/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 21/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 22/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 23/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 24/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 25/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 26/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 27/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 28/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 29/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 30/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 31/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 32/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 33/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
ANEXAMPLE
HASHTABLES 34/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPENADDRESSING
Open addressing uses an ordered sequence of
locations.
h(k,i)gives us the ithlocation for key k.
/angbracketlefth(k,0),h(k,1),h(k,2),.../angbracketrightis the probe
sequence .
Try these locations in order until an empty cell is
found and insert there.
HASHTABLES 35/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPENADDRESSING - INSERT
1funinsert (T,k) =
2let
3 funinsert/prime(T,k,i) =
4 case nth T h (k,i)of
5 NONE⇒update (h(k,i),k)T
6|⇒insert/prime(T,k,i+1)
7in
8 insert/prime(T,k,1)
9end
Tmust be an ST array - otherwise work and
span are not constant.
Need to check if table is full and the key is
already in the table or not.
HASHTABLES 36/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPENADDRESSING -SEARCH
1funfind (T,k) =
2let
3 funfind/prime(T,k,i) =
4 case nth T h (k,i)of
5 NONE⇒false
6|SOME (k/prime)⇒if(eq(k,k/prime))then true
7 else find/prime(T,k,i+1)
8in
9 find/prime(T,k,1)
10 end
HASHTABLES 37/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPENADDRESSING -DELETE
We can not just delete an items and set its cell to
NONE ! (Why ?)
find will stop searching if it encounters an
empty cell.
Uselazy delete
/trianglerightsldInstead of deleting, use a special value HOLD .
1datatypeαentry =EMPTY|HOLD|FULL ofα
Find and Insert will need to be changed
accordingly.
Lazy delete effectively increases load factor.
Rehashing to the rescue!
HASHTABLES 38/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
OPENADDRESSING
Linear Probing
Quadratic Probing
Double Hashing
HASHTABLES 39/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
LINEAR PROBING
We check cell at h(k,i) = (h(k) +i)mod min
ithprobe.
mpossible probe sequences.
Keys tend to cluster – primary clustering .
/trianglerightsldInserts add to a cluster
/trianglerightsldProbe sequences get longer and longer
HASHTABLES 40/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
IMPACT OF CLUSTERING
Assume table is half full ( λ=1/2)
Minimum clustering when every other cell is
empty!
Average probes for insert is 3/2
/trianglerightsldOne probe to check cell h(k)
/trianglerightsld+with 1/2 chance try the next cell (which by design
should be empty)
HASHTABLES 41/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
IMPACT OF CLUSTERING
Worst case: all keys are clustered to the second
half of the array. (Remember λ=1/2⇒m=2n)
How many probes for positions 0 through n−1?
/trianglerightsld1 (Why?)
How many probes when initial hash is to cell n?
/trianglerightsldn(Why?)
How many probes when initial hash is to cell
n+1?
/trianglerightsldn−1 (Why?)
Average is
(n+[n+(n−1)+(n−2)+....+1])/m=n/m+n(n+1)/2m≈n/4
Even though though the average cluster length is
2, the cost is about n/4 probes.
HASHTABLES 42/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
COSTS FOR LINEAR PROBING
Given a hash table of size mand with n=λm
keys.
The cost of an unsuccessful search/insert is
1
2/parenleftbigg
1+1
1−λ2/parenrightbigg
The cost of an successful search is
1
2/parenleftbigg
1+1
1−λ/parenrightbigg
.
HASHTABLES 43/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
COSTS FOR LINEAR PROBING
HASHTABLES 44/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
COSTS FOR LINEAR PROBING
HASHTABLES 45/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
COSTS FOR LINEAR PROBING
HASHTABLES 46/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
COSTS FOR LINEAR PROBING
HASHTABLES 47/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
QUADRATIC PROBING
We check cell at h(k,i) = (h(k) +i2)mod min
ithprobe.
Makes longer jumps
Avoids primary clustering
But has secondary clustering .
Since there are mpossible positions there are m
probe sequences.
Not all available cells get probed (Why?)
HASHTABLES 48/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
QUADRATIC PROBING
Ifmis prime and the table is at least half empty,
then quadratic probing will always ﬁnd an empty
location.
Furthermore, no locations are checked twice.
HASHTABLES 49/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
QUADRATIC PROBING
Consider two probe locations h(k) +i2and
h(k) +j2,0≤i,j<⌈m/2⌉.
Suppose the locations are the same but i/negationslash=j.
h(k) +i2≡(h(k) +j2)mod m
i2≡j2mod m
i2−j2≡0 mod m
(i−j)(i+j)≡0 mod m
Therefore, either i−jori+jare divisible by m.
But since both i−jandi+jare less than mand
mis prime, they cannot be divisible by m.
Thus the ﬁrst⌈m/2⌉probes are distinct and
guaranteed to ﬁnd an empty location.
HASHTABLES 50/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
QUADRATIC PROBING
Computing the next hash value is only slightly
more expensive
hi−hi−1≡(i2−(i−1)2)mod m
hi≡(hi−1+2i−1)mod m
If the table gets too full, one can resize and
rehash
/trianglerightsldConstant additional overhead
HASHTABLES 51/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
DOUBLE HASHING
Uses two hash-functions:
/trianglerightsldinitial location
/trianglerightsldsize of the jump
ithprobe is
h(k,i) = (h1(k) +i·h2(k))mod m.
Different keys are likely to have different values
jump function if they collide.
Avoids secondary clustering
h2(k)should be relatively prime to mto probe
each locations.
/trianglerightsldmprime and 0 <h2(k)<mis one option.
HASHTABLES 52/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
DOUBLE HASHING
The average number of probes for an
unsuccessful search or an insert is at most
1+λ+λ2+...=/parenleftbigg1
1−λ/parenrightbigg
/trianglerightsldWhy?
The average number of probes for a successful
search is
1
λ/parenleftbigg
1+ln/parenleftbigg1
1−λ/parenrightbigg/parenrightbigg
.
/trianglerightsldSame argument of averaging over probes at insertion
time.
HASHTABLES 53/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
DOUBLE HASHING
λ 1/4 1/2 2/3 3/4 9/10
successful 1.2 1.4 1.6 1.8 2.6
unsuccessful 1.3 1.5 2.0 3.0 5.5
Allows for smaller tables than linear or quadratic
probing
Higher cost for hash function
HASHTABLES 54/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
PARALLEL HASHING
injectCond (IV,S) : (int×α)seq×(αoption )seq→
(αoption )seq.
Conditionally writes each value vjinto location ijofS
/trianglerightsldif the location is set to NONE
1funinsert (T,K) =
2let
3 funinsert/prime(T,K,i) =
4 if|K|=0then T
5 else let
6 valT/prime=injectCond ({(h(k,i),k) :k∈K},T)
7 valK/prime={k:k∈K|T[h(k,i)]/negationslash=k}
8 in
9 insert/prime(T/prime,K/prime,i+1) end
10 in
11 insert/prime(T,k,1)
12 end
HASHTABLES 55/55
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS SPRING 2013
15-210
PARALLEL AND SEQUENTIAL
ALGORITHMS AND DATA
STRUCTURES
LECTURE 27
PRIORITY QUEUES
SYNOPSIS
Priority Queues
Heaps
Meldable Priority Queues
Leftist Heaps
PRIORITY QUEUES 2/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PRIORITY QUEUES
Abstract Data Type supporting
/trianglerightslddeleteMin /deleteMax
/trianglerightsldinsert
Used in many useful algorithms
/trianglerightsldDijkstra’ Algorithm
/trianglerightsldPrim’s Algorithm for MST
/trianglerightsldConstructing Huffman Codes
/trianglerightsldHeapsort
PRIORITY QUEUES 3/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
HEAPSORT
1funsort S=
2let
3 valpq=iter Q.insert Q.empty S
4 funsort/primepq=
5 let
6 case (PQ.deleteMin pq)of
7 NONE⇒ []
8|SOME (v,pq/prime)⇒v::sort/prime(pq/prime)
9in
10 Seq.fromList (sort/primepq)
11end
PRIORITY QUEUES 4/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
UNDERLYING IMPLEMENTATIONS
Sorted and Unsorted Lists/Arrays
/trianglerightsldOne of deleteMin andinsert is fast ( O(1))
/trianglerightsldThe other is slow. O(n)
Balanced binary search trees
/trianglerightsldBoth operations have O(logn)work and span.
Binary heaps
/trianglerightsldBoth operations have O(logn)work and span.
/trianglerightsldBut binary heaps provide a O(1)work findMin
operation.
PRIORITY QUEUES 5/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
HEAPS
Amin-heap (max-heap ) is a rooted tree
Key at every node is ≤(≥) all descendants.
Abinary heap is heap which has
/trianglerightsldShape property : The tree is a complete binary tree
⋆All levels of the tree are completely ﬁlled except the
bottom level, which is ﬁlled from the left
/trianglerightsldHeap Property
PRIORITY QUEUES 6/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BINARY HEAPS
A complete tree
PRIORITY QUEUES 7/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BINARY HEAPS
An incomplete tree
PRIORITY QUEUES 8/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BINARY HEAPS
Shape Property⇒binary heap can be
maintained in an array.
Index of a parent or a child is very easy to
compute
Operations ﬁrst restore shape property, then
heap property.
PRIORITY QUEUES 9/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BINARY HEAPS AND ARRAYS
PRIORITY QUEUES 10/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BUILDING PRIORITY QUEUES
We can insert elements one-by-one
/trianglerightsldWith balanced binary trees and binary heaps, work is
O(nlogn)
/trianglerightsldCan we do better?
Build the heap recursively
/trianglerightsldIf left and right sides are already heaps, just shift
down the root element.
PRIORITY QUEUES 11/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
BUILDING HEAPS DIRECTLY
1funsequentialFromSeq S=
2let
3 funheapify (S,i) =
4 if(i>=|S|/2)then S
5 else let
6 valS/prime=heapify (S,2∗i+1)
7 valS/prime/prime=heapify (S/prime,2∗i+2)
8 inshiftDown (S/prime/prime,i)end
9inheapify (S,0)end
PRIORITY QUEUES 12/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
COSTANALYSIS
shiftDown does O(logn)work on subtree of
sizen
W(n) =2W(n/2) +O(logn)∈O(n)
Opportunities for parallelism?
PRIORITY QUEUES 13/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLEL HEAPIFY
Green cells are OK
All the pinks cells can be shifted down in parallel
Then all purple cells can be shifted down in
parallel
(All) Red cell(s) can be shifted down in parallel
PRIORITY QUEUES 14/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLEL HEAPIFY
01234568710912111413
PRIORITY QUEUES 15/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLEL HEAPIFY
We use Single-threaded sequences
1funfromSeq S:’a seq =
2let
3 funheapify (S,d) =
4 let
5 valS/prime=shiftDown (S,/angbracketleftbig
2d−1, . . . , 2d+1−2/angbracketrightbig
,d)
6 in
7 if(d=0)then S/prime
8 else heapify (S/prime,d−1)
9inheapify (S,⌊log2n⌋−1)end
S(n) =S(n/2) +O(logn)∈O(log2n)
PRIORITY QUEUES 16/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PARALLEL HEAPIFY
01234568710912111413
d=2⇒shiftDown (S,<3, 4, 5, 6 >, 2)
d=1⇒shiftDown (S,<1, 2>, 1)
d=0⇒shiftDown (S,<0>, 0)
PRIORITY QUEUES 17/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PRIORITY QUEUES – SUMMARY
Data. Str. findMin deleteMin insert fromSeq
sorted linked O(1) O(1) O(n) O(nlogn)
list
unsorted linked O(n) O(n) O(1) O(n)
list
balanced O(logn) O(logn) O(logn)O(nlogn)
search tree
binary heap O(1) O(logn) O(logn) O(n)
PRIORITY QUEUES 18/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MELDABLE PRIORITY QUEUES
Priority Queues with an additional meld operation
/trianglerightsldJust like the union in BSTs
/trianglerightsldTakes two meldable PQs and returns the union as a
meldable PQ
Implementations uses leftist heaps
/trianglerightsldSame work and span as binary heaps for insert,
deletemin
/trianglerightsldMeld has O(logn+logm)work and span where m
andnare the heap sizes
PRIORITY QUEUES 19/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINHEAPS
Binary tree
Maintains the heap property
But does notmaintain the complete binary tree
property
Here is an example
o 3
/ \
7 o o 8
/ \
11 o o 15
/ \
22 o o 16
PRIORITY QUEUES 20/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINHEAPS
To implement deleteMin
/trianglerightsldRemove the root
7 o o 8
/ \
11 o o 15
/ \
22 o o 16
We can then use meld to union the heaps.
PRIORITY QUEUES 21/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
MINHEAPS
To implement insert
/trianglerightsldWe create a single node heap
/trianglerightsldmeld it with the original heap
fromSeq is also easy using reduce
val pq = Seq.reduce Q.meld Q.empty
(Seq.map Q.singleton S)
PRIORITY QUEUES 22/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEMELDOPERATION
So we only need the meld operation
Consider
4 o o 3
/ \ / \
11 o o 7 8 o o 5
/ \ /
19 o o 23 14 o
Which element goes to the root?
PRIORITY QUEUES 23/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEMELDOPERATION
Select the tree with the smaller root and
recursively meld with one of its children
o 3
/ \
8 o = meld ( 4 o , o 5 )
/ / \
14 o 11 o o 7
/ \
19 o o 23
PRIORITY QUEUES 24/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEMELDOPERATION
Applying recursively
o 3
/ \
8 o o 4
/ / \
14 o 11 o = meld ( o 7 o 5)
/ \
19 o o 23
PRIORITY QUEUES 25/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEMELDOPERATION
Applying recursively
o 3
/ \
8 o o 4
/ / \
14 o 11 o o 5
/ \ \
19 o o 23 = meld( o 7 ,empty)
Melding Awith an empty heap gives A
PRIORITY QUEUES 26/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
THEMELDOPERATION
1datatype PQ =Leaf|Node of(key×PQ×PQ)
2funmeld (A,B) =
3 case (A,B)of
4 (,Leaf )⇒A
5|(Leaf ,)⇒B
6|(Node (ka,La,Ra),Node (kb,Lb,Rb))⇒
7 case Key.compare (ka,kb)of
8 LESS⇒Node (ka,La,meld (Ra,B))
9|⇒Node (kb,Lb,meld (A,Rb))
Traverses the right spines of the trees
Could be Θ(|A|+|B|)in the worst case.
PRIORITY QUEUES 27/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LEFTIST HEAPS
When melding, keep trees deeper on the left.
Deﬁne
rank (x) = # of nodes on the right
spine of the subtree rooted at x,
For all nodes, rank can be inductively deﬁned
rank (leaf) =0
rank (node (,,R) =1+rank (R)
PRIORITY QUEUES 28/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LEFTIST PROPERTY
For all node xin a leftist heap,
rank (L(x))≥rank (R(x))
/trianglerightsldL(x)andR(x)are the left and child children of x
Allows for
o 1
/
o 2
/
o 3
.
.
o n
But this is OK (Why?)
PRIORITY QUEUES 29/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LEFTIST HEAPS
Most items pile to the left
Right spine is relatively short!
LEMMA
In a leftist heap with nentries, the rank of the root
node is at most log2(n+1).
PRIORITY QUEUES 30/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LEFTIST HEAPS
1datatype PQ =Leaf|Node of(int×key×PQ×PQ)
2funrank Leaf =0
3|rank (Node (r,,,)) = r
4funmakeLeftistNode (v,L,R) =
5 if(rank (L)<rank (R))
6 then Node (1+rank (L),v,R,L)
7 else Node (1+rank (R),v,L,R)
Puts lower rank subtree to the right!
PRIORITY QUEUES 31/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LEFTIST HEAPS
1funmeld (A,B) =
2 case (A,B)of
3 (,Leaf )⇒A
4|(Leaf , )⇒B
5|(Node (,ka,La,Ra),Node (,kb,Lb,Rb))⇒
6 case Key.compare (ka,kb)of
7 LESS⇒makeLeftistNode (ka,La,meld (Ra,B))
8|⇒makeLeftistNode (kb,Lb,meld (A,Rb))
PRIORITY QUEUES 32/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
LEFTIST HEAPS
THEOREM
IfAandBare leftists heaps then
themeld (A,B)algorithm runs in
O(log(|A|) +log(|B|))work, and
returns a leftist heap containing the union of A
andB.
Code traverses the right spines, one node at a
time
/trianglerightsldso needs at most rank (A) +rank (B)steps
/trianglerightsldEach step needs constant work
makeLeftistNode guarantees leftist result
PRIORITY QUEUES 33/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROVING THE LEMMA
CLAIM
If a heap has rank r, it contains at least 2r−1 entries.
n(r)≡nodes in the smallest heap of rank r
/trianglerightsldMonotone: if r/prime≥r, then n(r/prime)≥n(r)
/trianglerightsldn(0) =0
rank (L(x))≥rank (R(x)) = r−1
n(r) = 1+n(rank (L(x))) + n(rank (R(x)))
≥1+n(r−1) +n(r−1) =1+2·n(r−1).
n(r)≥2r−1
PRIORITY QUEUES 34/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
PROVING THE LEMMA
Apply the claim
Suppose leftist heap of nnodes has rank r
n≥n(r)≥2r−1
2r≤n+1⇒r≤log2(n+1)
Rank of a leftist node of nnodes is at most
log2(n+1)
PRIORITY QUEUES 35/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013
SUMMARY OF PRIORITY QUEUES
Implementation insert findMin deleteMin meld
(Unsorted) Sequence O(n) O(n) O(n) O(m+n)
Sorted Sequence O(n) O(1) O(n) O(m+n)
Balanced Tree O(logn) O(logn) O(logn)O(mlog(1+n
m))
Leftist Heap O(logn) O(1) O(logn) O(logm+logn)
PRIORITY QUEUES 36/36
CMU-Q 15-210 P ARALLEL AND SEQUENTIAL DATASTRUCTURES AND ALGORITHMS FALL2013

Discussion on Mechanical Learning and
Learning Machine
Chuyu Xiong
Independent researcher, New York, USA
Email: chuyux99@gmail.com
November 10, 2021
Abstract
Mechanical learning is a computing system that is based on a set of simple and xed rules, and
can learn from incoming data. A learning machine is a system that realizes mechanical learning.
Importantly, we emphasis that it is based on a set of simple and xed rules, contrasting to often
called machine learning that is sophisticated software based on very complicated mathematical
theory, and often needs human intervene for software ne tune and manual adjustments. Here,
we discuss some basic facts and principles of such system, and try to lay down a framework for
further study. We propose 2 directions to approach mechanical learning, just like Church-Turing
pair: one is trying to realize a learning machine, another is trying to well describe the mechanical
learning.
Keywords: Mechanical Learning, Learning Machine, Spatial Learning, Church-Turing
Thesis
The only way to rectify our reasonings is to make them as tangible
as those of the Mathematicians, so that we can nd our error at a glance,
and when there are disputes among persons, we can simply say:
Let us calculate, without further ado, to see who is right.
|-Gottfried Leibniz
1 Introduction
In recent years, machine learning becomes hot topic of research and IT development. Yet, there
are still some very fundamental problems need to be addressed. In the eort to understand these
problems, we brought up the term mechanical learning. Here, we will try to lay down the discussion
framework for mechanical learning.
While electronic devices can do numerical computation eectively, and actually can do many com-
plicated even intelligent things, however, inside the device there is a core that is very mechanical ,
i.e. the device is governed by a set of simple and xed rules . The ability of electronic device doing
complicated information processing comes from that it is running a pre-installed program that is from
human intelligence. In another words, the core of computing device is very mechanical, and its ability
of complicated information processing is endowed by human intelligence.
In machine learning software, the situation seems dierent. For a machine learning software, its
ability of information processing is, at least partially, from learning. Naturally, we would ask: can
Great thanks for whole heart support of my wife Yiping. Thanks for Internet and research contents contributers to
Internet.
1arXiv:1602.00198v1  [cs.AI]  31 Jan 2016
2 Mechanical Learning and Learning Machine
a computing device endows itself the ability of information processing by learning? It is not easy
to answer. Machine learning software seems acquires ability of information processing from learning,
however, if we look more deeply, we would notice that such learning very heavily depends on human
intervenes and involvements. This motivates us to consider to isolate the part of learning that does
not need human intervene. So, it follows to put the requirement of a set of simple and xed rules .
The purpose to use term mechanical is to emphasis a set of simple and xed rules , not to mean gears,
levers, pushers and pullers. We would like to call doing things by a set of simple and xed rules
as mechanical. This is in-line with historical usage, such as mechanical reasoning and mechanical
computing.
At rst, we introduce IPU (information processing unit), and base our discussions on IPU. We then
explain more why we are interested in mechanical learning , or learning by a set of simple and xed
rules. To demonstrate the eectiveness of such requirement, we show that we could reach some
important implication by this line of thinking.
Once we are thinking in this way, we naturally draw analogy between mechanical computing and
mechanical learning. Also, naturally, we recall the fundamental works of Turing and Church on
mechanical computing and reasoning. This strongly suggests we should go 2 dierent and equivalent
approaches to mechanical learning: to extend Church-Turing thesis, i.e., one way is to construct an
universal learning machine, and equivalently, another way is to well describe mechanical learning.
Church-Turing thesis gave people the key to understand mechanical computing, and we believe, the
extended Church-Turing thesis will give us good guidance on mechanical learning.
This paper is the rst one for our discussions on mechanical learning. We will write down our studies
in next papers. In last section, we put down some topics that we would like to discuss further.
In this discussion, we will restrict us to spatial learning, not consider temporal learning. First, let's
make the distinguish of spatial andtemporal roughly here. For learning machine, we of course concern
patterns. Roughly say, the pattern along the incoming space is spatial pattern, while the pattern along
the time line is temporal pattern. To say that we restrict us to spatial learning, it means that we
only consider the pattern of incoming space, not consider the pattern of several patterns sequentially
coming. For a simple example, consider letters: A, B, C, ..., Z, each single letter is one spatial pattern.
To restrict to spatial pattern, the learning machine will be able to consider (and possibly to learn)
patterns A, B, ..., Z, but will not, and is not able to consider (and possibly to learn) the sequence of
letters, such AB, CZW, etc. The meaning of term spatial will become more clear in later discussions.
By restricting us to spatial learning, we can simplify the discussion so that we can go in more depth.
Of course, any true learning system should consider both spatial and temporal learning together. But,
that is beyond the scope of current discussion.
2 Information Processing Unit and Mechanical Learning
Here, we try to formalize the mechanical learning and related principles. We will start from informa-
tion processing since learning is inseparably linked to information and how information is processed.
Actually, information processing is computing. Thus, what we are talking about here is actually to
view computing in a dierent angle.
Denition Information Processing Unit:
Information Processing Unit (IPU) is such an entity: it has input space and output space, input space
is oneN-bit binary array I; output space is one M-bit binary array O. For any input i2I, there will
be one corresponding output o2O. We will call it as N-MIPU.
Chuyu Xiong 3
Fig. 1. Illustration of N-MInformation Processing Unit
At this moment, we do not focus on how the information is processed inside IPU, instead, we focus on
information input and output. In our notation, for one input i2I, the output is o=P(i), we callP
the processing of IPU. So, in fact, the processing Pis one mapping between binary array, P:I!M,
i.e. for every member i2I, there is one o2O, so thato=P(i). Clearly, one particular processing
denes the particular behavior of IPU.
Here, we can make the term spatial and spatial learning more clear. Exactly, the meaning of spatial
is: for input i2I, the output o=P(i) will only depend on i, not on any other, no any context to
depend, such as the previous input or later input. So, the term spatial exactly means only to consider
input itself, and no any inuence from time line. If the output also depends on context, the IPU
would be called temporal. Clearly, we could make one IPU be temporal. But, we will restrict us on
spatial IPU. This is the exact mean for spatial .
We are not just interested in one particular processing. We are mostly interested in how processing
is changing and how processing is learned. Thus, we would consider all possible processing for N-M
IPU. We have this:
Lemma 1
ForN-MIPU, the total possible processing are 2M2N. Using the term of bits, the number of all
possible processing is M2Nbits.
The proof of Lemma 1 is very straightforward. But, this simple fact has great implications as we
will demonstrate later. Simply say, except very small N, forN-MIPU, the number of total possible
processing is extremely huge. To show this as one quick example, we can consider the cognition of
hand written digits. In this case, we need to consider input space of 28 28 black-white pixels. So,
the input space has N= 2828 = 784. Thus, all possible processing are in the order of 2784bits!
IPU actually can be thought as a computing unit. In fact, a computer CPU is one IPU. So, with
recursive usage and a well designed program to do the recursive usage, any computation can be done
by one IPU. In this sense, it seems no reason to introduce the term of IPU. However, the reason to
introduce such term is to emphasize the information processing: input Nbits information and output
Mbits information. In this way, we can focus on properties of information processing, specially how
the processing is learned.
To change the processing of one IPU has many ways. To change the processing inside it manually,
either by programming, or some ne tuning, clearly is one way. However, we only are interested in the
changes driven by experience/data. If for a IPU M, and if there is a sequence of data fik;okg;ik2
I;ok2O;k = 1;2;:::feed intoM, and under such data driving, the processing of Mis changing,
e.g. fromP1toP2, we would sayMis learning. So, we have:
Denition Learning of IPU:
For one IPUM, if the processing of Mis changing under the driving force of feed-in of input data
and feedback of output data, we call Mis learning.
For learning of IPU, our focus is: under what data, what changes of processing occur, and how.
Yet, there could still have too many things involved in changing of processing. For example, to
4 Mechanical Learning and Learning Machine
manually modify software sure could change the processing. More subtly, to manually put bias into
the computing system could also change the processing. Surely, we would like to exclude all such
factors. So, we have:
Denition Mechanical Learning of IPU:
For one IPUM, if the processing of Mis changing under the driving force of feed-in of input data
and feedback of output data, and the changing is according to a set of simple and xed rules , we call
Mis doing mechanical learning.
This denition could hardly be called as a mathematical denition. But, it is the best so far we could
make. We will discuss more in later section.
If we can build one computing system that can realize mechanical learning, we call it learning ma-
chine. A learning machine could be specialized hardware, or a pure software sitting in a computing
environment (cloud, supercomputer, PC, or even cell phone), or combination of them, etc. The most
important property of a learning machine is: it is doing mechanical learning.
One immediate consideration for a learning machine would be: universal learning.
Denition Universal Learning:
For one learning machine M, if for any giving processing P(i.e. one mapping from ItoO), no matter
what the current processing of Mis,Mcould learn P(i.e. its processing becomes P), then, we call
Muniversal.
Simply say, a universal learning machine can learning any processing (starting from any processing).
This is a very high requirement, however, as we will see later, this seems very high requirement is
actually quite necessary.
There is a group of learning machine we should specially notice: parameterized learning machine .
Denition Parameterized Mechanical Learning:
If a learning machine Mis well dened and be controlled by a group of parameters, 1;2;:::; L,
and the learning is by changing the parameters, we call such a learning as parameterized mechanical
learning.
Fig. 2. Illustration of Parameterized Mechanical Learning
Currently, almost all machine learning models are actually parameterized. This fact also has big
implications as we can see later.
For parameterized learning machine, the learning is actually realized by changing its parameters
1;2;:::; L. Naturally, a question follows: how many possible dierent processing could be allowed
by changing parameters 1;2;:::; L? So, we dene:
Denition Eects of Parameter on Processing:
For a parameterized learning machine M, ifis one of its parameter, and when varies in its full
range, the total possible dierent processing are less than a number e, we then say, this parameter
has at most eeects on processing. We often use bits, i.e. log2(e).
Chuyu Xiong 5
The relationship between parameters and the eects on processing is very complicated. However, if
we know all these parameters have nite values, we at least know the upper limit of total possible
dierent processing. This is true for most computing system. For example, if parameters 1;2;:::; L
are double precision oating numbers, then each nukhas at most 64 bits nite values, i.e. at most 64
bits eects on processing. This simple fact is also useful.
We talk some examples of IPU in next section. 2-1 IPU is the simplest IPU, yet it still reveals some
very interesting properties for us. See appendix for details.
3 Examples of Mechanical Learning
Now, we see some examples.
Examples of IPU
1. See simplest IPU 2-1 IPU in appendix.
2. One mathematical function is one IPU: P:I!O. Such function with Nbits variables and M
bits function value is one N-MIPU.
3. One software with well-dened input and no context dependence is one IPU. Such software with
Nbits input and Mbits output is one N-MIPU. Many statistics software would t in this
category.
4. One CPU with certain restriction so that it does not have any context is one IPU. Such CPU
actually could be viewed as one of mathematical function (but, its denition is complicated).
For example, one 64-bits CPU, if we take some restriction, is one 64-64 IPU.
5. One machine learning software is one IPU. Of course, its processing will be able to change
(learn).
6. Abstractly, and with certain restriction, some processing region in our brain neocortex (for
example, that is responsible for digits recognition) can be thought as one IPU ( Nmust be great,
andMis small).
7. Even more abstractly, and with certain restriction, one decision process is one IPU. Here, the
decision process can be in one animal's brain (or even more primary, such as ganglion of a y),
or a meeting of a company's board, etc. For such IPU, Nis big, butM= 1.
As we see in examples, IPUs are everywhere. Actually, the center of IPU is its ability of in-
formation processing. We are most interested in where such ability comes and how such ability
adapt/change/learn. Let's see some examples about ability of information processing.
Examples of IPU, about its information processing
1. For IPU formed by a mathematical function, its ability of processing comes from the denition
of mathematical function. If this function is computable, we can use computer to realize the
processing. So, the ability is from programing.
2. For the software with well-dened input, clearly, the ability is from programming.
3. For CPU, the ability clearly comes from hardware setup and software build into it.
4. For many machine learning software, one would suppose its ability of information processing
comes from learning. However, we should examine more deeply, we know that the ability actually
partially comes from programming and setup, and eects of both learning and setup are mixed,
and not easy to distinguish. This fact actually motivate us to bring up the term of mechanical
learning.
6 Mechanical Learning and Learning Machine
5. For the region in our brain that is responsible for digits recognition, it is safe to claim that the
information processing ability is from learning (but a very long learning, starting from baby
time, and from school days). We indeed learn this ability. However, the learning is also depends
on pre-wiring of the brain region. And we know that the learning is not mechanical.
6. For one decision process that we abstractly think as one IPU, the ability of information processing
is partially from programming, and partially from learning. For example, consider the decision
process of a company board as one IPU, then, its ability of information processing partially
comes from set up, e.g. the predened rules, and partially comes from learning, e.g. the success
or failure experienced. The learning clearly is not mechanical.
Of course, we are mostly interested in those IPU, whose information processing is changing, specially,
adapting and learning. We can see some examples below.
Examples of IPU, information processing is changing/adapting/learning
1. For IPU formed by a mathematical function, if the processing can change, then such property
must be built in the denition of mathematical function. Mostly likely, it is parameterized. That
is to say,P:I!Ois the mathematical function, which has parameters 1;2;:::, so that when
kchange values, the processing will change accordingly. Learning is to change the parameter
values. Actually, many, if not most, IPUs are this type.
2. For so called neuromorphic chip, such as Truenorth of IBM, it can change its information pro-
cessing. The ability to change the processing is built into the hardware. Such kind of hardware
are just at the very beginning of its development, a lot of modication of such chip will be
expected. However, we might be able to classify them as parameterized.
3. For one machine learning software, it indeed has ability to change its processing. For most
current machine learning software, we can classify them as parameterized.
4. For a statistics model, it often likes this: mathematical functions + database. This is IPU and its
processing is changing/adapting. Database is used to store the incoming data, and mathematical
function is statistical model that does calculations based on the data in database. Such IPU is
parameterized.
5. One particular ANN, Restricted Boltzmann Machine (RBM), is the center of many machine
learning software. Clearly, it is one N-MIPU. RBM is actually completely determined by its
entries matrix, a MxNmatrix. So, it is parameterized.
4 Why Mechanical Learning?
We dened mechanical learning and learning machine, and saw some examples in the previous sections.
Simply say, mechanical learning is: One computing system Mimproves its ability of information
processing according to a set of simple and xed rules under the driving of incoming data.
But, why are we interested in a set of simple and xed rules ? Let's rst explain our thoughts about
this.
Seems many current machine learning software are doing well, they do not emphasis mechanical side,
but, they emphasis how to make computing system learning from data, and how to do so better. This
is perfectly ne. So, is it necessary to bring up the term mechanical and post mechanical requirement
on learning?
Against such thought, Je Hawkins gave a very strong point [2]: In order to build a learning machine
that has potential to become next generation computing system, it must be independent from the
learning tasks. Recall history of computing could help us to better see this. Before von Neumann
architecture of computer, there were many systems or devices that could do eective job for certain
tasks. However, all of them disappeared. Requirement of "independent from any particular task" is
indeed playing the crucial role. Armed by this history knowledge, we would expect to see similar for
learning.
Chuyu Xiong 7
However, current machine learning software heavily depend on human intervenes and involvements,
and are quite depend on specic learning tasks. This motivates us to consider to isolate the part of
learning that does not need human intervene, and independent from learning tasks. For this reason,
we post the mechanical requirement.
Such a thought is not new. Many people have been trying to do so. Numenta developed CLA
algorithm trying to closely simulate human brain neocortex [1]. By doing so, it hopes to establish one
computing system that is independent from any particular learning tasks. Though, at current stage,
Numenta's CLA focuses on temporal learning. We think that spatial learning should be studied rst
and it is easier to deal with spatial learning rst. Nonetheless, CLA is an algorithm formed by a set of
simple and xed rules. Once it is setup, human intervene is not necessary and CLA is learning from
incoming data. In this sense, we can say, CLA is doing mechanical learning. Of course, CLA is still at
its rst stage of development, and might not fully realize its goal. However, at least, this is intention
of CLA.
Besides CLA, there are other eorts trying to build master algorithm independent from particular
task. For example, Pedro Domingos is trying to unite 5 kinds of learning methods [4]: logic learning,
connectionist learning, probabilistic learning, analogy, and evolution. If anyone can successfully unites
these learning methods, the underneath principle of new method must be simpler, not more compli-
cated. So, we should expect a simple and xed rules underneath those dierent types of methods.
Even more, people now start to question if we can capture the mathematical theory of human brain
(of course including learning). For example, see the famous 23 problems of DARPA [3].
Naturally, in order to do those tasks list above that is aiming very high, we can expect to consider rst
step: mechanical learning. If we could understand mechanical learning better, we are better prepared
for those high tasks.
Now, we can come back to the denition of mechanical learning we gave in section 1. We have to say,
it is not very precise. What is mean for "a set of simple and xed rules"? But, perhaps, this is the
best we can do up to now, we cannot give a better and more precise denition for mechanical learning.
However, on the other side, it is very important for us to post mechanical requirement on learning,
even though we do not know exactly this requirement really means. We can sense the importance of
such requirement and can only roughly grasp some basic skeleton of such requirement.
Again, in order to help us to see better, we will consult history of mechanical reasoning and me-
chanical computing. It is Leibniz rst requested "mechanical reasoning". After him, great amount
of eorts were paid to concretely realize "mechanical reasoning", from Cantor, Fred, Hilbert, Russell,
till Church and Turing. After many great works done by great scientists and mathematicians, now,
we know exactly what mechanical reasoning and computing means: It is what Turing machine does,
or equivalently, it is what -calculus describes. It is this great process of pursuing to understand
mechanical reasoning and mechanical computing gives us the key to modern computer era.
We see strong analogy between mechanical computing and mechanical learning. So, for mechanical
learning, we can fully expect similar: we do not know exactly mathematical denition of mechanical
learning, but, it will be productive if we post the mechanical requirement on learning. By pursuing
such requirements, we can propel us to the fully understanding of mechanical learning. This pursue
could be a long journey and it might not be easy. But, we can expect the time span is much shorter
since we already have the guidance of history of development of mechanical computing and mechanical
reasoning.
Currently, a lot of eorts are put on how to do machine learning, and how to do machine learning
better. But, in the process, some very fundamental questions have to be addressed. We can list some
here:
1. What is really learned? What is really learned in a deep learning software? This question can
not be answered precisely. What is really learned in a probabilistic learning module? Is it just
some parameter adapting? The question can not be answered precisely.
2. What could be learned by computing system? And what could not be learned by computing
system? No precise answers.
8 Mechanical Learning and Learning Machine
3. Why connectionist view is fundamentally important?
4. Can we integrate logic learning, connectionist learning, probabilistic learning and analogy to-
gether eective and smoothly? And how?
5. How to establish one architecture of learning machine independent from individual learning task,
so that this architecture will guide us for next generation of computing?
6. How to teach a computing system for certain tasks, instead of programming it? Or can we do
so? If we can, what is the ecient teaching language/method?
We think, by putting mechanical requirements on learning, we are actually starting to address these
fundamental questions, at least from certain point of view, view of rigorous mathematical reasoning.
To demonstrate this, we will go following arguments, which is quite simple, but reveal some important
implications.
From Lemma 1, we know N-Mlearning machine, the number of all possible processing is M2Nbits.
We can have a lemma for parameterized mechanical learning.
Lemma 2
For a parameterized learning machine M, if its parameters are 1;2;:::; L, and each parameter
has at most ebits eects on processing of M, thenMcould at most have eLbits many dierent
processing. In another words, Mat most could learn eLbits processing.
The proof is very simple. By combining Lemma 1 and Lemma 2 together, we then have:
Theorem 1
For a parameterized learning machine M, most likely, it is not universal learning.
The proof is short: Number of total possible processing of N-Mlearning machine is in order of M2N
bits. Most likely, lemma 2 could apply to M, so the number of processing that Mcould learn at most
in order ofeLbits. Thus, unless Lis in the order 2N,eLM2N. It means whatMcould learn
is much less than M2N, soMcould not be universal. But, it is extremely unlikely, one parameterized
learning machine Mcould have such a large group of parameters (even it has, how it can learn?).
Actually, in simple words, Theorem 1 tells us, in order to build an universal learning machine, we
could not use parameterized learning machine. This simple fact indicates that almost all current
machine learning models are not candidate for universal learning machine. Unlike most of them, CLA
of Numenta [1] might be a system that is not parameterized. However, no one has made a proof yet.
The arguments above are very simple and shallow. However, it already gives us some strong and
very useful indications. Thus, we have strong reason to believe, along this path, eorts could be very
fruitful.
5 How to Approach Mechancial Learning
How to approach and study mechanical learning? This is not an easy question. However, fortunately,
we have a better guide than pioneers of computing. We can recall history of computing to gain some
invaluable guidance.
Before modern computing, people thought about mechanical reasoning and mechanical computing
for many hundred years. In fact, people made many devices for such purposes, from ancient abacus,
to tablet machine, even to Babbage's mechanical computer. And, on theoretical side, people are
fascinated about the mechanical aspects human thoughts, especially computing, and wonder how to
explore and use such aspects. Such thoughts motivated many great scientists and mathematicians
working in this direction. And, big block knowledges are accumulated, such as mathematical logic.
But, until Turing and Church, thoughts were very scattered and not systematic, computing devices
were designed for special purpose, and without guidance of well developed theory. Simply, people still
did not know well what is mechanical logic reasoning and mechanical computing. It is Turing and
Church's theory laid down the foundation and let people start to fully understand what mechanical
computing is, and how to design universal computer that can do all possible mechanical computing.
Chuyu Xiong 9
We might express Church-Turing theory in this way: While Turing machine gives one real model
of mechanical computing, Church's -calculus gives a very precise description on objects that is
mechanically computable. Church-Turing thesis tells us: all mechanically computable (i.e. computable
by Turing machine) can be well described by -calculus, and vise versa.
Using such Church-Turing pair as a guide, in parallel, we will propose to go the same line of thoughts:
we should work on 2 equivalent directions: one is trying to establish a real learning machine that is
based on a set of simple and xed rules, and this learning machine can do universal learning; another
is trying to well describe the objects that can be learned mechanically, and exactly how the mechanical
learning is doing. Going to 2 equivalent directions, would be more fruitful than just going one. For
example, if through the well description, we understand that a learning machine should behave in
certain way, then, such information will help us to design a learning machine.
The second direction, could help us to establish teaching language/method to teach a learning ma-
chine. We would vision, just like programming is super important for computer, teaching would be
super important for learning machine. In another words, instead of programming a machine, we
will teach a machine. But, eective teaching needs good teaching language/method besides data.
A well description of mechanical learning could guide us to develop such teaching language, just like
-calculus guided us to develop programming language. Data is important for teaching. But, teaching
language/method are equally important, if not more.
In this way, Church-Turing pair will continue and be extended: we have a universal learning machine,
and we teach the universal learning machine with the well developed teaching language/methods and
data.
This is what we propose to do. Actually, we did some works in both directions. We will write down
them in dierent places.
6 About Further Works
Current article is the rst one for our discussions on mechanical learning and learning machine. We
will continue to work on the 2 directions talked in last section. We would like to list some topics here.
We would be very glad to see more studies on these topics from all possible point of view.
About Building Universal Learning Machine
In order to build one universal leaning machine, we think following topics are important and fruitful.
1.What way could achieve universal learning? As we discussed, any parameterized learning could
not be universal. However, how can we avoid parameterized? This is not easy at all. If we use well
known mathematical function or sophisticated software as the foundation of the learning machine, it
would inevitable become parameterized, since all such mathematical functions and software are all
parameterized. We think, from this point of view, connectionist view becomes important.
2.First spatial learning, then transit to temporal learning. Here, for the purpose of simplication,
we only discuss spatial learning. However, temporal learning is absolutely necessary. We should rst
study fully spatial learning, then, armed with the knowledge and tools from such studies, we move
to temporal learning, and spatial and temporal learning together. We guess, the transit might not
be super hard. After all, we can gain inspiration from human brain. Human brain surely can handle
spatial and temporal in uniformly way. This indicates to us, in mechanical learning, there could be a
way to handle spatial and temporal learning in uniformly way. True understanding of spatial learning
could be the very key to temporal learning, and vise versa. We have high hope on this part.
About Descriptions of Mechanical Learning
To well describe mechanical learning, we can list some areas below.
1.Generalization. Generalization is very desired for a learning machine. That is to say, only need
to teach some things to the learning machine, then the learning machine could automatically and
correctly generalize to more things. Can a mechanical learning machine to do so? Why this seems
intelligent behavior can be achieved by a mechanical learning machine? And how? We sure would like
to go in depth for this question. Actually, this is the exactly reason that we propose to study spatial
10 Mechanical Learning and Learning Machine
learning rst.
2.Abstraction. As generalization, abstraction is also very desired for a learning machine. Many
researchers have already thought that abstraction might be the key of further development of machine
learning. At the rst step, we need to nd some way to well describe abstraction in mechanical
learning.
3.Prior knowledge and continue learning. A learning machine could have prior knowledge. Yet, what
exactly is prior knowledge? In what form prior knowledge is in a learning machine? Can and how
we inject prior knowledge to a learning machine? How prior knowledge play in the learning/teaching
process?
4.Pattern Complexity vs. Capacity of Learning. Very naturally, learning is closely related to patterns.
We can intuitively say, more complex the pattern associated to learning is, harder the learning would
be. But, is it so? If it is so, can we measure the complexity and hardness exactly? Also, intuitively,
we can think that if one learning machine has a better capacity of learning, it can learn more complex
things. But, is it so? If so, can we say more exactly?
5.Teaching, training and data. For learning machine, programming could still be a way to make it
to do desired tasks. But, teaching or training would be more important and more often be used. So,
how to do teaching or training eciently and eectively? Should we have to use big data? If so, what
big data is really used for?
About Integration of Dierent Types of Machine Learning
Pedro Domingos listed 5 kinds of learning methods [4]: logic, connectionist, probabilistic, analogy,
and evolution. All of them have sounding supports, and each is doing better than others in some
areas. This indicates each of them indeed stands on some important part of the big subject: learning.
Naturally, it is best to integrate them, instead of to choose some and discard others.
We would think connectionist view is going to play a central role, since it is very hard to imagine logic
view could integrate connectionist view (specially, not parameterized), but conversely, it would be
easier to imagine (though, we do not know how at this time). Also, it might be easier to imagine that
a connectionist model can handle analogy. Can we imagine such a system: it is a connectionist system,
and inside it, it accomplishes logic view, probabilistic view and analogy naturally, and evolution is
helping this system improving? If we can achieve such a system, or at least partially, we would progress
very well.
References
[1] Je Hawkins. White Paper: Cortical Learning Algorithm, Numenta Inc, 2010.
http://www.numenta.org
[2] Je Hawkins. Talk on Numenta Software. http://www.numenta.org
[3] The world's 23 toughest math questions: DARPA's math challenges. The question 1.
http://www.networkworld.com/community/blog/worlds-23-toughest-math-questions
[4] Pedro Domingos. The Master Algorithm, Talks at Google.
https://plus.google.com/117039636053462680924/posts/RxnFUqbbFRc
Appendix
2-1 IPU
2-1 IPU is the simplest IPU. For a 2-1 IPU, there are totally 16 (222= 16) possible processing. We
can see all processing in following value table.
Tab. 1. Value table of all processing of 2-1IPU
Chuyu Xiong 11
P0P1P2P3P4P5P6P7P8P9P10P11P12P13P14P15
(0,0) 0 1 0 0 0 1 1 0 1 0 1 1 1 0 0 1
(1,0) 0 0 1 0 0 1 0 1 1 1 0 1 1 0 1 0
(0,1) 0 0 0 1 0 0 1 1 1 1 1 0 1 1 0 0
(1,1) 0 0 0 0 1 0 0 0 1 1 1 1 0 1 1 1
Some processing are quite familiar. For example, P7is atually XOR logical gate, P9is OR logical
gate,P4is AND logical gate. Also note, P8is ip ofP0,P9is ip ofP1, etc.
P2,P3andP4are most important, which are the building blocks in 2-1 IPU. P0is for processing
that output is always 0 no matter what input is. P1looks not for real. But it is for completeness of
discussion. It is easy to see that the rest of processing can be constructed by the above. For example,
P10=P1+P3+P4.
Above, we tell what 2-1 IPU is. And, we point out that we can design an eective learning methods
so that all processing could be learned. For 2-1 IPU, this is very simple. However, this simplest case
could still give us some good guide. For example, 2-1 IPU is embedded in any IPU. Therefore, any
learning machine should eectively handle all processing we listed above, at least.

Distributed Stochastic Multi-Task Learning with Graph
Regularization
Weiran Wang*, Jialei Wang†, Mladen Kolar‡, and Nathan Srebro*
*Toyota Technological Institute at Chicago, IL, USA
†Department of Computer Science, University of Chicago, IL, USA
‡Booth School of Business, University of Chicago, IL, USA
Abstract
We propose methods for distributed graph-based multi-task learning that are based on
weighted averaging of messages from other machines. Uniform averaging or diminishing stepsize
in these methods would yield consensus (single task) learning. We show how simply skewing
the averaging weights or controlling the stepsize allows learning diﬀerent, but related, tasks on
the diﬀerent machines.
1 Introduction
We consider a distributed learning problem in a multi-task setting: each machine ihas access to
samples from a diﬀerent data distribution Di, with potentially a diﬀerent optimal predictor, and
thus a diﬀerent learning task, but where we still assume some similarity between diﬀerent tasks.
The goal of each machine is to ﬁnd a good predictor for its own task, based on its own local data,
as well as communicating with the other machines so as to leverage the similarity to other related
tasks.
Distributed multi-task learning lies between a homogeneous distributed learning setting (e.g.
Shamir and Srebro, 2014), where all machines have data from the same source distribution, and
inhomogeneous consensus problems (e.g. Ram et al., 2010; Boyd et al., 2011; Balcan et al., 2012),
where each machine sees data from a diﬀerent source, but the goal is to reach a single consensus
predictor. In many distributed learning problems, diﬀerent machines do indeed see diﬀerent dis-
tributions. For example, machines might serve diﬀerent geographical regions. In a more extreme
“federated learning” (Konecny et al., 2015) scenario, each machine is a single user device, and its
data distribution might reﬂect e.g. the user’s speech, language biases, usage patterns, etc. Such
heterogeneity requires departing from a homogeneous model. But if the data distribution on each
machine is diﬀerent, we might as well learn a personalized predictor for each machine, while still
leveraging commonalities as in multi-task learning, instead of insisting on consensus. Unlike when
seeking consensus, we could learn a predictor entirely locally, ignoring data on other machines. But
the premise of multi-task learning is that by communicating with other machines we can improve
our predictions, reduce the sample complexity, and hopefully also reduce the computational cost
on each machine by distributing the computation.
1
arXiv:1802.03830v1  [stat.ML]  11 Feb 2018
Central to multi-task learning is the notion of relatedness between tasks. In a high-dimensional
setting, with large number of variables, we might expect a small common set of predictive variables,
where the form of the dependence on variables in this common set varies between tasks (Turlach
et al., 2005; Obozinski et al., 2011; Lounici et al., 2011; Wang et al., 2015). Another approach is
to assume that the predictors lie in a shared lower dimensional subspace (Ando and Zhang, 2005;
Yuan et al., 2007; Wang et al., 2016) or all have low-norm under some shared linear representation
(Amit et al., 2007; Argyriou et al., 2008). Both the shared sparsity and shared subspaces models
have recently been considered in a distributed learning setting (Wang et al., 2015, 2016), and
nuclear-norm regularized multi-task learning has been studied from a distributed optimization
perspective (Baytas et al., 2016).
In this paper, we consider graph-based multi-task learning, where relatedness between tasks is
speciﬁed through a weighted graph over the tasks. Neighboring tasks in the graph are expected
to be similar, with a penalty for dis-similarity speciﬁed by the weight between them (see precise
formulation in Section 2) (Maurer, 2006; Evgeniou et al., 2005). This also generalized a simpler
“fully connected” multi-task model where all predictors are close to each other (Evgeniou and
Pontil, 2004). A predictor-homogeneous assumption can also be viewed as an extreme case where
all weights go to inﬁnity, forcing all predictors to be identical. In distributed multi-task learning,
graph-based relatedness is especially appealing if the relatedness graph also matches the graph of
network links between machines, as might be the case, e.g. in a geographical setting or with physical
sensors. We therefor emphasize and prefer methods with communication only between neighboring
tasks on the graph.
In designing methods for graph-based multi-task learning, we are interested in methods that (1)
are natural and simple—all our algorithms have a similar and natural structure, involving weighted
averaging of messages from neighboring machines and a local gradient or prox calculation; (2) have
low communication costs, are sample eﬃcient, and preferably also have low computational cost; and
(3) are backed by rigorous guarantees on the amount of communication, samples and computation
required.
Graph-based multi-task learning has been recently studied by Vanhaesebrouck et al. (2017)
and Liu et al. (2017), both considering the problem as distributed optimization of the multi-
task regularized empirical objective, similar to our approach in Section 3.2). Vanhaesebrouck
et al. suggested an asynchronous gossip-type algorithms and an ADMM procedure, while Liu et al.
proposed using SDCA, and also considered learning the relatedness graph itself. Neither provides
any statistical analysis, nor analysis of the iteration complexity and communication cost based on
the methods. We conduct detailed comparison of convergence properties with these methods in
Appendix H, providing upper bounds of their iteration complexities when possible; our methods
have faster convergence than the guarantees we could obtain for them. Also, neither directly
considers the underlying learning problem (minimizing the actual expected errors), and so neither
studies stochastic methods (in the ﬂavor of our Section 4).
Here, we show how methods that arise naturally by skewing averaging weights or controlling
stepsize of consensus learning methods do yield good guarantees. We also propose stochastic
methods which allow reducing the computational cost, and we compare the empirical performance
of both our batch and stochastic methods to those of Vanhaesebrouck et al. (2017) and Ma et al.
(2015).
Notations In this paper, boldface lower-case letters denote column vectors, boldface capital
letters denote matrices, vec(U) is the vectorial form of a matrix Uwhich concatenates columns of
2
U, andU⊗Vis the Kronecker product between two matrices UandV. Furthermore,/angbracketleftu,v/angbracketright=u/latticetopv
denotes the inner product of two vectors uandv, while/angbracketleftU,V/angbracketright= tr/parenleftbig
U/latticetopV/parenrightbig
denotes inner product
of two matrices UandVof the same dimensions. We use /bardblu/bardbl=/radicalbig
/angbracketleftu,u/angbracketrightto denote the length of
a vector u,/bardblU/bardblF=/bardblvec(U)/bardblthe Frobenius norm of a matrix U, and/bardblU/bardblM=/radicalbig
tr (UMU/latticetop) =/radicalbig
/angbracketleftUM,U/angbracketrightthe norm of Uwith respect to some positive deﬁnite matrix M. A function f(x) is
Lipschitz if|f(x)−f(y)|≤L/bardblx−y/bardbl,∀x,y. A convex function f(x) isβ-smooth and µ-strongly
convex ifµ
2/bardblx−y/bardbl2≤f(x)−f(y)−/angbracketleft∇f(y),x−y/angbracketright≤β
2/bardblx−y/bardbl2,∀x,y. This deﬁnition extends
to functions of matrices, by replacing the vector norm with the Frobenius norm in the above
inequality.
2 Graph-based multi-task learning
Consider a distributed setting with mmachines, where each machine ihas access to a data dis-
tributionDiand would like to learn a predictor wi∈Rdfor each machines with small expected
lossFi(wi) =Ezi∼Di[/lscript(wi,zi)]. A known weighted graph, with known non-negative weights {aik},
speciﬁes the relatedness between tasks. Specially, we would like to consider predictor matrices
W= [w1,w2,...,wm]∈Rd×mfrom the set
Ω =/braceleftBig
W:/bardblwi/bardbl2≤B2,∀i= 1,...,m,
/summationdisplay
i/negationslash=kaik
2/bardblwi−wk/bardbl2≤S2/bracerightBig
,
i.e., we would like the norm of each individual predictor to be bounded (so that it has low complexity
and generalizes well), and the weighted dis-similarities between related predictors to also be small.
Taking an agnostic PAC-learning approach, our goal is to minimize the overall population ob-
jective
F(W) :=1
m/summationdisplaym
i=1Ezi∼Di[/lscript(wi,zi)], (1)
and be competitive with respect to predictors in the set Ω. Denoting W∗= arg minW∈ΩF(W) the
optimal predictor from Ω, and we would like to learn a predictor WwithF(W)≤F(W∗) +ε.
In our analysis, we take the instantaneous loss /lscript(w,z) to beL-Lipschitz continuous, and some-
times also assume it is smooth. In the latter case, we assume machine i’s loss/lscript(wi,zi) isβi-
smooth in wi, and so the global loss /hatwideF(W) is/parenleftBig
βF
m/parenrightBig
-smooth in WwithβF= maxi=1,...,mβi.
Even ignoring the constraint on the similarity between predictors, the sample complexity for each
individual task (i.e. the number of samples from Direquired to ensure Fi(wi)≤Fi(w∗
i) +ε) is
nL=O/parenleftBig
L2B2
/epsilon12/parenrightBig
. That is, with a total of O/parenleftBig
mL2B2
/epsilon12/parenrightBig
samples, we can learn Wwith the desired
guaranteeF(W)≤F(W∗) +εwithout any communication between the machines, by, e.g., solving
an independent /lscript2-regularized ERM problem on each machine. This local approach is the baseline
on which any method involving communication between the machines should improve.
Graph Laplacian The term/summationtext
i/negationslash=kaik
2/bardblwi−wk/bardbl2can be written equivalently using the graph
Laplacian. Let A= [aik]∈Rm×mbe the adjacency matrix, and L= diag ( A1)−Abe the
corresponding graph Laplacian ( Lik=/summationtext
l/negationslash=iailifi=k, and Lik=−aikotherwise), so that
3
/summationtext
i/negationslash=kaik
2/bardblwi−wk/bardbl2=/summationtext
i,kLik/angbracketleftwi,wk/angbracketright= tr/parenleftbig
WLW/latticetop/parenrightbig
. The eigenvalues of Lwill play an impor-
tant role and we denote them by 0 = λ1≤···≤λm.
Regularized ERM One way for learning the predictors is to solve the regularized empirical
risk minimization (ERM) problem. Let /hatwideFi(wi) =1
n/summationtextn
j=1/lscript(wi,zij) be the local empirical loss of
machinei, and letZ={zij:i= 1,...,m, j = 1,...,n}be the sample set. The regularized ERM
objective is
/hatwiderW= arg min
W1
m/summationdisplaym
i=1/hatwideFi(wi)
/bracehtipupleft/bracehtipdownright/bracehtipdownleft/bracehtipupright
/hatwideF(W)
+η
2m/summationdisplaym
i=1/bardblwi/bardbl2+τ
2mtr/parenleftBig
WLW/latticetop/parenrightBig
/bracehtipupleft /bracehtipdownright/bracehtipdownleft /bracehtipupright
R(W), (2)
whereη, τ≥0 are regularization parameters. Let /hatwiderW= arg minW/hatwideF(W) +R(W) be the solution
to (2).
To understand the statistical property of multi-task learning and facilitate further discussion, we
ﬁrst analyze the generalization error of /hatwiderW. Inspired by Maurer (2006), who showed essentially the
same learning guarantee for the solution of a constrained ERM problem (i.e., arg minW∈Ω/hatwideF(W)),
we provide guarantee for the regularized ERM solution /hatwiderW. Our motivation for studying regular-
ized ERM rather than constrained ERM is that it is easier to solve unconstrained problem using
(proximal) gradient methods, and we avoid computing projection onto the constraint set Ω, which
is diﬃcult in a distributed setting.1
While the analysis of Maurer (2006) was based on the Rademacher complexity of Ω (and re-
quired the solution to lie in Ω), our proof uses the stability based argument for generalization with
strongly convex regularizers (Shalev-Shwartz et al., 2009). Our analysis also reveals a fundamental
connection between single- and multi-task learning: to obtain generalization of a single task in the
distributed setting, we only need concentration for the sampling process of that task. In our case,
we consider strong convexity w.r.t. the /bardblW/bardblM-norm where M=I+τ
ηL.
Lemma 1. Assume that the instantaneous loss /lscript(w,z)isL-Lipschitz with respect to w. Then for
the ERM solution deﬁned in (2), we have EZ/bracketleftBig
F(/hatwiderW)−/hatwideF(/hatwiderW)/bracketrightBig
≤4L2
mn/summationtextm
i=11
η+τλi.
Corollary 2. Setη=2LB/radicalBig
1+m·ρ(B,S)
mn
B2 andτ=2LB/radicalBig
1+m·ρ(B,S)
mn
S2/min(2), where
ρ(B,S) :=1
m/summationdisplaym
i=21
1 +λimB2/S2.
Then EZ/bracketleftBig
F(/hatwiderW)−F(W∗)/bracketrightBig
≤4LB/radicalBig
1+m·ρ(B,S)
mn.
The quantity ρ(B,S) measures task relatedness and thus the beneﬁt of multi-task learning. It
depends on the parameters ( B,S) and the graph, but not the data. The value of ρ(B,S) ranges
from 0 (when λimB2/greatermuchS2) tom−1
m≤1 (whenλimB2/lessmuchS2), corresponding to two extreme cases.
1Although for convex optimization, the constrained form and the regularized form are equivalent due to the
Lagrange duality, solving the constrained form may still require repeatedly solving the regularized form and searching
for the Lagrange multiplier.
4
•WhenSis small and the graph is connected with high weights, the predictors are encouraged
to be similar to each other (we have a consensus problem if S= 0 and the graph is connected),
andρ(B,S) is close to 0. The generalization error is then O/parenleftBig
LB√mn/parenrightBig
, corresponding to that
of single task learning using mnsamples.
•WhenSis large or the graph is disconnected, tasks are not very related and ρ(B,S) is
close to 1. In this case, the generalization error behaves like O/parenleftBig
LB√n/parenrightBig
, and we are essentially
performing local learning with nsamples for each task.
For a ﬁxed number of machines mand graph Laplacian L, to achieve εexcess population error by
the above approach, the number of samples used by each machines is nC=O/parenleftBig
L2B2(1/m+ρ(B,S))
ε2/parenrightBig
=
O((1/m+ρ(B,S))·nL). Therefore, when the tasks are related and ρ(B,S) is small, the sample
complexity of multi-task learning is signiﬁcantly smaller than nLneeded by the local approach.
To implement the regularized ERM approach in the distributed setting, we could have each
machines send nCsamples to a central machine, and then minimize the regularized empirical loss
on that machine. We refer to this baseline as the centralized approach—it is sample eﬃcient, but
expensive in terms of communication and computation. We are interested in distributed multi-task
learning algorithms that are also sample eﬃcient, i.e. use only O(nC) samples on each machine
(or at least, not much more then this), but have low computation and communication costs. This
can be done either by low-communication distributed optimization of the regularized empirical
error (2).
3 Distributed algorithms for ERM
In this section, we propose eﬃcient distributed algorithms for minimizing the regularized empirical
objective (2). The simplest approach is perhaps to perform gradient descent on /hatwideF(W). Interestingly,
such updates take the form:
wt+1
i=/summationdisplaym
k=1µt+1
kiwt
k−αt+1∇/hatwideFi(wt
i), (3)
whereαt+1>0 is the stepsize at iteration t+1, and the weights for combining neighboring predictors
are
µt+1
ki=/braceleftBigg1−αt+1(η+τ/summationtext
k/primeaik/prime) : ifi=k,
αt+1τaik : otherwise.(4)
With an appropriate step-size schedule (or even a ﬁxed stepsize if the loss is smooth), this method
converges to /hatwiderW. Furthermore, the updates require only communication along the relatedness
graph, since the update for each machines involves only predictors from neighboring machines
(with nonzero aﬃnities). This is already a very natural and intuitive method for distributed multi-
task learning, and we will return to it later. When the loss is smooth, the method can be accelerated
using Nesterov’s techniques (Nesterov, 2004, as detailed in Appendix C) without any increase in
communication costs nor substantial increase in computation. But ﬁrst, we suggest two more
powerful alternatives.
Taking steps based on the gradients amounts to considering, in each iteration, a linearization
of the objective, that is of both the empirical loss /hatwideF(W) and the regularizer R(W). However, in
5
order to obtain a distributable update, it is suﬃcient to linearize only one of these components while
treating the other more explicitly, since each one of them separately can be eﬃciently optimized
in a distributed way: the empirical loss /hatwideF(W) decomposes over machines, and so can be directly
optimized in a distributed way, while R(W) is data independent and could be optimized implicitly
based on the common knowledge of the relatedness graph. In the following, we consider two
distributed schemes, each based on directly handling one of the components, and each preferable in
a diﬀerent regime depending on the relatedness graph and the structure and cost of communication.
3.1 Directly solving the regularizer
We ﬁrst consider methods which directly handle the regularization term R(W). To do so, we
consider the change of variable Ut=WtM1
2where M=I+τ
ηL, we can rewrite the ERM objective
as
min
U/hatwideF(UM−1
2) +η
2m/bardblU/bardbl2
F. (5)
We propose to optimize this objective using gradient descent with respect to U, which reduces to
the updates in the W-space: for t= 0,...,
Wt+1=/parenleftbig
1−αt+1η/parenrightbig
Wt−αt+1∇/hatwideF(Wt)·M−1(6)
whereαt+1>0 is the stepsize at iteration t+1. In each iteration, machine iperforms the following
update with µt+1
ki=αt+1(M−1)ki:
wt+1
i=/parenleftbig
1−αt+1η/parenrightbig
wt
i−m/summationdisplay
k=1µt+1
ki∇/hatwideFk(wt
k). (7)
This update can be implemented in the distributed setting with a broadcast channel: it requires
that each machine has access to gradients of all machines, which can be achieved using one round of
global, all-to-all communication (not respecting the graph). We could compute M−1oﬄine ahead
of time, and need not re-calculated at each iteration.
When the loss is smooth, we can accelerate (7) using Nesterov’s techniques without additional
communication costs. Setting a constant stepsize1
αt+1=βF+η, which is the smoothness param-
eter of the objective (5) in U2, to achieve /epsilon1-suboptimality in (2), the iteration complexity of the
accelerated algorithm is O/parenleftBig/radicalBig
βF+η
ηlog1
/epsilon1/parenrightBig
. To achieve εexcess error in the population loss, we set
the optimization error /epsilon1=O(ε) and plug in the choice of ηfrom Corollary 2, yielding the iteration
complexity/tildewideO/parenleftBig/radicalbig
βFB2/ε/parenrightBig
.
3.2 Directly optimizing the loss
The above algorithm requires dense, broadcast communication for solving the proximal step deﬁned
by the graph. In a decentralized setting, it is desired to develop algorithms which use only local,
2This is because∇2
vec(U)/hatwideF(UM−1
2) = (M−1
2⊗I)·∇2
vec(W)/hatwideF(W)·(M−1
2⊗I), and||∇2
vec(U)/hatwideF(UM−1
2)||≤
||M−1
2||·||∇2
vec(W)/hatwideF(W)||·||M−1
2||≤βF
m.
6
peer-to-peer communication. This can be achieved by the updates below, where we linearize the
graph regularizer but fully optimize over the loss:
Wt+1= arg min
W/angbracketleft∇R(Wt),W−Wt/angbracketright
+1
2mαt+1/vextenddouble/vextenddoubleW−Wt/vextenddouble/vextenddouble2
F+/hatwideF(W), (8)
whereαt+1is the stepsize at iteration t+1. As (8) decouples over machines, machine iindependently
computes a proximal operation using local data:
wt+1
i= arg minu
1
2αt+1/vextenddouble/vextenddoubleu−(wt
i−mαt+1∇wiR(Wt))/vextenddouble/vextenddouble2+/hatwideFi(u).
By the optimality condition of this update, we have
wt+1
i=/summationdisplaym
k=1µt+1
kiwt
k−αt+1∇/hatwideFi(wt+1
i), (9)
where the weights for combining neighboring predictors are the same as those in (4). Comparing (9)
with the similar update (3) where we linearized both the regularizer and the loss, we observe that (9)
is also a form of gradient method, with the gradient of loss evaluated at the “future” point.
The advantage of (9) is that the gradient ∇R(W) is data-independent and is obtained using
only one round of local communication from each machine to its neighbors. Furthermore, the
computation decouples over machines, and each machine optimizes the nonlinearized loss without
communication. In fact, we need not solve the proximal steps exactly since the (accelerated)
proximal gradient method is tolerant to errors in the steps (Schmidt et al., 2011), and suﬃciently
accurate solutions can often be obtained in time nearly linear in the number of examples processed
using variance-reduced ﬁnite-sum methods such as SVRG (Johnson and Zhang, 2013). Overall, this
is a communication-eﬃcient approach in which each machine tries to spend signiﬁcant amount of
time performing local computations on its own data, and to communicate only infrequently. Note
that similar proximal type operations also appear in the ADMM algorithm of Vanhaesebrouck
et al. (2017), but the decoupling of tasks is diﬀerent, because in the local problems of ADMM, each
machine optimizes over also a copy of neighboring predictors.
We can again accelerate (9) using Nesterov’s techniques, and set1
mαt+1=βR=η+τλm
m, which is
the smoothness parameter of R(W) inW. Then, to achieve εexcess error in the population objec-
tive, the number of iterations needed by the accelerated algorithm is /tildewideO/parenleftBig/radicalBig
βR
η/m/parenrightBig
=/tildewideO/parenleftbigg/radicalBig
λmmB2
S2/parenrightbigg
,
using the choice of ηandτfrom Corollary 2. We also show that this algorithm is tolerant to delay
and analyze its convergence under bounded delay in Appendix G.
4 Stochastic algorithms
In ERM, we collect training samples on each machine ahead of time, and solve a ﬁxed optimization
problem deﬁned by them. But in real-world scenarios, we might have access to virtually unlimited
data, or a constantly available stream of examples. In this case, it might be statistically wasteful
to reuse examples over iterations. Or, even if we do have a ﬁnite amount of data, as we shall see,
7
Table 1: Algorithms for distributed stochastic multi-task learning with graph regularization. Here ε
is the excess error in the population objective; nC=O/parenleftBig
L2B2·(1/m+ρ(B,S))
ε2/parenrightBig
andnL=O/parenleftBig
L2B2
/epsilon12/parenrightBig
;|E|
denotes the number of edges in the graph. For simplicity, schematic updates ignores acceleration,
but the rates are given for the accelerated algorithms. Each cell shall be interpreted as /tildewideO(·) which
hides poly-logarithmic dependencies.
AlgorithmsCommunication
roundsVectors (∈Rd)
communicated
per machineSample
complexity
per machineTotal Samples
processed
per machine
local 0 0nL=nC
1
m+ρ(B,S)nL
centralized nCnC=nL·
(1
m+ρ(B,S))m·nC
ERM: directly solving regularizer
1.gt+1
i=/summationtext
kµt+1
ki∇/hatwideFk(wt
k)
whereµt+1
ki=αt+1(M−1)ki
2.wt+1
i=wt
i−gt+1
i/radicalBig
B2
εm·/radicalBig
B2
εnCnC·/radicalBig
B2
ε
=nC·4√nL
ERM: directly optimizing loss
1./tildewidewt
i=/summationtext
kµt+1
kiwt
k
whereµt+1
ki= (I−αt+1ηM)ki
2.wt+1
i=/tildewidewt
i−αt+1∇/hatwideFt+1
i(wt+1
i)/radicalBig
λmmB2
S2|E|
m·/radicalBig
λmmB2
S2 nCnC·/radicalBig
λmmB2
S2
Stochastic: directly solving regularizer
Algorithm 2, b=O/parenleftbig
nC/radicalbigε
B2/parenrightbig
1.gt+1
i=/summationtext
kµt+1
ki∇/hatwideFt+1
k(wt
k)
whereµt+1
ki=αt+1(M−1)ki
2.wt+1
i=wt
i−gt+1
i/radicalBig
B2
εm·/radicalBig
B2
εnC nC
Stochastic: directly optimizing loss
1./tildewidewt
i=/summationtext
kµt+1
kiwt
k
whereµt+1
ki= (I−αt+1ηM)ki
2.wt+1
i=/tildewidewt
i−αt+1∇/hatwideFt+1
i(wt+1
i)|E|
m
per iterationnS, probably
∈(nC,nL)nS
we can get the same communication and statistical guarantee while processing only a minibatch
at a time, thus signiﬁcantly reducing computational cost. We consider stochastic variants of the
approaches in Section 3 to directly optimize the population loss F(W), using fresh samples in each
update.
4.1 Directly solving the regularizer
Analogous to (7), we could perform minibatch SGD with bsamples per machine to approximate
the gradient of the population loss: for t= 0,...,
wt+1
i=wt
i−/summationdisplaym
k=1µt+1
ki∇/hatwideFt+1
k(wt
k). (10)
where/hatwideFt+1
k(wt
k) =1
b/summationtextb
j=1/lscript(wt
k,zt+1
kj), and/braceleftBig
zt+1
kj:j= 1,...,b/bracerightBig
arebsamples drawn by machine
kat iteration t+ 1.
8
We can accelerate (10) using the accelerated stochastic approximation (AC-SA) algorithm of Lan
(2012). We provide the detailed accelerated algorithm in both the U-space and W-space in Algo-
rithm 2 (Appendix D). We have the following guarantee after running it for Titerations.
Theorem 3. Set the initialization W0=0and stepsizes θt+1=t+1
2,αt+1=t+1
2min/braceleftbigg
m
2βF,√
12mB2
(T+2)3
2σ/bracerightbigg
in Algorithm 2. Then E/bracketleftbig
F(WT
ag)−F(W∗)/bracketrightbig
≤O/parenleftBig
σ√
mB2√
bT+βFB2
T2/parenrightBig
.
Sample complexity Letn=bTbe the number of samples used in Algorithm 2. According
to Theorem 3, as long as the minibatch size b≤b∗=O/parenleftBig
n/radicalBig
ε(m,n)
βFB2/parenrightBig
, the ﬁrst term in the error
bound is dominant and we achieve the generalization error O/parenleftBig
σ√
mB2√n/parenrightBig
=O/parenleftbigg
LB/radicalBig
1+m·ρ(B,S)
mn/parenrightbigg
as
in ERM, so we are still sample eﬃcient in the stochastic setting.
Time complexity Algorithm 2 processes the drawn samples only once. While maintaining the
sample eﬃciency, we can set the minibatch size to the largest value b=b∗, and this leads to
the total number of iterations (and local communication rounds) T∗=n
b∗=O/parenleftBig/radicalBig
βFB2
ε(m,n)/parenrightBig
, also
matching that of ERM. However, since each stochastic gradient uses only b=o(n) samples, the
local computation ∇/hatwideFt+1(Wt) is signiﬁcantly reduced.
4.2 Directly optimizing the loss
Analogous to (8), we can use the stochastic algorithm where at iteration t+ 1, machine icomputes
wt+1
i= arg min
u1
2αt+1/vextenddouble/vextenddoubleu−/parenleftbig
wt
i−mαt+1∇wiR(Wt)/parenrightbig/vextenddouble/vextenddouble2
+1
b/summationdisplayb
j=1/lscript(u,zt+1
ij). (11)
Forb=n, it has the same per iteration computation cost as the ERM counterpart (both process
nsamples in each iteration). But, intuitively, it would outperform the ERM algorithm for the
same number of iterations/communications because it uses more fresh samples. We can prove the
convergence of this algorithm, but do not have a satisfactory analysis showing it is sample eﬃcient.
We conjecture that its sample complexity per machine, denoted by nS, is in the range ( nC,nL).
We implemented the accelerated version of this simple algorithm and this conjecture seems to be
supported by our experiments. In Appendix E, we provide a more complicated algorithm based
on the minibatch-prox algorithm of Wang et al. (2017), that is sample eﬃcient and trade oﬀ
communication and memory costs.
Comparison of the diﬀerent approaches Table 1 summarizes the communication and com-
putation complexities of the proposed algorithms. Some of our methods require solving local
regularized-ERM type problems on each machine. We do not analyze the precise complexity and
required accuracy of such local computation, but keep track of the number of samples processed
on each machine, i.e. sum of the sizes of the subproblems over the iterations, as the proxy for
computational complexity. We emphasize that, despite the simplicity of our ERM methods, their
9
have faster convergence than what we could obtain for previous methods; see detailed discussions
in Appendix H. Our stochastic algorithms mirror the ERM algorithms in terms of updates, but
can be computationally much more eﬃcient.
5 Connection to consensus learning
The iterations we consider all involve taking a weighted average of messages (iterates or gradients)
from other machines and a local gradient or prox computation. These same type of iterates have
also been suggested and studied as methods for solving the consensus problem—that is, ﬁnding a
single consensus predictor wthat is good for all machines and minimizes F(W) =1
m/summationtextm
i=1Fi(wi).
But the consensus problem is fundamentally diﬀerent from our “pluralistic” multi-task problem,
with a diﬀerent optimum. In this section we will understand what makes the same form of updates,
namely updates of the form (3), (7), (9) or their stochastic variants, converge to either the consensus
solution or to the pluralistic multi-task solution. In particular, we show how consensus methods
are obtained as special cases of these updates, or as limits of the multi-task approach.
Averaging gradients Let us begin with the update of the form (7) or its stochastic variant
(10), where we take a weighted average of gradients from other machines. When the averaging
weights are uniform, i.e. µt
ki=αt/mfor alli,k, and as long as all machines start from the same
initialization (e.g. wt
i= 0), the iterates will continue to be identical across machines throughout
optimization (i.e. we will have wt
i=wt
jfor alli,j,t), thus maintaining consensus. Furthermore,
the update (7) then boils down to precisely gradient descent on the empirical consensus objective
/hatwideF(W) +η
2m/bardblW/bardbl2
F, while the stochastic variant (7) is precisely a mini-batch stochastic gradient
descent update on the consensus objective, with a mini-batch consisting of the union of the samples
used across machines. Indeed, mini-batch SGD is a common approach for solving the distributed
consensus problem, or for distributed learning in a homogeneous setting (where we assume the same
distribution across machines, or at least the same good predictor). What we saw in Section 3, is
that by changing to non-uniform weights, given by µ∝M−1, we can allow pluralism and converge
to the multi-task solution.
We can furthermore observe how uniform weights (and therefor gradient descent/mini-batch
SGD on the consensus problem) are obtained as a limit of the multi-task weights µ∝M−1. If
the graph is connected, λ1= 0 is the only zero eigenvalue of the Laplacian Lwith an associated
eigenvector of u= [1,..., 1] (if the graph is not connected, we cannot expect consensus, as each
connected component will behave independently). Therefor M−1= (I+τ
ηL)−1has a leading
eigenvalue of 1 of multiplicity one, associated with the eigenvector u. AsS→0 and soτ→∞ , that
is we are demanding increasing similarity between machines, the leading eigenvalue of M−1remains
1 while all other eigenvalues go to zero, implying that M−1→1
muu/latticetopand soµt
ki=αtM−1
ki→αt/m.
That is, as we demand increasing similarity between machines, and thus converge to a consensus
situation, the updates converge to standard consensus gradient descent or mini-batch SGD updates.
Averaging iterates Let us now turn to updates of the form (3), the related prox updates (9),
and their stochastic variants. Nedi´ c and Ozdaglar (2009) proposed updates precisely of the form
(3) as a decentralized procedure for the consensus problem. They showed that when the averaging
weightsµt
kiare doubly stochastic and do not vary between iterations (i.e. µt
ki=µki,∀k/summationtext
iµki= 1
and∀j/summationtext
kµki= 1), and the stepsize on the gradient goes to zero, i.e. αtt→∞−−−→ 0, the updates (3)
10
C=1 C=5 C=10 C=10
0 20 40 60
iterationerrorlocal
central
ADMM
SDCA
BSR
BOL
SSR-50
SOL-50
0 20 40 60
iteration
0 20 40 60
iteration
0 50 100 150
iteration4.74.84.955.15.2errorlocal
central
SSR-40
SSR-80
SSR-100
SSR-200
SSR-500
SOL-40
SOL-80
SOL-100
SOL-200
SOL-500
0 20 40 60
computationerrorlocal
central
ADMM
SDCA
BSR
BOL
SSR-50
SOL-50
0 20 40 60
computation
0 20 40 60
computation
0 0.5 1 1.5 2
sample 1044.74.84.955.15.2error
Figure 1: Results for regularized ERM (left panel) and our stochastic methods with diﬀerent b
(right panel).
converge to the consensus solution. In our case, the averaging weights, as deﬁned in (4), deviate
from double-stochasticity, since/summationtext
kµt
ki= 1−αtη. Furthermore, and possibly more signiﬁcantly, to
obtain our convergence guarantees for smooth loss, we do not take αtto zero. Even if we were to
use diminishing stepsizes in our derivations, we would have αt→0, but in that case the averaging
weights would not be ﬁxed over iterations (as is the case in consensus optimization) and we would
haveµt→I.
To see how consensus updates are obtained as a limiting case of our multi-task setting, we
again consider a connected graph and study what happens as S→0 and soτ→∞ , whileB
and therefor ηremain ﬁxed. This corresponds to a ﬁxed amount of local regularization, and
increasing expectation that neighboring nodes are similar. Under this scaling, we would indeed
haveα= 1/(η+τλm)→0, whereλm>0 since the graph is connected. Furthermore, we have that
αη→0 whileατ→1/λm>0. Plugging this scaling into the multi-task averaging weights (4), we
obtain the doubly stochastic weights:
µt
ki→/braceleftbigg1−1
λm/summationtext
k/primeaik/prime: ifi=k,
1
λmaik : otherwise.(12)
To summarize, a signiﬁcant diﬀerentiation between consensus and multi-task learning is therefor in
whetherαtdiminishes relative to (µt−I). When our relatedness constraints approach consensus,
αtcan diminish while µtis non-trivial and doubly stochastic. In fact, in studying consensus
optimization, Yuan et al. (2016) recently noted that when αtdoes not diminish, the methods does
not converge to the consensus solution but only to a neighborhood of it. In light of our analysis, we
now understand that this “neighborhood” corresponds to the multi-task learning solution, which
indeed becomes increasingly similar to the consensus solution as S→0.
11
Connection to the decentralized algorithm of Scaman et al. (2017) When the graph is
connected, the consensus constraint w1=···=wmcan be equivalently written as W√
L=0,
since the null space of Lcontains only vectors of constants. Then the multi-task formulation (2) is
a relaxation of
min
W√
L=01
m/summationdisplaym
i=1/hatwideFi(wi) +η
2m/summationdisplaym
i=1/bardblwi/bardbl2(13)
with the quadratic termτ
2tr/parenleftbig
WLW/latticetop/parenrightbig
penalizing the constraint violation. The quadratic penalty
τ
2tr/parenleftbig
WLW/latticetop/parenrightbig
may lead to a large condition number for our algorithm (8) as τ→∞ .
Recently, Scaman et al. (2017) proposed an algorithm with optimal iteration/communication
complexities for decentralized consensus learning, which performs accelerated gradient descent on
the dual problem of (13), with updates (before acceleration):
Wt+1= arg maxW/angbracketleftVt,W/angbracketright−/hatwideF(W),
Vt+1=Vt−αWt+1L, (14)
where V0=0andα>0 is the stepsize. It can be seen that their algorithm consists of the same
type of basic operations (weighted local average of predictors, and solutions of local subproblems
involving non-linearized loss) as ours. As noted by the authors, this is a form of distributed
augmented Lagrangian method without the quadratic penalty.
6 Experiments
We examine diﬀerent graph-based multi-task learning methods on the task of least squares regres-
sion using synthetic data. More details of the experiments (including data generation and more
results) are given in Appendix I. The tasks are grouped into Cclusters and the true predictors
within the same cluster are generated from the same Gaussian distribution, thus smaller Cimplies
higher task relatedness. We have input dimension d= 100, number of tasks m= 100, training set
sizen= 500, and vary number of task clusters Cover{1,5,10,50}. We also generate a dev set
of 10000 samples per task for tuning hyper-parameters, and test set of 10000 samples per task for
approximately evaluating the population loss. The aﬃnity graph A∈R100×100is a (connected)
10-nearest neighbor graph with binary weights built on the true predictors.
The methods compared here are: Local , which solves a local ERM problem (with /lscript2-regularization)
withnsamples for each task; Centralized , which solves the regularized ERM problem (2) with n
samples for each task; ADMM , which is the synchronized version of the algorithm of Vanhaesebrouck
et al. (2017); SDCA , which is the algorithm used by Liu et al. (2017) for ﬁxed graph; our algorithms
are denoted as B/S (batch/stochastic) + SR/OL (solve regularizer/optimize loss).
Empirical risk minimization We ﬁst compare the iterative methods on the regularized ERM
problem (2), to which the analysis for ADMM andSDCA applies. We tune the /lscript2regularization param-
eter for Local and (η,τ) for Centralized , and then ﬁx the optimal ( η,τ) for other methods. We
also tune the quadratic penalty parameter for ADMM , the task separability and stepsize parameters
forSDCA , and stepsize parameter for BSR/BOL (although the default value based on the smooth-
ness parameter already works well for them). For SSR/SOL, we draw random samples from the
ﬁxed training set (with size n), and simply ﬁx the minibatch size to be n/10.
12
Figure 1 (left panel) shows for each method the estimated F(W) over iterations (or rounds of
communication) in the top row, and over the amount of computation (measured by the number of
passes over the training set) in the bottom row. Observe that all iterative algorithms converge to
the same ERM solution, our algorithms tend to consistently outperform ADMM andSDCA .
Stochastic optimization We next demonstrate the eﬃciency of true stochastic algorithms (using
fresh samples for each update) at C= 10. We allow the algorithms to process a total of 10000 fresh
samples on each machine, and vary the minibatch size bover{40,80,100,200,500}. The parameters
(η,τ) are ﬁxed to those used in the ERM experiments.
Figure 1 (right panel) shows for each method the estimated F(W) over iterations (or rounds
of communication) in the left plot, and over the amount of fresh samples processed (or total
computation cost) in the right plot. As a reference, the error of Local and Centralized (using
n= 500 samples per machine) are also given in the plots. We observe that with fresh samples,
stochastic algorithms are competitive to ERM algorithms in terms of sample complexity, while
being computationally more eﬃcient.
A Proof of Lemma 1
Recall that the ERM problem is deﬁned as
/hatwiderW= arg min
W/hatwideF(W) +R(W) :=1
mm/summationdisplay
i=1/hatwideFi(wi) +η
2mm/summationdisplay
i=1/bardblwi/bardbl2+τ
2mtr/parenleftBig
WLW/latticetop/parenrightBig
,
whereη, τ≥0 are regularization parameters, Z={zij:i= 1,...,m, j = 1,...,n}is the sample
set. And recall that λi,i= 1,...,m are the eigenvalues of L.
Assume that the instantaneous loss /lscript(w,z) isL-Lipschitz in w. We would like to show that
EZ/bracketleftBig
F(/hatwiderW)−/hatwideF(/hatwiderW)/bracketrightBig
≤4L2
mnm/summationdisplay
i=11
η+τλi.
Proof. In the following, we deﬁne M=I+τ
ηLwhich is positive deﬁnite. Furthermore, perform the
following change of variables
U=WM1
2,⇔ wi= (UM−1
2)·ei
where eiis thei-th standard basis in Rm.
We can then rewrite the losses using the new variables:
1
m/lscript(wi,zi) =1
m/lscript(UM−1
2ei,zi) =:hi(U,zi), fori= 1,...,m,
and the empirical objective as
min
U∈Rd×m/hatwideH(U) :=1
nn/summationdisplay
j=1h1(U,z1j) +
m/summationdisplay
i=21
nn/summationdisplay
j=1hi(U,zij) +η
2m/bardblU/bardbl2
F
. (15)
13
We can view (15) as performing ERM in the space of U, using the instantaneous loss h1(U,z1)
withnindependent samples {z1j}j=1,...,n, and using the term in bracket as the z1-independent
regularizer.
Recall that the ERM solution to an objective with Lipschitz loss and strongly convex regularizer
is stable. Obviously, the regularization term in (15) is/parenleftbigη
m/parenrightbig
-strongly convex in U. We now bound
the Lipschitz constant of h1(U,z1) inU. Observe that
∇Uh1(U,z1) =1
m∇wi/lscript(w1,z1)·e/latticetop
1M−1
2,
and as a result the Lipschitz constant is bounded by
/bardbl∇Uh1(U,z1)/bardblF=1
m/radicalBig
tr/parenleftbig
∇w1/lscript(w1,z1)·e/latticetop
1M−1e1·∇w1/lscript(w1,z1)/latticetop/parenrightbig
≤L/radicalbig
(M−1)11
m
where we have used the L-Lipschitz continuity of /lscript(w1,z1) which implies/bardbl∇w1/lscript(w1,z1)/bardbl≤L.
According to Shalev-Shwartz et al. (2009)[Theorem 6], for any ﬁxed {zij}i=2,...,m
j=1,...,n, it holds for
the ERM solution /hatwideU= arg minU/hatwideH(U) =/hatwiderWM1
2that
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE{z1j}
Ez1[h1(/hatwideU,z1)]−1
nn/summationdisplay
j=1h1(/hatwideU,z1j)
/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle≤4/parenleftBigg
L/radicalbig
(M−1)11
m/parenrightBigg2/slashbigg
(ηn/m ) =4L2(M−1)11
ηmn.
Translating this in terms of the original variables, we have
∀{zij}i=2,...,m
j=1,...,n,/vextendsingle/vextendsingle/vextendsingleE{z1j}/bracketleftBig
F1(/hatwiderW)−/hatwideF1(/hatwiderW)/bracketrightBig/vextendsingle/vextendsingle/vextendsingle≤4L2(M−1)11
ηn
whereF1(W) =Ez1[/lscript(w1,z1)] and/hatwideF1(W) =1
n/summationtextn
j=1/lscript(w1,z1j).
By the convexity of |·|and the Jensen’s inequality, this implies
/vextendsingle/vextendsingle/vextendsingleEZ/bracketleftBig
F1(/hatwiderW)−/hatwideF1(/hatwiderW)/bracketrightBig/vextendsingle/vextendsingle/vextendsingle=/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingleE{zij}i=2,...,m
j=1,...,n/bracketleftBig
E{z1j}/bracketleftBig
F1(/hatwiderW)−/hatwideF1(/hatwiderW)/bracketrightBig/bracketrightBig/vextendsingle/vextendsingle/vextendsingle/vextendsingle/vextendsingle
≤E{zij}i=2,...,m
j=1,...,n/vextendsingle/vextendsingle/vextendsingleE{z1j}/bracketleftBig
F1(/hatwiderW)−/hatwideF1(/hatwiderW)/bracketrightBig/vextendsingle/vextendsingle/vextendsingle≤4L2(M−1)11
ηn.
This result shows that, to obtain generalization for a single task, we only need concentration for
the sampling process of that task. By the same argument, we obtain similar inequalities regarding
stability for losses on each machine.
Finally, we have by the triangle inequality that
/vextendsingle/vextendsingle/vextendsingleEZ/bracketleftBig
F(/hatwiderW)−/hatwideF(/hatwiderW)/bracketrightBig/vextendsingle/vextendsingle/vextendsingle≤1
mm/summationdisplay
i=1/vextendsingle/vextendsingle/vextendsingleEZ/bracketleftBig
Fi(/hatwiderW)−/hatwideFi(/hatwiderW)/bracketrightBig/vextendsingle/vextendsingle/vextendsingle
≤1
mm/summationdisplay
i=14L2(M−1)ii
ηn
=4L2tr/parenleftbig
M−1/parenrightbig
ηmn
=4L2/summationtextm
i=11
1+τλi/η
ηmn
14
which is what we set out to prove.
B Proof of Lemma 2
Based on Lemma 1, we now show that by properly setting the regularization parameters in the
regularized ERM problem (2), i.e., η=2LB/radicalBig
1+m·ρ(B,S)
mn
B2 andτ=2LB/radicalBig
1+m·ρ(B,S)
mn
S2/m, we have that
EZ/bracketleftBig
F(/hatwiderW)−F(W∗)/bracketrightBig
≤4LB/radicalbigg
1 +m·ρ(B,S)
mn.
whereρ(B,S) :=1
m/summationtextm
i=21
1+λimB2/S2.
Proof. Observe that
EZ/bracketleftBig
F(/hatwiderW)/bracketrightBig
≤EZ/bracketleftBig
/hatwideF(/hatwiderW)/bracketrightBig
+4L2
mnm/summationdisplay
i=11
η+τλi
≤EZ/bracketleftBig
/hatwideF(/hatwiderW) +R(/hatwiderW)/bracketrightBig
+4L2
mnm/summationdisplay
i=11
η+τλi
≤EZ/bracketleftBig
/hatwideF(W∗) +R(W∗)/bracketrightBig
+4L2
mnm/summationdisplay
i=11
η+τλi
=F(W∗) +R(W∗) +4L2
mnm/summationdisplay
i=11
η+τλi
where we have used Lemma 1 in the ﬁrst inequality, and that /hatwiderWis the empiric risk minimizer in
the third inequality.
Since W∗∈Ω, we can bound the excess error as
ε(m,n) =EZ/bracketleftBig
F(/hatwiderW)−F(W∗)/bracketrightBig
≤R(W∗) +4L2
mnm/summationdisplay
i=11
η+τλi
≤1
2ηB2+1
2mτS2+4L2
mnm/summationdisplay
i=11
η+τλi. (16)
Now, setη=/epsilon1
B2andτ=m/epsilon1
S2for some/epsilon1that will be speciﬁed later. Continuing from (16) yields
ε(m,n)≤/epsilon1+4L2
mnm/summationdisplay
i=11
/epsilon1
B2+m/epsilon1
S2λi
=/epsilon1+1
/epsilon1·4L2B2
n·1
mm/summationdisplay
i=11
1 +λimB2/S2
≤/epsilon1+1
/epsilon1·/parenleftBigg
4L2B2
mn+4L2B2
n·1
mm/summationdisplay
i=21
1 +λimB2/S2/parenrightBigg
≤/epsilon1+1
/epsilon1·/parenleftbigg4L2B2
mn+4L2B2
n·ρ(B,S)/parenrightbigg
.
15
Minimizing the RHS over /epsilon1gives/epsilon1= 2LB/radicalBig
1
mn+ρ(B,S)
n, and
ε(m,n)≤4LB/radicalbigg
1
mn+ρ(B,S)
n.
C The accelerated proximal gradient algorithm
We provide the accelerated proximal gradient algorithms in Algorithm 1, which are used to ac-
celerate our ERM algorithms in the main text. The proximal operator is deﬁned as proxβ
h(x) =
arg minyβ
2/bardbly−x/bardbl2+h(y) whereβ >0 andh(x) is convex and possibly non-smooth.
Algorithm 1 ProxGrad(g,h,β,µ): Accelerated proximal gradient descent.
Input: Objective has the form f(w) =g(w) +h(w), whereg(w) isβ-smooth and µ-strongly
convex, and h(w) is convex.
Initialize w0,y1←w0
fort= 1,...,T do
wt←proxβ
h/parenleftBig
yt−1
β∇g(yt)/parenrightBig
, yt+1←wt+√β−√µ√β+√µ/parenleftbig
wt−wt−1/parenrightbig
end for
Output: wTis the approximate solution.
D Analysis of stochastic optimization by directly solving the reg-
ularizer
In each iteration of this algorithm, we draw bsamples per machine to approximate the gradient
of the population loss and perform minibatch SGD, which amounts to linearizing the loss on a
minibatch. The key to being sample eﬃcient is to respect the geometry imposed by the graph
Laplacian.
As in Section 3.1, deﬁne the change of variable Ut=WtM1
2where M=I+mB2
S2L. Our
population objective is F(W) =F(UM−1
2), and the predictor U∗=W∗M1
2satisﬁes the constraint
that/bardblU∗/bardbl2
F= tr/parenleftBig
W∗/parenleftBig
I+mB2
S2L/parenrightBig
(W∗)/latticetop/parenrightBig
≤2mB2. We can perform minibatch SGD in the U-
space:
Ut+1= arg min
Uαt+1/angbracketleft∇/hatwideFt+1(UtM−1
2)·M−1
2,U−Ut/angbracketright+1
2/vextenddouble/vextenddoubleU−Ut/vextenddouble/vextenddouble2
F, fort= 0,...,
where/hatwideFt+1(Wt) =1
mb/summationtextm
i=1/summationtextb
j=1/lscript(wt
i,zt+1
ij) and/braceleftBig
zt+1
ij/bracerightBig
j=1,...,barebsamples drawn by machine i
at iteration t+ 1, andαt+1>0 is a stepsize parameter. In the W-space, the above update reduces
to
Wt+1=Wt−αt+1∇/hatwideFt+1(Wt)·M−1,
Clearly, this update requires inverting the graph Laplacian.
16
We can further accelerate this method using the accelerated stochastic approximation (AC-
SA) algorithm of Lan (2012). We give the detailed stochastic algorithm by directly solving the
regularizer (with linearized loss) in Algorithm 2.
Algorithm 2 Accelerated minibatch SGD. This algorithm maintains three iterate sequences:/braceleftbig
Ut/bracerightbig
is the sequence of prox centers,/braceleftbig
Ut
md/bracerightbig
is the “middle” sequence with which we evaluate the sto-
chastic gradient and build models (approximations) of the objective, and/braceleftbig
Ut
ag/bracerightbig
is the “aggregated”
sequence with which we evaluate the objective values.
Input: The stepsize sequences/braceleftbig
θt+1/bracerightbig
and/braceleftbig
αt+1/bracerightbig
fort= 0,....
Initialize W0←0,W0
ag←W0/braceleftbig
U0←0,U0
ag←U0/bracerightbig
fort= 0,...,T−1do
Wt
md←/parenleftbig
θt+1/parenrightbig−1Wt+ (1−/parenleftbig
θt+1/parenrightbig−1)Wt
ag/braceleftBig
Ut
md←/parenleftbig
θt+1/parenrightbig−1Ut+ (1−/parenleftbig
θt+1/parenrightbig−1)Ut
ag/bracerightBig
Wt+1←Wt−αt+1∇/hatwideFt+1(Wt
md)·M−1/braceleftBig
Ut+1←Ut−αt+1∇/hatwideFt+1(Ut
mdM−1
2)·M−1
2/bracerightBig
Wt+1
ag←/parenleftbig
θt+1/parenrightbig−1Wt+1+ (1−/parenleftbig
θt+1/parenrightbig−1)Wt
ag/braceleftBig
Ut+1
ag←/parenleftbig
θt+1/parenrightbig−1Ut+1+ (1−/parenleftbig
θt+1/parenrightbig−1)Ut
ag/bracerightBig
end for
Output: WT
ag(or equivalently UT
ag) is the approximate solution.
The key quantity for analyzing the convergence property of minibatch SGD is the variance
of stochastic gradients in the U-space, which we now derive. We can view ξ= (z1,...,zm) as
the combined sample, /lscriptmulti (W,ξ) =1
m/summationtextm
i=1/lscript(wi,zi) as the averaged instantaneous loss, so that
/hatwideFt+1(W) =1
b/summationtextb
j=1/lscriptmulti(W,ξt+1
j) approximates Eξ[/lscriptmulti (W,ξ)] withbcombined samples. The
lemma below bounds the variance of stochastic gradient estimated with one combined sample.
Lemma 4. The variance of stochastic gradient in the U-space is bounded:
Eξ/vextenddouble/vextenddouble/vextenddouble∇/lscriptmulti/parenleftBig
UM−1
2,ξ/parenrightBig
·M−1
2−Eξ/bracketleftBig
∇/lscriptmulti/parenleftBig
UM−1
2,ξ/parenrightBig
·M−1
2/bracketrightBig/vextenddouble/vextenddouble/vextenddouble2
F≤σ2
whereσ2:=4L2
m2(1 +m·ρ(B,S)).
Proof. By direct calculation, we have
Eξ/vextenddouble/vextenddouble/vextenddouble∇/lscriptmulti/parenleftBig
UM−1
2,ξ/parenrightBig
·M−1
2−Eξ/bracketleftBig
∇/lscriptmulti/parenleftBig
UM−1
2,ξ/parenrightBig
·M−1
2/bracketrightBig/vextenddouble/vextenddouble/vextenddouble2
F
=1
m2Eξ/bardbl[∇w1/lscript(w1,z1)−Ezi[∇w1/lscript(w1,z1)],...,∇wm/lscript(wm,zm)−Ezm[∇wm/lscript(wm,zm)]]/bardbl2
M−1
=1
m2/summationdisplay
i,kEzi,zk/angbracketleft∇wi/lscript(wi,zi)−Ezi[∇wi/lscript(wi,zi)],∇wk/lscript(wk,zk)−Ezk[∇wk/lscript(wk,zk)]/angbracketright·(M−1)ik
=1
m2m/summationdisplay
i=1/bardbl∇wi/lscript(wi,zi)−Ezi[∇wi/lscript(wi,zi)]/bardbl2·(M−1)ii (17)
≤4L2
m2tr/parenleftbig
M−1/parenrightbig
=4L2
m2m/summationdisplay
i=11
1 +λimB2/S2=4L2
m2(1 +m·ρ(B,S)) =σ2
where we have used the independence between ziandzkfori/negationslash=kso that the cross terms vanishes
in (17), and the triangle inequality and that /bardbl∇wi/lscript(wi,zi)/bardbl≤Lin the inequality.
17
Averaging the bindependent stochastic gradients on a minibatch reduces the gradient variance
toσ2/b(see, e.g., Dekel et al., 2012, eqn 7). Note thatβF
mis the smoothness parameter of F(UM−1
2)
w.r.t. U, and the distance generating function1
2/bardblU/bardbl2
Fis 1-strongly convex w.r.t. the /bardblU/bardblF-norm.
Plugging these problem parameters into (Lan, 2012)(Corollary 1) yields Theorem 3.
E A sample-eﬃcient stochastic algorithm by directly optimizing
the loss
The key to sample eﬃciency in the stochastic setting is to couple the individual learning tasks
with the graph, and respect the geometry of the U-space (e.g., in deriving the generalization
performance in Lemma 1, we rely on strong convexity in the norm /bardblU/bardblF). This motivates us to
derive a sample-eﬃcient stochastic algorithm based on the minibatch-prox method (Wang et al.,
2017). The minibatch-prox method solves a subproblem involving nonlinearized loss on a minibatch
in each iteration, and was shown to have the optimal sample complexity for stochastic convex
optimization regardless of the minibatch size (recall from Section 4.1 that mnibatch SGD achieves
the optimal sample complexity only for small enough minibatch size), and it was the basis for
developing communication- and memory-eﬃcient algorithm for distributed stochastic consensus
learning in Wang et al. (2017).
We detail the minibatch-prox based algorithm in Algorithm 3, which consists of two nested
loops. In the outer loop, we perform minibatch-prox in the space of U; in each iteration of the
outer loop we use bsamples per machines to approximate the nonlinearized loss, and approximately
solves a subproblem involving the full Laplacian in the W-space. The solutions to the subproblems
(which is then a small ERM problem with ﬁxed samples) are computed approximately by the inner
loops, where we perform acclerated gradient descent in the space of W.
Algorithm 3 Distributed minibatch prox.
Initialize W0←0
fort= 0,...,T−1do
Approximately solve
Wt+1≈/hatwiderWt+1= arg minWγ
2tr/parenleftbig
(W−Wt)M(W−Wt)/latticetop/parenrightbig
+/hatwideFt+1(W)
toζt+1-suboptimality using the accelerated proximal gradient algorithm
ProxGrad (γtr/parenleftbig
(W−Wt)M(W−Wt)/latticetop/parenrightbig
,/hatwideFt+1(W), γ(1 +mB2
S2λm), γ)
end for
Output: W=1
T/summationtextT
t=1Wtis the approximate solution.
The minibatch-prox algorithm for minimizing F(UM−1
2) works as follows:
Ut+1≈/hatwideU= arg min
Uγ
2/vextenddouble/vextenddoubleU−Ut/vextenddouble/vextenddouble2
F+/hatwideFt+1(UM−1
2), fort= 0,..., (18)
where in each iteration we draw bfresh samples per machine to approximate F(W) by/hatwideFt+1(W) =
1
mb/summationtextm
i=1/summationtextb
j=1/lscript(wi,zt+1
ib). Note that we allow inexact solutions to the objective in (18). The
corresponding update of (18) in the W-space is Wt+1≈/hatwiderWt+1= arg minW/hatwideft+1(W) where
/hatwideft+1(W) =γ
2tr/parenleftBig
(W−Wt)M(W−Wt)/latticetop/parenrightBig
+/hatwideFt+1(W). (19)
18
We provide the learning guarantee of the minibatch-prox algorithm in the following theorem.
Theorem 5. Suppose that we initialize Algorithm 3 with W=0and setγ= 2/radicalBig
T
b·L√
1+m·ρ(B,S)
m3
2B.
Assume that for all t≥0, the error in minimizing (19) satisﬁes
/hatwideft+1(Wt+1)−min
W/hatwideft+1(W)≤ζt+1= min/parenleftBigg/parenleftbiggT
b/parenrightbigg1
2
,/parenleftbiggT
b/parenrightbigg3
2/parenrightBigg
·LB(1 +m·ρ(B,S))3
2
m5
2t3.
Then for WT=1
T/summationtextT
t=1Wt, we have E/bracketleftBig
F(WT)−F(W∗)/bracketrightBig
=O/parenleftbigg
LB√
1+m·ρ(B,S)√
mbT/parenrightbigg
.
Proof. LetLU=L√
tr(M−1)
mwhere tr/parenleftbig
M−1/parenrightbig
= 1 +m·ρ(B,S). By an analysis similar to that of
Lemma 1 (and essentially due to /hatwideft+1(W)’sγ-strong convexity w.r.t. the norm /bardbl·/bardblM), we obtain the
“stability” of the exact minimizer to (19), i.e.,/vextendsingle/vextendsingle/vextendsingleE[/hatwideFt+1(/hatwiderWt+1)−F(/hatwiderWt+1)]/vextendsingle/vextendsingle/vextendsingle≤4L2tr(M−1)
γm2b=4L2
U
γb.
Furthermore, if the suboptimality of Wt+1satisﬁes/hatwideft+1(Wt+1)−/hatwideft+1(/hatwiderWt+1)≤ζt+1, by the
γ-strong convexity of /hatwideft+1(W) w.r.t. the Euclidean norm, we have
/vextenddouble/vextenddoublewt+1
i−/hatwidewt+1
i/vextenddouble/vextenddouble≤/radicalBigg
2ζt+1
γ, fori= 1,...,m,
and consequently by the Lipschitz continuity of the loss, we have
/hatwideFt+1(Wt+1)−/hatwideF(/hatwiderWt+1)≤/radicalBigg
2L2ζt+1
γ=/radicalBigg
2L2
U
γ·m2ζt+1
tr (M−1).
This reconstructs the essential lemma required by the minibatch-prox analysis (Wang et al., 2017,
Lemma 2). We can then invoke the learning guarantee of minibatch-prox (Wang et al., 2017,
Theorem 7), by using our LUin place of their L, and ourm2ζt+1
tr(M−1)in place of their ηt.
In the end, we have
E/bracketleftBig
F(WT)−F(W∗)/bracketrightBig
≤O/parenleftBigg
LB/radicalbig
tr (M−1)√
mbT/parenrightBigg
=O/parenleftBigg
LB/radicalbig
1 +m·ρ(B,S)√
mbT/parenrightBigg
.
For ﬁxedn=bT, minibatch-prox attains the generalization error O/parenleftbigg
LB/radicalBig
1+m·ρ(B,S)
mn/parenrightbigg
for any
minibatch size b. Though the error in solving each subproblem (19) seems stringent as it decreases
over iterations, we can apply the linearly convergent accelerated proximal gradient method in
the inner loops to the subproblems. For any minibatch size b, the number of outer iterations
isT=n
b, and the number of inner iterations for each outer iteration (the initial error for the
subproblems are bounded with a warm-start, see Appendix F) is /tildewideO/parenleftbigg/radicalBig
λmmB2
S2/parenrightbigg
, so the total
number of communication rounds is the multiplication /tildewideO/parenleftbigg
n
b·/radicalBig
λmmB2
S2/parenrightbigg
.
19
This algorithm allows us to trade oﬀ communication and memory: We could use small number
of samples bin each outer iteration (limited by the local memory), but the total number com-
munication rounds increase with1
b. The most communication-eﬃcient setting is b=n, in which
case we are essentially solving one ERM problem with mnsamples (by linearzing the regular-
izer). Finally, we note that each update of the simple algorithm (11) (without the outer+inner
loop structure) and a single inner iteration of the minibatch-prox subproblem (19) have the same
communication/computation costs.
F Warm start when directly optimizing the loss
Lemma 6. Consider the objective of the proximal operator
min
yf(y) =β
2/bardbly−x/bardbl2+h(y).
whereh(y)isL-Lipschitz, and let x∗= arg minyf(y). Then we have
/bardblx∗−x/bardbl≤L/β,
and the suboptimality of xis bounded
f(x)−f(x∗)≤L2/β.
Proof. By the ﬁrst-order optimality of x∗, we have
0=β(x∗−x) +∇h(x∗)
where∇h(x∗) is a subgradient of hatx∗. By the assumption that h(y) isL-Lipschitz, we have
/bardbl∇h(x∗)/bardbl≤Land consequently /bardblx∗−x/bardbl=/bardbl∇h(x∗)/bardbl/β≤L/β.
For the suboptimality of x, it follows again from the Lipschitz continuity of hthat
f(x)−f(x∗) = 0 +h(x)−β
2/bardblx∗−x/bardbl2−h(x∗)
≤h(x)−h(x∗)
≤L/bardblx−x∗/bardbl
≤L2/β.
This lemma indicates that for solving the local objectives when directly optimizing the loss,
e.g., (8), we can initialize from Wt−1
β∇R(Wt) which mixes the local predictor with those of the
neighbors, and the initial suboptimality of this warm start is bounded by O/parenleftBig
L2
βF/parenrightBig
.
A similar result holds when the distance term is deﬁned by other non-Euclidean norms. For
example, in Section 4.1, we need to solve subproblems of the form (19), where the distance in the
W-space is deﬁned by the /bardblW/bardblM-norm. By an analysis similar to that of Lemma 6 and noting
that/vextenddouble/vextenddoubleM−1/vextenddouble/vextenddouble≤1, we obtain the distance between wt
iand the optimal solution /tildewidewt+1
iis at mostL/γ.
As a result, the suboptimality of solving (19) when initialized from Wtis at mostL2/γ.
20
G Directly optimizing the loss with bounded delays
When directly optimizing the loss (while linearizing the regularizer), consider the case where the
synchronization step is not perfect. Instead of waiting for neighboring machines to ﬁnish their
local proximal step and sending in their new weight parameters, each machine can use the stale
parameters for neighboring machines. Can we still solve the original ERM problem in this case?
Consider the iteration t+ 1 on machine i(with delays, tis now considered a local iteration
counter). Let the set of neighboring machines be Ni. Due to delay in communication, we have a
noisy gradient
/tildewide∇iR(Wt) =1
m/parenleftBigg
ηwt
i+τ/summationdisplay
kaik(wt
i−wt−dik(t)
k)/parenrightBigg
, i = 1,...,m.
Heredik(t)∈[0,Γ] is the delay of machine krelative to machine i(at iteration t+ 1): Machine i
is using the weight of machine kfromdik(t) steps ago. In this section, we allow the delay to vary
over time, as long as it is upper bounded by Γ.
Based on this noisy gradient, machine icomputes the following proximal gradient step
wt+1
i= proxβ
/hatwideFi
m/parenleftbigg
wt
i−1
β/tildewide∇iR(Wt)/parenrightbigg
(20)
with some stepsize β > 0. We need to analyze the convergence of the proximal gradient method
with errors in the gradient, as done by Schmidt et al. (2011). The diﬀerence from their work is
that the error in our gradients comes from delay (stale weight parameters).
Comparing with the case without delay, we have the “error” in the local gradient:
/tildewide∇iR(Wt)−∇iR(Wt) =τ
m/summationdisplay
kaik(wt
k−wt−dik(t)
k).
From iteration t−dik(t) to iteration t, thek-th machine has performed dik(t) gradient proximal
operations. The intuition is that, by the non-expansiveness of the proximal operator, the error
in gradient would not cause too much error in the iterates, and then by the smoothness of the
objective, this would in turn only results in small error in gradient of the next step. It is important
to note that, all machines are inﬂuenced by each other and the local errors are propagated to the
entire graph.
Based on the non-expansive property of the proximal operator and the additional assumption
of the adjacency matrix being doubly-stochastic, it is straightforward to show the following conver-
gence guarantee for the (non-accelerated) proximal gradient algorithm. The algorithm converges
at a slower linear rate than without delays.
Theorem 7. Assume that the aﬃnity matrix Ais doubly-stochastic, i.e.,/summationtext
k∈Niaik= 1for alli,
and the delay in the update rule (20) has delay bounded by Γ. Set the inverse stepsize β=η+τ
m.
Then after t≥1iterations of the algorithm, we have
max
i=1,...,m/vextenddouble/vextenddoublewt
i−/hatwidewi/vextenddouble/vextenddouble≤/parenleftbigg
1−η
η+τ/parenrightbiggt
1+Γ
·max
i=1,...,m/vextenddouble/vextenddoublew0
i−/hatwidewi/vextenddouble/vextenddouble.
21
Proof. Since/hatwiderWis the optimal solution to the ERM problem, we have that
/hatwidewi= proxβ
/hatwideFi
m/parenleftbigg
/hatwidewi−1
β∇iR(/hatwiderW)/parenrightbigg
, i = 1,...,m.
Then, by the non-expansiveness of the proximal operator, we obtain
/vextenddouble/vextenddoublewt+1
i−/hatwidewi/vextenddouble/vextenddouble=/vextenddouble/vextenddouble/vextenddouble/vextenddoubleproxβ
/hatwideFi
m/parenleftbigg
wt
i−1
β/tildewide∇iR(Wt)/parenrightbigg
−proxβ
/hatwideFi
m/parenleftbigg
/hatwidewi−1
β∇iR(/hatwiderW)/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg
wt
i−1
β/tildewide∇iR(Wt)/parenrightbigg
−/parenleftbigg
/hatwidewi−1
β∇iR(/hatwiderW)/parenrightbigg/vextenddouble/vextenddouble/vextenddouble/vextenddouble
=/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/parenleftbigg
1−η+τ/summationtext
k∈Niaik
βm/parenrightbigg
(wt
i−/hatwidewi) +/summationdisplay
k∈Niτaik
βm(wt−dik(t)
k−/hatwidewk)/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble/vextenddouble
≤/parenleftbigg
1−η+τ/summationtext
k∈Niaik
βm/parenrightbigg/vextenddouble/vextenddoublewt
i−/hatwidewi/vextenddouble/vextenddouble+τ
βm/summationdisplay
k∈Niaik/vextenddouble/vextenddouble/vextenddoublewt−dik(t)
k−/hatwidewk/vextenddouble/vextenddouble/vextenddouble
≤/parenleftbigg
1−η+τ/summationtext
k∈Niaik
βm/parenrightbigg/vextenddouble/vextenddoublewt
i−/hatwidewi/vextenddouble/vextenddouble+τ
βm/summationdisplay
k∈Niaikmax
t−Γ≤t/prime≤t/vextenddouble/vextenddouble/vextenddoublewt/prime
k−/hatwidewk/vextenddouble/vextenddouble/vextenddouble (21)
where we have used the triangle inequality in the second inequality.
Assume that the aﬃnity matrix Ais doubly-stochastic, so that/summationtext
k∈Niaik= 1 for all i. De-
noteV(t) = max i=1,...,m/vextenddouble/vextenddoublewt
i−/hatwidewi/vextenddouble/vextenddouble. Then (21) implies that/vextenddouble/vextenddoublewt+1
i−/hatwidewi/vextenddouble/vextenddouble≤/parenleftBig
1−η+τ
βm/parenrightBig
V(t) +
τ
βmmaxt−Γ≤t/prime≤tV(t/prime) holds for all i, and as a result
V(t+ 1)≤/parenleftbigg
1−η+τ
βm/parenrightbigg
V(t) +τ
βmmax
t−Γ≤t/prime≤tV(t/prime).
As long asβ≥η+τ
m, we have/parenleftBig
1−η+τ
βm/parenrightBig
∈[0,1]. Then according to Feyzmahdavian et al. (2014,
Lemma 3), we have
V(t)≤/parenleftbigg
1−η
βm/parenrightbiggt
1+Γ
V(0).
Settingβto be the smallest possible valueη+τ
myields the desired result.
H Comparisons with previous distributed multi-task learning al-
gorithms
We now provide upper bounds of the iteration complexities for the distributed multi-task learning
algorithms of Vanhaesebrouck et al. (2017) and Liu et al. (2017) in the ERM setting. We convert
their notations into ours to be consistent.
22
H.1 Iteration complexity of the algorithm of Liu et al. (2017)
The full algorithm of Liu et al. (2017) performs alternating optimization over the task relationship
and the local predictors on each machine. In order to to compare their algorithm with ours on the
eﬃciency of learning predictors, we consider a ﬁxed task correlation matrix M=I+τ
ηLin their
objective (corresponding to Ω in eqn (1) of their paper).
With ﬁxed M, their algorithm performs distributed SDCA (Ma et al., 2015) for optimizing over
the predictors. In each round of distributed SDCA, one constructs an upper bound of the objective
that is separable over the machines (predictors), so that each machine solves a subproblem deﬁned
by its local data, and then one around of communication is used to aggregate local updates.
When the instantaneous losses are βF-smooth and each local subproblem is solved exactly (i.e.,
we set Θ = 0 in their analysis), the number of global (communication) rounds needed for obtaining
an approximate solution is, according to Liu et al. (2017, Lemma 7 and Theorem 8), of the order
(ignoring the logarithmic factor on ﬁnal optimization error)
max
αα/latticetopKα/summationtextm
i=1α/latticetop
[i]Kα[i]·max
i/parenleftbig
M−1/parenrightbig
ii·βF
η.
Here, the ﬁrst term measures the “task separability” with value in [1 ,m] (see the deﬁnitions of K
andα[i]in their Theorem 1, and the discussion of separability in Section 6.3). On the other hand,
we have max i/parenleftbig
M−1/parenrightbig
ii≤σmax/parenleftbig
M−1/parenrightbig
≤1. As a result, the iteration complexity of distributed
SDCA is
/tildewideO/parenleftbiggβF
η/parenrightbigg
×(task separability in [1,m]) .
This iteration complexity is similar to that of our ERM algorithm by directly solving the regular-
izer (/tildewideO/parenleftBig/radicalBig
βF
η/parenrightBig
), but has worse dependence on the condition number and an unclear multiplicative
constant on the tasks separability.
H.2 Comparison with the collaborative algorithm of Vanhaesebrouck et al.
(2017)
We now compare with the collaborative learning algorithm of Vanhaesebrouck et al. (2017) in
the synchronous and decentralized setting. In their algorithm, each machine augments its local
optimization parameters to include a copy of predictor from each neighboring machine. Let Θ ibe
the set of|Ni|+ 1 variables wkfork∈Ni∪{i}, and Θk
iis the copy of wkon machine i. We can
reformulate the global objective (2) as
arg min
{Θi}m
i=1m/summationdisplay
i=1Hi(Θi) whereHi(Θi) =1
m/hatwideFi(Θi
i) +η
2m/vextenddouble/vextenddoubleΘi
i/vextenddouble/vextenddouble2+τ
4m/summationdisplay
k∈Niaik/vextenddouble/vextenddouble/vextenddoubleΘi
i−Θk
i/vextenddouble/vextenddouble/vextenddouble2
subject to Θi
i= Θi
k,for all (i,k) s.t.k∈Ni. (22)
Vanhaesebrouck et al. (2017) then introduce variables associated with each edge (4 set of vari-
ables per edge) and apply ADMM to the resulting problem. An advantage of ADMM is that it
allows decoupling of the local problems when updating primal variables, where the local problem
involves the nonlinearized loss function.
23
Although Vanhaesebrouck et al. (2017) suggest that the convergence results of synchronous
decentralized ADMM (Wei and Ozdaglar, 2013; Shi et al., 2014) apply to this formulation (see
their Appendix D), we note however that (22) is not in the standard form covered by these results.
In particular, the classical decentralized concensus problem has the form
min
x1,...,xmm/summationdisplay
i=1fi(xi) s.t. xi=xjfor all (i,j) wherej∈Ni.
Here, neighboring machines share the same set of optimization parameters and they would like to
reach complete consensus, whereas in (22) neighboring machines can have diﬀerent set of variables
and they only try to achieve consensus on the shared parameters. As a result, it is nontrivial to
derive the iteration complexity of the collaborative learning algorithm of Vanhaesebrouck et al.
(2017) based on the same quantities used in the analysis of our algorithms.
I Experiments
In this section we examine the empirical performance of the proposed algorithms. We consider the
problem of linear regression on synthetic data. For the i-th task, we generate data from
y=/angbracketleftw∗
i,x/angbracketright+/epsilon1,
where/epsilon1is noise drawn from the Normal distribution N(0,3),x∈Rdis drawn from a multivariate
Normal distribution with mean zero and covariance matrix Σwhere Σij= 2−|i−j|/3, and w∗
i∈Rd
is a coeﬃcient vector for the i-th task generated from the following clustered multi-task structure.
Each w∗
iis drawn from a mixture of Cclusters; there is a reference model rjfor each cluster
j= 1,...,C , and the task speciﬁc model w∗
iis a small perturbation of the corresponding cluster
reference model:
w∗
i=rj+ξi,ifw∗
iis drawn from cluster j.
The cluster reference model rjis generated by sampling each entry i.i.d. from Unif [−0.5,0.5],
while the perturbation vector ξiis generated by sampling each entry i.i.d. from Unif [−0.05,0.05].
This construction gives us task speciﬁc models which are similar to each other when they belong to
the same cluster. The corresponding similarity graph is a 10-nearest neighbor graph (so the graph
is connected) with binary weights built on {wi}i=1,...,m, i.e., each task is connected to 10 other
tasks whose models are most similar.
We tested a few graph-based multi-task learning methods.
•Local : solves a local ERM problem (with only /lscript2regularization) with nsamples for each
task.
•Centralized : solves the graph-regularized ERM problem (2) with nsamples for each task.
•ADMM : the synchronized version of the ADMM algorithm of Vanhaesebrouck et al. (2017).
•SDCA : the distributed SDCA algorithm of Liu et al. (2017) for ﬁxed graph.
•Our algorithms: denoted as B/S (batch/stochastic) + SR/OL (solve regularizer/optimize
loss).
24
C=1 C=5 C=10 C=50
0 20 40 60
iterationerrorlocal
central
ADMM
SDCA
BSR
BOL
SSR-50
SOL-50
0 20 40 60
iteration
0 20 40 60
iteration
0 20 40 60
iteration
0 20 40 60
computationerrorlocal
central
ADMM
SDCA
BSR
BOL
SSR-50
SOL-50
0 20 40 60
computation
0 20 40 60
computation
0 20 40 60
computation
Figure 2: Performance of diﬀerent methods for regularized empirical risk minimization.
In the experiments below, we have problem dimension d= 100, number of tasks m= 100,
training set size n= 500, and vary number of task clusters Cover{1,5,10,50}(smallerCim-
plies overall stronger task similarity). We also generate a dev set of 10000 samples per task for
tuning hyper-parameters, and test set of 10000 samples per task for approximately evaluating the
population loss.
Empirical risk minimization We ﬁst compare the iterative methods on the regularized ERM
problem (2), to which the analysis for ADMM andSDCA applies. We tune the /lscript2regularization param-
eter for Local and (η,τ) for Centralized , and then ﬁx the optimal ( η,τ) for other methods. We
also tune the quadratic penalty parameter for ADMM , the task separability and stepsize parameters
forSDCA , and stepsize parameter for BSR/BOL (although the default value based on the smooth-
ness parameter already works well for them). For SSR/SOL, we draw random samples from the
ﬁxed training set (with size n), and simply ﬁx the minibatch size to be n/10.
Figure 2 shows for each method the estimated F(W) over iterations (or rounds of communica-
tion) in the top row, and over the amount of computation (measured by the number of passes over
the training set) in the bottom row. Observe that all iterative algorithms converge to the same
ERM solution, our algorithms tend to consistently outperform ADMM andSDCA .
Stochastic optimization We next demonstrate the eﬃciency of true stochastic algorithms (using
fresh samples for each update) at C= 10. We allow the algorithms to process a total of 10000 fresh
samples on each machine, and vary the minibatch size bover{40,80,100,200,500}. The parameters
(η,τ) are ﬁxed to those used in the ERM experiments.
Figure 3 shows for each method the estimated F(W) over iterations (or rounds of communica-
tion) in the left plot, and over the amount of fresh samples processed (or total computation cost)
in the right plot. As a reference, the error of Local andCentralized (usingn= 500 samples per
machine) are also given in the plots. We observe that with fresh samples, stochastic algorithms are
25
0 50 100 150
iteration4.74.84.955.15.2errorlocal
central
SSR-40
SSR-80
SSR-100
SSR-200
SSR-500
SOL-40
SOL-80
SOL-100
SOL-200
SOL-500
0 0.5 1 1.5 2
sample 1044.74.84.955.15.2errorFigure 3: Performance of stochastic algorithms with various minibatch sizes. Here C= 10.
competitive to ERM algorithms in terms of sample complexity, while being computationally more
eﬃcient.
References
Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classiﬁ-
cation. In Proceedings of the 24th international conference on Machine learning , pages 17–24.
ACM, 2007.
R. K. Ando and T. Zhang. A framework for learning predictive structures from multiple tasks and
unlabeled data. J. Mach. Learn. Res. , 6:1817–1853, 2005.
A. Argyriou, T. Evgeniou, and M. Pontil. Convex multi-task feature learning. Mach. Learn. , 73
(3):243–272, 2008.
M.-F. Balcan, A. Blum, S. Fine, and Y. Mansour. Distributed learning, communication complexity
and privacy. In S. Mannor, N. Srebro, and R. Williamson, editors, JMLR W&CP 23: COLT
2012, volume 23, pages 26.1–26.22, 2012.
I. M. Baytas, M. Yan, A. K. Jain, and J. Zhou. Asynchronous multi-task learning. In IEEE
International Conference on Data Mining (ICDM) , 2016.
S. P. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. Distributed optimization and statistical
learning via the alternating direction method of multipliers. Found. Trends Mach. Learn. , 3(1):
1–122, 2011.
O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao. Optimal distributed online prediction using
mini-batches. Journal of Machine Learning Research , 13:165–202, 2012.
T. Evgeniou and M. Pontil. Regularized multi-task learning. In Proceedings of the tenth ACM
SIGKDD international conference on Knowledge discovery and data mining , pages 109–117.
ACM, 2004.
26
T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks with kernel methods. In J.
Mach. Learn. Res. , pages 615–637, 2005.
H. R. Feyzmahdavian, A. Aytekin, and M. Johansson. A delayed proximal gradient method with
linear convergence rate. In 2014 IEEE International Workshop on Machine Learning for Signal
Processing (MLSP) , 2014.
R. Johnson and T. Zhang. Accelerating stochastic gradient descent using predictive variance re-
duction. In Advances in Neural Information Processing Systems (NIPS) , pages 315–323, 2013.
J. Konecny, B. McMahan, and D. Ramage. Federated optimization: Distributed optimization
beyond the datacenter. arXiv:1511.03575 [cs.LG], 2015.
G. Lan. An optimal method for stochastic composite optimization. Math. Prog. , 133(1–2):365–397,
2012.
S. Liu, S. J. Pan, and Q. Ho. Distributed multi-task relationship learning. arXiv:1612.04022 [cs.LG],
2017.
K. Lounici, M. Pontil, A. B. Tsybakov, and S. A. van de Geer. Oracle inequalities and optimal
inference under group sparsity. Ann. Stat. , 39:2164–204, 2011.
C. Ma, V. Smith, M. Jaggi, M. I. Jordan, P. Richtarik, and M. Takac. Adding vs. averaging in
distributed primal-dual optimization. In Proc. of the 32st (ICML 2015) , 2015.
A. Maurer. The Rademacher complexity of linear transformation classes. In G. Lugosi and H.-U.
Simon, editors, Annual Conference on Learning Theory , pages 65–78, 2006.
A. Nedi´ c and A. Ozdaglar. Distributed subgradient methods for multi-agent optimization. IEEE
Trans. Automat. Contr. , 54(1):48–61, 2009.
Y. Nesterov. Introductory Lectures on Convex Optimization. A Basic Course . Number 87. Springer-
Verlag, 2004.
G. Obozinski, M. J. Wainwright, and M. I. Jordan. Support union recovery in high-dimensional
multivariate regression. Ann. Stat. , 39(1):1–47, 2011.
S. S. Ram, A. Nedi´ c, and V. V. Veeravalli. Distributed stochastic subgradient projection algorithms
for convex optimization. Journal of optimization theory and applications , 147(3):516–545, 2010.
K. Scaman, F. Bach, S. Bubeck, Y. T. Lee, and L. Massoulie. Optimal algorithms for smooth and
strongly convex distributed optimization in networks. arXiv:1702.08704 [math.OC], 2017.
M. Schmidt, N. L. Roux, and F. Bach. Convergence rates of inexact proximal-gradient methods
for convex optimization. pages 1458–1466, 2011.
S. Shalev-Shwartz, O. Shamir, N. Srebro, and K. Sridharan. Stochastic convex optimization. In
S. Dasgupta and A. Klivans, editors, Proc. of the 22th Annual Conference on Learning Theory
(COLT’09) , Montreal, Quebec, 2009.
27
O. Shamir and N. Srebro. Distributed stochastic optimization and learning. In 52nd Annual Allerton
Conference on Communication, Control, and Computing (Allerton), 2014 , pages 850–857. IEEE,
2014.
W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin. On the linear convergence of the ADMM in
decentralized consensus optimization. IEEE Trans. Signal Processing , 62(7):1750–1761, 2014.
B. A. Turlach, W. N. Venables, and S. J. Wright. Simultaneous variable selection. Technometrics ,
47(3):349–363, 2005.
P. Vanhaesebrouck, A. Bellet, and M. Tommasi. Decentralized collaborative learning of personalized
models over networks. In Int. Workshop on Artiﬁcial Intelligence and Statistics , pages 509–517,
2017.
J. Wang, M. Kolar, and N. Srebro. Distributed multitask learning. ArXiv e-prints,
arXiv:1510.00633 , 2015, arXiv:1510.00633 .
J. Wang, M. Kolar, and N. Srebro. Distributed multi-task learning with shared representation.
2016, arXiv:1603.02185 .
J. Wang, W. Wang, and N. Srebro. Memory and communication eﬃcient distributed stochastic
optimization with minibatch prox. In S. Kale and O. Shamir, editors, Annual Conference on
Learning Theory , Amsterdam, Netherlands, 2017.
E. Wei and A. Ozdaglar. On the o(1/k) convergence of asynchronous distributed alternating direc-
tion method of multipliers. arXiv:1307.8254 [math.OC], 2013.
K. Yuan, Q. Ling, and W. Yin. On the convergence of decentralized gradient descent. SIAM
Journal on Optimization , 26(3):1835–1854, 2016.
M. Yuan, A. Ekici, Z. Lu, and R. Monteiro. Dimension reduction and coeﬃcient estimation in
multivariate linear regression. J. R. Stat. Soc. B , 69(3):329–346, 2007.
28

arXiv:1809.00832v1  [cs.LG]  4 Sep 2018Improving the Expressiveness of
Deep Learning Frameworks with Recursion†
Eunji Jeong∗, Joo Seong Jeong∗, Soojeong Kim, Gyeong-In Yu, Byung-Gon Chun‡
Seoul National University
{ejjeong, joosjeong, soojeong kim, gyeongin, bgchun }@snu.ac.kr
Abstract
Recursive neural networks have widely been used by re-
searchers to handle applications with recursively or hiera rchi-
cally structured data. However, embedded control ﬂow deep
learning frameworks such as TensorFlow, Theano, Caffe2, an d
MXNet fail to efﬁciently represent and execute such neural n et-
works, due to lack of support for recursion. In this paper, we add
recursion to the programming model of existing frameworks b y
complementing their design with recursive execution of dat aﬂow
graphs as well as additional APIs for recursive deﬁnitions. Un-
like iterative implementations, which can only understand the
topological index of each node in recursive data structures , our
recursive implementation is able to exploit the recursive r ela-
tionships between nodes for efﬁcient execution based on par allel
computation. We present an implementation on TensorFlow an d
evaluation results with various recursive neural network m odels,
showing that our recursive implementation not only conveys the
recursive nature of recursive neural networks better than o ther
implementations, but also uses given resources more effect ively
to reduce training and inference time.
1 Introduction
Recursive neural networks have widely been used by research ers
to handle applications with recursively or hierarchically struc-
tured data, such as natural language processing [ 27,25,3] and
scene parsing [ 23,25,22,13].
In order to implement such models, embedded control ﬂow
deep learning frameworks (in short, embedded control ﬂow
frameworks ), such as TensorFlow [ 1], Theano [ 30], Caffe2 [ 6],
and MXNet [ 4], embed control ﬂows within dataﬂow graphs,
i.e., the control ﬂow is represented as a type of operation of
the dataﬂow graph, which can trigger conditional execution or
iterative computation. However, the programming model pro -
posed by such frameworks fails to efﬁciently represent and e x-
ecute neural networks with recursive structures. The desig ns
of these frameworks do not consider recursive models and in-
stead urge users to either write their models with iterative con-
† Appeared in EuroSys ’18
* Both authors contributed equally to the paper
‡ Corresponding authorstructs [ 29] or completely unroll models without exploiting con-
trol ﬂow at all [ 28,18]. Meanwhile, non-embedded control ﬂow
deep learning frameworks (in short, non-embedded control ﬂow
frameworks ) such as PyTorch [ 20] or DyNet [ 19] allow users to
deﬁne control ﬂows from the client-side, creating new compu ta-
tion graphs for all possible control ﬂow paths of a model. Thi s
approach trades performance for programmability, losing o pti-
mization opportunities because each graph is usually execu ted
only once.
An important example of recursive neural networks is the
TreeLSTM [ 27] model, a tree-shaped network with recur-
sively deﬁnable nodes, demanding complicated execution me ch-
anisms. In existing frameworks, the TreeLSTM network is han -
dled by either statically unrolling the full network graph b efore-
hand [ 20,19], or using a single LSTM cell to iteratively com-
pute all intermediate nodes [ 1,30]. For the former case, it is
difﬁcult to process multiple data instances together becau se the
tree structure differs for each instance. For the latter cas e, the
iterative execution is inherently sequential and thus is in capable
of computing multiple nodes in parallel.
In this paper, we introduce recursive deﬁnitions into the pro-
gramming model of existing embedded control ﬂow frame-
works [ 1,6,4,30], adding ﬁrst-class support for recursion. By
allowing users to directly express recursive deﬁnitions in appli-
cation code with enhanced programmability, models with rec ur-
sive data structures such as trees or graphs can be written wi th-
out requiring users to use a separate complex API to express
the control ﬂow [ 15]. Also, optimization opportunities can be
exploited to boost performance, such as concurrently execu ting
child nodes in tree structures that have no dependencies bet ween
each other.
We make recursive deﬁnitions possible by introducing a spe-
cial graph operation, InvokeOp , that abstracts the execution of
aSubGraph . Users can incorporate recursion in models by
invoking a SubGraph within the InvokeOp that abstracts the
sameSubGraph . The framework handles the execution of an
InvokeOp as the initiation of a new SubGraph containing a bun-
dle of inner operations, which are treated the same as the ori ginal
running operations.
We implemented support for recursively deﬁned dataﬂow
graphs on TensorFlow [ 1], a widely used deep learning (DL)
1
framework. To show the expressive power and the performance
of recursive graphs, we implemented three applications usi ng
our framework: sentiment analysis with the TreeRNN [ 25],
RNTN [ 26], and TreeLSTM [ 27] models. For every model, we
succeeded in capturing the recursive semantics of the compu ta-
tion graph, and achieved competitive performance compared to
other state-of-the-art deep learning frameworks such as Te nsor-
Flow [ 1] and PyTorch [ 20].
The rest of the paper is organized as follows. Section 2ex-
plains the limitations of existing embedded control ﬂow fra me-
works regarding recursive models, and Section 3provides a
high-level API for efﬁciently representing such recursive mod-
els. Section 4describes the design aspects of our framework,
and Section 5presents the implementation details. Section 6
presents evaluation results on various applications. Sect ion7
covers related work and Section 8concludes.
2 Motivation
2.1 Embedded Control Flow Frameworks and
Their Limitations
Modern deep learning frameworks use directed acyclic graph s
(DAGs) to represent mathematical computations of deep lear n-
ing applications and the execution order of such computatio ns.
The vertices of graphs represent the mathematical operatio ns,
while the edges represent the dependencies between two oper -
ations. An edge from operation ato operation bimplies that
the output of ais fed into bas the input value. As the execu-
tion order between any two operations in the computation gra ph
is statically determined, it is a non-trivial task to repres ent dy-
namic control ﬂow within computations, such as conditional ly
executing only a part of the graph, or jumping to a nonadjacen t
operation.
Based on how to handle dynamic control ﬂow, we can di-
vide deep learning frameworks into two categories: embedded
control ﬂow frameworks and non-embedded control ﬂow frame-
works. Embedded control ﬂow frameworks such as Tensor-
Flow [ 1] and Theano [ 30] include control ﬂow concepts inside
the computation graph. They deﬁne special kinds of control ﬂ ow
operations to embed the control ﬂow within the graph. This wa y,
a single computation graph is able to express multiple contr ol
ﬂow paths. Since these frameworks can build a single graph an d
execute it repeatedly, aggressive performance optimizati on can
be done while hiding the optimization overhead.
On the other hand, non-embedded control ﬂow frameworks
including PyTorch [ 20], DyNet [ 19], and Chainer [ 31] do not
represent the control ﬂow inside the computation graph. Ins tead,
they create a new static computation graph for every activat ed
control ﬂow. This approach enables fast prototyping and eas y
development of various deep neural networks. However, this
approach leaves little room to optimize the performance of c om-
putation graph execution, because each graph gets executed only
once.
Embedded control ﬂow frameworks. In embedded control
ﬂow frameworks, graph vertices represent not only arithmet icoperations (e.g., AddorMatMul ) and data transformations (e.g.,
Concat ), but also data-dependent control ﬂow mechanisms.
Conditional expressions are often made available by many em -
bedded control ﬂow frameworks. A predicate is expected as th e
ﬁrst input argument, and two other operation groups as the true
andfalse inputs. Based on the predicate value, only one of
the two operation groups are executed and passed to the out-
put operation. Another useful control ﬂow construct in exis ting
deep learning frameworks is the iterative loop construct, n amely
thewhileloop operation in TensorFlow and the Scan operator
in Theano. This kind of API enables adding a group of opera-
tions, referred to as a loop body, to be executed multiple tim es
iteratively. Conditional expressions are usually used wit h loop
constructs to denote the termination condition of the loop b ody.
By planting dynamic control ﬂow inside the computation
graph and thus decoupling the client-side code execution fr om
computation graph execution, frameworks can exploit paral -
lelism while executing jobs by handling mutually independe nt
operations in a concurrent manner, and can also exploit grap h
optimization techniques for faster execution that would ot her-
wise be impossible for non-embedded control ﬂow frameworks .
This paper will focus on embedded control ﬂow frameworks,
building up on the provided optimizations to produce maximu m
performance.
Limitations of embedded control ﬂow frameworks. The
computation graphs of embedded control ﬂow frameworks do
not fully cover every possible control ﬂow construct, howev er.
Designing recursive neural networks efﬁciently using embe dded
control ﬂow of iterative loop constructs is difﬁcult. Not on ly
is it unclear how to parallelize independent operations wit h it-
erative loops, recursion and iteration are fundamentally d iffer-
ent and thus converting one into another involves a nontrivi al
conversion process [ 14,8,7]. The following subsection shows
an example demonstrating the difﬁculties of designing recu rsive
neural networks with just loop constructs.
2.2 Example: TreeLSTM
The long short-term memory [ 9] (LSTM) cell is a block of func-
tions that is well-known for its ability to “remember” past c om-
putations of a neural network, and is often used for networks that
process data of sequential characteristics such as text dat a with
sentence structures.
TreeLSTM [ 27] is a widely used recursive neural network
based on LSTMs that is known to perform well for applications
with tree-shaped data instances such as parse trees for natu ral
language processing and hierarchical scene parsing [ 25]. In an
ordinary linear recursive neural network, LSTM cells are pl aced
sequentially regardless of the input data structure. On the other
hand, in the TreeLSTM model, LSTM cells are combined to
form a tree, mimicking the shape of the input data tree. Sen-
timent analysis is often used as an application of the TreeLS TM.
For example, with movie review sentences and the correspond -
ing ratings as training input data and labels, the TreeLSTM n et-
work can be trained to predict the sentiment of each movie re-
view sentence.
2
1states = array()
2
3defcomputeleaf(idx):
4currstate = lstm(embed(tree.leaves[idx]))
5states.insert(idx, curr state)
6
7defcomputeinternal(idx):
8leftidx, rightidx = tree.children[idx]
9leftstate = states.get(left idx)
10rightstate = states.get(right idx)
11currstate = lstm(left state, right state)
12states.insert(idx, curr state)
13
14forloop(range(numleaves), compute leaf)
15forloop(range(numinternals), compute internal)
16
17rootstate = states[root idx]
Figure 1: Iterative implementation of the TreeLSTM model in
pseudocode.
There are two approaches to implement this TreeLSTM net-
work with current deep learning frameworks, both having its
own limitations.
The ﬁrst approach is unrolling the whole tree structure to the
computation graph, so that LSTM cells are duplicated for eac h
tree node. To train multiple trees with this approach, howev er, a
new graph must be created for all input training instances. N ot
only does this result in an excessive amount of graph objects
and signiﬁcant construction overhead, the effect of compil e-time
graph optimization is near zero as all graphs are used only on ce.
The second approach is using iterative control ﬂow operations
provided by frameworks. Figure 1shows pseudocode of an it-
erative implementation of the TreeLSTM model. In this imple -
mentation, a single LSTM cell can be used multiple times for
multiple input data instances. After the leaf nodes are proc essed
sequentially in Line 14, the internal nodes with their depen den-
cies resolved get processed in Line 15. In order for this ap-
proach to work, the input tree must be preprocessed so that it s
nodes are assigned with topologically sorted indices, i.e. , exe-
cuting the tree nodes in an iterative manner does not violate the
computational dependencies. Since the recursive nature of the
tree structure cannot be directly represented by iteration , it is
difﬁcult to write and understand this code.
The process of topologically sorting the tree nodes loses th e
parent-child node relationships of the tree, and thus the it erative
implementation can only view the tree nodes as a linearly or-
dered list. A recursive formulation, on the other hand, woul d
be able to utilize the information on parent-child relation ships to
concurrently execute nodes, and is inherently more suitabl e for
representing recursive neural networks, preserving their recur-
sive nature.2.3 Recursion in Embedded Control Flow
Frameworks
The drawbacks of the unrolling method and the iterative meth od
suggest the need for a more effective and intuitive solution to
implement TreeLSTMs, and recursive neural networks in gen-
eral. We propose that recursively deﬁning and executing rec ur-
sive neural networks is a simple yet powerful approach.
Recursive execution of computation graphs has many similar -
ities with recursive invocation of functions in general pro gram-
ming languages. Recursive function invocation in programm ing
languages is supported by allowing a function to call itself inside
the function body. This is usually more complicated than exe -
cuting non-recursive functions, since when parsing the sou rce
code of a recursive function, the recursive function call mu st be
processed before the parsing of the function gets ﬁnished.
Inspired by the concept of functions and function invocatio ns,
we propose to design similar ideas in embedded control ﬂow
frameworks to support recursive execution. First, a progra m-
ming interface for deﬁning a subset of the computation graph
that will be executed recursively is required. Then, an invo ca-
tion operation inside the graph subset is also needed, to tri gger
the recursive execution of the graph subset. No modern embed -
ded control ﬂow framework supports these functionalities a nd,
at the same time, is able to train a recursive neural network, to
the best of our knowledge.
Our observations above suggest that an implementation of re -
cursion, for embedded control ﬂow frameworks, must satisfy
two conditions. First, recursion must be expressible as par t of a
valid computation graph. Despite the fact that recursion im plies
the usage of a call stack of arbitrary length, the graph repre senta-
tion of recursion must be ﬁnite and executable by the framewo rk.
The graph representation of recursion corresponds to the re cur-
sivefunction deﬁnition; the deﬁnition simply denotes what com-
putation is involved and how the recursion occurs, while not ac-
tually running the function. Moreover, this representatio n must
be usable together with other non-recursive parts of the com pu-
tation graph (Section 3.1).
Second, an operation included in a recursive computation
graph must be able to trigger the surrounding computation gr aph
recursively. The operation triggering the recursive graph execu-
tion corresponds to the function invocation , which can further
unfold the computation until the recursion termination con dition
is satisﬁed (Section 3.2).
3 Programming Model
In this section, we describe our modiﬁcations to the program -
ming model of existing embedded control ﬂow frameworks, as
well as how they are translated into dataﬂow graph component s.
3.1 Unit of Recursion: SubGraph
It is infeasible to implement the dynamism and recurrences o f
recursive computations using the static components of data ﬂow
graphs provided by existing embedded control ﬂow framework s.
3
In response to this shortcoming, we ﬁrst propose an abstrac-
tion,SubGraph , that represents basic recursive blocks and, at
the same time, can be used in conjunction with existing opera -
tions to create a dataﬂow graph with recursive computations .
SubGraph s are created by grouping operations of a given
computation graph that will be executed recursively. SubGraph s
represent fractions of a dataﬂow graph. Executing a SubGraph
refers to executing the operations that belong to that SubGraph .
The inputs and outputs of operations that are connected to
outer operations (operations that reside outside of the cur rent
SubGraph ) are assigned as inputs and outputs of the SubGraph
itself. During execution, the inputs of a SubGraph are passed to
the corresponding inner operations, while operation outpu ts that
must be shipped out to outer operations are passed as SubGraph
outputs. A SubGraph can be regarded as a function in general
programming languages.
Additionally, we allow SubGraph s to invoke other Sub-
Graph s. ASubGraph invocation within an outer SubGraph is
connected to the other inner operations to form an inner data ﬂow
graph, just as the outer SubGraph is connected to outer oper-
ations. A SubGraph invocation in a SubGraph simply implies
that there is yet another group of operations to be executed a t that
particular graph position. Coming back to the function anal ogy,
placing a SubGraph invocation within a SubGraph is identical
to calling a function within another function.
More importantly, a SubGraph may recursively invoke itself.
This aspect makes possible the deﬁnition of a recursive comp u-
tation; we deﬁne a recursive block as a SubGraph and insert a
invocation to itself in the same SubGraph .
Figure 2shows the recursive implementation of the Tree-
LSTM model, with details omitted for brevity. After deﬁning
aSubGraph for the TreeLSTM model in Line 2, we reuse the
deﬁnition in Lines 10-11 to complete the recursive tree stru cture
of the model. Notice how a conditional expression is used ( ifin
Line 14) to separate the base case from the recursive case. Co m-
paring with Figure 1, this recursive version follows the deﬁnition
of the TreeLSTM model more clearly; the recursive nature of t he
tree structure is explicitly represented in this implement ation.
3.2 Recursion in Dataﬂow Graphs: InvokeOp
WhileSubGraph s provide a convenient way to deﬁne recursive
computations, the framework is still left with the task of ac tually
executing the operations gathered as SubGraph s. However, as
SubGraph operations are expected to be executed in a recursive
fashion, an additional mechanism for “re-executing” the op era-
tions ofSubGraph s repeatedly (until some termination condition
is met) is required. To this end, we introduce a new operation
namedInvokeOp .
AnInvokeOp is an operation that takes a set of tensors as
input, runs an associated SubGraph (i.e., executes the inner op-
erations of the SubGraph ) with the provided inputs, and returns
the resulting tensors as output. InvokeOp s are execution ob-
jects instantiated from SubGraph invocations; as SubGraph s
are semantically close to function deﬁnitions, InvokeOp s can
be considered as function calls to the functions speciﬁed by1# TreeLSTM: index(int32) −>hiddenstate(Tensor)
2withSubGraph()asTreeLSTM:
3idx =TreeLSTM.input(int32)
4
5defcomputeleafnode():
6returnLSTM(embed(tree.leaves[idx]))
7
8defcomputeinternalnode():
9leftidx, rightidx = tree.children[idx]
10leftstate =TreeLSTM(leftidx)
11rightstate =TreeLSTM(rightidx)
12returnLSTM(leftstate, right state)
13
14TreeLSTM.output(if(is leafnode(idx),
15 computeleafnode,
16 computeinternalnode))
17
18rootstate =TreeLSTM(rootidx)
Figure 2: Recursive implementation of the TreeLSTM model
withSubGraph deﬁnitions. After declaring the start of a
SubGraph in Line 2, we indicate the inputs of the SubGraph
in Line 3. The body of the SubGraph is deﬁned in Lines 5-
16, while recursive calls are made on Lines 10-11. Note that
SubGraph outputs must be given as in Lines 14-16. The com-
pletedSubGraph deﬁnition can now be used in Line 18.
SubGraph s. As such, it is possible for a single SubGraph to
be associated with more than one InvokeOp .
Despite the special property of having an associated
SubGraph , anInvokeOp is fundamentally the same as other op-
erations such as Add orMatMul , and is generally treated as an
ordinary operation. The difference with other operations c omes
from the operation kernel implementation; instead of perfo rming
a mathematical calculation, an InvokeOp abstracts the execution
of an entire SubGraph . This difference also affects a process
called automatic differentiation, a core functionality pr ovided by
modern deep learning frameworks for training models. Inste ad
of calculating mathematical derivates of some numerical fu nc-
tion like other operations, the framework must take into acc ount
the associated SubGraph of anInvokeOp . We will discuss this
further in Section 4.2.2 .
3.3 TreeLSTM with SubGraph s &InvokeOp s
Figure 3portrays an example on how InvokeOp s are used to
represent the TreeLSTM (Section 2.2) model with recursion. A
completely unrolled depiction of the model for a full binary tree
is shown in Figures 3(a) and 3(b). It is not hard to observe that
the model can be expressed using recursion: the embed oper-
ation and the LSTM cell at the leaves form the base case (Fig-
ure3(a)), while the two-input LSTM cell at the intermediate notes
corresponds to the recursive case (Figure 3(b)).
Merging the base case and the recursive case into a SubGraph
with a conditional branch ( if), we now have a concise represen-
4
tation of the TreeLSTM model, as shown in Figure 3(c). Note
that the condensed SubGraph is able to represent TreeLSTMs of
arbitrary height or shape, and not just a single particular s truc-
ture.InvokeOp s are inserted at all inner recursive call points.
4 System Design
In this section, we discuss various system design aspects fo r sup-
porting the recursive programming model of the previous sec -
tion.
Our design complements existing embedded control ﬂow
frameworks with additional APIs for declaring recursive gr aphs
and core changes for executing such recursive graphs. Mod-
els declared using the SubGraph API from Section 3are trans-
formed into a dataﬂow graph containing InvokeOp s. In turn,
the framework core engine runs the resulting graph with the
same mechanism used to run non-recursive graphs, accessing
additional graph and value cache structures when dealing wi th
InvokeOp s. The design does not involve any implementation
details of a particular framework, and can be applied to any
DL framework that incorporates control ﬂows in computation
graphs.
4.1 Graph Execution
4.1.1 Background: Execution Model of Existing Frame-
works
The execution model of embedded control ﬂow frameworks can
be characterized by three components: the client who builds and
submits dataﬂow graphs to the framework, the master which
parses the given graphs and schedules the execution of opera -
tions, and one or more workers that carry out the actual compu-
tation of the operations. The master coordinates the execut ion
of operations on the workers, running operations in an order that
respects the inter-operation dependencies.
Steps (1)-(3) of Figure 4displays an illustration of the execu-
tion model, with only one worker shown for simplicity. When
the master ﬁrst analyzes the input dataﬂow graph, operation s that
require no inputs are enqueued directly into the ready queue of
the worker, whereas operations that need at least one input a re
put on standby. Next, execution threads of the worker’s execu-
tion thread pool grab operations from the operation queue and
perform the computation for those operations in parallel. W hen
an execution thread ﬁnishes running an operation, the maste r
checks the waiting operations that have a dependency on the
completed operation, and enqueues operations whose depend en-
cies have all been resolved to the ready queue. This process i s
repeated until all operations have been processed.
4.1.2 Recursive Execution
The execution mechanism for executing a recursively deﬁned
dataﬂow graph is no different from the mechanism for execut-
ing non-recursive graphs. This is possible because the exec ution
of anInvokeOp mimics the initiation of a new dataﬂow graph,
with the exception of reusing the same master scheduler as we llLSTM 
LSTM 
LSTM 
embed LSTM 
embed LSTM 
LSTM 
embed LSTM 
embed 
(a)
LSTM 
LSTM 
LSTM 
embed LSTM 
embed LSTM 
LSTM 
embed LSTM 
embed 
(b)
InvokeOp 
InvokeOp InvokeOp LSTM LSTM LSTM if leaf? 
T F
embed 
(c)
Figure 3: An illustration of how an unrolled computation gra ph
of the TreeLSTM model (a, b) can be transformed into a recur-
sive graph with InvokeOp s (c). The base case, depicted in the
boxes of (a), and the recursive case, indicated by the boxes i n
(b), can be combined to succinctly describe the model as a re-
cursiveSubGraph as shown in (c). InvokeOp s have been added
at the appropriate places to mark the points where a recursiv e
function call to the SubGraph must occur.
5
Operation 
InvokeOp 
Master 
Graph Parser Execution 
Thread Pool 
Waiting 
Operations Ready 
Queue Worker 
Client 
(1) (2) (3) 
(4) 
(2) SubGraph 
Figure 4: The execution model of embedded control ﬂow frame-
works with InvokeOp s. (1) After the client initiates the job with
a dataﬂow graph, (2) the master decomposes the graph into op-
erations and places them into either the ready queue or the wa it-
ing line of the worker, depending on the number of unresolved
inputs. (3) Operations are dequeued from the queue by idle ex -
ecution threads, while new operations are enqueued when inp ut
dependencies are resolved. (4) When an InvokeOp gets exe-
cuted, its associated SubGraph is passed to and processed by
the master, similar to step (1). Only one worker is shown for t he
sake of clarity.
as the same worker ready queues, as illustrated in step (4) of Fig-
ure4. When an InvokeOp becomes executable and is dequeued
by an execution thread, the graph associated with the InvokeOp
is processed by the master, similar to how a graph submitted b y
the client is parsed by the master. Operations that are immed i-
ately ready to be run are enqueued into the ready queue, behin d
the existing operations. Likewise, operations that have at least
one unresolved input dependency are added to the waiting lis t,
together with other previous standby operations.
This design allows recursive dataﬂow graphs to be processed
on existing embedded control ﬂow frameworks without dras-
tic changes. Recursive graphs can enjoy graph optimization s
supplied by such frameworks and achieve good performance
while providing intuitive, recursive APIs at the same time. In
fact, from the framework’s point of view, a recursive graph i s
the more general representation, while non-recursive grap hs are
simply special cases which have no recursive SubGraph s and
InvokeOp s.
It is also worth noting that performing priority scheduling
of operations instead of simple FIFO scheduling may possibl y
yield signiﬁcant effects on the execution time of recursive com-
putation graphs, depending on the inter-operation depende ncies
of the given recursive model. For example, if the model con-
tains aSubGraph whose inner operation must be processed in
order for many outer operations to be enqueued into the ready
queue, then a scheduling decision of processing inner opera tions
before others would lead to a shorter execution time overall . Al-though this is an interesting problem, it is usually not an is sue
for servers with many parallel computation threads to spare and
thus we leave this as future work.
Graph execution stack. When a function is invoked in pro-
gramming languages, the language runtime maintains a call
stack to record the call history and relevant information. T his
enables the program to correctly return from the callee func tion
to the corresponding caller function, and also provides hel p-
ful information to programmers such as backtrace informati on
when an exception occurs while executing the function. A sim i-
lar process of keeping track of the SubGraph invocation history
is required for the recursive graph execution engine. Howev er,
the caller-callee relationship of InvokeOp s cannot be managed
with a linear stack, because an InvokeOp can branch out into
multiple child InvokeOp s in parallel. Rather, the relationship is
maintained as a tree, where each InvokeOp holds a pointer to its
parentInvokeOp (i.e., return location).
4.2 Graph Backpropagation
4.2.1 Background: Automatic Differentiation
Neural networks are normally trained via the backpropagati on
algorithm [ 21]. Backpropagation calculates the errors of all op-
eration output values, by ﬁrst comparing the ﬁnal outputs of
a neural network with expected ground-truth values (labels ) to
compute output errors, and then propagating the output erro rs
all the way back up to the input according to the chain rule. Th e
calculated errors – often referred to as gradients – are used to
update model parameters so that operation outputs are pushe d
towards the expected values.
Backpropagation of a simple linear network is shown in Fig-
ure5(a). Starting from operation op1, all forward operations
op1,op2, and op3are computed in succession to produce val-
uesa,b, and c, respectively. The ﬁnal output cis checked with
the expected value c*to produce the loss value E, as well as
the gradient of Ewith respect to c, denoted as dE/dc . Next,
the other gradients are generated one by one, this time throu gh
the backpropagation line of operations, op3-grad ,op2-grad , and
op1-grad .
Note that in order to calculate a certain gradient, both the p re-
vious gradient and the corresponding forward value are requ ired.
For instance, the gradient dE/db is computed with the previous
gradient dE/dc and the forward value b(op3-grad ). Likewise,
dE/db andaare used to compute dE/da (op2-grad ). This results
in a ﬁnal dataﬂow graph where a forward operation shares its i n-
puts with its backpropagation equivalent (e.g. op2andop2-grad
both take aas input). As a precaution to prevent values from be-
ing invalidated (released from memory) before being consum ed
by all dependent operations, DL frameworks always keep all o p-
eration outputs as valid data until that particular iterati on termi-
nates.1
Automatic differentiation. Deep learning frameworks re-
lieve users from the burden of manually inserting backpropa ga-
1Technically, we could recompute the forward operation valu es during
backpropagation instead of retaining them to save memory. H owever, this in-
curs a signiﬁcant increase in training time and is generally not preferred.
6
a
b
c
Feedforward Backpropagation c* dE / dc op1 
op2 
op3 op2-grad op1-grad 
op3-grad 
loss label dE / db dE / da a
b
(a)
Feedforward Backpropagation op1 
op2 
op3 op2-grad op1-grad 
op3-grad 
InvokeOp InvokeOp 
(b)
Figure 5: Backpropagation of dataﬂow graphs with and with-
outInvokeOp s. (a) A simple linear feedforward network is
shown on the left, while the backpropagation side of the same
network is shown on the right. All gradient operations recei ve
previous gradient values from its gradient predecessor as w ell
as the original feedforward value from the feedforward netw ork.
(b) AnInvokeOp and its gradient operation for backpropaga-
tion are shown on the left and right, respectively. Notice ho w
(a) and (b) are structurally very similar, except for the enc losing
InvokeOp s.
tion operations, with the help of a process called automatic dif-
ferentiation. In the case of embedded control ﬂow framework s,
after a user submits a feedforward neural network to the fram e-
work, the framework automatically adds all operations requ ired
for computing gradients to the given dataﬂow graph. Maintai n-
ing a catalogue of predeﬁned gradient operations, the frame work
backtracks along the feedforward path and adds the correspo nd-
ing gradient for each and every feedforward operation. The r e-
sulting computation graph can then be processed by the frame -
work for execution. As setting up the backpropagation path i s
usually much more tedious than deﬁning the forward path, the
automatic differentiation process is very helpful for user s and
currently supported by all deep learning frameworks.
4.2.2 Recursive Backpropagation
Backpropagation of a recursive dataﬂow graph is similar to
backpropagation of a non-recursive dataﬂow graph. The onlynontrivial issue is how to deﬁne and calculate gradients for
InvokeOp s. As the feedforward output of an InvokeOp is the
execution output of its associated SubGraph , naturally the gra-
dient of an InvokeOp is also generated from the gradients of the
associated SubGraph .
During automatic differentiation, we inspect the SubGraph s
associated with InvokeOp s and perform automatic differentia-
tion on them as well. For each SubGraph , we collect the gradi-
ent operations that were inserted by automatic differentia tion. At
this point, it is possible to simply add the inserted gradien t op-
erations to the backpropagation path of the computation gra ph.
However, in case the SubGraph was used for recursion, the gra-
dients for the inner recursive computations would not be gen -
erated and thus backpropagation would be returning incorre ct
results.
Instead, we wrap each set of gradient operations from
SubGraph s with yet another SubGraph . If a feedforward
SubGraph contains a recursive invocation to itself, then its cor-
responding backpropagation SubGraph will also hold a recur-
sive invocation, at the same position. Later, InvokeOp s are in-
serted atSubGraph invocation points for both the feedforward
SubGraph and the backpropagation SubGraph to complete the
computation graph.
Figure 5(b) illustrates how a gradient operation of an
InvokeOp is formed. The associated SubGraph is shown in
the inner side of the feedforward InvokeOp , while the corre-
sponding gradient operations of the SubGraph are shown inside
the backpropagation InvokeOp . Carrying over operation out-
puts from the feedforward phase to the backpropagation phas e
is done by connecting the outputs and inputs of the relevant o p-
erations, same as in Section 4.2.1 .
5 Implementation
We implemented our framework atop TensorFlow [ 1] 1.3.0.
Framework changes, including the kernel implementation of
InvokeOp as well as internal data structures, were done in the
C++ core, while client-side APIs for recursive deﬁnitions a re
exposed in Python. Here, we describe several implementatio n
issues of our framework.
Forward declaration. In embedded control ﬂow frame-
works, all operations must have well-deﬁned inputs and out-
puts before they are used (comparable to function signature s in
programming languages). InvokeOp s are not exceptions; the
framework does not allow the creation of a recursive InvokeOp
unless the operation deﬁnition for the recursive call is spe ci-
ﬁed beforehand. This rule can be bypassed by using forward
declarations for InvokeOp s that are recursively deﬁned; when
aSubGraph is deﬁned, we ﬁrst predeclare an empty InvokeOp
that has the same signature as the SubGraph , and later “register”
theSubGraph deﬁnition to the empty InvokeOp . Note that this
procedure is automatically done by the framework, and is not
exposed to users. Gradients for backpropagation are deﬁned in
a similar manner, with the operation declaration coming bef ore
the actual deﬁnition.
Backpropagation cache implementation. As described in
7
Invoke 
Op Invoke 
Op LSTM LSTM 
Feedforward Backpropagation HashTable 
concurrent
addition concurrent
retrievalInvoke 
-grad Invoke 
-grad LSTM- 
grad InvokeOp InvokeOp 
Figure 6: Concurrent hash table being used between multi-
ple forward and backward executions of the same operation
(InvokeOp ).
Section 4.2, operation output values from the feedforward phase
must be retained until backpropagation and be fed into the co r-
responding gradient operations. For non-recursive comput ation
graphs, embedded control ﬂow frameworks would accomplish
this simply by holding a feedforward value entry for each re-
quired operation and later passing the values to the appropr iate
gradient operations. Unfortunately, for recursive graphs , an op-
eration within a SubGraph may be called more than one time
across multiple recursions. Multiple output values genera ted
during multiple executions must all be passed to the corresp ond-
ing gradient operation, without losing their topological p osition
information.
We resolve this issue by maintaining a concurrent hash table
for storing and fetching operation output values of SubGraph s.
Figure 6describes the whole procedure of passing multiple feed-
forward output values from InvokeOp s. A hash table is exter-
nally generated and managed per SubGraph , and a unique hash
entry key is used to distinguish table entries. During the fe ed-
forward phase, we store all output values of InvokeOp instances
that come from the SubGraph in the table. An InvokeOp ’s key
is deﬁned by combining the InvokeOp ’s topological position
within the SubGraph with the key of the parent InvokeOp , guar-
anteeing uniqueness. By using a concurrent hash table, mult iple
instances of the same operation in the graph can concurrentl y
access and update the hash table.
Next, during backpropagation, we perform a hash table
lookup for each gradient operation of the InvokeOp instances
and feed the stored value as input. This enables feedforward
output values to be retained and correctly retrieved for bac kprop-
agation. While the concurrent insert operations may incur m inor
overhead, the lookup operations are thread-safe and are neg li-
gible. Is it notable that using a simple queue or stack to stor e
activation values is inadequate, as the order of enqueue and de-
queue operations or push and pop operations is not determini stic
and thus output values may be directed to the wrong gradient
operation.
Outer reference. It is common for a recursive SubGraph to
not refer to the external input values explicitly, but rathe r im-
plicitly. For instance, a static value that is required for a ll lev-
els of recursion of a SubGraph should not be included as aninput of the SubGraph , as the value does not change anyway.
Nonetheless, the TensorFlow framework regards a SubGraph
and the outer graph as two separate graphs and is unable to
understand the identities of such implicit external values un-
less they are speciﬁed as actual operation inputs. Therefor e,
when aSubGraph is created, we analyze the operations to check
whether there are any external inputs that have not already b een
speciﬁed as the SubGraph ’s inputs and add them to the input
list.
Implementation on other frameworks. Recursively deﬁned
SubGraph s andInvokeOp s can be implemented on not only
TensorFlow but any other embedded control ﬂow DL frame-
works as well, with the computation graph and the operations as
its elements. A SubGraph can be provided as an abstraction that
is similar to the framework’s original graph structure but c on-
tains only a subset of all operations to mark a recursion bloc k.
AnInvokeOp can be implemented as a new kind of user-deﬁned
operation type, which recursively executes a SubGraph .
For instance, Caffe2 [ 6] uses aNetDef protocol buffer as
its computation graph, and allows feeding NetDef s as inputs
to operators. By extending NetDef s to recursively represent
subgroups of operators, we can create a Caffe2 version of
InvokeOp that receives such subgroups as inputs and executes
them. Theano [ 30] provides the Scan operator which abstracts
the execution of a loop in a single operator. Although the Scan
operator is usually used to express iterative control ﬂow, t he con-
cept of maintaining a separate graph isolated from the main c om-
putation graph ﬁts well with SubGraph s and is a good starting
point for implementing recursive computations.
6 Evaluation
We evaluate our framework while focusing on the performance
beneﬁts of the recursive nature of the framework, mostly mad e
possible by exploitation of parallelism in recursive neura l net-
works.
6.1 Experimental Setup
Applications. We implemented a variety of neural network
models from the recursive neural network family, namely
the aforementioned TreeLSTM [ 27] model as well as the
TreeRNN [ 25] and the RNTN [ 26] model. All models were
trained and tested to perform sentiment analysis on the Larg e
Movie Review [ 16] dataset, where data instances are sentences
in the form of binary parse trees. For this dataset, we used a p re-
trained network (for each model) to label all nodes. For all m od-
els, we used the same hyperparameters as the ones suggested
in the original papers. We also considered smaller batch siz es
to investigate the performance trends of our framework with out
mixing in additional performance gains obtainable from bat ch-
ing instances.
Frameworks. Along with our implementation of recursive
dataﬂow graphs (built on TensorFlow 1.3.0), we also imple-
mented neural networks on other frameworks, including Tens or-
Flow [ 1] (TF 1.3.0) without recursive graphs, which allows an
8
1 10 25
Batch size050100150Throughput (instances/s)
46.6
125.2
129.717.3
38.1
55.94.1
4.3
4.3(a) TreeRNN
1 10 25
Batch size0204060Throughput (instances/s)
23.4
39.2
44.88.1
26.8
40.81.5 1.5
1.5(b) RNTN
1 10 25
Batch size0246Throughput (instances/s)
4.8
4.2
3.62.5
4.0
5.52.0 2.0
2.0(c) TreeLSTMRecursive Iterative Unrolling
Figure 7: Training throughput for the TreeRNN, RNTN, and Tre eLSTM models with the Large Movie Review dataset. Numbers
are shown for our recursive implementation, TensorFlow’s i terative implementation, and PyTorch’s static unrolling i mplementation.
Our recursive implementation outperforms the other framew orks for all models and all batch sizes except when training T reeLSTM
with a batch size of 25, at which point the amount of system res ources is insufﬁcient to completely parallelize the comput ation. We
did not observe any signiﬁcant performance gain for the stat ic unrolling approach when the batch size was increased.
1 10 25
Batch size0200400600800Throughput (instances/s)
159.0
552.0
693.995.8
270.3
427.36.5
7.6
6.8(a) TreeRNN
1 10 25
Batch size0100200300400500Throughput (instances/s)
98.7
321.6
399.419.2
69.1
131.42.6
2.5
2.7(b) RNTN
1 10 25
Batch size0100200300Throughput (instances/s)
81.4
217.9
269.919.2
49.3
72.13.5
3.5
2.8(c) TreeLSTMRecursive Iterative Unrolling
Figure 8: Inference throughput for the TreeRNN, RNTN, and Tr eeLSTM models with the Large Movie Review dataset. Mea-
surements are presented for our recursive implementation, TensorFlow’s iterative implementation, and PyTorch’s sta tic unrolling
implementation. Our recursive implementation outperform s the other frameworks for all models and all batch sizes.
iterative way of programming, and PyTorch [ 20] (PTH 0.3.1),
which only supports the static unrolling technique. Since na-
tive TensorFlow does not support recursive deﬁnitions, we u sed
TensorFlow’s control ﬂow operators to train the neural netw orks
in an iterative fashion, as shown in Section 2. For PyTorch,
we dynamically create a new graph structure for each sentenc e.
Although implementing the static unrolling technique on Te n-
sorFlow is possible, the graph generation overhead tends to be
very large; instead, we use PyTorch for the unrolling techni que,
which incurs negligible graph construction overhead.
Hardware speciﬁcation. All numbers reported in this pa-
per were retrieved from experiment results on a single machi ne
of two 18-core Intel Xeon E5-2695 @ 2.10 GHz processors
with 256GB RAM, unless otherwise speciﬁed. We also used an
NVIDIA Titan X GPU for certain models. Unlike other common
neural networks such as convolutional models, the unstruct ured
input data of recursive neural networks makes it difﬁcult to ex-
ploit the full computational power of GPUs. Thus, we use GPUs
only if they introduce performance gain compared to CPU-onl yexecution. Our implementation and TensorFlow showed great er
performance on CPUs, while PyTorch performed better on a
GPU.
6.2 Throughput and Convergence Time
We start our analysis by measuring the training and inferenc e
throughputs with the recursive, iterative, and static unro lling im-
plementations.
Training throughput. Figure 7shows the throughput of
training the TreeRNN, RNTN, and TreeLSTM models using the
Large Movie Review dataset with recursive, iterative, and s tatic
unrolling implementations. The models were trained with ba tch
sizes of 1, 10, and 25.2
Thanks to the parallelism exploited by recursive dataﬂow
graphs, our implementation outperforms other implementat ions
for the TreeRNN and RNTN models at all batch sizes, by up to
2The original TreeRNN, RNTN, and TreeLSTM papers state that u sing a
batch size of 25 yielded the best model.
9
0 1774460
Time (s)5060708090100Accuracy (%)(a) Validation accuracy for TreeRNN
Recursive
Iterative
0 77832208
Time (s)5060708090100Accuracy (%)(b) Validation accuracy for RNTN
Recursive
Iterative
0 1870 1372
Time (s)5060708090100Accuracy (%)(c) Validation accuracy for TreeLSTM
Recursive
Iterative
Figure 9: Validation accuracy for the binary sentiment clas siﬁcation task with (a) TreeRNN, (b) RNTN, and (c) TreeLSTM m odels.
Results are shown for training each model with the recursive and iterative implementations, using the Large Movie Revie w dataset.
The time to reach 93% accuracy for each setup is also plotted, showing that the recursive implementation converges faste r for all
models.
3.3x improved throughput over the iterative implementatio n, and
30.2x improved throughput over the static unrolling approa ch.
Note that the performance gap between the recursive and iter a-
tive approach for the TreeRNN model is bigger than that of the
RNTN model. This is due to the fact that the TreeRNN model
involves much less computation in its recursive function bo dy
compared to the RNTN model, therefore having bigger room for
performance optimization via computation parallelizatio n. We
will further discuss the effectiveness of parallelization in Sec-
tion6.3.
For the TreeLSTM model, our implementation performs bet-
ter than other frameworks at batch sizes 1 and 10. On the other
hand, at a batch size of 25, our implementation is slower than
the iterative implementation. Generally, recursion has ad ditional
overheads compared to iteration, including passing around argu-
ments and return values, caller and callee context setup, et c. We
also have additional overheads related to backpropagation , as
discussed in previous sections. Consequently, our recursi ve im-
plementation exhibits excessively high resource utilizat ion for
computing large batches, making the throughput lower than t he
iterative computation.
Inference throughput. Inference refers to the process of
computing the operation output values of the feedforward ph ase,
stopping before backpropagation. Aside from training thro ugh-
put, inference throughput is also a useful metric for comput ing
the performance of a neural network, indicating how quickly a
deployed model can process unseen data, e.g., in serving sys -
tems.
Figure 8shows the inference throughput, with identical envi-
ronments with the previous experiments on training through put.
Our framework demonstrates throughput up to 5.4x higher tha n
the iterative implementation, and 147.9x higher than the st atic
unrolling approach. Unlike training throughput, our recur sive
implementation greatly dominates other implementations, since
our framework can fully utilize the given resources and the a ddi-
tional overheads introduced by backpropagation are not pre sent.
Convergence. We also measured how the accuracy of the
model increases as the training progresses, in Figure 9. Since our
implementation calculates numerically identical results as the it-1 2 4 8
Number of machines0102030Training throughput
(instances/s)
1.00x1.85x3.65x7.34x
Figure 10: Training throughput for the TreeLSTM model on our
recursive implementation, using varying numbers of machin es
for data parallelism. The performance increases almost lin early
as more machines are used for training.
erative implementation, the accuracy improvement per epoc h is
the same. However, thanks to our higher throughput, trainin g
with our framework results in faster convergence than the it era-
tive implementation.
Training throughput with multiple machines. One way to
overcome the resource limitations is scaling out to multipl e ma-
chines. Figure 10shows how the training throughput for the
TreeLSTM model on our recursive implementation changes, as
the number of machines used in training gradually grows from 1
to 8. Utilizing the well-known data parallelism technique [ 12],
the training throughput improves almost linearly up to 8 ma-
chines.
6.3 Analysis of Recursive Graphs:
Parallelization
The performance difference between the iterative and recur sive
implementation of the same recursive neural network mainly
comes from the parallelization of recursive operations. In this
subsection, we analyze how the performance varies dependin g
on various aspects related to parallelization.
10
0 100 200
Number of words050100150200250Training time (ms)Recursive
Iterative
0 100 200
Number of words0102030405060Inference Time (ms)Recursive
Iterative
Figure 11: Time taken for processing each data instance, in t he
TreeLSTM model using the Large Movie Review dataset. The
bold lines represent the average time for each speciﬁc sente nce
length in the whole dataset, and the enclosing colored areas rep-
resent the range of time taken to process the speciﬁc length o f
sentences. No batching is used for this experiment. As the nu m-
ber of words inside a data instance increases, our recursive im-
plementation outperforms the iterative implementation th anks to
the parallelized execution of tree cells. For inference, th e com-
putation load is low enough for the framework to utilize syst em
resources without hitting the resource limit, and the proce ssing
time of the recursive implementation is O(logN), where Nis the
number of words.
Sentence length. A close inspection of the training time per
data instance sorted by sentence length gives us interestin g re-
sults. As shown in Figure 11, the time required for processing a
single sentence generally increases as sentences become lo nger,
regardless of whether the implementation is based on native Ten-
sorFlow or our recursive implementation. This is an expecte d
phenomenon, because longer sentences form larger tree stru c-
tures consisting of more cells which require more computati on.
However, there is a clear difference in the increasing slope ;
the training time grows at a steeper slope for TensorFlow tha n
that of our implementation. This is because the recursive im -
plementation allows tree cells to be processed concurrentl y,
whereas the iterative TensorFlow implementation is only ca pa-
ble of processing one tree cell at a time. Theoretically, our im-
plementation is able to process a tree structure consisting ofN
cells in O(logN)time (native TensorFlow requires O(N)time),
though the parallelization effect is diminished by the fram ework
overhead and therefore the performance is more close to a lin ear
trend rather than a logarithmic trend. On inference workloa ds
with much less resource needs, the trend is clearly closer to a
logarithmic scale.
Balancedness of trees. To analyze the inﬂuence of tree bal-
ancedness on training throughput on our recursive implemen ta-
tion, we prepared several modiﬁed versions of the Large Movi e
Review dataset, that contain the same sentences as the origi nal
dataset but have different parse tree shapes. Speciﬁcally, we pre-
pared 1) a balanced dataset consisting of only complete binary
trees, 2) a moderate dataset that contains moderately balanced
binary trees, and 3) a linear dataset comprising only extremely
unbalanced binary trees.Batch sizeThroughput (instances/s)
Balanced Moderate Linear
1 46.7 27.3 7.6
10 125.2 78.2 22.7
25 129.7 83.1 45.4
Table 1: Throughput for the TreeRNN model implemented with
recursive dataﬂow graphs, using datasets of varying tree ba l-
ancedness. The balanced dataset exhibits highest throughp ut
thanks to the high degree of parallelization, but at the same time
does not improve as well as the linear dataset when the batch
size increases from 1 to 25, because there is only a small room
of performance improvement left, w.r.t parallelization.
Table 1shows the throughput of training the TreeRNN model
using these three datasets. For all batch sizes, the trainin g
throughput on the balanced dataset is the highest, while the
throughput on the linear dataset is the lowest. This trend oc -
curs because the maximum possible execution concurrency of a
tree is affected by the balancedness of the tree. A full binar y
tree of Ncells can be processed concurrently with at mostN+1
2
threads, because allN+1
2leaf nodes are mutually independent.
On the other hand, an extremely unbalanced binary tree can be
processed with only one or two threads at a time due to the lin-
earity of the tree. As a result, our implementation can train input
data of balanced trees with greater throughput than input da ta of
unbalanced trees.
Resource Utilization. Another interesting fact in Table 1is
that the training throughput on the linear dataset scales be tter
than the throughput on the balanced dataset, as the batch siz e
increases. For the balanced dataset, the recursive impleme nta-
tion efﬁciently utilizes many threads to process the data ev en at
a small batch size of 1, and thus increasing the batch size lea ds
to a relatively small speed boost. On the contrary, for the li near
dataset, the recursive implementation fails to efﬁciently make
use of CPU resources and thus the performance gain provided
by increasing the batch size is relatively high.
6.4 Comparison with Folding
The performance improvement of our recursive framework dis -
cussed in previous subsections comes from executing multip le
tree nodes in parallel. On the other hand, another approach f or
efﬁciently executing recursive neural networks exists: id entify-
ing concurrently executable nodes and batching them into a s in-
gle node to be run on GPUs. We refer to this technique as fold-
ing, following the name of a framework, TensorFlow Fold [ 15],
that implements this technique.
The folding technique hardly suffers from resource limita-
tions, as GPUs are very efﬁcient in batching computations.
However, batching multiple nodes leads to overheads that ar e not
present in other approaches. Due to the various tree structu res
in the input data, the batching decision must be done in a dept h-
wise manner, thus the ungrouping and regrouping of tree node s
across multiple depths lead to numerous memory reallocatio ns
11
Batch
sizeThroughput (instances/s)
Inference Training
Iter Recur Fold Iter Recur Fold
1 19.2 81.4 16.5 2.5 4.8 9.0
10 49.3 217.9 52.2 4.0 4.2 37.5
25 72.1 269.9 61.6 5.5 3.6 54.7
Table 2: Throughput for processing the TreeLSTM model on
our recursive framework, Fold’s folding technique, and Ten sor-
Flow’s iterative approach, with the Large Movie Review data set.
The recursive approach performs the best on inference with e f-
ﬁcient parallel execution of tree nodes, while the folding t ech-
nique shows better performance on training thanks to its GPU
exploitation.
and copies. Moreover, folding is applicable only if the tree struc-
ture of the input data is known before executing the computat ion;
for dynamically structured models the folding technique ca nnot
be implemented. Here, we discuss and compare our recursive
framework with the folding technique. Experiment results f or
folding were obtained using the TensorFlow Fold framework.
6.4.1 Statically Structured Models
Table 2compares the throughput of performing inference and
training on the TreeLSTM model using our implementation, th e
iterative approach, and the folding technique. The amount o f
resources is sufﬁcient for executing forward computations , and
therefore our framework outperforms the folding technique for
the inference task with up to 4.93x faster throughput. Unlik e
folding, the recursive approach does not have any overheads re-
garding batch regrouping, since the calculated values can b e di-
rectly passed between caller and callee SubGraph s.
However, when the resource usage is high, not every sched-
uled tree node in the worker ready queue can be executed con-
currently, even if the dependencies have been fully resolve d.
While the scalability of the recursive approach is limited b y this
drawback for the training task, the folding technique can ex ploit
the GPU and scales better. As a result, the folding technique per-
forms better than the recursive approach for the training ta sk. We
can improve the performance of the recursive approach by con -
ditionally deciding whether to batch the operations or not s imilar
to the folding technique, and we leave this as future work.
6.4.2 Dynamically Structured Models
While the models presented in the previous sections demand
support for dynamic control ﬂow, there is yet another collec tion
of models that boast an even greater degree of dynamism, in
which the model structure is gradually formed depending on i n-
termediate values calculated from prior steps. Top-down Tr eeL-
STM [ 33] (TD-TreeLSTM) is a dynamic model proposed for
sentence completion and dependency parsing. When a trained
model receives root node information as an input, the model
can generate child nodes based on the information and com-Batch sizeThroughput (instances/s)
Iterative Recursive Folding
1
640.30
0.345.59
9.30Not supported
Table 3: Throughput for evaluating the TD-TreeLSTM model
on our recursive framework and TensorFlow’s iterative impl e-
mentation, on batch sizes of 1 and 64.3Being able to execute
tree nodes in parallel lets our framework perform better tha n the
iterative approach. Fold’s folding technique is inapplica ble to
the TD-TreeLSTM model.
pletes the rest of the tree sentence. The decision of generat ing
a child node or stopping tree expansion is conditionally mad e
based on the computed value of the current node at runtime, so
the structure of the complete tree is not known before actual ly
executing the graph. DRNN [ 2] is a neural network model that
can generate tree-structured data from sequences, and ther efore
the tree structure in unknown before graph execution, simil ar to
TD-TreeLSTM. The Hierarchical Mixtures of Experts [ 11,24]
model has a similar structure, where the whole tree structur e is
decided at runtime. The network structure of HMRNN [ 5] is
also dynamically determined by the intermediate computati on
values.
Our framework performs well for such dynamic models. Ta-
ble3shows the throughput of the sentence completing task with
the TD-TreeLSTM model. Our implementation performs better
than the iterative approach by up to 18.6x, since multiple tr ee
nodes are executed in parallel. For this kind of model, tech-
niques that rely on heavy preprocessing of input data to batc h
operations (folding) are ineffective because the model str uctures
are unknown until the main computation. We note that it is im-
possible to express such models using the API provided by the
Fold framework.
7 Related Work
Embedded control ﬂow frameworks. DL frameworks with a
computation graph comprised of control ﬂow operators along
with the mathematical operators to represent a DL job are cal led
embedded control ﬂow frameworks [ 1,30,6,4]. This class of
frameworks does not use the programming language’s control
ﬂow support (e.g., Python’s ifclause) for representing dynamic
control ﬂow. Instead, they provide certain primitives for e mbed-
ding dynamic control ﬂow in the dataﬂow graph; the framework
cores evaluate a boolean expression and decide what to apply for
the next operation at graph execution time.
Although our implementation is based on the embedded con-
trol ﬂow framework TensorFlow [ 1], the key difference is the
ability to express recursive functions. In our implementat ion, a
user can deﬁne an arbitrary function and use it as an operatio n to
compose a graph. The arbitrary function can call another fun c-
tion including itself without restriction, allowing recur sive deﬁ-
3We follow the suggestions of the original TD-TreeLSTM paper to use a
batch size of 64.
12
nitions of functions. TensorFlow and Theano [ 30] also let users
write user-deﬁned functions, but do not support recursion; the
user must not create a cycle of dependencies between functio ns.
Non-embedded control ﬂow frameworks. Unlike embed-
ded control ﬂow frameworks, PyTorch [ 20], DyNet [ 19], and
Chainer [ 31] do not embed control ﬂow operators inside their
computation graphs. Rather, the computation occurs along w ith
the dynamic control ﬂow of the host language, removing the
need to embed the control ﬂow operators inside the computati on
graph. In other words, these non-embedded control ﬂow frame-
works behave just like numerical computation libraries suc h as
NumPy [ 32] and MKL [ 10], so one can directly exploit the un-
derlying language’s abilities for handling conditional br anches,
loops, and recursive functions. Thanks to this behavior, a u ser
can easily build a prototype of a new neural network architec ture
or optimization algorithm.
However, since neural networks are usually trained for nume r-
ous steps until they converge, non-embedded control ﬂow fra me-
works suffer from repetitive construction of graphs compos ed
of hundreds or thousands of nodes, resulting in substantial ob-
ject creation and removal overhead. More importantly, embe d-
ded control ﬂow frameworks employ graph compilation tech-
niques like operation fusion or in-place operation convers ion to
optimize execution, but non-embedded control ﬂow framewor ks
cannot since they do not reuse the graphs.
Recursive dataﬂow graphs are designed to provide a similar
level of programmability to non-embedded control ﬂow frame -
works, without losing optimization opportunities by using an
embedded control ﬂow framework (TensorFlow) to declare com -
putations with recursion.
Other frameworks with recursion support. TensorFlow
Fold [ 15], a library for handling models with dynamic computa-
tion, allows recursion for writing computation graphs. Fol d pro-
vides a number of new APIs for creating and managing blocks
(sets of low-level operations). A block behaves as a schedul ing
unit to enable dynamic batching of different computation gr aphs.
Using these blocks, Fold constructs an execution loop that r e-
sembles recursion and starts running the loop from base case s,
wiring intermediate results to the appropriate positions f or sub-
sequent recursive cases. From the perspective of programma bil-
ity, Fold provides a whole new set of functional programming
style APIs to preprocess input data and build the computatio n
graph. It is required to mix the control ﬂow API of Fold and the
computational API of TensorFlow to represent a complete DL
job, which is not a trivial task. Also, since the structure an d exe-
cution order of the computation graph becomes completely di f-
ferent after graph preprocessing, it becomes impossible to pin-
point the location of errors on failures, resulting in poor d ebug-
gability.
On the other hand, our framework adds a simple abstrac-
tion,SubGraph , to the programming model to support recur-
sion.SubGraph s can be used with existing operations anal-
ogously and does not import any additional execution detail s
other than those already provided by the underlying embed-
ded control ﬂow framework. Moreover, the ﬁnal computation
graph ofInvokeOp s retains the original position information ofSubGraph s, allowing the same debugging experience as the un-
derlying framework.
CIEL [ 17] is a dynamic task (operator) creation framework
that allows users to declare data processing jobs recursive ly. The
operators of CIEL are relatively more coarse-grained compa red
to DL frameworks, which means the number of recursion calls
is not large. The different granularity comes from the chara cter-
istics of the target domain; CIEL targets batch processing a ppli-
cations, whereas recursively deﬁned graphs were designed f or
deep learning. More fundamentally, CIEL cannot be integrat ed
with modern DL frameworks because CIEL does not consider
DL-speciﬁc mechanisms such as backpropagation or typed op-
erator deﬁnitions, which are highly important for DL applic a-
tions.
8 Conclusion
In this paper, we have introduced recursive declarations an d re-
cursive execution mechanisms for running recursive neural net-
works on top of existing embedded control ﬂow frameworks.
With recursively deﬁned computation graphs, recursive neu ral
networks can be implemented in a fashion that better portray s
the recursion aspect, and be executed efﬁciently by letting the
framework exploit parallel execution of computations, bot h of
which were very difﬁcult to achieve on existing frameworks d ue
to the lack of support for recursion. To achieve this goal, we
designed and implemented a programming model and a run-
time execution model, including automatic differentiatio n sup-
port for deep learning jobs. We have demonstrated the expres -
sive power and performance of recursive graphs by implement -
ing various recursive neural network models using our progr am-
ming model and comparing them with iterative and unrolling i m-
plementations, showing that recursive graphs outperform o ther
approaches signiﬁcantly.
Acknowledgement
This research was supported by the MSIT (Ministry of Science
and ICT), Korea, under the SW Starlab support program (IITP-
2018-R0126-18-1093) supervised by the IITP (Institute for In-
formation & communications Technology Promotion), and by
the ICT R&D program of MSIT/IITP (No.2017-0-01772, De-
velopment of QA systems for Video Story Understanding to pas s
the Video Turing Test).
References
[1] M. Abadi et al. TensorFlow: A system for large-scale ma-
chine learning. In OSDI , 2016.
[2] D. Alvarez-Melis and T. S. Jaakkola. Tree-structured de -
coding with doubly-recurrent neural networks. In ICLR ,
2017.
[3] S. R. Bowman, J. Gauthier, A. Rastogi, R. Gupta, C. D.
Manning, and C. Potts. A fast uniﬁed model for parsing
and sentence understanding. In ACL, 2016.
13
[4] T. Chen, M. Li, Y . Li, M. Lin, N. Wang, M. Wang, T. Xiao,
B. Xu, C. Zhang, and Z. Zhang. MXNet: A ﬂexible and
efﬁcient machine learning library for heterogeneous dis-
tributed systems. In Workshop on Machine Learning Sys-
tems in NIPS , 2015.
[5] J. Chung, S. Ahn, and Y . Bengio. Hierarchical multiscale
recurrent neural networks. In ICLR , 2017.
[6] Facebook. Caffe2, 2017. https://caffe2.ai .
[7] A. Filinski. Recursion from Iteration, 1994. Lisp and Sym-
bolic Computation , 7, 1, 11-37.
[8] D. P. Friedman and M. Wand. Essentials of Programming
Languages . MIT Press, 2008.
[9] S. Hochreiter and J. Schmidhuber. Long Short-Term Mem-
ory, 1997. Neural Computation , 9, 8, 1735–1780.
[10] Intel Corporation. Intel Math Kernel Library Reference
Manual . 2009.
[11] M. I. Jordan and R. A. Jacobs. Hierarchical Mixtures of
Experts and the EM Algorithm, 1994. Neural Computa-
tion, 6, 2, 181–214.
[12] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed,
V . Josifovski, J. Long, E. J. Shekita, and B.-Y . Su. Scaling
distributed machine learning with the parameter server. In
OSDI , 2014.
[13] L. Lin, G. Wang, R. Zhang, R. Zhang, X. Liang, and
W. Zuo. Deep structured scene parsing by learning with
image descriptions. In CVPR , 2016.
[14] Y . A. Liu and S. D. Stoller. From recursion to iteration:
What are the optimizations? In PEPM , 2000.
[15] M. Looks, M. Herreshoff, D. Hutchins, and P. Norvig.
Deep learning with dynamic computation graphs. In ICLR ,
2017.
[16] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y . Ng,
and C. Potts. Learning word vectors for sentiment analysis.
InACL, 2011.
[17] D. G. Murray, M. Schwarzkopf, C. Smowton, S. Smith,
A. Madhavapeddy, and S. Hand. CIEL: A universal execu-
tion engine for distributed data-ﬂow computing. In NSDI ,
2011.
[18] MXNet. RNN Cell API, 2018. https://mxnet.
incubator.apache.org/api/python/rnn.html .
[19] G. Neubig et al. DyNet: The Dynamic Neural Network
Toolkit, 2017. arxiv preprint arXiv:1701.03980 .
[20] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang,
Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer.
Automatic differentiation in pytorch. In Autodiff Workshop
in NIPS , 2017.[21] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn-
ing Representations by Back-propagating Errors, 1986.
Nature , 323, 6088, 533–536.
[22] A. Sharma, O. Tuzel, and D. W. Jacobs. Deep hierarchical
parsing for semantic segmentation. In CVPR , 2015.
[23] A. Sharma, O. Tuzel, and M.-Y . Liu. Recursive context
propagation network for semantic scene labeling. In NIPS ,
2014.
[24] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le,
G. Hinton, and J. Dean. Outrageously large neural net-
works: The sparsely-gated mixture-of-experts layer. In
ICLR , 2017.
[25] R. Socher, C. C.-Y . Lin, A. Y . Ng, and C. D. Manning.
Parsing natural scenes and natural language with recursive
neural networks. In ICML , 2011.
[26] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning,
A. Ng, and C. Potts. Recursive deep models for semantic
compositionality over a sentiment treebank. In EMNLP ,
2013.
[27] K. S. Tai, R. Socher, and C. D. Manning. Improved
semantic representations from tree-structured long short -
term memory networks. In ACL, 2015.
[28] TensorFlow. Recurrent Neural Networks, 2018.
https://www.tensorflow.org/versions/master/
tutorials/recurrent .
[29] TensorFlow Whitepaper. Implementation of Control Flow
in TensorFlow . 2017.
[30] Theano Development Team. Theano: A Python Frame-
work for Fast Computation of Mathematical Expressions,
2016. arXiv preprint arXiv:1605.02688 .
[31] S. Tokui, K. Oono, S. Hido, and J. Clayton. Chainer: a
next-generation open source framework for deep learning.
InWorkshop on Machine Learning Systems in NIPS , 2015.
[32] S. V . D. Walt, S. C. Colbert, and G. Varoquaux. The
NumPy Array: a Structure for Efﬁcient Numerical Com-
putation, 2011. Computing in Science & Engineering , 13,
2, 22-30.
[33] X. Zhang, L. Lu, and M. Lapata. Top-down tree long short-
term memory networks. In NAACL , 2016.
14

This is the authors’ final version. The authoritative version will appear in the proceedings of ICCAD 2018.
Hardware-Aware Machine Learning: Modeling and Optimization
(Invited paper )
Diana Marculescu, Dimitrios Stamoulis, Ermao Cai
Department of ECE, Carnegie Mellon University, Pittsburgh, PA
Email: dianam@cmu.edu, dstamoul@andrew.cmu.edu, ermao@cmu.edu
ABSTRACT
Recent breakthroughs in Machine Learning (ML) applications, and
especially in Deep Learning (DL), have made DL models a key
component in almost every modern computing system. The in-
creased popularity of DL applications deployed on a wide-spectrum
of platforms (from mobile devices to datacenters) have resulted in a
plethora of design challenges related to the constraints introduced
by the hardware itself. “What is the latency or energy cost for an
inference made by a Deep Neural Network (DNN)?” “Is it possible
to predict this latency or energy consumption before a model is
even trained?” “If yes, how can machine learners take advantage
of these models to design the hardware-optimal DNN for deploy-
ment?” From lengthening battery life of mobile devices to reducing
the runtime requirements of DL models executing in the cloud, the
answers to these questions have drawn significant attention.
One cannot optimize what isn’t properly modeled. Therefore, it
is important to understand the hardware efficiency of DL models
during serving for making an inference, before even training the
model. This key observation has motivated the use of predictive
models to capture the hardware performance or energy efficiency
of ML applications. Furthermore, ML practitioners are currently
challenged with the task of designing the DNN model, i.e., of tuning
the hyper-parameters of the DNN architecture, while optimizing
forboth accuracy of the DL model and its hardware efficiency.
Therefore, state-of-the-art methodologies have proposed hardware-
aware hyper-parameter optimization techniques. In this paper, we
provide a comprehensive assessment of state-of-the-art work and
selected results on the hardware-aware modeling and optimization
for ML applications. We also highlight several open questions that
are poised to give rise to novel hardware-aware designs in the
next few years, as DL applications continue to significantly impact
associated hardware systems and platforms.
1 INTRODUCTION
Recent advances in Deep Learning (DL) have enabled state-of-the-
art results in numerous areas, such as text processing and computer
vision. These breakthroughs in Deep Neural Networks (DNN) have
been fueled by newly discovered ML model configurations [ 2] and
advances in hardware platforms. However, the demand for better
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ICCAD ’18, November 5–8, 2018, San Diego, CA, USA
©2018 Association for Computing Machinery.
ACM ISBN 978-1-4503-5950-4/18/11. . . $15.00
https://doi.org/10.1145/3240765.3243479performance in real-world deployment results in increased com-
plexity for both the hardware systems and the DL models. As mod-
ern DNNs grow deeper and more complex, hardware constraints
emerge as a key limiting factor. This “hardware wall” manifests its
unintended, negative effects in several ways.
First, the energy requirements of DNNs have emerged as a key
impediment preventing their deployment on energy-constrained
embedded and mobile devices, such as Internet-of-Things (IoT)
nodes and wearables. For instance, image classification with AlexNet
[26] can drain the smartphone battery within an hour [ 38]. This
design challenge has resulted in a plethora of recent methodolo-
gies that focus on developing energy-efficient image classification
solutions [ 31]. However, identifying the most energy-efficient im-
plementation on a given platform or the right DNN model-hardware
platform pair can be challenging. Recent work shows that, while
several DNN architectures can achieve a similar accuracy level [ 3],
the energy consumption differs drastically among these various
iso-accuracy DNN configurations, many times by as much as 40 ×.
Second, there is an ever-increasing number of mobile applica-
tions that use on-device Machine Learning (ML) models employed
directly into the smartphone, rather than relying on the cloud to pro-
vide the service. Thus, recent work on the design of DNNs shows an
ever-increasing interest in developing platform- and device-aware
DL models [ 11,37]. For instance, state-of-the-art hardware-aware
methodologies from Google Brain consider DL optimization and
design techniques that optimize for both the accuracy of DNNs for
image classification and the runtime of the model on Google Pixel
smartphones [37].
Finally, edge-cloud communication constraints are an important
design consideration for learning on the edge, e.g., for commer-
cially important indoor localization applications based on low-cost
sensing data ( e.g., Radio-frequency identification RFID tags and
WiFi signals) [ 28]. Hence, towards enabling hardware-aware ML
applications, the aforementioned “hardware wall” gives rise to two
main challenges:
Challenge 1: Characterizing hardware performance of DL
models : The efficiency of DL models is determined by their hard-
ware performance with respect to metrics such as runtime or en-
ergy consumption, not only by their accuracy for a given learning
task [ 37]. To this end, recent work explores modeling methodologies
that aim to accurately model and predict the hardware efficiency
of DNNs. In this paper, we review state-of-the-art modeling tools,
such as the ones employed in Eyeriss [ 5,38], or the Paleo [ 27] and
NeuralPower [3] frameworks.
Challenge 2: Designing DL models under Hardware Con-
straints : The hyper-parameter optimization of DL models, i.e.,
the design of DL models via the tuning of hyper-parameters such
as the number of layers or the number of filters per layer, hasarXiv:1809.05476v1  [cs.LG]  14 Sep 2018
emerged as an increasingly expensive process, dubbed by many
researchers to be more of an art than a science . Hyper-parameter
optimization is a challenging design problem due to several rea-
sons. First, in commercially important Big Data applications ( e.g.,
speech recognition), the construction of DL models involves many
tunable hyper-parameters [ 32] and the training of each configura-
tion takes days, if not weeks, to fully train. More importantly, the
ability of a human expert to identify the best performing DL model
could be hampered significantly if we are to consider hardware
constraints and design considerations imposed by the underlying
hardware [ 34]. To this end, there is an ever-increasing interest in
works that co-optimize for both the hardware efficiency and the
accuracy of the DL model. In this paper, we investigate the main
tools for employing hardware-aware hyper-parameter optimiza-
tion, such as methodologies based on hardware-aware Bayesian
optimization [ 34,35], multi-level co-optimization [ 30] and Neural
Architecture Search (NAS) [11, 37].
2 RELATED WORK
Reducing the complexity of the ML models has long been a concern
for machine learning practitioners. Hence, while this paper focuses
on hardware-aware modeling and optimization methodologies,
there are orthogonal techniques that have been previously explored
as means to reduce the complexity of DL models. In this section, we
briefly review these approaches as they relate to hardware-aware
modeling and optimization, and we highlight their limitations.
Pruning : Prior art has investigated pruning-based methods that
aim at reducing the ML model complexity by gradually removing
weights of the model and by retraining it [ 8,19,38]. The key insight
behind this type of work, e.g., deep compression [ 18], is the fact that
modern DL models exhibit redundancy to their features and a small
degradation of accuracy can be traded off for a significant reduction
in the computational cost. This insight has resulted in a significant
body of work that incorporates regularizing terms directly in the
objective function used for the training of DL model training. These
loss regularizers, e.g., L1 norm terms, “zero out” several connections,
thus reducing the overall model complexity directly at training.
For instance, in [ 15], the authors propose MorhpNet, a resource-
constrained regularization that outperforms prior methodologies in
terms of accuracy given a maximum FLOP constraint. Nonetheless,
these methods use the number of floating points operations or the
number of model parameters as a proxy for the model complexity,
without explicitly accounting for hardware-based metrics such as
energy or power consumption.
Quantization : Beyond pruning-based methodologies that re-
duce the DL model complexity, other works reduce directly their
computational complexity via quantization [ 7]. The key insight is
that, while the number of FLOPs remains the same, the hardware
execution cost is reduced by reducing the bit-width per multiply-
accumulate (MAC) operation. In turn, the reduced cost per opera-
tion results in overall hardware efficiency, compounded by lower
storage requirements. Several methodologies have explored this
trade-off by considering different bit-width values [ 9,10,17,24].
Nevertheless, the effectiveness of quantization methods relies heav-
ily on the performance of the pre-quantized (seed) network [ 15],
without providing any insight on how the sizing of different DLhyper-parameters affects the overall hardware efficiency and accu-
racy of the DL model.
Discussion : While pruning- and quantization-based methods
have been shown to significantly reduce the computational com-
plexity of ML models, recent work has highlighted some key limi-
tations. First, these methodologies focus on reducing the storage
required by model weights or the number of FLOPs. However, Yang
et al. show that FLOP reduction does not yield the optimal DNN
design with respect to power or energy consumption [ 38] largely
because the parameter-heavy layers of a DL model are not necessar-
ily the most energy-consuming. Hence, pruning and quantization
methodologies rely on formulations that are hardware-unaware
and they do not necessarily result in Pareto-optimal configura-
tions in terms of hardware efficiency, even though the targeted
FLOP constraint is satisfied. In contrast, hardware-aware modeling
methodologies can pave the way to directly account for energy
consumption or runtime, as discussed in the next section.
Second, pruning methodologies traverse the design space only
locally around the original pre-trained DL model used as a seed.
Consequently, if the configuration of the DL model is viewed as a
hyper-parameter optimization problem to be globally solved, prun-
ing methodologies can only reach locally optimal solutions, whose
effectiveness is bound by the quality of the seed design. Neverthe-
less, Tan et al. show that that hardware-constrained models such as
MobileNets [ 23] are not Pareto-optimal with respect to accuracy for
a given level of hardware complexity. Instead, state-of-the-art re-
sults in platform-aware hyper-parameter optimization [ 37] yield DL
design closer to the Pareto front by globally solving the hardware-
aware optimization problem. In this paper, we therefore focus on
hardware-aware optimization techniques based on Bayesian opti-
mization and Neural Architecture Search (NAS).
3 HARDWARE-AWARE RUNTIME AND
ENERGY MODELING FOR DL
3.1 DL model-hardware platform interplay
As mentioned before, effective optimization requires efficient and
reliable models. It is not surprising, therefore, that the first efforts
in hardware models for DL have been developed in the context
of hardware efficient DNNs. In [ 38], Yang et al. have shown that
energy-based pruning achieves better energy-efficiency compared
to its FLOPs-based counterpart. During each iteration of pruning-
finetuning, Yang et al. use an energy consumption model to decide
on which DNN layers to prune next. To this end, the authors de-
velop a predictive model based on measurements on their hardware
accelerator, namely Eyeriss [ 5]. Their formulation models energy
consumption as a weighted function of the number of accessed to
the different levels of the memory hierarchy, wherein each level
is profiled based on the energy consumption per operation or per
memory access pattern ( e.g., overhead to access the register file, the
global buffer, the main memory, etc.).
Specifically, they calculate the energy consumption for each
layer in two parts, computation energy consumption ( Ecomp ), and
data movement energy consumption ( Edata ).Ecomp is calculated
by multiplying the number of MACs in the layer by the energy
consumption for running each MAC operations in the computation
core. Edata is the total summation of energy consumption of all the
Machine learnersTarget software/ hardware platformLayer-level modelsNetwork-level modelsCNN architectureBuildDetailed power, runtime & energy, with breakdownsAnalysis & guidanceFigure 1: Overview of NeuralPower [3].
memory access for each level of energy. In addition, they account
for the impact of data sparsity and bitwidth reduction on energy
consumption. This is based on their assumption that a MAC and its
memory access can be skipped completely when the input activa-
tion or weight is zero. The impact of bitwidth is also calculated by
scaling the energy consumption of corresponding hardware units.
This work provides a good model to evaluate the energy cost
for running DNNs on the ASICs. However, transferring the model
or methodology to other platforms is not trivial as it may require
additional model selection and training.
In a more recent development, the Paleo framework proposed
by Qi et al. [27] proposes a methodology for modeling of hardware
metrics, i.e., runtime of DNNs, without resorting to FLOPs as a
proxy for hardware efficiency. As opposed to Eyeriss, Paleo has
been developed with general purpose platforms (GPUs) in mind.
In their approach, the authors present an analytical method to
determine the runtime of DNNs executing on various platforms.
The framework has been shown to flexibly scale across different
GPU platforms and DNN types.
Specifically, Paleo divides the runtime for each layer into two
parts, computation time and communication time. For a layer u,
and the operation fon a device d, the total execution time T(u)
can be expressed as:
T(u)=R(Pa(u))+C(f,d)+W(f,d) (1)
whereR(Pa(u))is the time to fetch the input produced by its par-
ent layers,W(f,d)is the time to write the outputs to the local
memory, andC(f,d)is the total computation time with f. Assum-
ing that we are interested in metrics for a device dunder aver-
age speed conditions, the computation time can be determined by:
C(f,d)=FLOPs(f)/speed(d). Following a similar assumption on
IO bandwidth,RandWcan be calculated as the amount of the
reading/writing data divided by the average IO bandwidth. In the
case of multiple workers, the model still holds by replacing the av-
erage IO bandwidth with the communication bandwidth between
two devices.
Since the peak FLOPs and peak bandwidth provided by the man-
ufacturers are usually different than the actual speed of the devices
running specific DNNs, the authors introduce the concept of plat-
form percent of peak (PPP) to capture the average relative ineffi-
ciency of the platform compared to peak performance. However,
for different devices, the average computation and communication
speeds can vary from one to another. Therefore, one needs to evalu-
ate those metrics with many different tests to obtain reliable values.
In addition, Paleo does not consider models for predicting power
or energy consumption.3.2 The NeuralPower framework
To provide adaptive and reliable prediction of runtime, power, and
energy for DNNs simultaneously, Cai et al. [3] have introduced
NeuralPower, a layer-wise predictive framework based on sparse
polynomial regression for determining the serving energy consump-
tion of convolutional neural networks (CNNs) deployed on GPU
platforms. Given the architecture of a CNN, NeuralPower provides
an accurate prediction and breakdown for power and runtime across
all layers in the whole network. In their framework, the authors
also provide a network-level model for the energy consumption of
state-of-the-art CNNs.
Specifically, NeuralPower introduces a flexible and comprehen-
sive way to build the runtime, power, and energy models for various
DNNs on a variety of platforms. The modeling can be divided into
two parts, as shown in Figure 1.
The layer-level approach provides accurate models for running
a specific layer on each of the considered platform. Instead of using
proxies to determine how fast a layer runs as in Paleo, NeuralPower
learns the models by actually running the DL models on the target
platform. For example, the runtime ˆTof a layer can be expressed
as:
ˆT(xT)=Õ
jcj·DTÖ
i=1xqij
i+Õ
sc′
sFs(xT) (2)
where xT∈RDT;qij∈N;∀j,DTÕ
i=1qij≤KT.
The model in Equation 2 consists of two parts. The first part is
the regular degree- KTpolynomial terms which are a function of
the features in the input vector xT∈RDT.xiis the i-th component
ofxT.qijis the exponent for xiin the j-th polynomial term, and
cjis the coefficient to learn. This feature vector of dimension DT
includes layer configuration hyper-parameters, such as the batch
size, the input size, and the output size. For different types of layers,
the dimension DTis expected to vary. For convolutional layers,
for example, the input vector includes the kernel shape, the stride
size, and the padding size, whereas such features are not relevant
to the formulation/configuration of a fully-connected layer. The
second part is comprised of special polynomial terms F, which
represent physical operations related to each layer ( e.g., the total
number of memory accesses and the total FLOPs). The number of
the special terms differs from one layer type to another. Finally, c′s
is the coefficient of the s-th special term to learn.
Similarly, a power model can be constructed and the total energy
consumption can be calculated by the product of power consump-
tion and runtime. The models are trained on the real data obtained
from real GPU platforms running state-of-the-art DNNs.
One can compare NeuralPower against Paleo with respect to
runtime prediction only, as the latter does not address energy or
power modeling. Table 1 shows the modeling results for each layer.
As it can be seen, NeuralPower outperforms Paleo in terms of model
accuracy for the most widely used types of layers in CNNs.
Table 1: Comparison of runtime models for common CNN
layers between NeuralPower [3] and Paleo [27].
LayerNeuralPower [3] Paleo [27]
Model size RMSPE RMSE (ms) RMSPE RMSE (ms)
CONV 60 39.97% 1.019 58.29% 4.304
FC 17 41.92% 0.7474 73.76% 0.8265
Pool 31 11.41% 0.0686 79.91% 1.763
For the network level model, NeuralPower uses the summation
of the layer level results to achieve total runtime and total energy.
The average power can be calculated by dividing total energy by
total runtime. Table 2 shows the runtime predictions at network
level for NeuralPower and Paleo. From Table 1 and Table 2, we
can see NeuralPower generally achieves better accuracy in both
layer-level and network-level runtime prediction.
Table 2: Comparison of runtime models for common CNNs
between NeuralPower [3] and Paleo [27].
CNN Paleo[27] NeuralPower[3] Actual runtime
VGG-16 345.83 373.82 368.42
AlexNet 33.16 43.41 39.02
NIN 45.68 62.62 50.66
Overfeat 114.71 195.21 197.99
CIFAR10-6conv 28.75 51.13 50.09
Lastly and most importantly, NeuralPower is currently the state-
of-the-art method in terms of prediction error when capturing the
runtime, power, and energy consumption for various CNNs, with
mean squared error less than 5%. Moreover, NeuralPower achieves
a70%accuracy improvement compared the previously best model,
i.e., Paleo. Using learning-based polynomial regression models, Neu-
ralPower can be flexibly employed for prediction across different
DL software tools ( e.g., Caffe [ 25] or TensorFlow [ 1]) and GPU plat-
forms ( i.e., both low-power Nvidia Tegra boards and workstation
Nvidia Titan X GPUs).
4 HARDWARE-AWARE OPTIMIZATION FOR
DL
4.1 Model-based optimization
Background : Even without hardware-related design objectives,
the hyper-parameter optimization of DL models, i.e., deciding on
tunable hyper-parameters such as the number of features per layer
or the number of layers, is a challenging design task. Early attempts
towards efficiently solving this problem use sequential model-based
optimization (SMBO) when the objective function ( i.e., test error
of each candidate DNN configuration) has no simple closed form
and its evaluations are costly. SMBO methodologies use a surro-
gate (cheaper to evaluate) probabilistic model to approximate the
objective function.
Figure 2: Model-based optimization (denoted as BayesOpt)
outperforms alternative methods such as random search
and genetic algorithms in terms of finding more accurate,
energy-efficient designs closer to the Pareto front [29].
Different formulations have been used for SMBO probabilistic
models, such as Gaussian Processes (GP) [ 32] or tree-structured
Parzen estimators (TPE) [ 2]. Regardless of the choice of the SMBO
formulation, intuitively the probabilistic model encapsulates the
belief about the shape of functions that are more likely to fit the
data observed so far, providing us with a cheap approximation for
the mean and the uncertainty of the objective function.
Each SBMO algorithm has three main steps: (i) maximization of
acquisition function : to select the point ( i.e., next candidate DNN
configuration) at which the objective will be evaluated next, SMBO
methods use the so-called acquisition function. Intuitively, the ac-
quisition function provides the direction in the design space toward
which there is an expectation of improvement of the objective. By
evaluating cheaply the acquisition function at different candidate
points ( i.e., different DNN configurations), SMBO selects the point
with maximum acquisition function value. (ii) evaluation of the ob-
jective : the currently selected DNN design is created and trained to
completion to acquire the validation error. (iii) probabilistic model
update : based on the value of the objective at the currently consid-
ered DNN design, the probabilistic model is updated.
Hardware-aware SMBO : If Gaussian processes are used to for-
mulate the probabilistic model, the resulting special case SMBO con-
stitutes a powerful approach, namely Bayesian optimization [ 32].
Prior art has proposed general frameworks for employing Bayesian
optimization with multiple objective and constraint terms [ 21]. To
this end, a natural extension is to use these formulations to solve
the hyper-parameter optimization of DNNs under hardware con-
straints.
Hernández-Lobato et al. has successfully used Bayesian optimiza-
tion to design DNNs under runtime constraints [ 21]. More inter-
estingly, the authors have investigated the co-design of hardware
accelerators and DNNs in [ 22,29], where the energy consumption
and the accuracy of a DNN correspond to objective terms that de-
pend on both DNN design choices ( e.g., number of features per layer
or number of layers) and hardware-related architectural decisions
(e.g., bit width and memory size).
Figure 3: Overview of HyperPower flow and illustration of the Bayesian optimization procedure during each iteration [34].
In Figure 2, we can observe that the SMBO-based method (de-
noted as BayesOpt) outperforms alternative methods such as ran-
dom search and genetic algorithms in terms of finding more accu-
rate, energy-efficient designs closer to the Pareto front. This is to
be expected, since the model-based approach provides a hardware-
aware model to guide the design space exploration faster towards
Pareto-optimal designs.
4.2 Multi-layer co-optimization
From a different perspective, Minerva [ 30] presents an automated
co-optimization method including multiple layers in the system
stack: algorithm, architecture and circuit levels. In that approach,
five stages are used: 1. Training space exploration; 2. Microarchitec-
ture design space; 3. Data type quantization; 4. Selective operation
pruning; 5. SRAM fault mitigation. Based on these five steps, the
DNNs can be trained, pruned and deployed to Minerva with low
power consumption, without sacrificing their model accuracy.
In general, the savings from optimization at the software, archi-
tecture, and circuit level, can be combined together. According to
tests on five different datasets, including MNIST, Forest, Reuters,
WebKB, and 20NG, Minerva achieves an average power saving of
8.1×compared with a baseline accelerator. The authors conclude
that fine-grain, heterogeneous datatype optimization, aggressive
prediction and pruning of small activity values, and active hardware
fault detection coupled with domain-aware error mitigation are the
main contributors for the savings. Given its low footprint, Minerva
can be applied to IoT and mobile devices, but it doesn’t rely on a
comprehensive optimization framework that can be extended to
generic platforms. We present next one such approach.
4.3 HyperPower framework
A special case of Bayesian optimization is one where constraints
can be expressed and known a priori ; these formulations enable
models that can directly capture candidate configurations as valid
or invalid [ 14]. In HyperPower [ 34], Stamoulis et al. investigate
the key insight that predictive models (as discussed in the previous
section) can provide an a priori knowledge on whether the energy
consumption of any DNN configuration violates the energy budget
or not. In other words, predictive models can be used in the context
of Bayesian optimization and can be formulated as a priori known
constraints.
More specifically, HyperPower introduces a hardware-aware
acquisition function that returns zero for all constraint-violatingcandidate designs, hence effectively guiding the design space ex-
ploration toward hardware constraint-satisfying configurations
with optimal DNN accuracy. By accounting for hardware con-
straints directly in the SMBO formulation, HyperPower reaches the
near-optimal region 3.5×faster compared to hardware-unaware
Bayesian optimization. The proposed acquisition function uses pre-
dictive models for power consumption and memory utilization, thus
showing the interplay of both modeling and optimization towards
enabling hardware-aware DL.
The general structure of HyperPower is shown in Figure 3.
Bayesian optimization is a sequential model-based approach that ap-
proximates the objective function with a surrogate (cheaper to eval-
uate) probabilistic model M, based on Gaussian processes (GP). The
GP model is a probability distribution over the possible functions of
f(x), and it approximates the objective at each iteration n+1based
on data X:=xi∈Xn
i=1queried so far. HyperPower assumes that
the values f:=f1:nof the objective function at points Xare jointly
Gaussian with mean mand covariance K,i.e.,f|X∼N( m,K).
Since the observations fare noisy with additive noise ϵ∼N( 0,σ2),
the GP model can be written as y|f,σ2∼N( f,σ2I). At each point
x, GP provides a cheap approximation for the mean and the un-
certainty of the objective, written as pM(y|x)and illustrated in
Figure 3 with the black curve and the grey shaded areas.
Each iteration n+1of a Bayesian optimization algorithm consists
of three key steps:
Maximization of acquisition function : one first needs to se-
lect the point xn+1(i.e., next candidate NN configuration) at which
the objective ( i.e., the test error of the candidate NN) will be eval-
uated next. This task of guiding the search relies on the so-called
acquisition function α(x). A popular choice for the acquisition func-
tion is the Expectation Improvement (EI) criterion, which computes
the probability that the objective function fwill exceed (negatively)
some threshold y+,i.e.,EI(x)=∫∞
−∞max{y+−y,0}·pM(y|x)dy.
Intuitively, α(x)provides a measure of the direction toward which
there is an expectation of improvement of the objective function.
The acquisition function is evaluated at different candidate points
x, yielding high values at points where the GP’s uncertainty is high
(i.e., favoring exploration), and where the GP predicts a high objec-
tive ( i.e., favoring exploitation) [ 32]; this is qualitatively illustrated
in Figure 3 (blue curve). HyperPower selects the maximizer of α(x)
as the point xn+1to evaluate next (green triangle in Figure 3). To en-
able power- and memory-aware Bayesian optimization, HyperPower
incorporates hardware-awareness directly into the acquisition func-
tion.
Evaluation of the objective : Once the current candidate NN
design xn+1has been selected, the NN is generated and trained
to completion to acquire the test error. This is the most expensive
step. Hence, to enable efficient Bayesian optimization, HyperPower
focuses on detecting when this step can be bypassed.
Probabilistic model update : As the new objective value yn+1
becomes available at the end of iteration n+1, the probabilistic
model pM(y)is refined via Bayesian posterior updating (the pos-
terior mean mn+1(x)and covariance covariance Kn+1can be ana-
lytically derived). This step is quantitatively illustrated in Figure 3
with the black curve and the grey shaded areas. Attention can be
paid to how the updated model has reduced uncertainty around
the previous samples and newly observed point. For an overview
of GP models the reader is referred to [32].
To enable a priori power and memory constraint evaluations that
are decoupled from the expensive objective evaluation, HyperPower
models power and memory consumption of a network as a function
of theJdiscrete (structural) hyper-parameters z∈ZJ
+(subset of
x∈X); it trains on the structural hyper-parameters zthat affect
the NN’s power and memory ( e.g., number of hidden units), since
parameters such as learning rate have negligible impact.
To this end, HyperPower employs offline random sampling by
generating different configurations based on the ranges of the con-
sidered hyper-parameters z. Since the Bayesian optimization corre-
sponds to function evaluations with respect to the test error[ 33],
for each candidate design zlHyperPower measures the hardware
platform’s power Pland memory Mlvalues during inference and
not during the NN’s training. Given the Lprofiled data points
{(zl,Pl,Ml)}L
l=1, the following models that are linear with respect
to both the input vector z∈ZJ
+and model weights w,m∈RJare
trained, i.e.:
Power model :P(z)=JÕ
j=1wj·zj (3)
Memory model :M(z)=JÕ
j=1mj·zj (4)
HyperPower trains the models above by employing a 10-fold cross
validation on the dataset {(zl,Pl,Ml)}L
l=1. While the authors ex-
perimented with nonlinear regression formulations which can be
plugged-in to the models ( e.g., see recent work [ 3]), these linear func-
tions provide sufficient accuracy. More importantly, HyperPower
selects the linear form since it allows for the efficient evaluation
of the power and memory predictions within the acquisition func-
tion (next subsection), computed on each sampled grid point of the
hyper-parameter space.
HW-IECI : In the context of hardware-constraint optimization,
EI allows to directly incorporate the a priori constraint informa-
tion in a representative way. Inspired by constraint-aware heuris-
tics [ 14] [16], the authors propose a power and memory constraint-
aware acquisition function:
a(x)=∫∞
−∞max{y+−y,0}·pM(y|x)·
I[P(z)≤PB]·I[M(z)≤MB]dy(5)
where zare the structural hyper-parameters, pM(y|x)is the pre-
dictive marginal density of the objective function at xbased on
Figure 4: Assessment of HyperPower (denoted as HW-IECI)
against hardware-unaware methods. Best observed test er-
ror against the number of function evaluations [34].
surrogate model M.I[P(z)≤PB]andI[M(z)≤MB]are the indi-
cator functions, which are equal to 1if the power budget PB and
the memory budget MB are respectively satisfied. Typically, the
thresholdy+is adaptively set to the best value y+=max i=1:nyi
over previous observations [32][14].
HyperPower captures the fact that improvement should not be
possible in regions where the constraints are violated. Inspired
by the integrated expected conditional improvement (IECI) [ 16]
formulation, the authors refer to this proposed methodology as
HW-IECI . Uncertainty can be also encapsulated by replacing the
indicator functions with probabilistic Gaussian models as in [ 16],
whose implementation is already supported by the used tool [33].
NeuralPower exploits the use of trained predictive models based
on the power and memory consumption of DNNs, allowing the
HyperPower framework to navigate the design space in a constraint
“complying” manner. As shown in Figure 4, the authors observe that
the proposed HyperPower methodology (denoted as HW-IECI) con-
verges to the high-performing, constraint-satisfying region faster
compared to the hardware unaware one.
A key advantage of model-based optimization methodologies
is that, given their black-box optimization nature, they can be ex-
tended to various type of hardware constraints and design con-
siderations. For instance, Stamoulis et al. [35] show that Bayesian
optimization can be used to efficiently design adaptive DNNs un-
der edge-node communication constraints. In particular, hardware-
aware adaptive DNNs achieve 6.1×more energy efficient designs
than state-of-the-art for same accuracy.
4.4 Platform-aware NAS
Traditional SMBO-based black-box optimization methods fit a prob-
abilistic model over the entire design, i.e., the hyper-parameters of
all layers can be simultaneously changed. Hence, while Bayesian op-
timization methodologies are extremely efficient for design spaces
with up to 20 hyper-parameters, they could suffer from high di-
mensionality in larger DL models, e.g, parameter-heavy Recurrent
Neural Networks (RNNs) used in speech recognition applications.
To address this challenge, recent breakthroughs in hyper-parameter
optimization focus on the following insight: DL models designed
by human experts exhibit pattern repetition, such as the residual
cell in ResNet-like configurations [ 20]. To this end, recent work
focuses on the hyper-parameter optimization of a repetitive mo-
tif, namely cell, which is identified on a simpler dataset. After
the optimal cell is identified, it is repeated several times to con-
struct larger, more complex DL models that achieve state-of-the-art
performance on larger datasets and more complex learning tasks.
Figure 5: Pareto-optimal candidates (colored dots) identified
by platform-aware NAS-based method [11].
Intuitively, this transferability property of the optimal cell allows
the hyper-parameter optimization probabilistic model to focus on a
smaller structured design spaces with fewer hyper-parameters. Re-
cent work uses techniques such as Reinforcement learning (RL) or
Evolutionary algorithms (EA) to guide the exploration towards the
optimal cell. The body of work based on this insight is called Neural
Architecture Search (NAS). Nevertheless, initial NAS-based meth-
ods focused explicitly on optimizing accuracy, without considering
hardware-related metrics.
Motivated by this observation, the recently published NAS frame-
works MnasNet [ 37] and DPP-Net [ 11] propose formulations that
co-design the cell for both DNN accuracy and runtime. More specif-
ically, DPP-Net proposes a progressive search for Pareto-optimal
DNNs, where a probabilistic model is obtained on smaller cell de-
signs and new, more complex cells are being proposed by the prob-
abilistic model as the search progresses. The DPP-Net NAS-based
progressive model ranks candidate cell configurations based on
a weighted ranking of both the anticipated DL accuracy and the
runtime of each configuration measured on a hardware platform.
Similarly, MnasNet considers a multi-objective term to be optimized,
then accounts for both runtime (on a Google Pixel phone platform)
and accuracy of candidate cell designs. Figure 5 in [ 11] shows the
Pareto optimal DNN candidates with respect to error and FLOPS.
The NAS-based method, i.e., DPP-Net, is able to identify models
with a large number of parameters and slow inference time. The
DPP-Nets can achieve better trade-off among multiple objectives
compared with state-of-the-art mobile CNNs and models designed
using architecture search methods. Similarly, the MNAS method
outperforms the state-of-the-art handcrafted designs, namely Mo-
bileNet [23], by identifying designs closer to the Pareto front.
The ever-increasing interest of ML practitioners in NAS meth-
ods shows that these methodologies will provide the foundation
for novel hardware-aware optimization techniques. Several open
questions remain to be investigated, as we motivate further in the
following section.
5 UNEXPLORED RESEARCH DIRECTIONS
5.1 Hardware-aware modeling
As motivated in the previous section, the development of predictive
models to capture the hardware performance of DL application
has already played a critical role in the context of hyper-parameter
optimization. To this end, we postulate that predictive models are
poised to have significant impact towards enabling hardware-awareML. However, the following directions of research are crucial for
applicability of these models in a co-design environment
5.1.1 Hardware-aware models for DL with general topologies.
Current work has addressed building models largely for linear or
pipelined neural network structures. However, as mentioned before,
such predictive models should be suitably extended or developed
anew for nonlinear DNN structures [ 3]. This is of critical impor-
tance especially in the context of NAS-based methods, where the
cell design consists of several concatenating operations that are
executed in a nonlinear fashion with several non-sequential depen-
dencies, unlike the case of traditional DNN designs. To this end, the
development of hardware-aware predictive models for NAS-like
frameworks is essential.
5.1.2 Cross-platform hardware-aware models. Existing runtime
and energy models currently consider specific types of platforms,
with Nvidia GPUs being the hardware fabric in both the Paleo [ 27]
and the NeuralPower [ 3] frameworks. It is therefore important
to explore other platforms and segments of computing systems
spanning the entire edge to server continuum. It becomes crucial
to develop predictive models for capturing the runtime or energy
consumption for other types of neural accelerators and platforms,
such as reconfigurable architectures presented in [ 12]. Towards this
direction, we postulate that cross-platform models or models that
account for chip variability effects [ 4,6,36] can significantly help
ML practitioners to transfer knowledge from one type of hardware
platform to another and to choose the best model for that new
platform.
5.2 Hardware-aware optimization
There are several directions to advance state-of-the-art in the con-
text of hardware-aware hyper-parameter optimization, especially
given the recent interest in NAS-based formulations.
5.2.1 Multi-objective optimization. The design of DL models
is a challenging task that requires different trade-offs beyond the
currently investigated accuracy versus runtime/energy cases. For
instance, while the use of a simpler DNN design can improve the
overall runtime, it could significantly degrade the utilization or
throughput achieved given a fixed underlying hardware platform.
Hence, we believe that several novel approaches that focus on
hardware-aware hyper-parameter optimization would be extending
current SMBO models to multiple design objectives.
5.2.2 Hardware-DL model co-optimization. We postulate that
model-based hyper-parameter optimization approaches will allow
innovation beyond the design of DL models, since the predictive
models can be viewed also as a function of the underlying hardware
platform. That is, Bayesian optimization- and NAS-based formula-
tions can be extended to cases where both the DL model and the
hardware are co-designed. For instance, the optimal NAS-like cell
can be identified while varying the hardware hyper-parameters of
a reconfigurable architectures presented, such as the number of
processing elements [ 12]. Initial efforts focusing on the design of
hardware accelerators based on 3D memory designs [ 13] already
exploit energy-based models [5] for design space exploration.
6 CONCLUSION
To conclude, tools and methodologies for hardware-aware machine
learning have increasingly attracted attention of both academic
and industry researchers. In this paper, we have discussed recent
work on modeling and optimization for various types of hardware
platforms running DL algorithms and their impact on improving
hardware-aware DL design. We point out several potential new di-
rections in this area, such as cross-platform modeling and hardware-
model co-optimization.
ACKNOWLEDGMENTS
This research was supported in part by NSF CNS Grant No. 1564022
and by Pittsburgh Supercomputing Center via NSF CCR Grant No.
180004P.
REFERENCES
[1]Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al .
2016. TensorFlow: A system for large-scale machine learning. arXiv preprint
arXiv:1605.08695 (2016).
[2]James S Bergstra, Rémi Bardenet, Yoshua Bengio, and Balázs Kégl. 2011. Al-
gorithms for hyper-parameter optimization. In Advances in neural information
processing systems . 2546–2554.
[3]Ermao Cai, Da-Cheng Juan, Dimitrios Stamoulis, and Diana Marculescu. 2017.
Neuralpower: Predict and deploy energy-efficient convolutional neural networks.
arXiv preprint arXiv:1710.05420 (2017).
[4]Ermao Cai, Dimitrios Stamoulis, and Diana Marculescu. 2016. Exploring aging
deceleration in FinFET-based multi-core systems. In Computer-Aided Design
(ICCAD), 2016 IEEE/ACM International Conference on . IEEE, 1–8.
[5]Yu-Hsin Chen, Tushar Krishna, Joel S Emer, and Vivienne Sze. 2017. Eyeriss:
An energy-efficient reconfigurable accelerator for deep convolutional neural
networks. IEEE Journal of Solid-State Circuits 52, 1 (2017), 127–138.
[6]Zhuo Chen, Dimitrios Stamoulis, and Diana Marculescu. 2017. Profit: priority
and power/performance optimization for many-core systems. IEEE Transactions
on Computer-Aided Design of Integrated Circuits and Systems (2017).
[7]Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua
Bengio. 2016. Binarized neural networks: Training deep neural networks with
weights and activations constrained to+ 1 or-1. arXiv preprint arXiv:1602.02830
(2016).
[8]Xiaoliang Dai, Hongxu Yin, and Niraj K Jha. 2017. NeST: A Neural Network Syn-
thesis Tool Based on a Grow-and-Prune Paradigm. arXiv preprint arXiv:1711.02017
(2017).
[9]Ruizhou Ding, Zeye Liu, RD Shawn Blanton, and Diana Marculescu. 2018. Quan-
tized deep neural networks for energy efficient hardware-based inference. In
Design Automation Conference (ASP-DAC), 2018 23rd Asia and South Pacific . IEEE,
1–8.
[10] Ruizhou Ding, Zeye Liu, Rongye Shi, Diana Marculescu, and RD Blanton. 2017.
LightNN: Filling the Gap between Conventional Deep Neural Networks and
Binarized Networks. In Proceedings of the on Great Lakes Symposium on VLSI 2017 .
ACM, 35–40.
[11] Jin-Dong Dong, An-Chieh Cheng, Da-Cheng Juan, Wei Wei, and Min Sun. 2018.
DPP-Net: Device-aware Progressive Search for Pareto-optimal Neural Architec-
tures. arXiv preprint arXiv:1806.08198 (2018).
[12] J. Fowers, K. Ovtcharov, M. Papamichael, T. Massengill, M. Liu, D. Lo, S. Alkalay,
M. Haselman, L. Adams, M. Ghandi, S. Heil, P. Patel, A. Sapek, G. Weisz, L. Woods,
S. Lanka, S. K. Reinhardt, A. M. Caulfield, E. S. Chung, and D. Burger. 2018. A
Configurable Cloud-Scale DNN Processor for Real-Time AI. In 2018 ACM/IEEE
45th Annual International Symposium on Computer Architecture (ISCA) .
[13] Mingyu Gao, Jing Pu, Xuan Yang, Mark Horowitz, and Christos Kozyrakis. 2017.
Tetris: Scalable and efficient neural network acceleration with 3d memory. ACM
SIGOPS Operating Systems Review 51, 2 (2017), 751–764.
[14] Michael A Gelbart, Jasper Snoek, and Ryan P Adams. 2014. Bayesian optimization
with unknown constraints. arXiv preprint arXiv:1403.5607 (2014).
[15] Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and
Edward Choi. 2018. Morphnet: Fast & simple resource-constrained structure
learning of deep networks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR) .
[16] Robert B Gramacy and Herbert KH Lee. 2010. Optimization Under Unknown
Constraints. arXiv preprint arXiv:1004.4027 (2010).
[17] Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
2015. Deep learning with limited numerical precision. In International Conferenceon Machine Learning . 1737–1746.
[18] Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing
deep neural networks with pruning, trained quantization and huffman coding.
arXiv preprint arXiv:1510.00149 (2015).
[19] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights
and connections for efficient neural network. In NIPS . 1135–1143.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proceedings of the IEEE conference on computer
vision and pattern recognition . 770–778.
[21] José Miguel Hernández-Lobato, Michael A Gelbart, Ryan P Adams, Matthew W
Hoffman, and Zoubin Ghahramani. 2016. A general framework for constrained
Bayesian optimization using information-based search. The Journal of Machine
Learning Research 17, 1 (2016), 5549–5601.
[22] José Miguel Hernández-Lobato, Michael A Gelbart, Brandon Reagen, Robert
Adolf, Daniel Hernández-Lobato, Paul N Whatmough, David Brooks, Gu-Yeon
Wei, and Ryan P Adams. 2016. Designing neural network hardware accelerators
with decoupled objective evaluations. In NIPS workshop on Bayesian Optimization .
l0.
[23] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun
Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. Mobilenets:
Efficient convolutional neural networks for mobile vision applications. arXiv
preprint arXiv:1704.04861 (2017).
[24] Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew
Howard, Hartwig Adam, and Dmitry Kalenichenko. 2017. Quantization and
training of neural networks for efficient integer-arithmetic-only inference. arXiv
preprint arXiv:1712.05877 (2017).
[25] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long,
Ross Girshick, Sergio Guadarrama, and Trevor Darrell. 2014. Caffe: Convolutional
architecture for fast feature embedding. arXiv preprint arXiv:1408.5093 .
[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifica-
tion with deep convolutional neural networks. In Advances in neural information
processing systems . 1097–1105.
[27] Hang Qi, Evan R. Sparks, and Ameet Talwalkar. 2017. Paleo: A Performance
Model for Deep Neural Networks. In Proceedings of the International Conference
on Learning Representations .
[28] Sreeraj Rajendran, Wannes Meert, Domenico Giustiniano, Vincent Lenders, and
Sofie Pollin. 2017. Distributed deep learning models for wireless signal classifica-
tion with low-cost spectrum sensors. arXiv preprint arXiv:1707.08908 (2017).
[29] Brandon Reagen, José Miguel Hernández-Lobato, Robert Adolf, Michael Gelbart,
Paul Whatmough, Gu-Yeon Wei, and David Brooks. 2017. A case for efficient
accelerator design space exploration via Bayesian optimization. In Low Power
Electronics and Design (ISLPED, 2017 IEEE/ACM International Symposium on . IEEE,
1–6.
[30] Brandon Reagen, Paul Whatmough, Robert Adolf, Saketh Rama, Hyunkwang Lee,
Sae Kyu Lee, José Miguel Hernández-Lobato, Gu-Yeon Wei, and David Brooks.
2016. Minerva: Enabling low-power, highly-accurate deep neural network ac-
celerators. In ACM SIGARCH Computer Architecture News , Vol. 44. IEEE Press,
267–278.
[31] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C.
Berg, and Li Fei-Fei. 2015. ImageNet Large Scale Visual Recognition Challenge.
International Journal of Computer Vision (IJCV) 115, 3 (2015), 211–252.
[32] Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Fre-
itas. 2016. Taking the human out of the loop: A review of bayesian optimization.
Proc. IEEE 104, 1 (2016), 148–175.
[33] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. 2012. Practical bayesian
optimization of machine learning algorithms. In Advances in neural information
processing systems . 2951–2959.
[34] Dimitrios Stamoulis, Ermao Cai, Da-Cheng Juan, and Diana Marculescu. 2018.
HyperPower: Power-and memory-constrained hyper-parameter optimization for
neural networks. In Design, Automation & Test in Europe Conference & Exhibition
(DATE), 2018 . IEEE, 19–24.
[35] Dimitrios Stamoulis, Ting-Wu Chin, Anand Krishnan Prakash, Haocheng Fang,
Sribhuvan Sajja, Mitchell Bognar, and Diana Marculescu. 2018. Designing Adap-
tive Neural Networks for Energy-Constrained Image Classification. arXiv preprint
arXiv:1808.01550 (2018).
[36] Dimitrios Stamoulis and Diana Marculescu. 2016. Can we guarantee performance
requirements under workload and process variations?. In Proceedings of the 2016
International Symposium on Low Power Electronics and Design . ACM, 308–313.
[37] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V Le. 2018.
MnasNet: Platform-Aware Neural Architecture Search for Mobile. arXiv preprint
arXiv:1807.11626 (2018).
[38] Tien-Ju Yang, Yu-Hsin Chen, and Vivienne Sze. 2017. Designing energy-efficient
convolutional neural networks using energy-aware pruning. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR) .

From Server-Based to Client-Based Machine Learning: A
Comprehensive Survey
RENJIE GU, CHAOYUE NIU, FAN WU∗, and GUIHAI CHEN, Shanghai Jiao Tong University,
China
CHUN HU, CHENGFEI LYU, and ZHIHUA WU, Alibaba Group, China
In recent years, mobile devices have gained increasing development with stronger computation capability and
larger storage space. Some of the computation-intensive machine learning tasks can now be run on mobile
devices. To exploit the resources available on mobile devices and preserve personal privacy, the concept
of client-based machine learning has been proposed. It leverages the users’ local hardware and local data
to solve machine learning sub-problems on mobile devices and only uploads computation results rather
than the original data for the optimization of the global model. Such an architecture can not only relieve
computation and storage burdens on servers but also can protect the users’ sensitive information. Another
benefit is the bandwidth reduction because various kinds of local data can be involved in the training process
without being uploaded. In this paper, we provided a literature review on the progressive development of
machine learning from server-based to client-based. We revisited a number of widely-used server-based and
client-based machine learning methods and applications. We also extensively discussed the challenges and
future directions in this area. We believe that this survey will give a clear overview of client-based machine
learning and provide guidelines on applying client-based machine learning to practice.
CCS Concepts: •Computing methodologies →Machine learning ;Multi-agent systems ;Mo-
bile agents ;Distributed algorithms .
Additional Key Words and Phrases: Mobile intelligence, Machine learning, Distributed System, Decen-
tralized training, Federated learning.
1 INTRODUCTION
Machine learning, especially deep learning, has become a hot topic, attracting tremendous attention
from both academia and industry. The core idea of machine learning is to use large amounts of data
to train a model that can generalize well to unseen test samples. However, with the increase of data
volume and the enhancement of model capacity, it is infeasible for a single server to accomplish
complex learning tasks in a centralized way. To address this problem, the concept of server-based
distributed machine learning was proposed in [ 82], where multiple servers, connected through
shared data buses or a fast local area network, exchange essential information (e.g., training losses
and gradients) to collaboratively train a model. Although this framework is highly scalable and
has been widely deployed in practice, it may not always be cost effective and efficient to build a
Comments: Accepted to ACM CSUR 2021, Volume 54, Issue 1, Pages 6:1 - 6:36. Webpage: https://doi.org/10.1145/3424660.
This work was supported in part by National Key R&D Program of China No. 2019YFB2102200, in part by China NSF grant
No. 61972252, 61972254, 61672348, and 61672353, in part by Joint Scientific Research Foundation of the State Education
Ministry No. 6141A02033702, and in part by Alibaba Group through Alibaba Innovation Research Program. The opinions,
findings, conclusions, and recommendations expressed in this paper are those of the authors and do not necessarily reflect
the views of the funding agencies or the government. The authors also want to sincerely thank Dr. Shuo Yang for offering
valuable suggestions on polishing the initial version of this work.
∗Fan Wu is the corresponding author.
Authors’ addresses: Renjie Gu, grj165@sjtu.edu.cn; Chaoyue Niu, rvince@sjtu.edu.cn; Fan Wu, fwu@cs.sjtu.edu.cn; Gui-
hai Chen, gchen@cs.sjtu.edu.cn, Shanghai Jiao Tong University, 800 Dongchuan Rd., Shanghai, China, 200240; Chun Hu,
shiji.hc@alibaba-inc.com; Chengfei Lyu, chengfei.lcf@alibaba-inc.com; Zhihua Wu, zhihua.wzh@alibaba-inc.com, Alibaba
Group, 969 Wenyi Rd. (W), Hangzhou, China, 311121.arXiv:1909.08329v2  [cs.LG]  28 Sep 2021
2 Gu et al.
high-performance server cluster. In addition to cost, security and privacy are major concerns when
machine learning involves sensitive user data, such as typed texts in natural language processing,
user profiles in personalized recommendations, and health records in medical diagnosis. Specifically,
the servers in both centralized and distributed machine learning frameworks require direct accesses
to training data and thus need to collect and store user data, which inevitably suffers outsider and
insider attacks [ 79–81]. For example, a malicious hacker may invade the datacenter, compromise
part of servers, and leak private databases. Further, if the servers are untrusted, they may share
user data with other unauthorized entities or even trade for profits. In a nutshell, how to reduce
the server cluster’s operation cost, how the trusted servers can securely maintain user data, and
how to defend untrusted servers are bottlenecks of the server-based machine learning.
Meanwhile, with rapid proliferation and development of mobile devices, the idea of doing
machine learning tasks on mobile devices has also emerged. For example, applications, such as face
recognition and speech recognition, are all based on machine learning and are common among
mobile phones. To support these applications, a full-sized machine learning model is first trained
on servers using large amounts of data, and then it is tailored and delivered to mobile devices to do
inference and make predictions locally. This framework brings all the burdens to the central servers,
wasting the resources of mobile devices, whose processors, memory space, and disk space are now
powerful and abundant enough to support various kinds of computation tasks. In addition, many
off-the-shelf machine learning frameworks (e.g., the TensorFlow Lite module in TensorFlow [ 1]) are
now available. Developers can now readily adopt these end-to-end tools to build machine learning
models for their mobile applications. The above evidences have shown that it is feasible to deploy
distributed training tasks on mobile devices, which is also called client-based training.
Client-based training has advantages in cost reduction and privacy preservation. In particular,
machine learning problems are distributed to mobile devices and solved locally so that high-
performance servers and user data transmission/maintenance are no longer required. The idea
of client-based training can be traced back to 2015, when Shokri et al. [ 100] proposed distributed
deep learning without sharing datasets among multiple parties. Later that year, Google researchers
designed federated optimization [ 54], aiming to improve communication efficiency during learning
with decentralized datasets. The idea was also referred to as Federated Learning and further
developed in the following years [ 53,55,56,70]. These works can be generally viewed as a specific
type of client-based training which mainly focuses on how to make use of data without uploading
them to the server, so that the privacy of users can be well preserved. If we enable on-device training
and only upload the computation results, the leakage of sensitive information can be relieved. The
reason is that attacks against the computation results without accessing the raw data are much
harder. Moreover, since raw data is processed locally, client-based training is now able to make use
of the data that is too much to be uploaded (so that centralized machine learning doesn’t take them
into consideration), which gives us great opportunity to improve the model performance.
Compared to existing surveys in this area, this survey focuses on the evolution process of
machine learning. From server-based machine learning to client-based machine learning, the
application scenarios and the research problems have greatly changed. We summarize the changes
and further investigate the underlying motivations. We also review the research focuses at different
stages of machine learning development. In particular, considering that a lot of new features and
new demands have emerged in client-based machine learning, we analyze the applicability of
existing server-based algorithms to client-based machine learning. We finally point out some
future directions of client-based machine learning. In a nutshell, this survey not only is a review of
the development of machine learning, but also can work as a good reference for designing new
client-based learning algorithms on the basis of conventional server-based methods.
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 3
Machine Learning
input: , output: ?
Data:
Samples (with label)
Tasks:Data:
Samples (without label)
Tasks:
Clustering
(e.g., Recommender System)
Dimensionality Reduction
(e.g.,  Feature Extraction)
Association
(e.g.,  Market Basket Analysis)Data:
Interaction between
Tasks:
Gaming
Robot Navigation
Driverless V ehicleRegression
(e.g., Market Forecasting)
Classiﬁcation
(e.g.,  Image Classiﬁcation)Agent And Env .output: prediction Supervised
LearningUnsupervised
Learning
output: summary Reinforcement
Learning
output: action  to
maximize reward 
(a) Machine Learning
Supervised Learning
Linear Regression
ﬁt a line based on samples
Logistic Regression
predict probability of events
Support V ector Machine
ﬁnd a hyperplane to seperate samples
(perform well even with small  amount
Neural Network / Deep Learning
ﬁt the mapping function using  neurons
(large deep neural networks perform well of training data)
for learning from massive training data)Discriminative Model
directly learn conditional distributionGenerative Model
learn joint distribution and then
calculate conditional distribution
Naive Bayes
based on Bayes' Theorem
Markov Process
learn state transition probabilities
Hidden Markov Model
learn the hidden states behind
the observed  state sequence(b) Supervised Learning
Fig. 1. Taxonomy of machine learning and supervised learning.
The rest of this survey is organized as follows. In Section 2 and Section 3, we introduce the
general machine learning process and the methodology of server-based distributed training. In
Section 4 and Section 5, we explain the motivations of client-based machine learning (including
inference and training), list the challenges, and discuss current advances of client-based training
using federated learning and split learning as examples. In Section 6, we discuss the open problems
and future directions of client-based training. Finally, we conclude the survey in Section 7.
2 CENTRALIZED MACHINE LEARNING
Machine learning [ 11] is a study of mathematical models that can automatically learn and make
predictions based on a set of observed data. The concept of centralized machine learning means
that operations and executions of the model are all done on a central machine. Centralized machine
learning has been widely used to extract insights behind huge amounts of data.
In this section, we briefly review concepts and techniques of centralized machine learning.
To better understand the methodology of machine learning, we first introduce the paradigms of
machine learning methods in Section 2.1. Next, several concepts that are commonly used in machine
learning will be illustrated in Section 2.2. Then, the task of machine learning will be defined in
Section 2.3. A general process of machine learning will be presented in Section 2.4. After that,
several machine learning optimizers will be introduced in Section 2.5. Finally, the applicability of
these optimizers to client-based training will be discussed in Section 2.6.
2.1 Machine Learning Paradigm
If we classify machine learning algorithms based on the kind of input data samples and the kind of
output, we have the following three basic machine learning paradigms as shown in Fig. 1a.
Supervised Learning .In supervised learning, every data sample is made up of several input
features and a label. The learning process is to approximate a mapping function from the features
to the label. After that, given new input features, the label for the data can be predicted using the
mapping function. This scheme is the most popular machine learning scheme which has been used
in a variety of tasks. An example of supervised learning is the classification task, which is to classify
an object based on its features, such as classifying fruit according to its color, shape, and weight. If
4 Gu et al.
the supervised learning task is to predict a continuous variable such as market pricing, then this is
a regression task. We can further classify supervised learning based on the model type, as shown in
Fig. 1b. We mainly focus on supervised learning (especially discriminative models) in this survey.
Unsupervised Learning .In contrast to supervised learning, unsupervised learning is where
we only have input features but no corresponding labels. Thus, the goal for unsupervised learning
is to learn the distribution of the data and show how the data points are different from each other.
A typical example of unsupervised learning is the clustering problem, which is to discover the
groupings of the data, such as grouping users by their behaviors.
Reinforcement Learning .Reinforcement learning is quite different from supervised learning
and unsupervised learning. Labelled input/output pairs and explicit correction on sub-optimal
options are not needed for training an agent using reinforcement learning. Instead, the agent tries to
find a trade-off between exploration and exploitation through its interaction with the environment.
For good choices or actions, the agent gains rewards from the interpreter. Otherwise, it is punished.
Reinforcement learning is widely used in research about robots or computer gaming agents.
Others .In addition to the three basic paradigms, there also exists some other paradigms that can
be viewed as a combination of the basic ones (e.g., semi-supervised learning). Usually, the choice of
machine learning paradigms depends on the kind of the problem going to be solved.
2.2 Machine Learning Concepts
To better understand machine learning, in this section, we introduce and illustrate several key
concepts that are commonly used in machine learning (especially supervised learning).
Stage and Dataset .The whole process of machine learning is mainly made up of three kinds of
stages, which are training, validation, and test. The dataset 𝐷={𝑑1,𝑑2,...,𝑑 𝑛}is the set of all 𝑛data
samples used for the machine learning process. A data sample 𝑑𝑖=(𝑥𝑖,𝑦𝑖)contains a feature vector
𝑥𝑖and a corresponding label 𝑦𝑖. Usually𝐷can be divided into three disjoint sub-sets which are the
training set 𝐷𝑡𝑟𝑎𝑖𝑛, the validation set 𝐷𝑣𝑙𝑑, and the test set 𝐷𝑡𝑒𝑠𝑡, corresponding to the three stages.
𝐷𝑡𝑟𝑎𝑖𝑛 accounts for the largest proportion of 𝐷. In the training stage, loss is calculated on 𝐷𝑡𝑟𝑎𝑖𝑛
and then optimization to the model is done. The validation stage aims to prevent overfitting. Since
samples in𝐷𝑣𝑙𝑑are not used in any training iterations, the error calculated on them will increase
significantly if the model overfits 𝐷𝑡𝑟𝑎𝑖𝑛. Moreover,𝐷𝑣𝑙𝑑is also helpful for tuning hyperparameters
between training epochs. The test stage is to evaluate the performance of the final model on 𝐷𝑡𝑒𝑠𝑡.
Thus, a typical machine leaning process is as follows: (1) Run training and validation iteratively
until the model converges; and (2) Run test to evaluate the model performance.
Feature .In machine learning, a feature vector 𝑥is a𝑘-dimensional vector containing numerical
features which are observable or measurable properties of instances, objects, or phenomena. We
input feature vectors to the machine so that it knows how instances are different from each other.
Label .A label𝑦is the identity of the instance. Unlike features that describe the instances to the
machine, labels tell the machine what the instances really are.
Model .The machine learning model 𝑓(𝑤;·)includes a structure 𝑓and a parameter vector 𝑤.
The model𝑓(𝑤;·)is the core of machine learning. It serves as the mapping function that maps the
input feature vector 𝑥to the output label 𝑓(𝑤;𝑥). For a good model, its output label 𝑓(𝑤;𝑥)is close
to the true label 𝑦of the feature vector 𝑥. Usually, the structure and the parameters of the model are
stored as multiple vectors or matrices. The mapping process is done through matrix multiplication.
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 5
Various kinds of models such as support vector machines and artificial neural networks have been
designed for different machine learning tasks.
Loss Function .The loss function 𝐿(𝑦,𝑓(𝑤;𝑥))(e.g., mean square error, hinge loss, and softmax
loss) describes how far the predicted label 𝑓(𝑤;𝑥)deviates from the true label 𝑦. Since a pair(𝑥,𝑦)
is also known as a data sample 𝑑, we can also denote the loss function by 𝐿(𝑤;𝑑)for convenience.
The design of the loss function 𝐿reflects the goal of the training process which is to train the model
to make predictions as accurate as possible. For example, mean square error is defined as
MSE=𝑛∑︁
𝑖=1(𝑦𝑖−𝑓(𝑤;𝑥𝑖))2, (1)
where𝑛is the number of samples, 𝑤stands for the model parameters, 𝑥𝑖is the feature vector of
the𝑖-th sample, 𝑓(𝑥𝑖)is the predicted label for the 𝑖-th sample, and 𝑦𝑖is the true label of the 𝑖-th
sample. Thus, in a regression problem using mean square error as the loss function, the model 𝑤
will get more punishment if the predicted label 𝑓(𝑥𝑖)is further from the true label 𝑦𝑖. The value of
the loss function 𝐿over𝐷𝑡𝑟𝑎𝑖𝑛 is calculated as
𝐿(𝑤;𝐷𝑡𝑟𝑎𝑖𝑛)=1
𝑛𝑡𝑟𝑎𝑖𝑛𝑛𝑡𝑟𝑎𝑖𝑛∑︁
𝑖=1𝐿(𝑤;𝑑𝑖)=1
𝑛𝑡𝑟𝑎𝑖𝑛𝑛𝑡𝑟𝑎𝑖𝑛∑︁
𝑖=1𝐿(𝑦𝑖,𝑓(𝑤;𝑥𝑖)), (2)
where𝑛𝑡𝑟𝑎𝑖𝑛 is the number of samples in 𝐷𝑡𝑟𝑎𝑖𝑛. Here𝐿(𝑤;𝐷𝑡𝑟𝑎𝑖𝑛)is also known as the empirical
risk. The change in the value of 𝐿(𝑤;𝐷𝑡𝑟𝑎𝑖𝑛)shows the training progress of the model 𝑤. The lower
the value of 𝐿(𝑤;𝐷𝑡𝑟𝑎𝑖𝑛)is, the better the model 𝑤is trained on 𝐷𝑡𝑟𝑎𝑖𝑛. Note that norms should
be added to the loss function to avoid the overfitting problem. In particular, overfitting means the
model is too closely fit to 𝐷𝑡𝑟𝑎𝑖𝑛 and has a poor performance when dealing with new data.
Training (Optimization) .After the loss is calculated, the model will be optimized iteratively
to achieve a better generalization ability. This is also known as the training process. Typical
optimization algorithms such as gradient decent are used to adjust parameters of the model
according to the loss and the model structure. Considering that heavy calculation is required in this
step, computational tricks like back-propagation are usually applied to improve system efficiency.
Inference (Prediction) .With a trained model 𝑓(𝑤;·), we can input a feature vector 𝑥to it. Then,
after some calculation, the model outputs a predicted label 𝑓(𝑤;𝑥). This process is also known as
the inference process. Usually inference is done by doing several times of matrix multiplication.
2.3 Task Definition
In machine learning, we first design the model structure 𝑓according to what kind of problem
we are going to solve. Then the goal is to find out a parameter vector 𝑤which minimizes the
expectation of the loss function. This can be expressed as
arg min
𝑤E[𝐿(𝑦,𝑓(𝑤;𝑥))], (3)
where𝑥∈𝑋is the input feature and 𝑦∈𝑌is the output label. It is assumed that the feature
space𝑋and the label space 𝑌obey a joint probability distribution 𝑃(𝑥,𝑦). Since the real 𝑃(𝑥,𝑦)
is unknown and is impossible to be figured out on most occasions, we are not able to directly
optimize our model parameter 𝑤to minimize the expectation of the loss function E[𝐿(𝑦,𝑓(𝑤;𝑥))].
As an approximation, we minimize the empirical risk 𝐿(𝑤;𝐷𝑡𝑟𝑎𝑖𝑛)on our training set 𝐷𝑡𝑟𝑎𝑖𝑛. This
approximation requires 𝐷𝑡𝑟𝑎𝑖𝑛 to be a set which contains Independent and Identically Distributed
6 Gu et al.
(IID) random variables drawn from 𝑃(𝑥,𝑦). Now the core learning task is expressed as
arg min
𝑤𝐿(𝑤;𝐷𝑡𝑟𝑎𝑖𝑛))=arg min
𝑤1
𝑛𝑡𝑟𝑎𝑖𝑛𝑛𝑡𝑟𝑎𝑖𝑛∑︁
𝑖=1𝐿(𝑤;𝑑𝑖)=arg min
𝑤1
𝑛𝑡𝑟𝑎𝑖𝑛𝑛𝑡𝑟𝑎𝑖𝑛∑︁
𝑖=1𝐿(𝑦𝑖,𝑓(𝑤;𝑥𝑖)),(4)
where𝑛𝑡𝑟𝑎𝑖𝑛 is the number of samples in 𝐷𝑡𝑟𝑎𝑖𝑛,𝑑𝑖=(𝑥𝑖,𝑦𝑖)is the𝑖-th sample in 𝐷𝑡𝑟𝑎𝑖𝑛.
2.4 Machine Learning Process
The procedure of machine learning can be divided into two parts: (1) Designing a suitable model
structure based on the machine learning task; and (2) Examining the performance of the model and
optimize it accordingly using large amounts of data.
Model Design .For the model design part, there already exists many well-designed models
that have been proposed to solve different kinds of tasks, as shown in Fig. 1b. A good survey
on popular traditional machine learning algorithms and models can be found in [ 116]. What’s
more, in recent years, deep learning [ 58] is proved to be very effective in many areas such as
image recognition and natural language processing. The word “deep” is used to describe the
multi-layer structure of the neural network used by it. The concept of deep learning can be
further divided into Convolutional Neural Networks (CNN), Recursive Neural Networks (RNN),
Generative Adversarial Networks (GAN), and so on according to the neural network structure.
Considering that the scale of a deep learning model can be very large, it is usually trained on
high-performance servers. In general, deep learning is a powerful machine learning technique
which is based on neural network and benefited from the increase in the amount of training data.
Model Optimization .Regarding the model optimization part, it is done through machine learn-
ing optimizers. Generally speaking, an optimizer focuses on how to optimize the model to reach
its best performance based on the given dataset. For example, Gradient Descent (GD) calculates
the gradient of the loss function and uses this gradient to optimize the model parameters. In each
training iteration, according to the calculated gradient, the model parameters 𝑤takes a step toward
the optimal point where 𝐿(𝑤;𝐷𝑡𝑟𝑎𝑖𝑛)is minimized. However, the computation cost of GD is so
high that it is not suitable for being applied to those models with a large number of parameters.
Thus, many different schemes have been designed to find a better balance between the convergence
speed and the computational cost. For those machine learning algorithms which are not suitable
for using GD-based methods as their optimizers, they have their specific optimization algorithms,
such as Sequential Minimal Optimization (SMO) for SVM and Canopy for 𝑘-means.
2.5 GD-Based Optimizers
Although there are many kinds of optimizers in centralized machine learning, in this survey, we
mainly focus on the GD-based optimizers which originate from centralized machine learning and
are now being widely used in distributed training and deep learning. In this section, we introduce
several GD-based optimizers and analyze their advantages. We list the basic information of them
in Table 1. Note that the column “Speed” in the table has considered both the computational cost
per iteration and the total number of iterations to reach convergence. Among these optimizers,
SGD, SVRG, ADAM, and Hogwild! are designed for serial centralized machine learning or parallel
multi-threads centralized machine learning. ASGD, EASGD, and DC-ASGD are actually designed
for a server-worker scheme, which means they should be classified as distributed training techniques.
However, we still choose to introduce them here for the convenience of comparing them with other
optimizers and studying how GD-Based optimizers have developed from centralized to distributed.
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 7
Table 1. GD-based optimizers.
Name Mathematical Format Convergence Speed Experimental Scale
SGD Gradient (1 sample) Sublinear (all) Baseline An email dataset (100 machines) [127]
SVRG Gradient (all samples)Linear (strongly convex)
Sublinear (convex)
Sublinear (non-convex)Slow↓ MNIST [59] and CIFAR-10 [39] (1 machine)
ADAM SGD + Moment Not Guaranteed Fast⇈ MNIST and CIFAR-10 (1 machine)
Hogwild! SGDLinear (strongly convex,
constant stepsize)EquivalentRCV1 [60], Netflix, KDD, Jumbo, DBLife,
and Abdomen (10 machines)
ASGD SGD Sublinear (strongly convex) Equivalent A speech dataset and ImageNet [90] (128 workers)
EASGD SGD + Elastic Update ? (complicated form) Fast↑ CIFAR-10 (16 workers), ImageNet (8 workers)
DC-ASGD SGD + Delay Compensation Sublinear (strongly convex) Fast↑ CIFAR-10 (8 workers), ImageNet (16 workers)
SGD .Stochastic Gradient Descent (SGD) [ 89] was first proposed in 1951. The main advantage of
SGD is that it greatly reduces the computational cost in each iteration compared with GD. Its core
equations are very similar to those of GD and are shown as follows:
𝑔𝑡=∇𝐿(𝑤𝑡−1;𝑑𝑖), 𝑤 𝑡=𝑤𝑡−1−𝜂·𝑔𝑡, (5)
where𝑡is the timestamp for the current training iteration, 𝑤𝑡−1is the model at time 𝑡−1,𝑔𝑡is the
gradient of the loss function 𝐿with model𝑤𝑡−1and a randomly selected data sample 𝑑𝑖, and𝜂is the
learning rate. The feature that it only uses one sample to compute gradients in each iteration greatly
reduces the computational cost. However, SGD should not be directly applied to client-based training
since it cannot handle the bias caused by the non-IID local dataset (will be introduced in Section 5.4).
Modification to SGD is necessary to fit the scenario of client-based training (e.g., FedAvg [ 70]).
Although the solution for client-based training on mobile devices is unlikely to choose the original
SGD as the optimizer, we have to say that SGD still works well on many other occasions (such as
server-based distributed training) due to its low computational cost and high training efficiency.
SVRG .Stochastic Variance Reduced Gradient (SVRG) [ 46] aims at accelerating the convergence
speed of SGD by applying noise reduction methods. Compared with GD, SGD does much less
computation in each iteration but has a lower convergence speed. Bottou et al. [ 14] discovered
that one reason for this is the existence of noise in the estimate of the gradient, which can be also
considered as the variance of gradients. Thus, SVRG uses the overall gradient to make corrections
and thus reduces the noise. The core equations are shown as follows:
𝑔′
𝑡=∇𝑓(𝑤𝑡−1;𝑑𝑖)−∇𝑓(¯𝑤;𝑑𝑖)+¯𝑔, 𝑤 𝑡=𝑤𝑡−1−𝜂·𝑔′
𝑡, (6)
where ¯𝑤is an averaged model which is updated every 𝑘iterations and ¯𝑔is the gradient averaged
among all data samples at point ¯𝑤. The first term∇𝑓(𝑤𝑡−1;𝑑𝑖)on the right side of the first equation is
exactly the𝑔𝑡used in SGD. The core idea of SVRG is to make the upper bound of gradients’ variance
keep reducing during training by using correction (−∇𝑓(¯𝑤;𝑑𝑖)+¯𝑔). McMahan et al. [ 70] found out
that SVRG can cooperate well with some distributed optimization algorithms like DANE [ 96] by
working as the local optimizer. The main factor which limits SVRG’s applicability to client-based
training is the high computational cost of periodically calculating the overall gradient ¯𝑔.
ADAM .Adam is an optimization algorithm based on SGD aiming to accelerate the convergence
speed by adaptively tuning the learning rate. It was proposed in 2014 [ 50]. Before Adam, there al-
ready exists some algorithms trying to improve SGD through making use of the moment/momentum
of gradients, such as SGD with Momentum (SGDM) [ 83], AdaGrad [ 26], and RMSProp [ 111]. Adam
8 Gu et al.
combines the advantage of AdaGrad and RMSProp and uses both the first moment estimate and
the second moment estimate. Its equations are given as follows:
𝑚𝑡=𝛽1·𝑚𝑡−1+(1−𝛽1)·𝑔𝑡, 𝑣 𝑡=𝛽2·𝑣𝑡−1+(1−𝛽2)·(𝑔𝑡)2,
ˆ𝑚𝑡=𝑚𝑡
1−(𝛽1)𝑡, ˆ𝑣𝑡=𝑣𝑡
1−(𝛽2)𝑡, 𝑤 𝑡=𝑤𝑡−1−𝜂·ˆ𝑚𝑡√ˆ𝑣𝑡+𝜖.(7)
Here,𝑚𝑡is the first moment estimate and 𝑣𝑡is the second moment estimate. 𝛽1,𝛽2∈[0,1)are
exponential decay rates for the moment estimates. ˆ𝑚𝑡andˆ𝑣𝑡are the bias-corrected version of the
moment estimates. 𝜖is a small constant used to prevent division by zero. Experiments show that
Adam can accelerate training. Considering that the number of communication rounds may be
limited in client-based training, an efficient optimizer which can speed up the training process
may help a lot. However, Wilson et al. [ 115] and Reddi et al. [ 92] discovered that on some special
occasions, ADAM may fail to reach convergence. This can be the main barrier for applying ADAM
to client-based training.
Hogwild! .Hogwild! [ 86] aims to prove that parallel SGD can be implemented without any
locking. It shows that when the optimization problem is strongly convex and sparse, most updates
only modify small subsets of all parameters, which means the whole update process can be run
asynchronously without locking. However, it has only been tested on traditional machine learning
problems like sparse SVM and matrix completion. Whether Hogwild! is suitable for complex tasks,
such as deep learning and client-based training, still remains unknown.
ASGD .Asynchronous SGD (ASGD) [ 24] is a simple attempt for making use of more workers to
train a huge deep network through asynchronous methods with SGD. Compared with synchro-
nous SGD, ASGD won’t suffer from the straggler problem which is a huge obstacle to deploying
distributed machine learning on heterogeneous mobile devices. Since no waiting is needed, all
workers can make best use of their resources and together accelerate the training. The problem is
that in asynchronous methods, the delayed gradients may be unsuitable for being applied to the
current updated model. The delay error can cause fluctuation in weights and have negative effects
on the model according to [ 8,63]. This delay error problem can be even worse in client-based
training due to the large number of workers and the high frequency of model update.
EASGD .The purpose of Elastic ASGD (EASGD) [ 123] is to reduce the communication cost
between workers and the parameter server during parallel training. Each worker’s local model is not
replaced by the global model in each communication round. The communication and coordination
of work among all workers is controlled by an elastic force that links the local parameters with a
center variable stored by the parameter server. The update rules are shown as follows:
𝑤𝑖
𝑡=𝑤𝑖
𝑡−1−𝜂· 𝑔𝑖
𝑡+𝜌· 𝑤𝑖
𝑡−1−¯𝑤𝑡−1, ¯𝑤𝑡=¯𝑤𝑡−1+𝜂·𝑝∑︁
𝑖=1𝜌· 𝑤𝑖
𝑡−1−¯𝑤𝑡−1, (8)
where𝑖is a random index of a worker, 𝑝is the number of workers, 𝜌is the control parameter for
the elasticity, and ¯𝑤𝑡is the center variable. The center variable ¯𝑤𝑡is updated as a moving average
which is taken in both time and space over all local parameters. The elastic design allows workers
to do more exploration in its nearby parameter space, which can do good to the model performance.
The effectiveness of EASGD has only been analyzed for quadratic and strongly convex objectives.
It is worthy to worry about that the model may become even worse if the elastic hyperparameter
isn’t set properly and causes the workers to explore too far away from the center variable.
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 9
DC-ASGD .Delay Compensated ASGD (DC-ASGD) [ 126] focuses on mitigating the error caused
by delayed gradients in ASGD. The main idea of DC-ASGD is to use the first-order term in Taylor
series to compensate for the delayed gradient and use an approximation of the Hessian Matrix to
reduce the computational cost. At time 𝑡+𝜏, to update model 𝑤𝑡+𝜏, the original delayed gradient
𝑔(𝑤𝑡)for old model 𝑤𝑡will be replaced by the delay-compensated gradient which is expressed as:
𝑔(𝑤𝑡)+𝜆𝑔(𝑤𝑡)⊙𝑔(𝑤𝑡)⊙(𝑤𝑡+𝜏−𝑤𝑡), (9)
where𝜆is a variance control parameter set by the server. The only additional information needed for
compensation is the historical model 𝑤𝑡, which means this method is easy to implement. However,
since experiments have been done with no more than 16 workers, DC-ASGD’s performance under
large numbers of workers still needs to be studied. In addition, another key problem of applying
it to client-based training is that the server needs to spend large additional storage to store the
historical models for all workers.
2.6 Applicability of GD-Based Optimizers to Client-Based Training
In this section, we compare the applicability of the GD-based optimizers to client-based training in
detail, as shown in Table 2. The main difficulty of applying these optimizers to client-based training is
that mobile devices may not have enough resources to run them on large models. A possible solution
is to use distributed optimization algorithms (will be introduced in Section 3.5) to decompose an
original large problem into multiple small sub-problems. The aforementioned optimizers should
cooperate with distributed optimization algorithms and work as the local optimizer. Therefore,
we analyze the applicability of GD-based optimizers from two aspects: (1) whether the time and
space complexities are acceptable and affordable for mobile devices; and (2) whether the optimizers
support distributed computing in terms of scalability, asynchronization, and delay solution .
Time Complexity .For client-based training, optimizers with lower time complexity are pre-
ferred since they require fewer resources. Hard et al. [ 37] demonstrate that running SGD on mobile
devices is feasible. We regard SGD as the baseline here. For SVRG, its periodical calculation of the
overall gradient incurs huge local computation overhead. For ADAM, to accelerate the gradient
descent step, it introduces additional gradient processing steps which contains several times of
matrix multiplication. So ADAM’s time complexity is relatively higher but still acceptable. For other
GD-based optimizers, the local computation step is just SGD. Thus, regarding the time complexity,
most of these GD-based optimizer (except for SVRG) are applicable to client-based training.
Space Complexity .Some optimizers use additional information to accelerate training or mitigate
errors. This incurs additional space overhead. For SVRG, it keeps an overall gradient and a model to
reduce the variance of gradients. For ADAM, it keeps the last gradient and the moment to accelerate
training. For DC-ASGD, it needs to store a historical model for each worker for delay compensation.
Therefore, these three optimizers require additional memory and storage space. For other GD-based
optimizers, they do not need additional space. Therefore, regarding the space complexity, SGD,
Hogwild!, ASGD, and EASGD are preferred.
Scalability .We use “machines” to express that multiple machines cooperate with each other.
We use “workers” to show that a group of workers are managed by a central server and may not
need to communicate with each other. For synchronous optimizers, their scalability is limited by
the time-consuming synchronization step. The “thundering herd” problem also add difficulty to
implementing large-scale synchronous systems. For asynchronous optimizers, the obstacles are the
update covering problem (i.e., later updates rewrite parameters and cover earlier updates.) and the
delay error problem. With more workers, asynchronous training becomes less stable. However,
10 Gu et al.
Table 2. Applicability of GD-based machine learning optimizers to client-based training.
Name Time Complexity Space Complexity Scalability Asynchronization Delay Solution
SGD Baseline! Basic Model ! 100 machines ! No% /
SVRG High⇈%Historical Gradient,
Historical Model %1 machine No% /
ADAM High↑ Historical Gradient % 1 machine No% /
Hogwild! Equivalent ! Basic Model ! 10 machines Yes! None%
ASGD Equivalent ! Basic Model ! 128 workers ! Yes! None%
EASGD Equivalent ! Basic Model ! 8 workers Yes! Not Needed !
DC-ASGD Equivalent ! Historical Model % 16 workers Yes! Compensating !
the impact of these problems has not been clearly analyzed yet. Thus, we only list the number of
machines/workers used in experiments to reflect the potential scalability. According to Table 2,
SGD and ASGD have shown good scalability.
Asynchronization .SGD, SVRG, and ADAM are designed for centralized machine learning
with only one machine. They require a costly synchronization step to ensure convergence when
being applied to parallel training. By allowing the asynchronous update of the global model, the
time-consuming synchronization step can be avoided. For example, Hogwild!, ASGD, EASGD, and
DC-ASGD are designed for asynchronous machine learning.
Delay Solution .The unstable network condition and the limited resources in client-based
training can easily cause update delay. In asynchronous training scheme, the global model may
have already been updated by others when the gradient arrives at the server. The delayed gradient
thus becomes less accurate for the current global model. Directly applying delayed gradients to the
global model can slow down the convergence speed due to the delay error. Hogwild! and ASGD
have no solution for delayed gradients. They just ignore the delay error. EASGD does not need a
delay solution because its global model is a moving average instead of being updated by gradients.
DC-ASGD compensates delayed gradients by using Taylor series expansion. Therefore, regarding
delay solution, EASGD and DC-ASGD outperform Hogwild! and ASGD.
3 SERVER-BASED DISTRIBUTED TRAINING
Although centralized machine learning has shown good performance in many kinds of tasks, it
cannot catch up with the growing demand of processing more data and training larger models.
Under this circumstance, server-based distributed training techniques have been developed. The
concept of server-based training means that operations and executions of the machine learning
model are all done on servers. In this section, we first introduce the motivations and then discuss
distributed parallelism categories and distributed optimization algorithms.
3.1 Motivations
Training Acceleration .The original centralized machine learning scheme can only use the com-
putation power of a single machine, which indicates that it may require a long time to train a good
model when dealing with large amounts of data. A potential solution is to distribute data on different
machines and let them process data samples simultaneously. As the heavy calculation is distributed
to multiple machines and executed parallel, the training process is significantly accelerated.
Large Model Support .Large-scale deep learning has shown its effectiveness in many areas and
has gained rapid development. However, for those large models that have billions and trillions of
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 11
DataModel
Machine 3 Machine 2
Machine 1Machine 4
(a) Model Parallelism
Data 1ModelMachine 1Parameter Server
Data 2ModelMachine 2
Data 3ModelMachine 3 (b) Data Parallelism
Fig. 2. Architectures of model parallelism and data parallelism for server-based distributed training.
parameters to be optimized, the resources on a single machine can hardly support the learning
task. Thus, researchers have studied the feasibility of dividing the large model into different parts
and training them in parallel with multiple machines.
3.2 Task Definition
In Section 2.3, we have mentioned that the objective function is: arg min𝑤𝐿(𝑤;𝐷𝑡𝑟𝑎𝑖𝑛)). There are
three key components in the formula: (1) Loss Function 𝐿; (2) Model parameters 𝑤; and (3) Dataset
𝐷𝑡𝑟𝑎𝑖𝑛. For these three components, we determine which of them to be divided:
•Loss Function 𝐿: Since𝐿usually takes only a little storage, it is not necessary to divide it.
•Dataset𝐷𝑡𝑟𝑎𝑖𝑛: If𝐷𝑡𝑟𝑎𝑖𝑛 is large, we can divide and store it on several machines. After machines
generate model updates through local training, the updates are transferred and aggregated
to generate a new global model. This is known as distributed training with data parallelism.
•Model𝑤: If𝑤is large and contains huge amounts of parameters, we can let each machine
process only one part of 𝑤. Intermediate results over parts of the model are transferred
between machines. This is known as distributed training with model parallelism.
Note that both data parallelism and model parallelism let the whole dataset flow through the whole
model, which guarantees the effectiveness of training. We normally do not divide the dataset and the
model at the same time because this may cause incomplete training and result in performance loss.
3.3 Parallel Training Categories
As shown in Fig. 2, there are two complementary architectures for distributed machine learning,
namely data parallelism and model parallelism.
Data Parallelism .In most occasions, a large dataset that contains various kinds of data samples
is very helpful for training a well-performed model. Sometimes the dataset is so large that it cannot
be stored on a single machine. It is also possible that the huge amount of data results in prohibitively
slow training process. According to [ 77], distributed machine learning with data parallelism has
emerged to solve the data storage problem and accelerate the training. In this scheme, the whole
dataset is divided into sub-sets and distributed on machines. Each machine keeps a copy of the
model and trains it based on the locally available part of data. After several iterations of training, the
local models may become quite different from each other. The information is gathered to generate
12 Gu et al.
Storage
Data 1
Data 4Data 3Data 2Mapper 1
Map()
Mapper  2
Map()
Mapper  3
Map()T emp
Map Result 1
Map Result 2
Map Result 3
Map Result 4
Map Result 5
Map Result 6Reducer 1
Reduce()
Reducer 2
Reduce()Storage
Result
Part 1
Result
Part 2
Fig. 3. Process of MapReduce.
an updated global model. This process is called data aggregation. Then, if the performance of the
new global model is still not satisfying, another round of training is started. With data parallelism,
more data is processed simultaneously, which means it speeds up the training.
Model Parallelism .In some special machine learning tasks, the model can be so large that it is
too slow and even not able to be trained and run on a single machine. This problem is particularly
serious in deep learning tasks. Thus, large-scale distributed deep networks are proposed in [ 24]
trying to deal with it. Model parallelism methods are adopted and used to train large models with
billions of parameters. In this scheme, each machine keeps a small part of the whole model. During
training, the data flows through machines in order to be processed by the local sub-models. On
most occasions, every round of training needs the cooperation of all machines. Therefore, this
process should be done sequentially since the inputs of some machines depend on the outputs
of others. In this situation, using a scheduler to manage the training process may be helpful for
solving the dependency between machines. Compared with data parallelism, model parallelism is
more complex and also harder to implement due to the strong cooperation among machines.
3.4 Parallel Communication Frameworks
Parallel communication frameworks help realize parallel training. Without a well-designed commu-
nication scheme, the limitation on the network bandwidth may become a troublesome bottleneck
for the whole system. The frameworks can be divided into the following three kinds: (1) MapRe-
duce/AllReduce ; (2)Parameter Server ; and (3) Data Flow . We briefly introduce their design in
Section 3.4.1 - 3.4.3. Then we give a simple discussion on this topic in Section 3.4.4.
3.4.1 MapReduce/AllReduce.
Design of MapReduce .In MapReduce, the map operation distributes data and tasks to workers
and the reduce operation aggregate all results. The general process of MapReduce is shown in
Fig. 3. To accomplish the distributed computation task, several mappers and reducers are set on
available nodes. Mappers read data from the storage, perform the mapping in parallel, and generate
intermediate results. Reducers then aggregate intermediate results and generate the final result.
Design of AllReduce .One problem of MapReduce is that it takes huge communication costs
to transfer intermediate results to the reducers. To deal with this problem, AllReduce is proposed
and integrated into Message Passing Interface (MPI). In AllReduce, all worker nodes also work as
reducers. Parts of the intermediate results are transferred between workers if necessary. With this
design, the amount of transferred data decreases. The network burden is also relieved since all the
workers’ bandwidths are used during reducing. AllReduce has been realized using many different
topologies [ 76]. Here we use ring topology (Fig. 4) as an example to explain AllReduce. For 𝑘(here
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 13
Result 1.2
Result 1.3
Result 2.1
Result 2.2
Result 2.3Result 3.1
Result 3.2
Result 3.3Reducer 1
1.1
1.2
1.3 + 3.3
1.1 + 2.1
2.33.1
2.2 + 3.2
3.3Result 1.1
2.21.1
1.2 + 2.2 + 3.2
1.3 + 3.3
1.1 + 2.1
1.3 + 2.3 + 3.31.1 + 2.1 + 3.1
2.2 + 3.2
3.32.2
State 0, to do Step 1Reducer 1 Reducer 1
Reducer 2 Reducer 3 Reducer 2 Reducer 2 Reducer 3 Reducer 3
State 1, to do Step 2 State 2, to do Step 3
Fig. 4. Process of Ring AllReduce.
𝑘=3) nodes, we first use (𝑘−1)steps to make each node keeps1
𝑘of the final aggregated result.
Then with another (𝑘−1)ring communication steps, the result on each node can be completed.
Pros/Cons .In machine learning, MapReduce/AllReduce is commonly used to make multiple
machines cooperate with each other. One advantage of MapReduce/AllReduce is that it is easy to
deploy. MapReduce has been applied to server-based distributed training in [ 33,72]. Here, mapping
is to retrieve the global model and generate model updates. Reducing is to aggregate model updates
and update the global model. Baidu brought Ring AllReduce technique to deep learning in 2017 [ 34].
AllReduce has now been supported in TensorFlow. In addition, we can use a third-party library,
called Horovod [ 95], to simplify the implementation of AllReduce in deep learning. One problem of
MapReduce/AllReduce is that it can be easily blocked by stragglers as synchronization is needed.
3.4.2 Parameter Server.
Design of Parameter Server .The parameter server (PS) can be either a single server or a server
cluster that takes charge of the task arrangement but doesn’t do the task by itself. The tasks are
actually done by the workers. Each worker only has to communicate with the central server to
pull or push data and has no need to be aware of other participators. This means one worker’s
computation task is independent of others’ so that asynchronous working is possible. PS is more
robust than MapReduce/AllReduce since it supports asynchronous communication and thus won’t
suffer from the straggler problem. An example of PS has been shown in Fig. 2b.
Development of Parameter Server .The idea of PS came from the parallel Latent Dirichlet Al-
location (LDA) architecture [ 107]. This first-generation PS used Memcached as the storage of
parameters and managed the synchronization of workers. The lack of flexibility and performance is
its main disadvantage. After that, YahooLDA [ 4] and Distbelief [ 24] followed this idea and improved
PS’s design for specific applications. Petuum [ 40] was a more general platform based on YahooLDA
but it placed more constraints on worker threading models. These works are all considered as the
second-generation PS. To build a more robust system, the third-generation PS was proposed and
implemented in [61]. PS has been applied to distributed deep learning tasks by Google in [24].
Pros/Cons .Here, we first introduce the advantages of the third-generation PS: (1) Efficient
Communication: Communication has been optimized for learning tasks to reduce overhead, and
asynchronous communication is supported; (2) Flexible Consistency: The system allows three
consistency models, including sequential consistency, eventual consistency, and bounded delay
consistency; (3) Elastic Scalability: New servers can join without rebooting the whole system; and
(4)Fault Tolerance and Durability: Chain replication is used to backup data entries on servers. The
vector clock design enables a failed node to be quickly recovered to its original working status.
14 Gu et al.
Compared to AllReduce, one disadvantage of PS is that all transferred data must pass through the
server. This can bring huge communication burden to the server. On the other side, the support
for asynchronous training and the elastic scalability are great advantages of PS. Although the PS
architecture is originally designed for server-based machine learning, some of its features such as
efficient communication are also strongly needed in client-based machine learning.
3.4.3 Data Flow.
Design of Data Flow .For distributed machine learning with model parallelism, a specially
designed scheme called data flow can be applied. Unlike the above-mentioned two schemes in
which each node has similar functions for the whole task, in data flow, different parts of the
model are distributed on different machines, so their jobs vary from one to another. The whole
computation process is organized using a directed acyclic graph. Nodes are units of the model,
and edges describe how data flows. If data flows between two units which are stored on different
machines, communication will be taken place. An example of this scheme has been shown in Fig. 2a.
Pros/Cons .The disadvantage of this scheme is that the failure of any machine can cause the
graph to be incomplete and the system can no longer run. If we use redundancy to solve this
problem, backups for every machine is necessary. This may result in an expensive cost. Thus, the
data flow scheme is more suitable for the cooperation among several powerful and stable machines.
3.4.4 Discussion. From these techniques, we can see that server-based distributed training focuses
on the cooperation between powerful machines. Its core idea is using data parallelism or model
parallelism to solve large-scale learning tasks. The different communication schemes are designed
for dealing with the low bandwidth of the local area network compared with the shared memory.
However, in client-based training, the challenges are mainly due to the limited resources and the
unstable network, which is quite different from those of server-based distributed training. Even if
some techniques such as data parallelism and parameter server can be referred to for designing
client-based training, we still have to pay attention to the special limitations in its scenario.
3.5 Distributed Optimization Algorithms
This kind of algorithms focus on how to better manage a large scale of workers and how to use
their resources to solve a huge complex task in a distributed way. A typical solution is to transform
the original hard problem into much easier sub-problems. In this section, we introduce several
distributed optimization algorithms. We list their key features in Table 3.
ADMM .Alternating Direction Method of Multipliers (ADMM) [ 15] makes use of both the
decomposability of Dual Ascent and the superior convergence properties of Method of Multipliers.
Machines distributedly use augmented Lagrangian methods to solve local sub-problems based
on local data and alternately compute some shared variables to solve the global problem. The
idea of this algorithm can be traced back to the mid-1970s and has been used in distributed SVM
training [ 30] in 2010. Although ADMM may take a large number of iterations to converge to high
accuracy, it can usually reach modest accuracy within tens of iterations in practice.
DANE .Distributed Approximate NEwton (DANE) [ 96] is an approximate Newton-like method.
In every iteration, each worker separately takes an approximate Newton step with implicitly using
its local Hessian and makes two rounds of communication. In contrast to ADMM, DANE can
benefit from the fact that sub-problems are often similar in applications of machine learning. DANE
performs well on smooth and strongly convex problems. It is proved that DANE can achieve linear
convergence if the learning rate is close to 1 and the approximation for Hessian is good.
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 15
Table 3. Distributed optimization algorithms for server-based distributed training.
Name Math Tool Objective Convergence Speed Experimental Scale
ADMMDual Decomposition,
Method of MultipliersClosed, Proper,
ConvexReach optimal as iteration number 𝑘→∞ .Tested on 30GB data
with 80 workers.
DANEApproximate
Newton MethodStrongly Convex,
SmoothLinear convergence
for quadratic objectives.Tested on CoverType [25], MNIST,
and ASTRO-PH [45] with 64 workers.
CoCoAUse dual variables to
do efficient merge.Convex,
SmoothConvergence rate is𝑚−1+Θ
𝑚.
(𝑚: number of workers, Θ: convergence
rate of the local optimization method)CoverType with 4 workers.
RCV1 with 8 workers.
ImageNet with 32 workers.
CoCoA+Use dual variables to do
efficient additive merge.ConvexLinear convergence for smooth
convex objectives. Independent
of the number of workers.CoverType with 16 workers.
RCV1 with 16 workers.
Epsilon with 100 workers.
DiSCOInexact Damped
Newton MethodStrongly Convex,
Smooth,
Self-ConcordantLinear convergence.
Communication rounds 𝑡≈𝑂((𝑚𝑑)1
4log(1
𝜖)).
(𝑚: number of workers, 𝑑: number of features)Tested on CoverType, RCV1,
and News20 [87] with 64 workers.
HydraRandomized Coordinate
Descent MethodStrongly Convex,
SmoothRoughly linear convergence.
Reach𝜖-accurate with possibility at least
(1−𝜌) after𝑂(𝑑
𝑚log(1
𝜖𝜌))iterations.
(𝑚: number of workers, 𝑑: number of features)Speedup tested on 3TB data with
512 workers. Convergence tested on
ASTRO-PH with 32 workers.
CoCoA .Communication-efficient distributed dual Coordinate Ascent (CoCoA) was first proposed
in 2014 [ 43]. By making use of convex duality, after dividing the task into approximate local sub-
problems, we can choose whether to solve the primal sub-problem or the dual problem. The main
advantage of CoCoA is its flexibility. It allows machines to choose an local optimization method
and train the model to arbitrary accuracy. What’s more, the trade-off between local computation
and communication can also be tuned easily by setting a specific parameter. Experiments show
that CoCoA can achieve up to a 50×speedup when dealing with problems like SVM, logistic
regression, and lasso. Note that CoCoA [ 43] was improved to a more general case CoCoA’ [ 106]
in 2017 (CoCoA [ 43] and CoCoA+ [ 66] are predecessors of CoCoA’ [ 106].). The convergence for
non-smooth or non-strongly convex objectives has been analyzed for CoCoA’.
CoCoA+ .CoCoA+ [ 66] also makes use of the primal-dual problem to get optimization. Compared
with CoCoA, CoCoA+ additionally studies and proves the convergence on non-smooth loss functions.
Linear convergence is proved for convex smooth objectives, while sub-linear convergence is proved
for convex non-smooth objectives. Furthermore, to get rid of the slowdown caused by averaging
updates, CoCoA+ choose to add all updates. Experiments show that CoCoA+ only slows down a
little as the number of workers increases, and it is faster than CoCoA for a large number of workers.
DiSCO .Distributed Self-Concordant Optimization (DiSCO) [ 124] is a Newton-type method.
Compared with DANE, DISCO uses a distributed preconditioned conjugate gradient method to
compute inexact Newton steps in each iteration and gets a superior communication efficiency. One
significant advantage of DiSCO is that compared with other algorithms, it has fewer parameters
to be paid attention and adjusted. According to the experiments, when the number of workers
increases to 16 and 64, DiSCO significantly outperforms ADMM and DANE on the convergence
speed. DiSCO-S [ 67] is an improved version of DiSCO. It uses an approximated Hessian as its
preconditioning matrix and uses Woodbury Formula to deal with the linear system more efficiently.
Hydra .HYbriD cooRdinAte (Hydra) [ 88] is a randomized coordinate descent method. This
kind of methods are becoming popular in many learning tasks such as boosting and large-scale
regression. In Hydra’s design, the original data are partitioned and assigned to one node from
16 Gu et al.
Table 4. Applicability of distributed optimization algorithms to client-based training.
Name Local ComplexityComm. Overhead
(per worker per round)Scalability Other Pro(s)/Con(s)
ADMMA serially solvable
convex problem.AllReduce(model)Not theoretically analyzed.
Perform well in experiments.Tested with numerical experiments.
Implemented in C using MPI [108]. !
DANE Do mirror descent. 3·Sizeof(model)Convergence rate is independent
of the number of workers. !Perform well only for
quadratic objectives. %
CoCoADepend on local dual
optimization method.2·Numof(features) !Slow down as the number
of workers grows. %Allow steering the trade-off between
communication and local computation. !
CoCoA+Depend on local dual
optimization method.2·Numof(features) !Convergence rate is independent
of the number of workers. !Allow steering the trade-off between
communication and local computation. !
DiSCOCompute gradients
and Hessians.Numof(features)2!Slow down as the number
of workers grows. %Require self-concordant objectives. %
Hydra Do coordinate decent. Not clearly analyzed. %Speed up as the number
of workers grows.!Partition dataset by features,
require redistribution of data. %
the cluster of machines. Each node independently updates a random subset of its data based on a
designed closed-form formula in each iteration. The updates are all parallelized.
3.6 Applicability of Distributed Optimization Algorithms to Client-Based Training
Compared with server-based distributed training, client-based distributed training is harder to
implement due to the large number of workers and the relatively limited resources. Thus, some dis-
tributed optimization algorithms designed for server-based training may no longer be applicable to
client-based training. In what follows, we discuss the applicability of existing distributed optimiza-
tion algorithms to client-based training. As shown in Table 4, we investigate applicability mainly
from three aspects: (1) Local Complexity ; (2)Communication Overhead ; and (3) Scalability .
They are key concerns in the context of client-based training. We also list some other pros and
cons of these distributed optimization algorithms.
Local Complexity .Limited by the relatively poor computation resources (CPU and memory)
available on mobile devices, client-based training is very sensitive to the local complexity of
algorithms. However, in server-based training, local complexity has not been paid much attention
as the worker nodes are all regarded as powerful machines. We can hardly find accurate local
complexity information in the above-mentioned distributed optimization work. Here, in Table 4,
we just list the local computation methods which may reflect the local complexity to some degree.
In ADMM, DANE, DiSCO, and Hydra, the local optimization step is fixed. However, CoCoA and
CoCoA+ allow multiple choices on the local solver (Please refer to [ 106] for detailed suggestions.),
which means we can choose an appropriate local solver according to workers’ available resources.
This design is useful in client-based training since it improves the algorithm’s compatibility for het-
erogeneous mobile devices. Thus, regarding the local complexity, CoCoA and CoCoA+ outperform
the others.
Communication Overhead .Communication overhead is another important issue in client-
based training. Considering the complex network condition of mobile devices, high communication
overhead not only brings expensive data transfer cost, but also increases the risk of transfer failure.
Communication overhead can be further divided into the number of communication rounds and
the amount of data transferred in each round. The number of communication rounds is reflected by
the convergence speed listed in Table 3. Except for ADMM which requires large number of rounds
to converge, the other algorithms can all achieve linear convergence.
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 17
The amount of transferred data per worker per round is listed in the Communication Overhead
column in Table 4. In ADMM, the communication step is accomplished by using AllReduce to
update the model parameters. In DANE, the amount of data transferred per round is triple as much
as the size of the model. Since the size of the model is not accurately provided, we cannot get the
accurate communication overhead of ADMM and DANE. In CoCoA and CoCoA+, the amount of
data transferred by each worker in each round is twice as much as the number of the features. They
are quite communication-efficient when dealing with tasks with small feature space. In DiSCO, the
amount of data transferred in each round is square as much as the number of features, which may
also be acceptable. Although several communication schemes have been provided in Hydra, the
amount of transferred data has not been clearly analyzed. Regarding communication overhead,
CoCoA, CoCoA+, and DiSCO outperform the other algorithms.
Scalability .Good scalability means we can assign the task to more workers and make use
of more parallel resources. We compare the scalability of distributed optimization algorithms
by checking if their convergence speed is affected by the number of workers. Results are listed
in Table 4.
In ADMM, although the scaled version of the algorithm is given, the scalability is only discussed
for the scale of the datasets. Whether the scale of workers has an impact on ADMM is not theoreti-
cally analyzed. We could only say that the performance of ADMM is not greatly changed as the
number of workers grows according to the comparison experiments between DANE and ADMM
done in [ 96]. In DiSCO and CoCoA, according to Table 3, the increase in the number of workers
causes the slowdown of the convergence speed. This indicates that the scalability of DiSCO and
CoCoA is not so good. In DANE and CoCoA+, the convergence rate is independent of the number
of workers. In Hydra, according to Table 3, since it can reach 𝜖-accurate with possibility at least
(1−𝜌) after𝑂(𝑑
𝑚log(1
𝜖𝜌))iterations, the number of iterations actually decreases as the number
of workers𝑚increases. This means Hydra speeds up as the scale of workers grows. Regarding
scalability, DANE, CoCoA+, and Hydra outperform the other algorithms.
Other Pros/Cons .Besides the above three key aspects which are mostly concerned by client-
based training, these distributed algorithms also have some other pros/cons that should be incorpo-
rated for evaluating their applicability to client-based training. Details are presented in Table 4.
DANE and DiSCO require strict assumptions on the objective function. Hydra partitions training
data by features, which means it may require a data redistribution step. Thus, DANE, DiSCO, and
Hydra have additional cons. ADMM has been implemented using MPI in [ 108] and thus is easier to
be applied. CoCoA and CoCoA+ allow steering the trade-off between communication and local
computation, which gives us the freedom to change the training strategy according to available
resources. For this part, we could say that ADMM, CoCoA, and CoCoA+ outperform the others.
Another problem is that all these distributed optimization algorithms require a synchronous
update aggregation process in each communication round. Since the heterogeneity of mobile
devices can easily cause the straggler problem, the synchronization may significantly degrade the
system efficiency. This disadvantage should be carefully considered when applying distributed
optimization algorithms to client-based training. Considering that some optimizers introduced in
Section 2.5 have already supported the asynchronous scheme, we can refer to them and design new
asynchronous distributed optimization algorithms for client-based training.
4 CLIENT-BASED INFERENCE
In contrast to server-based machine learning where all operations over the model (including training
and inference) are conducted on servers, client-based machine learning intends to pull down some
18 Gu et al.
computation tasks to clients so that the quality of service can be improved in terms of response
latency, personalization, security privacy, and so on. Based on the execution phase, we divide
client-based machine learning into client-based inference and client-based training. Client-based
inference focuses on the local execution of trained machine learning models. In this section, we
discuss the necessity and the feasibility of client-based inference.
4.1 Motivations
Motivations for client-based inference can be concluded as follows: (1) Client-based inference can
reduce service latency and preserve user privacy; (2) Client-based inference helps lower the cost of
cloud platforms for service producers; (3) It is feasible to deploy client-based inference thanks to
the rapid development of mobile chipsets; and (4) It is convenient to deploy client-based inference
using the off-the-shelf mobile machine learning frameworks.
4.2 Challenges
One major challenge is that (1) inference tasks using complex or large-scale models can
hardly be solved on mobile devices. Even though the power of mobile chipsets has grown fast,
it is still far away from that of high-performance servers. The CPU power and the memory capacity
of mobile devices may have trouble supporting complex models, especially deep learning models.
Another challenge is that (2) the trade-off between inference accuracy and resource con-
sumption needs to be carefully decided. This is a fine-grained resource control problem. As
client-based inference aims at improving the quality of service, we should never let client-based
inference take too much resources or drain the battery. Considering that multiple models may be
run simultaneously to provide different services, the trade-off between inference accuracy and
resource consumption must be carefully decided to prevent resource contention.
4.3 Current Advances
In the past few years, deep learning has become the main trend of machine learning. Mobile phones,
as the most popular device in people’s daily life, becomes the most promising platform for deep
learning. Ravi [ 85] has proposed a method to generate compact on-device neural networks, which
brings convenience to the deployment of client-based deep learning inference. Deep learning based
apps such as [ 57,73,122] have been built. General mobile deep learning frameworks [ 7,10,28,
35,44,110] have been developed. Here, we introduce several mobile deep learning applications to
show the current development of client-based inference.
Mobile Computer Vision (CV) .According to [ 117], photo beautify and face recognition together
cover over a half of all mobile deep learning applications. For CV applications, server-based inference
can bring high privacy risk as the raw sensitive images and videos have to be transferred through
network. To solve this problem, DeepEye [ 68] and DeepMon [ 41] were proposed to support the
on-device execution of deep vision models. Another problem is that the huge latency incurred by
server-based inference may be unacceptable for applications such as mobile augmented reality.
As a solution, DeepDecision [ 84] designs an on-device small deep learning model tiny-YOLO and
automatically decides the inference to be done locally or remotely. By using optional client-based
inference, the real-time capability of the system is guaranteed.
Mobile Natural Language Processing (NLP) .Another hot topic is to solve NLP tasks such
as sentiment analysis, translation, and question answering on mobile devices. To better preserve
users’ privacy as well as reduce the service latency, DeQA [ 18] adapts existing question answering
systems to suit on-device running. Client-based inference is also applied to voice assistant and
voice input applications to ensure that they can still function normally even the device is offline.
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 19
Siri, an voice assistant developed by Apple, uses on-device deep learning [ 19,104] to improve the
text-to-speech synthesis process. Google makes use of client-based inference to realize personalized
speech recognition [69]. Georgiev et al. [32] use mobile GPU to accelerate mobile audio sensing.
Other Applications .Client-based inference also plays an important role in other applications
that require real-time responses or involve sensitive data processing. Liu et al. [ 102] designed
UbiEar to do smartphone-based acoustic event sensing and notification. This work greatly improves
the quality of life for Deaf or Hard-of-Hearing (DHH) people. Fang et al. [ 29] designed NestDNN
to dynamically select the structure of on-device deep learning CV models based on the trade-off
between accuracy and current available resources. In addition to these published works, client-
based inference are also applied to various kinds of applications (e.g., recommendation, movement
tracking, and identity recognition) without being noticed [ 117]. It is certain that people’s daily lives
have already been deeply permeated and improved by client-based inference techniques.
5 CLIENT-BASED TRAINING
Client-based training is mainly motivated by the strong need of making the best use of user data
generated on mobile devices and protecting users’ privacy at the same time. Since we can hardly
find any work and research paper about client-based local training, for the rest of this survey paper,
client-based distributed training is referred to as client-based training for convenience. In contrast
to traditional machine learning that require centralized datasets, client-based training uses mobile
devices to solve machine learning problems according to local user data. The server aggregates all
intermediate results and gets the trained model. Generally speaking, client-based training aims to
transfer some computation tasks from centralized servers to decentralized mobile devices. It can
not only offload servers’ burden but also make use of the growing processing power on mobile
devices. Furthermore, considering that data is processed locally in client-based training, the user
data that is either too much to be uploaded or too sensitive to be uploaded can now participate in
machine learning tasks. This gives possibility to improve the model’s accuracy and accelerate the
training process. More importantly, besides the benefits mentioned above, client-based training can
even better preserve users’ privacy. In this section, we first introduce and explain the motivations
of client-based training in Section 5.1. We formally define the task in Section 5.2. Then we discuss
the general constraints in Section 5.3. The main challenges faced by client-based training will be
claimed in Section 5.4. Finally, we use federated learning and split learning as examples to show
the current development of client-based training in Section 5.5 and Section 5.6.
5.1 Motivations
We conclude motivations for client-based training as follows: (1) Client-based training keeps all
advantages of client-based inference as client-based inference can be viewed as part of it; (2)
In client-based training, the on-device model has a much shorter update cycle compared with
server-based training and thus may perform better; (3) Client-based training is able to preserve
user privacy and make full use of on-device sensitive user data at the same time; and (4) Some work
in this direction has shown the feasibility and effectiveness of client-based training.
One problem faced by server-based training is that the cost (communication cost and computation
cost) is high. In server-based training, user data is generated on users’ devices and then collected.
However, in many cases, the data is so fine-grained and much that it incurs huge communication
overhead when being transferred to the server. In addition, processing huge amounts of data and
running large-scale machine learning on the server is not only time-consuming but also costly. Even
if the model is finally trained to good performance on the server, the long training cycle will result in
the delay of the model, which can cause decrease in its performance as users’ behavior patterns may
20 Gu et al.
Model
Model DataProcessorPrediction
(a) Client-Based Inference
Model
Model DataProcessorModel’
 Model’ (b) Client-Based Training
Fig. 5. Illustrations of inference and distributed training in client-based machine learning.
have already changed. Another problem in server-based training is the privacy risk. Specifically,
the server in both centralized and distributed machine learning frameworks requires direct access
to training data and thus need to store raw user data, which inevitably suffers outsider and insider
attacks [ 79–81]. For example, a malicious hacker may invade the datacenter, compromise part of
the server, and leak private databases. Further, if the server is untrusted, it may share user data
with unauthorized entities or even trade for profits. To address these problems and disadvantages
of server-based training, attempts on client-based training has been made.
The difference between client-based inference and client-based training is shown in Fig. 5. From
the figure we can see that the client-based inference process just returns a prediction result which
may be used to produce a service for the user locally. It has nothing to do for the server. However,
if some part of the training process can be additionally done on-device, the generated trained
model can not only become personalized but also provide valuable information for the global
model stored on the server if the update of the local model is uploaded. This is exactly the idea of
client-based training. As an example of client-based training, federated learning has already shown
its effectiveness, which will be introduced later in Section 5.5. Besides federated learning, some
other works that are closely related to client-based training have also been proposed. He et al. [ 38]
designed a framework for decentralized on-device linear learning. Koloskova et al. [ 51,52] further
studied decentralized deep learning with compressed communication. They together show us the
feasibility of on-device client-based training.
5.2 Task Definition
We first introduce some notations and definitions in client-based training: (1) A real matrix W∈
R𝑑1×𝑑2is the model learned from decentralized data; (2) A list {𝐶1,𝐶2,...,𝐶 𝑚}contains all𝑚clients
that are run on different mobile devices. They are also known as the workers in client-based
training; (3) A list{𝐷1,𝐷2,...,𝐷 𝑚}contains all local datasets used by the clients; and (4) A server
𝑆that communicates with clients and arranges their tasks. 𝑆can either be a single computer or a
server cluster whose structure is transparent to the clients.
5.2.1 Workflow. As shown in Fig. 6, a general workflow of client-based training consists of five
phases: (1) Initialization: The workers{𝐶1,𝐶2,...,𝐶 𝑚}run on mobile devices download hyperpa-
rameters from the server 𝑆and get themselves initialized; (2) Distribution: The server𝑆arranges
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 21
3. Local  Training
(•Machine Learning Optimizers)
4. Uploading
1. Initialization (Hyperparameters)
5. Aggregation
(Data Aggregation Methods)
2. Distribution
(•Distributed Optimization Algorithms)
𝑪𝑪𝟏𝟏𝑪𝑪𝟐𝟐𝑪𝑪𝒎𝒎
𝑪𝑪𝟏𝟏𝑪𝑪𝟐𝟐𝑪𝑪𝒎𝒎 𝑺𝑺𝑺𝑺𝑾𝑾 𝑫𝑫𝟏𝟏 𝑫𝑫𝟐𝟐𝑫𝑫𝒎𝒎
𝑫𝑫𝟏𝟏 𝑫𝑫𝟐𝟐𝑫𝑫𝒎𝒎
Fig. 6. Workflow of client-based training.
the tasks and sends them to the workers {𝐶1,𝐶2,...,𝐶 𝑚}. The model to be trained should also be
distributed to the workers in this step; (3) Local Training: The workers{𝐶1,𝐶2,...,𝐶 𝑚}train the
local models based on the local datasets {𝐷1,𝐷2,...,𝐷 𝑚}. Updates will be generated after a certain
number of local training iterations; (4) Uploading: The workers{𝐶1,𝐶2,...,𝐶 𝑚}upload the updates
to the server 𝑆; and (5) Aggregation: The server𝑆aggregates all updates to generate a final update
and applies it to the global model W. A single round of learning ends here. Go back to step 2 to start
a new round of learning. A similar workflow has been adopted in federated learning protocol [ 12].
5.2.2 System Modules. To accomplish client-based training, the system is made up of three modules:
local training module, communication module, and aggregation module. We show how they
cooperate with each other and what kinds of techniques can be used to improve them.
The local training module is implemented on the worker. It works in step 3. Besides the data
preprocessing part, the core component of the local training module is the local machine learning
optimizer. Considering that the resources available for client-based training are very limited, the
local optimizer must be chosen and designed carefully. It should be run on mobile devices without
significant bad effect on users’ daily usage experience. For the local training module, the input is
the initialized model and the output is the update. The update can be generated based on either the
new model which has been trained on the local dataset or the difference between the two models.
The Communication module takes part in step 1, step 2 and step 4. It is implemented on both the
server and the worker, which means the server and the worker have to cooperate with each other
for communication. The communication module is responsible for two jobs: task arrangement
and data transfer. For the server, it should concern about the task arrangement part whose main
purpose is to divide the original complicated problem into easier sub-problems. This can be done
through distributed optimization algorithms. The input is the original problem and the output is a
set of sub-problems. From the worker’s perspective, data transfer is the main challenge as mobile
network resources are limited and expensive. Compression methods can be used to reduce the
communication cost. Here, the input is the original data and the output is the compressed data.
The aggregation module is implemented on the server. It works in step 5. Since the number of
updates received from workers can be very large, it is inefficient and even impossible to apply all
updates to the global model one by one. Thus, the main job of the data aggregation module is to
extract and make the best use of the information contained in the updates. It takes all received
updates as inputs. Its output can be either a final update which can be directly applied to the global
model or a new model which replaces the old one.
22 Gu et al.
5.3 Constraints
The only difference between server-based training and client-based training is where the training
process is done. However, this change introduces additional constraints for the whole system due
to the limitations on mobile devices: (1) Compared with PCs and servers, the hardware resources
available on mobile devices are poor and limited; (2) The network condition for mobile devices is
unstable, and the communication cost is very high; and (3) Unlike servers which are owned and
managed by data centers, mobile devices are not totally under control, thus the reliability and the
security of the whole decentralized training process are not guaranteed.
The first limitation is similar to the first challenge of client-based inference as both of them
require plenty of computation. The second limitation is introduced considering that client-based
training additionally needs the communication process to accomplish distributed training. The
third limitation indicates that client-based training is more vulnerable to attacks. In what follows,
we show how these three limitations lead to six concrete constraints of client-based training.
Simple On-Device Task .The machine learning sub-problems solved on mobile devices should
be easy. This constraint is desired according to the first limitation. Although the development of
mobile devices has kept accelerating in recent years, their computation power is still far less than
that of personal computers, not to mention the server clusters. The key reason for this phenomenon
is that mobile devices need to be portable. In this situation, chipsets are specially designed for
mobile devices to balance between the need for lower energy consumption and the need for greater
power. Thus, unlike the CPUs used on personal computers which can easily reach 100W in power
with stable power supply, the peak power of CPUs used on mobile devices is usually under 10W.
Considering that mobile devices have to spare some resources and energy to support its basic
functions, the part which can be used for supporting mobile machine learning becomes even
less. Moreover, the mobile devices usually have no additional cooler and just use passive cooling
methods. This also limits the maximum power of them. Considering the facts given above, the
necessity of the simple on-device task constraint has been clear. First, mobile devices cannot handle
complex machine learning tasks because the power is limited. Second, since only passive cooling
methods are used, a hard machine learning task is very likely to cause the mobile device becomes
hot. This must not happen as it has huge negative effect on user experience. Therefore, the machine
learning sub-problems solved on mobile devices should be easy enough so that the users are even
unaware of its running at all.
Short Training Time .The time spent on one round of local on-device training should be
short. We have mentioned that the hardware resources available on mobile devices are poor and
limited compared with personal computers. Thus, the systems run on mobile devices are specially
designed. Background apps may be paused by the system to save resources for the foreground app.
Considering that most users just use an app for a little while at one time, it is better to finish the
training task during the time that the app is running in the foreground. Otherwise, if the training
has not been finished when the app is paused or quit, the generated update may not be sent to the
server in time and become useless at the next start due to the long delay.
No Uploading .Local private user data should not be uploaded to the server in principle. For
now, although the 4G network has already been deployed widely and the 5G network is coming,
the cost of data on mobile devices is still expensive. What’s more, local data on mobile devices may
contain sensitive user information, which means it is not suitable for being uploaded to the server.
Thus, to best preserve user privacy, the local raw user data should not be uploaded in principle.
However, considering that uploading a small subset of local data is feasible and may be helpful for
the aggregation process on the server, it can be permitted under special situations with guaranteed
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 23
privacy. Note that No Uploading is one important cause of the significant difference between server-
based distributed training and client-based training. Without uploading data, the local datasets used
for training are specific to their owners and can never become Independent and Identically Dis-
tributed (IID) which is a necessary condition for many machine learning algorithms. Moreover, No
Uploading also causes the sizes of local datasets to be unbalanced, and results in the information
contained in different workers’ updates being unbalanced. Hence, how to determine which updates
are more valuable (contains more information) becomes another problem to be solved.
Low Communication Overhead .The data transferred in one round of learning should not
be too much. This constraint also aims at protecting the experience of users’ daily usage. If too
much data has to be transferred, it may be a heavy burden for the mobile devices even if they are
connected to WLANs and can cause other apps that are using the network to be blocked.
Low Communication Frequency .The communication frequency should be low and it is better
to let the workers decide when to communicate. As the network condition for mobile devices is
unstable, a scheme with frequent communication is unsuitable. What’s more, a low communication
frequency with a long training time is beneficial for reducing the communication cost. The reason
why we suggest letting the workers decide when to make the communication is that the high
heterogeneity of workers makes the server have no idea when the sub-problems will be solved. By
letting workers make the decision, we may also reduce the communication cost as workers can
choose to do the communication at the time when they are connected to WLAN.
High Awareness of Data Reliability .The server should be highly aware of the data reliability
of the received updates. Since the decentralized training scheme cannot guarantee the reliability
of the received data, client-based training suffers from fake updates or low-quality updates. To
prevent machine learning poisoning attacks, a possible solution is to let the server evaluate the data
reliability by making use of update diversity or additional information such as worker reputation.
With high awareness of data reliability, the server can distinguish and reject low-quality updates
and make the training process become more secure and stable.
5.4 Challenges
From the above six constraints, we can summarize the major challenges of client-based training:
(1) Transforming original machine learning tasks to easier sub-problems which can be solved on
mobile devices effectively and efficiently; (2) Dealing with the unbalanced non-IID dataset and
cover its negative effects; (3) Compressing the transferred data between the server and the clients;
(4) Lowering communication frequency and overhead; and (5) Ensuring the efficiency and security
of data reliability evaluation.
For challenge (1), although we can refer to distributed optimization algorithms, none of them
guarantees that it performs well with millions or even billions of workers. The huge number of
workers and the limited resources available on workers may cause the optimization problem to
become so difficult that we must design a brand new solution for it.
For challenge (2), some existing methods (e.g., [ 125]) use data-sharing strategies to mitigate
the negative effect of the unbalanced non-IID dataset. The problem of data-sharing strategies is
that raw data transfer can be costly and risky for mobile devices. Another possible solution is to
design new machine learning algorithms which are insensitive to the data distribution. For example,
semi-cyclic SGD [3] can adapt to data with block-cyclic pattern.
Regarding challenge (3) and (4), they have already been widely studied in other areas (e.g., data
compression and computer network). However, considering that general methods normally may
24 Gu et al.
not well adapt to every scenario, specific algorithms that are closely combined with client-based
training must be more effective and thus are needed urgently.
For challenge (5), to prevent model poisoning attacks [9, 31, 98], some existing studies [31] use
update diversity to distinguish low-reliability updates. Some others [ 48,49] choose to calculate the
reputation of workers and use the reputation score to represent the data reliability. Techniques
such as blockchain have been used to achieve secure reputation management. Although these
methods can effectively protect the global model training process from the attacks, they incur large
computational cost as the number of workers increases. Thus, the main task here is to design an
algorithm which can guarantee both security and efficiency.
To better learn about these challenges and their possible solutions, we use federated learning
and split learning as two examples and introduce their current advances.
5.5 Current Advances in Federated Learning
In recent years, attempts at the implementation of client-based training have occurred. Federated
Learning, as an example, was proposed by researchers from Google in 2015. Generally speaking,
federated learning means to do machine learning tasks federatedly among a large number of mobile
devices with on-device private data protected. Its core idea is to train the model locally on user
devices and aggregate updates on the server without uploading raw user data, which shares a
similar scheme of distributed machine learning. From 2016 to 2018, Google published several related
articles to complement federated learning’s framework. In 2019, applications of federated learning
appear. A detailed survey [ 47] about federated learning is given in December, 2019. Federated
learning can be viewed as an attempt at client-based training with privacy preservation as the
primary goal. Existing federated learning work mainly focuses on studying how to improve the
final model’s performance and how to design an efficient communication scheme. In this section,
problems and current advances in federated learning are introduced and discussed.
5.5.1 Model Performance.
Literature Review .The idea of federated learning can be traced back to Distributed Selec-
tive Stochastic Gradient Descent (DSSGD) [ 100]. It was published in October 2015 and is about
distributed deep learning without sharing datasets.
Following the idea of distributed learning and privacy preservation, in November 2015, re-
searchers from Google submitted their first attempt on federated learning [ 54]. Federated learning
can be viewed as an improved version of DSSGD which is optimized for mobile devices. In this work,
three basic properties of federated learning’s scenario were given: (1) Non Independent and Iden-
tically Distributed (non-IID) Data ; (2)Unbalanced Data ; and (3) Massively Distributed Data . (Note
that the fourth property (4) Limited Communication was introduced in [ 70] and will be discussed
in Section 5.5.2.) Also, this work proposed an efficient federated optimization algorithm called
Distributed Stochastic Variance Reduced Gradient (DSVRG) based on SVRG [46] and DANE [96].
Later in 2016, Konečn `y et al. [ 55] complemented and improved the above work. DSVRG was
renamed as Federated SVRG (FSVRG). Equations and mathematical proofs for it were also provided.
After the above works had formulated the basic training scheme of federated learning, researchers
continued to study the effectiveness of federated learning and tried to improve its performance.
Zhao et al. [ 125] analyzed the negative effect of non-IID datasets on model performance and
provided a simple data sharing strategy to deal with it. Yu et al. [ 121] studied on why model
averaging works for deep learning tasks. Eichner et al. [ 27] discovered that cyclic patterns in the
data samples is hard to be avoided in federated learning and does harm to the performance of
SGD. They proposed Semi-Cyclic SGD to correct this problem when optimizing convex objectives.
Mohri et al. [ 74] proposed a new framework of agnostic federated learning to avoid the federated
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 25
model being biased towards different clients. Chen et al. proposed FedMeta [ 21] which combines
federated learning with meta-learning. To better evaluate various kinds of federated learning
algorithms, LEAF [ 17], a modular benchmarking framework for learning in federated settings, was
proposed. It contains open-source federated datasets and is continuously updated.
Discussion .While federated leaning training algorithms have developed from DSSGD to FSVRG,
the most important non-IID data problem has not been solved well yet. What’s more, the cyclic
pattern of data is also harmful and should be taken into consideration. In conclusion, to further
improve the model performance of federated learning, we need to deal with the cyclic unbalanced
non-IID data problem under the non-convex objective condition.
5.5.2 Communication Efficiency.
Literature Review .The first aspect of improving communication efficiency is reducing commu-
nication rounds. In [ 70], Federated Averaging (FedAvg) was proposed to deal with the fourth basic
characteristics of federated learning Limited Communication . By enabling multiple local training
iterations and using model averaging methods, the number of communication rounds is reduced.
Similarly, based on parallel restarted SGD [ 121], Yu et al. [ 120] proposed parallel restarted SGD
with momentum to enlarge local training steps and thus reduce the total number of rounds.
Meanwhile, reducing the size of communication data also helps improve communication effi-
ciency. According to [ 56], as on most occasions the bandwidth of the uplink is much poorer than
that of the downlink, reducing the uplink communication cost is a more urgent task. Two kinds of
approaches ( Structured Updates andSketched Updates ) which can lower the size of the updates are
implemented and tested in federated learning in [ 56]. Besides the general compression methods,
Caldas et al. [ 16] proposed Federated Dropout. With Federated Dropout, each worker trains a
smaller sub-model instead of the whole global model. Then the size of updates is also reduced.
Discussion .Although the above-mentioned methods do improve the communication efficiency,
they are still using the two-tier server-worker communication structure. This scheme not only
brings huge communication burden to the central server, but also suffers from the instability of
mobile workers’ network. Perhaps we can consider using a multi-tier communication structure to
further improve both communication efficiency and communication stability.
5.5.3 Security & Privacy.
Literature Review .After the development of FedAvg [ 70], to better ensure the security of
the aggregation process in federated learning, Google proposed a practical secure aggregation
method [ 13] for privacy-preserving machine learning. In the secure aggregation process, the secure
multiparty computation technique is used to compute sums of model parameter updates. What’s
more, model poisoning attacks [ 9,31,98] toward federated learning have also been studied. The
main purpose of model poisoning is to make the trained model output wrong answers or even
attacker-chosen answers. To prevent model poisoning attacks, update diversity [ 31] and worker
reputation [48, 49] can be used to recognize unreliable updates.
For better privacy preservation, McMahan et al. [ 71] applied differential privacy methods to
FedAvg and only resulted in a negligible cost in inference accuracy. Agarwal et al. [ 3] proposed
cpSGD to achieve both differential privacy and communication efficiency in federated learning
settings. Niu et al. [ 20] proposed a secure federated submodel learning scheme with tunable property
which enables the workers to tune privacy and utility. Considering that the maximum contribution
is an important parameter for differential privacy algorithms (the noise to be added to data is closely
related to it), Amin et al. [ 5] studied how to bound user contributions in federated learning. Yang et
al. [23] proposed a novel lossless privacy-preserving tree-boosting system called SecureBoost.
26 Gu et al.
Discussion .Regarding better privacy, although federated learning can naturally protect sensitive
raw user data, we still need to make sure that the attacker cannot infer information from the
transferred updates. For the security part, since federated learning distributes the training process
to unreliable mobile devices, attacks which aim at misleading the final model can be more easily
implemented. For example, we should pay more attention to the potential data poisoning attacks.
5.5.4 Applications. Federated learning has already been used in some applications and shown
satisfying performance. Google first applied federated learning to Gboard to improve its query
suggestions [ 119]. They tested federated learning on 100 clients. Results shew that federated learning
do improve the performance of the deployed LSTM model. After that, they also applied federated
learning to the mobile keyboard next-word prediction task [ 37]. According to the evaluation
done on server-hosted logs data, the federated-trained CIFG model performs nearly as well as the
centralized-trained CIFG model. And for evaluation done on client-owned data caches, federated
learning even outperforms centralized learning. Google has also applied federated learning to learn
Out-Of-Vocabulary (OOV) words [ 22]. This work conducted both simulated federated learning on
a non-IID dataset and real-word federated learning on data hosted on user mobile devices. Results
shew that the federated learning method can learn OOV words effectively. Google introduced their
system design for federated learning at scale in [ 12], which mainly focus on how to design an
elastic parameter server to support large number of clients in real-word settings.
Besides Google, many other researchers were also exploring federated learning. Smith et al. [ 105]
combined federated learning with multi-task learning. Intel [ 99] used federated learning to do
multi-institutional deep learning for brain tumor segmentation without sharing patient data. Yang et
al. [64] proposed Federated Transfer Learning (FTL). This work shew that federated learning is
also suitable for being applied to machine learning tasks in the scenario of cooperation among
banks where sensitive information mustn’t be shared. Felix et al. [ 93] proposed Sparse Ternary
Compression (STC). STC is more robust to non-IID datasets and works as a substitute for FedAvg.
5.5.5 Surveys. Google has published a very detailed survey paper [ 47] to summarize all works
and researches related to federated learning. Advances of federated learning and open problems in
this area have also been discussed. Yang et al. provided a survey [ 118] about federated learning’s
concept and applications. In this survey paper, Federated Learning is extended and classified into
Horizontal Federated Learning (HFL) and Vertical Federated Learning (VFL). Li et al. [ 62] write a
survey to discuss challenges, methods, and future directions of federated learning.
5.6 Current Advances on Split Learning
Just like federated learning can be viewed as a special case of data parallel distributed learning
where datasets are distributed on mobile workers, split learning can be considered as a special
case of model parallel distributed learning where the model is distributed on the server and mobile
workers. Here, we continue to introduce current advances in split learning.
5.6.1 Model Performance.
Literature Review .The idea of split learning first appeared in [ 36] and was proposed by a
research team from MIT. It is motivated by the need of using multiple agents to collaboratively
train a deep neural network without transferring raw sensitive user data, which is very similar
to the motivation of federated learning. To achieve this goal, Split Neural Network (SplitNN) was
designed. In SplitNN, only bottom parts of the model are trained on an worker who owns the raw
data. The gradients together with the labels for the training data are transmitted to the server who
accomplishes the rest training process for the top parts of the model. Considering that the labels
may also reveal sensitive user information, U-shaped SplitNN is designed. In U-shaped SplitNN,
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 27
the server only processes the middle layers of the model. The bottoms layers and the top layers are
stored on workers to process the raw data and the corresponding labels. Experiments have been
done to demonstrate the effectiveness of SplitNN.
Later, the concept of split learning was formally proposed in [ 113]. SplitNN, as a method of split
learning, has been improved in this work. Several possible privacy-preserving structures of SplitNN
have been provided and discussed. We will introduce this part later in Section 5.6.3. By comparing
SplitNN with large-scale SGD and FedAvg on CIFAR 10 and CIFAR 100, it is shown that SplitNN
can achieve higher validation accuracy with much less computation.
Discussion .In SplitNN, although a training algorithm using multiple workers have been de-
signed, the whole training process is still run sequentially. According to the algorithm, a worker
must fetch the latest model before it starts training. After a training iteration, this worker’s updated
model will be marked as the latest model. This scheme implies that only one worker can train
the latest model at the same time. Although Singh et al. [ 103] proposed an approach called “split
learning without any client weight synchronization”, how it works and how it performs are not
described. On the other hand, since there is no model aggregation process, multiple workers training
the latest model in parallel will result in conflicts. This lack of parallel training scheme makes
SplitNN fail to get benefit from parallel training acceleration. Moreover, while local raw data is not
transferred in split learning, the impact of local non-IID dataset has not been analyzed yet.
5.6.2 Communication Efficiency.
Literature Review .A detailed comparison of communication efficiency of split learning and
federated learning was given in [ 103]. The theoretical analysis shows that split learning becomes
more communication-efficient as the number of clients increases and can well adapt to big models.
One shortcoming of this work is that the theoretical analysis has not been validated by experiments.
Discussion .The communication efficiency of split learning has not been well studied yet. As
workers need to transfer data with the server in each training iteration, the network condition (e.g.,
bandwidth and latency) can greatly influence the system efficiency. What’s more, the applicability
of communication compression methods needs to be further examined.
5.6.3 Security & Privacy.
Literature Review .Attacks on split learning have been studied in [ 2]. This work shows that it
is possible to reconstruct the raw data from the worker’s outputs if only a few convolution layers
is trained on the workers. In addition, neither introducing additional hidden layers nor applying
differential privacy to the split layer can effectively mitigate this shortcoming. Both of the two
attempts cause an unacceptable decrease in the model accuracy.
Several privacy-preserving structures of SplitNN have been introduced in [ 113]. As multiple
institutions might own different modalities of the same user’s data, vertical split learning is designed
to deal with this kind of data which is partitioned by features. Extended vanilla split learning and
Tor-like multi-hop split learning arrange additional workers to process the middle layers of the
model. This structure helps to cover the identities of the bottom workers who use their sensitive
raw data to train the bottom layers, and thus the privacy of the bottom workers are better preserved
by the anonymity. Besides the basic structure design, Vepakomma et al. [ 112] used two losses in
one model to reduce data leakage for SplitNN. Sharma et al. [ 97] proposed ExpertMatcher to do
model matching for split learning with only the encoded hidden representation of local raw data
shared. By hiding the raw data representation, user privacy is better preserved.
28 Gu et al.
Discussion .One important problem not studied yet is how to trade off between on-device model
complexity and user privacy. By putting some complex layers of the model on workers, the privacy
may be better preserved as it becomes harder to infer raw data. Meanwhile, split learning also
suffers model poisoning attacks. Now that some parts of the model even do not exist on the server,
model poisoning attacks become much easier and should be more carefully handled.
5.6.4 Applications. Split learning has been tested in the medical field in [ 78]. U-shaped SplitNN
has been implemented to enable the collaborative machine learning between several hospitals.
Experiments are done on two medical datasets: retinal fundus photos and chest X-rays. Results
show that split learning outperforms non-collaborative methods greatly.
5.6.5 Surveys. Vepakomma et al. [ 114] have surveyed methods for deep learning without revealing
raw data, including large batch SGD, federated learning, and split learning. Key ideas, limitation,
and future trends of them have been simply discussed. In addition, this survey paper has introduced
several cryptographic techniques which can be used to further preserve privacy in machine learning
area, including homomorphic encryption, oblivious transfer, and garbled circuits.
6 FUTURE DIRECTIONS
In this section, we introduce some potential research directions of client-based training. Most of
them are motivated by the problems and challenges discussed in Section 5 that have not been
studied well (e.g., Non-IID Training Sets). Others are ideas which can improve the robustness or
the adaptivity of the system (e.g., General Mobile Training Framework).
Non-IID Training Sets .This problem was first introduced in federated learning. It also exists
in the scenario of other client-based training methods such as split learning as we can no longer
upload the original data and do a shuffle. Zhao et al. [ 125] has shown the negative effect of non-IID
datasets on model convergence and proposed a data sharing strategy to deal with it. However, it
may be difficult for mobile devices to share a small part of the local dataset with others because the
process is hard to be managed and the communication cost can be high. We also have to ensure
that the privacy is still preserved during the whole data transmission process. Another possible
solution is to develop new machine learning algorithms which are not sensitive to the distribution
of training set. However, this direction does not have much existing work that can be referenced
and may become a new hard machine learning problem.
Aggregation Methods .For existing data aggregation methods designed for server-based dis-
tributed training (e.g., FedAvg [ 70], Ensemble-Compression [ 109], and Codistillation [ 6]), none of
them guarantees a fast convergence. Since the number of workers is usually under one hundred
in server-based distributed training, no one has the experience to aggregate tens of thousands
of updates in a round in client-based training. Moreover, the above-introduced methods all have
disadvantages. The averaging-based methods may cause a significant decrease in the convergence
speed as they reduce the scale of updates on weights. They may also not be suitable for non-convex
problems. The distillation-based methods may not be able to merge tens of thousands of models
because they need much additional computation to handle this complex task. Thus, how to design
an effective, efficient, and robust aggregation method is an urgent problem to be solved.
Security & Privacy .Although client-based training can prevent the leakage of raw user data,
Shokri et al. [ 101] have already shown the feasibility of the attack against machine learning models.
This work demonstrated how to inference user membership only based on the trained model.
Differential privacy, as a solution to this problem, requires additional computation on mobile
clients and can cause decrease in model accuracy. Secure aggregation [ 13] does no harm to the
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 29
Table 5. Available datasets for client-based training in LEAF.
Name #samples #users#samples per userTask
mean stdev
FEMNIST 805,263 3,550 226.83 88.94 Image Classification
Shakespeare 4,226,158 1,129 3,743.28 6,212.26 Sentiment Analysis
Twitter 1,600,498 660,120 2.42 4.71 Next-Character Prediction
CelebA 200,288 9,343 21.44 7.63 Image Classification
Reddit 56,587,343 1,660,820 34.07 62.9 Next-word Prediction
Synthetic Dataset 107,553 1,000 107.55 213.22 Classification
model performance but can hardly be deployed in large-scale client-based training due to its high
complexity. On the other hand, malicious clients may adjust their update data in order to affect or
control the behavior of the final model and gain benefits for themselves [ 9]. These facts imply that
better privacy and security in the scenario of client-based training is still an open problem.
Communication with 5G .The 5th generation mobile network (5G) brings high-bandwidth and
low-latency network to mobile devices. The training slow-down caused by network latency and the
client drop-off caused by network instability can be greatly relieved by 5G. Since communication
efficiency is a key concern for the implementation of client-based training, the high bandwidth,
low latency, and good stability of 5G may help client-based training become more robust [65].
While current client-based training methods are commonly using server-client communication
scheme (e.g., parameter server), 5G provides us with the opportunity to extend the communication
scheme to device-to-device (D2D) and vehicle-to-vehicle (V2V). Some work (e.g., [ 91]) has already
tried to combine federated learning with V2V networks to achieve low-latency neighbor cooperation.
We can also use 5G D2D network [ 94] to add middle layers between the server and the clients to
realize multi-tier network structure which is more robust and scalable.
What’s more, as 5G has brought stronger connectivity to mobile devices, secure data utilization
strategy becomes an urgent problem. Thus, besides being improved by 5G techniques, client-
based training has also been used to deal with 5G problems such as Network Data Analytics
Function (NWDAF) [ 42,75]. These facts show that combining client-based learning with 5G is
beneficial to both sides and thus will be an important future direction.
Standardization & Benchmark .Up till now, there still does not exist a white paper to com-
prehensively define and claim the standard of client-based training. Since deploying client-based
training requires a trade-off between a lot of properties (e.g., model accuracy, communication
overhead, complexity, privacy, and the scale of supported clients), the comparison between different
client-based training algorithms will be difficult without pre-defined standards and evaluation
metrics. Moreover, as client-based training can be applied to various kinds of tasks, it is also
important to collect and release corresponding benchmark datasets which satisfy the client-based
training settings. We recall that the No Uploading constraint in client-based training forces the local
datasets to be unbalanced non-IID datasets which are partitioned by users. LEAF [ 17] can be taken
as a reference. Table 5 has listed the currently available datasets in LEAF. All these datasets allow
partition by user, which means they can be used to test client-based training algorithms under
the non-IID condition. For now, LEAF datasets have covered only a few tasks. More benchmark
datasets are needed to support the development of client-based training in different areas.
30 Gu et al.
Table 6. Mobile machine learning frameworks.
Name(Server-Based)
Centralized(Server-Based)
DistributedMobile
InferenceMobile
Training
TensorFlow Lite [35] TensorFlow ! TensorFlow ! ! %
Core ML [7] Create ML ! % ! %
PyTorch Mobile [28] PyTorch! PyTorch! ! %
NCNN [110] % % ! %
Paddle Lite [10] Paddle! Paddle! ! %
MNN [44] MNN! % ! !
Deployment Scenarios .Since both federated learning and split learning are general client-based
training frameworks which concern about the overall learning process and scheme, they have to
be combined with concrete machine learning methods and models when being applied to real-life
applications. However, as they are newly emerging techniques and haven’t been tested on many
tasks, the suitable deployment scenarios for any of them are still not clear. For now, client-based
training has caught people’s eyes because it can guarantee user data privacy sacrificing a little
model performance and training speed. That is the reason why federated learning and split learning
have been applied to applications whose user data is sensitive (e.g., Gboard) and the health area.
Considering that client-based training is experiencing rapid development with its performance,
efficiency, and robustness all being improved, it is urgent to figure out what else can client-based
training do and how to better deploy it on mobile devices in various kinds of real-life scenarios.
General Mobile Training Framework .We list some commonly used mobile machine learning
frameworks in Table 6. Although the computation power of mobile devices is already sufficient for
training small models, many existing open-source mobile machine learning frameworks such as
TensorFlow Lite [ 35] and PyTorch Mobile [ 28] still only support inference operations, which means
they are actually mobile inference frameworks. Without a general mobile training framework,
implementing client-based training on mobile applications can be inefficient and time-consuming
because developers have to realize all training operations by themselves for each task. To deal with
this problem, MNN [ 44] from Alibaba has provided an on-device training module. MNN supports
constructing a model from zero and training it totally on mobile devices. We hope that other mobile
machine learning frameworks can also add support for mobile on-device training.
7 CONCLUSION
In this survey, we have provided a thorough overview of the development of machine learning in
recent years, from traditional server-based machine learning to emerging client-based learning.
We have discussed their purposes and demonstrated the sufficiency and necessity of client-based
machine learning. Specifically, for client-based inference, we have discussed its challenges and
demonstrated its current advances, especially in the fields of computer vision and natural language
processing. In addition, for client-based training, we have illustrated motivations and bottlenecks,
given a clear task definition, and further offered a general guideline for practicers. As typical
examples of client-based training, we have introduced the concepts of federated learning and
split learning and also reviewed their current advances. We finally have pointed out some future
research directions of client-based machine learning in both academia and industry. In summary,
applying client-based machine learning to real-world industrial applications is still faced with many
challenges and opportunities, which calls for more attention to be paid on its future development.
We hope that this survey can be a good starting point.
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 31
REFERENCES
[1]Martín Abadi, Ashish Agarwal, Paul Barham, et al .2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous
Systems. https://www.tensorflow.org/ Software available from tensorflow.org.
[2]Sharif Abuadbba, Kyuyeon Kim, Minki Kim, Chandra Thapa, Seyit A Camtepe, Yansong Gao, Hyoungshick Kim, and
Surya Nepal. 2020. Can We Use Split Learning on 1D CNN Models for Privacy Preserving Training? arXiv preprint
arXiv:2003.12365 (2020).
[3]Naman Agarwal, Ananda Theertha Suresh, Felix Xinnan X Yu, Sanjiv Kumar, and Brendan McMahan. 2018. cpSGD:
Communication-efficient and differentially-private distributed SGD. In Advances in Neural Information Processing
Systems 31 . Curran Associates, Inc., 7564–7575.
[4]Amr Ahmed, Moahmed Aly, Joseph Gonzalez, Shravan Narayanamurthy, and Alexander J Smola. 2012. Scalable
inference in latent variable models. In Proceedings of the fifth ACM international conference on Web search and data
mining . 123–132.
[5]Kareem Amin, Alex Kulesza, Andres Munoz, and Sergei Vassilvtiskii. 2019. Bounding User Contributions: A Bias-
Variance Trade-off in Differential Privacy. In Proceedings of the 36th International Conference on Machine Learning ,
Vol. 97. 263–271.
[6]Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E Hinton. 2018. Large
scale distributed neural network training through online distillation. arXiv preprint arXiv:1804.03235 (2018).
[7] Apple. 2017. Core ML. https://developer.apple.com/documentation/coreml. Last accessed: Jan. 20, 2020.
[8]Haim Avron, Alex Druinsky, and Anshul Gupta. 2015. Revisiting asynchronous linear solvers: Provable convergence
rate through randomization. Journal of the ACM (JACM) 62, 6 (2015), 51.
[9]Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. 2018. How to backdoor
federated learning. arXiv preprint arXiv:1807.00459 (2018).
[10] Baidu. 2019. Paddle Lite. https://github.com/PaddlePaddle/Paddle-Lite. Last accessed: Jan. 20, 2020.
[11] Christopher M Bishop. 2006. Pattern recognition and machine learning . springer.
[12] Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chloé Kiddon,
Jakub Konecný, Stefano Mazzocchi, H. Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and
Jason Roselander. 2019. Towards Federated Learning at Scale: System Design. arXiv preprint arXiv:1902.01046 (2019).
[13] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel
Ramage, Aaron Segal, and Karn Seth. 2017. Practical Secure Aggregation for Privacy-Preserving Machine Learning.
InProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security . 1175–1191.
[14] Léon Bottou, Frank E Curtis, and Jorge Nocedal. 2018. Optimization methods for large-scale machine learning. Siam
Review 60, 2 (2018), 223–311.
[15] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. 2011. Distributed optimization and
statistical learning via the alternating direction method of multipliers. Foundations and Trends ®in Machine learning
3, 1 (2011), 1–122.
[16] Sebastian Caldas, Jakub Konečny, H Brendan McMahan, and Ameet Talwalkar. 2018. Expanding the Reach of Federated
Learning by Reducing Client Resource Requirements. arXiv preprint arXiv:1812.07210 (2018).
[17] Sebastian Caldas, Peter Wu, Tian Li, Jakub Konečn `y, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar.
2018. Leaf: A benchmark for federated settings. arXiv preprint arXiv:1812.01097 (2018).
[18] Qingqing Cao, Noah Weber, Niranjan Balasubramanian, and Aruna Balasubramanian. 2019. DeQA: On-Device
Question Answering. In Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and
Services . 27–40.
[19] Tim Capes, Paul Coles, Alistair Conkie, et al .2017. Siri On-Device Deep Learning-Guided Unit Selection Text-to-Speech
System. In Proc. Interspeech 2017 . 4011–4015.
[20] Niu Chaoyue, Wu Fan, Tang Shaojie, Hua Lifeng, Jia Rongfei, Lv Chengfei, Wu Zhihua, and Chen Guihai. 2020.
Billion-Scale Federated Learning on Mobile Clients: A Submodel Design with Tunable Privacy. In Proceedings of the
26th Annual International Conference on Mobile Computing and Networking .
[21] Fei Chen, Zhenhua Dong, Zhenguo Li, and Xiuqiang He. 2018. Federated Meta-Learning for Recommendation. arXiv
preprint arXiv:1802.07876 (2018).
[22] Mingqing Chen, Rajiv Mathews, Tom Ouyang, and Françoise Beaufays. 2019. Federated Learning Of Out-Of-Vocabulary
Words. arXiv preprint arXiv:1903.10635 (2019).
[23] Kewei Cheng, Tao Fan, Yilun Jin, Yang Liu, Tianjian Chen, and Qiang Yang. 2019. SecureBoost: A Lossless Federated
Learning Framework. arXiv preprint arXiv:1901.08755 (2019).
[24] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc 'aurelio Ranzato, Andrew
Senior, Paul Tucker, Ke Yang, Quoc V. Le, and Andrew Y. Ng. 2012. Large Scale Distributed Deep Networks. In
Advances in Neural Information Processing Systems 25 . Curran Associates, Inc., 1223–1231.
[25] Dheeru Dua and Casey Graff. 2017. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml
32 Gu et al.
[26] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic
optimization. The Journal of Machine Learning Research 12, Jul (2011), 2121–2159.
[27] Hubert Eichner, Tomer Koren, Brendan Mcmahan, Nathan Srebro, and Kunal Talwar. 2019. Semi-Cyclic Stochastic
Gradient Descent. In Proceedings of the 36th International Conference on Machine Learning , Vol. 97. 1764–1773.
[28] Facebook. 2017. PyTorch. https://pytorch.org/. Last accessed: Jan. 20, 2020.
[29] Biyi Fang, Xiao Zeng, and Mi Zhang. 2018. NestDNN: Resource-Aware Multi-Tenant On-Device Deep Learning
for Continuous Mobile Vision. In Proceedings of the 24th Annual International Conference on Mobile Computing and
Networking . 115–127.
[30] Pedro A Forero, Alfonso Cano, and Georgios B Giannakis. 2010. Consensus-based distributed support vector machines.
The Journal of Machine Learning Research 11, May (2010), 1663–1707.
[31] Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. 2018. Mitigating sybils in federated learning poisoning. arXiv
preprint arXiv:1808.04866 (2018).
[32] Petko Georgiev, Nicholas D. Lane, Cecilia Mascolo, and David Chu. 2017. Accelerating Mobile Audio Sensing
Algorithms through On-Chip GPU Offloading. In Proceedings of the 15th Annual International Conference on Mobile
Systems, Applications, and Services . 306–318.
[33] A. Ghoting, R. Krishnamurthy, E. Pednault, B. Reinwald, V. Sindhwani, S. Tatikonda, Y. Tian, and S. Vaithyanathan.
2011. SystemML: Declarative machine learning on MapReduce. In 2011 IEEE 27th International Conference on Data
Engineering . 231–242.
[34] Andrew Gibiansky. 2017. Bringing HPC Techniques to Deep Learning. https://andrew.gibiansky.com/blog/machine-
learning/baidu-allreduce/. Last accessed: June 10, 2020.
[35] Google. 2017. TensorFlow Lite. https://www.tensorflow.org/lite/. Last accessed: Jan. 20, 2020.
[36] Otkrist Gupta and Ramesh Raskar. 2018. Distributed learning of deep neural network over multiple agents. Journal of
Network and Computer Applications 116 (2018), 1–8.
[37] Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Françoise Beaufays, Sean Augenstein, Hubert
Eichner, Chloé Kiddon, and Daniel Ramage. 2018. Federated learning for mobile keyboard prediction. arXiv preprint
arXiv:1811.03604 (2018).
[38] Lie He, An Bian, and Martin Jaggi. 2018. COLA: Communication-Efficient Decentralized Linear Learning. arXiv
preprint arXiv:1808.04883 (2018).
[39] Geoffrey E Hinton. 2007. Learning multiple layers of representation. Trends in cognitive sciences 11, 10 (2007), 428–434.
[40] Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B Gibbons, Garth A Gibson, Greg Ganger,
and Eric P Xing. 2013. More effective distributed ml via a stale synchronous parallel parameter server. In Advances in
neural information processing systems . 1223–1231.
[41] Loc N. Huynh, Youngki Lee, and Rajesh Krishna Balan. 2017. DeepMon: Mobile GPU-Based Deep Learning Framework
for Continuous Vision Applications. In Proceedings of the 15th Annual International Conference on Mobile Systems,
Applications, and Services . 82–95.
[42] Martin Isaksson and Karl Norrman. 2020. Secure Federated Learning in 5G Mobile Networks. arXiv preprint
arXiv:2004.06700 (2020).
[43] Martin Jaggi, Virginia Smith, Martin Takác, Jonathan Terhorst, Sanjay Krishnan, Thomas Hofmann, and Michael I
Jordan. 2014. Communication-efficient distributed dual coordinate ascent. In Advances in neural information processing
systems . 3068–3076.
[44] Xiaotang Jiang, Huan Wang, Yiliu Chen, Ziqi Wu, Lichuan Wang, Bin Zou, Yafeng Yang, Zongyang Cui, Yu Cai,
Tianhang Yu, Chengfei Lv, and Zhihua Wu. 2020. MNN: A Universal and Efficient Inference Engine. In MLSys .
[45] Thorsten Joachims. 2006. Training linear SVMs in linear time. In Proceedings of the 12th ACM SIGKDD international
conference on Knowledge discovery and data mining . 217–226.
[46] Rie Johnson and Tong Zhang. 2013. Accelerating stochastic gradient descent using predictive variance reduction. In
Advances in neural information processing systems . 315–323.
[47] Peter Kairouz, H. Brendan McMahan, Brendan Avent, et al .2019. Advances and Open Problems in Federated Learning.
arXiv preprint arXiv:1912.04977 (2019).
[48] Jiawen Kang, Zehui Xiong, Dusit Niyato, Shengli Xie, and Junshan Zhang. 2019. Incentive mechanism for reliable
federated learning: A joint optimization approach to combining reputation and contract theory. IEEE Internet of
Things Journal 6, 6 (2019), 10700–10714.
[49] Jiawen Kang, Zehui Xiong, Dusit Niyato, Yuze Zou, Yang Zhang, and Mohsen Guizani. 2020. Reliable federated
learning for mobile networks. IEEE Wireless Communications 27, 2 (2020), 72–80.
[50] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980
(2014).
[51] Anastasia Koloskova, Tao Lin, Sebastian U Stich, and Martin Jaggi. 2019. Decentralized deep learning with arbitrary
communication compression. arXiv preprint arXiv:1907.09356 (2019).
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 33
[52] Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi. 2019. Decentralized stochastic optimization and gossip
algorithms with compressed communication. arXiv preprint arXiv:1902.00340 (2019).
[53] Jakub Konecn `y. 2017. Stochastic, Distributed and Federated Optimization for Machine Learning. arXiv preprint
arXiv:1707.01155 (2017).
[54] Jakub Konečn `y, Brendan McMahan, and Daniel Ramage. 2015. Federated optimization: Distributed optimization
beyond the datacenter. arXiv preprint arXiv:1511.03575 (2015).
[55] Jakub Konečn `y, H Brendan McMahan, Daniel Ramage, and Peter Richtárik. 2016. Federated optimization: Distributed
machine learning for on-device intelligence. arXiv preprint arXiv:1610.02527 (2016).
[56] Jakub Konečn `y, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and Dave Bacon. 2016.
Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492 (2016).
[57] Nicholas D. Lane, Petko Georgiev, and Lorena Qendro. 2015. DeepEar: Robust Smartphone Audio Sensing in
Unconstrained Acoustic Environments Using Deep Learning. In Proceedings of the 2015 ACM International Joint
Conference on Pervasive and Ubiquitous Computing . 283–294.
[58] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep learning. nature 521, 7553 (2015), 436.
[59] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document
recognition. Proc. IEEE 86, 11 (1998), 2278–2324.
[60] David D Lewis, Yiming Yang, Tony G Rose, and Fan Li. 2004. Rcv1: A new benchmark collection for text categorization
research. Journal of machine learning research 5, Apr (2004), 361–397.
[61] Mu Li, David G. Andersen, Jun Woo Park, Alexander J. Smola, Amr Ahmed, Vanja Josifovski, James Long, Eugene J.
Shekita, and Bor-Yiing Su. 2014. Scaling Distributed Machine Learning with the Parameter Server. In 11th USENIX
Symposium on Operating Systems Design and Implementation (OSDI 14) . 583–598.
[62] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2019. Federated learning: Challenges, methods, and
future directions. arXiv preprint arXiv:1908.07873 (2019).
[63] Xiangru Lian, Yijun Huang, Yuncheng Li, and Ji Liu. 2015. Asynchronous parallel stochastic gradient for nonconvex
optimization. In Advances in Neural Information Processing Systems . 2737–2745.
[64] Yang Liu, Tianjian Chen, and Qiang Yang. 2018. Secure Federated Transfer Learning. arXiv preprint arXiv:1812.03337
(2018).
[65] Dumitrel Loghin, Shaofeng Cai, Gang Chen, Tien Tuan Anh Dinh, Feiyi Fan, Qian Lin, Janice Ng, Beng Chin Ooi,
Xutao Sun, Quang-Trung Ta, et al .2020. The Disruptions of 5G on Data-Driven Technologies and Applications. IEEE
Transactions on Knowledge and Data Engineering 32, 6 (2020), 1179–1198.
[66] Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I Jordan, Peter Richtárik, and Martin Takáč. 2015. Adding vs.
averaging in distributed primal-dual optimization. arXiv preprint arXiv:1502.03508 (2015).
[67] Chenxin Ma and Martin Takáč. 2016. Distributed inexact damped newton method: Data partitioning and load-
balancing. arXiv preprint arXiv:1603.05191 (2016).
[68] Akhil Mathur, Nicholas D. Lane, Sourav Bhattacharya, Aidan Boran, Claudio Forlivesi, and Fahim Kawsar. 2017.
DeepEye: Resource Efficient Local Execution of Multiple Deep Vision Models Using Wearable Commodity Hardware.
InProceedings of the 15th Annual International Conference on Mobile Systems, Applications, and Services . 68–81.
[69] I. McGraw, R. Prabhavalkar, R. Alvarez, M. G. Arenas, K. Rao, D. Rybach, O. Alsharif, H. Sak, A. Gruenstein, F. Beaufays,
and C. Parada. 2016. Personalized speech recognition on mobile devices. In 2016 IEEE International Conference on
Acoustics, Speech and Signal Processing (ICASSP) . 5955–5959.
[70] H. Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas. 2016. Communication-efficient
learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629 (2016).
[71] H. Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. 2017. Learning Differentially Private Language
Models Without Losing Accuracy. arXiv preprint arXiv:1710.06963 (2017).
[72] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkataraman, Davies Liu, Jeremy Freeman, DB
Tsai, Manish Amde, Sean Owen, Doris Xin, Reynold Xin, Michael J. Franklin, Reza Zadeh, Matei Zaharia, and Ameet
Talwalkar. 2016. MLlib: Machine Learning in Apache Spark. Journal of Machine Learning Research 17, 34 (2016), 1–7.
[73] Gaurav Mittal, Kaushal B. Yagnik, Mohit Garg, and Narayanan C. Krishnan. 2016. SpotGarbage: Smartphone App to
Detect Garbage Using Deep Learning. In Proceedings of the 2016 ACM International Joint Conference on Pervasive and
Ubiquitous Computing . 940–945.
[74] Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. 2019. Agnostic Federated Learning. In Proceedings of the
36th International Conference on Machine Learning , Vol. 97. 4615–4625.
[75] Solmaz Niknam, Harpreet S Dhillon, and Jeffery H Reed. 2019. Federated learning for wireless communications:
Motivation, opportunities and challenges. arXiv preprint arXiv:1908.06847 (2019).
[76] Pitch Patarasuk and Xin Yuan. 2009. Bandwidth optimal all-reduce algorithms for clusters of workstations. J. Parallel
and Distrib. Comput. 69, 2 (2009), 117–124.
34 Gu et al.
[77] Diego Peteiro-Barral and Bertha Guijarro-Berdiñas. 2013. A survey of methods for distributed machine learning.
Progress in Artificial Intelligence 2, 1 (2013), 1–11.
[78] Maarten G Poirot, Praneeth Vepakomma, Ken Chang, Jayashree Kalpathy-Cramer, Rajiv Gupta, and Ramesh Raskar.
2019. Split Learning for collaborative deep learning in healthcare. arXiv preprint arXiv:1912.12115 (2019).
[79] Raluca Ada Popa. 2014. Building practical systems that compute on encrypted data . Ph.D. Dissertation. Massachusetts
Institute of Technology, Department of Electrical Engineering and Computer Science.
[80] Raluca Ada Popa, Catherine Redfield, Nickolai Zeldovich, and Hari Balakrishnan. 2011. CryptDB: protecting confi-
dentiality with encrypted query processing. In Proceedings of the Twenty-Third ACM Symposium on Operating Systems
Principles . 85–100.
[81] Raluca Ada Popa, Catherine Redfield, Nickolai Zeldovich, and Hari Balakrishnan. 2012. CryptDB: processing queries
on an encrypted database. Commun. ACM 55, 9 (2012), 103–111.
[82] Foster J Provost and Daniel N Hennessy. 1996. Scaling up: Distributed machine learning with cooperation. In
AAAI/IAAI, Vol. 1 . 74–79.
[83] Ning Qian. 1999. On the momentum term in gradient descent learning algorithms. Neural networks 12, 1 (1999),
145–151.
[84] X. Ran, H. Chen, X. Zhu, Z. Liu, and J. Chen. 2018. DeepDecision: A Mobile Deep Learning Framework for Edge
Video Analytics. In IEEE INFOCOM 2018 - IEEE Conference on Computer Communications . 1421–1429.
[85] Sujith Ravi. 2019. Efficient On-Device Models using Neural Projections. In Proceedings of the 36th International
Conference on Machine Learning , Vol. 97. 5370–5379.
[86] Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu. 2011. Hogwild: A lock-free approach to parallelizing
stochastic gradient descent. In Advances in Neural Information Processing Systems . 693–701.
[87] Jason Rennie. 2007. 20 Newsgroups. http://qwone.com/~jason/20Newsgroups/. Last accessed: June 22, 2020.
[88] Peter Richtárik and Martin Takáč. 2016. Distributed coordinate descent method for learning with big data. The
Journal of Machine Learning Research 17, 1 (2016), 2657–2681.
[89] Herbert Robbins and Sutton Monro. 1951. A stochastic approximation method. The annals of mathematical statistics
(1951), 400–407.
[90] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, et al .2015. Imagenet large scale visual recognition challenge. International journal
of computer vision 115, 3 (2015), 211–252.
[91] Sumudu Samarakoon, Mehdi Bennis, Walid Saad, and Merouane Debbah. 2018. Federated learning for ultra-reliable
low-latency V2V communications. In 2018 IEEE Global Communications Conference (GLOBECOM) . IEEE, 1–7.
[92] J REDDI Sashank, KALE Satyen, and KUMAR Sanjiv. 2018. On the convergence of adam and beyond. In International
Conference on Learning Representations .
[93] Felix Sattler, Simon Wiedemann, Klaus-Robert Müller, and Wojciech Samek. 2019. Robust and communication-efficient
federated learning from non-iid data. arXiv preprint arXiv:1903.02891 (2019).
[94] Stefano Savazzi, Monica Nicoli, and Vittorio Rampa. 2020. Federated learning with cooperating devices: A consensus
approach for massive IoT networks. IEEE Internet of Things Journal 7, 5 (2020), 4641–4654.
[95] Alexander Sergeev and Mike Del Balso. 2018. Horovod: fast and easy distributed deep learning in TensorFlow. arXiv
preprint arXiv:1802.05799 (2018).
[96] Ohad Shamir, Nati Srebro, and Tong Zhang. 2014. Communication-efficient distributed optimization using an
approximate newton-type method. In International conference on machine learning . 1000–1008.
[97] Vivek Sharma, Praneeth Vepakomma, Tristan Swedish, Ken Chang, Jayashree Kalpathy-Cramer, and Ramesh Raskar.
2019. ExpertMatcher: Automating ML Model Selection for Clients using Hidden Representations. arXiv preprint
arXiv:1910.03731 (2019).
[98] Muhammad Shayan, Clement Fung, Chris JM Yoon, and Ivan Beschastnikh. 2018. Biscotti: A ledger for private and
secure peer-to-peer machine learning. arXiv preprint arXiv:1811.09904 (2018).
[99] Micah J Sheller, G Anthony Reina, Brandon Edwards, Jason Martin, and Spyridon Bakas. 2018. Multi-institutional deep
learning modeling without sharing patient data: A feasibility study on brain tumor segmentation. In International
MICCAI Brainlesion Workshop . 92–104.
[100] Reza Shokri and Vitaly Shmatikov. 2015. Privacy-preserving deep learning. In Proceedings of the 22nd ACM SIGSAC
conference on computer and communications security . 1310–1321.
[101] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. 2017. Membership inference attacks against
machine learning models. In Security and Privacy (SP), 2017 IEEE Symposium on . 3–18.
[102] Liu Sicong, Zhou Zimu, Du Junzhao, Shangguan Longfei, Jun Han, and Xin Wang. 2017. UbiEar: Bringing Location-
Independent Sound Awareness to the Hard-of-Hearing People with Smartphones. Proceedings of the ACM on Interactive,
Mobile, Wearable and Ubiquitous Technologies 1, 2 (2017), 1–21.
From Server-Based to Client-Based Machine Learning: A Comprehensive Survey 35
[103] Abhishek Singh, Praneeth Vepakomma, Otkrist Gupta, and Ramesh Raskar. 2019. Detailed comparison of communi-
cation efficiency of split learning and federated learning. arXiv preprint arXiv:1909.09145 (2019).
[104] Team Siri. 2017. Deep learning for Siri’s voice: on-device deep mixture density networks for hybrid unit selection
synthesis. Apple Machine Learning J 1, 4 (2017).
[105] Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. 2017. Federated Multi-Task Learning. In
Advances in Neural Information Processing Systems 30 . Curran Associates, Inc., 4424–4434.
[106] Virginia Smith, Simone Forte, Chenxin Ma, Martin Takáč, Michael I Jordan, and Martin Jaggi. 2017. CoCoA: A general
framework for communication-efficient distributed optimization. The Journal of Machine Learning Research 18, 1
(2017), 8590–8638.
[107] Alexander Smola and Shravan Narayanamurthy. 2010. An architecture for parallel topic models. Proceedings of the
VLDB Endowment 3, 1-2 (2010), 703–710.
[108] Boyd Stephen, Parikh Neal, Chu Eric, Peleato Borja, and Eckstein Jonathan. 2010. MPI example for alternating
direction method of multipliers. https://stanford.edu/~boyd/papers/admm/mpi/. Last accessed: June 26, 2020.
[109] Shizhao Sun, Wei Chen, Jiang Bian, Xiaoguang Liu, and Tie-Yan Liu. 2017. Ensemble-compression: A new method for
parallel training of deep neural networks. In Joint European Conference on Machine Learning and Knowledge Discovery
in Databases . 187–202.
[110] Tencent. 2017. ncnn. https://github.com/Tencent/ncnn. Last accessed: Jan. 20, 2020.
[111] T Tieleman and Geoffrey Hinton. 2012. Neural Networks for Machine Learning. https://www.cs.toronto.edu/~tijmen/
csc321/slides/lecture_slides_lec6.pdf.
[112] Praneeth Vepakomma, Otkrist Gupta, Abhimanyu Dubey, and Ramesh Raskar. 2019. Reducing leakage in distributed
deep learning for sensitive health data. In International Conference on Learning Representations .
[113] Praneeth Vepakomma, Otkrist Gupta, Tristan Swedish, and Ramesh Raskar. 2018. Split learning for health: Distributed
deep learning without sharing raw patient data. arXiv preprint arXiv:1812.00564 (2018).
[114] Praneeth Vepakomma, Tristan Swedish, Ramesh Raskar, Otkrist Gupta, and Abhimanyu Dubey. 2018. No Peek: A
Survey of private distributed deep learning. arXiv preprint arXiv:1812.03288 (2018).
[115] Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. 2017. The Marginal Value of
Adaptive Gradient Methods in Machine Learning. In Advances in Neural Information Processing Systems 30 . Curran
Associates, Inc., 4148–4158.
[116] Xindong Wu, Vipin Kumar, J. Ross Quinlan, et al .2008. Top 10 algorithms in data mining. Knowledge and information
systems 14, 1 (2008), 1–37.
[117] Mengwei Xu, Jiawei Liu, Yuanqiang Liu, Felix Xiaozhu Lin, Yunxin Liu, and Xuanzhe Liu. 2019. A First Look at Deep
Learning Apps on Smartphones. In The World Wide Web Conference . 2125–2136.
[118] Qiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. 2019. Federated machine learning: Concept and applications.
ACM Transactions on Intelligent Systems and Technology (TIST) 10, 2 (2019), 12.
[119] Timothy Yang, Galen Andrew, Hubert Eichner, Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ramage, and Françoise
Beaufays. 2018. Applied federated learning: Improving google keyboard query suggestions. arXiv preprint
arXiv:1812.02903 (2018).
[120] Hao Yu, Rong Jin, and Sen Yang. 2019. On the Linear Speedup Analysis of Communication Efficient Momentum SGD
for Distributed Non-Convex Optimization. In Proceedings of the 36th International Conference on Machine Learning ,
Vol. 97. 7184–7193.
[121] Hao Yu, Sen Yang, and Shenghuo Zhu. 2019. Parallel restarted SGD with faster convergence and less communication:
Demystifying why model averaging works for deep learning. In Proceedings of the AAAI Conference on Artificial
Intelligence , Vol. 33. 5693–5700.
[122] Xiao Zeng, Kai Cao, and Mi Zhang. 2017. MobileDeepPill: A Small-Footprint Mobile Deep Learning System for
Recognizing Unconstrained Pill Images. In Proceedings of the 15th Annual International Conference on Mobile Systems,
Applications, and Services . 56–67.
[123] Sixin Zhang, Anna E Choromanska, and Yann LeCun. 2015. Deep learning with elastic averaging SGD. In Advances
in Neural Information Processing Systems . 685–693.
[124] Yuchen Zhang and Xiao Lin. 2015. DiSCO: Distributed optimization for self-concordant empirical loss. In International
conference on machine learning . 362–370.
[125] Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra. 2018. Federated learning with
non-iid data. arXiv preprint arXiv:1806.00582 (2018).
[126] Shuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen, Nenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu. 2017. Asynchronous
stochastic gradient descent with delay compensation. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70 . 4120–4129.
[127] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. 2010. Parallelized stochastic gradient descent. In
Advances in neural information processing systems . 2595–2603.

arXiv:1911.03886v2  [eess.SP]  14 Jul 20211
Performance Analysis on Machine Learning-Based
Channel Estimation
Kai Mei, Jun Liu, Xiaochen Zhang, Nandana Rajatheva and Jibo Wei
Abstract —Recently, machine learning-based channel estima-
tion has attracted much attention. The performance of machi ne
learning-based estimation has been validated by simulatio n
experiments. However, little attention has been paid to the
theoretical performance analysis. In this paper, we invest igate
the mean square error (MSE) performance of machine learning -
based estimation. Hypothesis testing is employed to analyz e its
MSE upper bound. Furthermore, we build a statistical model
for hypothesis testing, which holds when the linear learnin g
module with a low input dimension is used in machine learning -
based channel estimation, and derive a clear analytical rel ation
between the size of the training data and performance. Then,
we simulate the machine learning-based channel estimation in
orthogonal frequency division multiplexing (OFDM) system s to
verify our analysis results. Finally, the design considera tions for
the situation where only limited training data is available are
discussed. In this situation, our analysis results can be ap plied
to assess the performance and support the design of machine
learning-based channel estimation.
Index Terms —channel estimation, performance analysis, MSE,
machine learning, OFDM
I. I NTRODUCTION
IN wireless communication systems, the transmitted signals
are corrupted by many detrimental factors, such as mul-
tipath propagation, and mobility, etc [1], [2]. To recover t he
transmitted data accurately, channel estimation is an esse ntial
module in the coherent receiver. A portion of the transmitte d
signals are known at the receiver and used for the channel
estimation, which are called pilot signals [3]. Among the
pilot-aided channel estimation methods, least-squares (L S)
estimation has the lowest complexity, which simply divides
the received signal value by the pilot [4], [5]. However, its
performance is sensitive to noise. To improve the performan ce
of LS estimation, the correlation in time, frequency, and sp ace
domain can be exploited in linear minimum mean square error
(LMMSE) estimation, where the correlated LS estimates are
used to enhance an estimate of interest [6], [7]. Although
LMMSE estimation has the optimal estimation performance
[8], the computational complexity is much higher than LS
estimation and it requires the knowledge of second-order
channel statistics. Moreover, LMMSE estimation draws on th e
(c) 2021 IEEE. Personal use is permitted, but re-
publication/redistribution requires IEEE permission. Se e
https://www.ieee.org/publications/rights/index.html for more information.
Citation information for this article : DOI 10.1109/TCOMM. 2021.3083597.
Kai Mei, Jun Liu, Xiaochen Zhang, and Jibo Wei are with the Col lege of
Electronic Science and Technology, National University of Defense Technol-
ogy, Changsha 410073, China (E-mail: meikai11, liujun15, z hangxiaochen14,
wjbhw@nudt.edu.cn).
Nandana Rajatheva is with Center for Wireless Communicatio ns, University
of Oulu, Oulu 90570, Finland (E-mail: nandana.rajatheva@o ulu.ﬁ).condition that the channel is linear and stationary. When th e
channel is non-linear or non-stationary, LMMSE estimation
suffers from performance degradation and the closed-form
expression of the optimal estimation, i.e., minimum mean
square error (MMSE) estimation, turns out to be intractable
[9], [10].
While there were early works on machine learning (ML)-
based channel estimation [11]–[13], the recent re-emergen ce
of machine learning has motivated the use of neural net-
works for channel estimation again [9], [10], [14]–[24]. In
ML-based channel estimation, the LS estimates are fed into
a neural network, and then the neural network yields the
enhanced channel estimates. As a model-free approach, ML-
based channel estimation merely needs a dataset labeled by
true channel responses to optimize the parameters of the neu ral
network. An effective estimator can still be learned under
complex channel conditions. In [9], a non-stationary chann el
is considered, where the expression of the MMSE estimation
cannot be calculated in closed form. The heuristic structur e
deduced under a given distribution of the second-order chan nel
statistics is used as a blueprint to design the neural networ k.
Through training the neural network, the estimator perform s
well under arbitrary channel models. Until now, ML-based
channel estimation has been developed for wireless energy
transfer (WET) systems [10], orthogonal frequency divisio n
multiplexing (OFDM) systems [14], multiple-input multipl e-
output (MIMO) [9], [15], [16], and massive MIMO [17]–
[19]. Deep learning (DL) techniques including fully connec ted
deep neural network (FC-DNN) [20], recurrent neural networ k
(RNN) [21], and convolution neural network (CNN) [22],
[23] have been leveraged to enhance the channel estimation
performance.
In the literature, the merits of ML-based channel estimatio n
have been demonstrated by experiments. However, the theo-
retical analysis of ML-based channel estimation has been pa id
little attention. Speciﬁcally, it has been shown by simulat ions
that ML-based channel estimation can approach the optimal
estimation [9], i.e., MMSE estimation. The mean square erro r
(MSE) difference between the two methods, however, has not
been investigated analytically. In addition to performanc e, the
requirements on the training dataset, which plays an import ant
role in the ML-based channel estimation, have not been studi ed
analytically as well. In this paper, we analyze the ML-based
channel estimation from a theoretical perspective, and our
main contributions are the following.
•We investigate the MSE difference between the ML-based
channel estimation and the MMSE channel estimation
employing hypothesis testing. The analysis shows that
2
there is an upper bound of the MSE difference even if the
inputs are not from the training dataset, which provides
theoretical support for ML-based channel estimation.
•We further specify the distribution functions in hypothesi s
testing, which holds for the situation where the learning
module is linear with a low input dimension. Then, we
deﬁne the scaled MSE difference as the performance
measure of the ML-based channel estimation and derive
the analytical relation between the training dataset size
and performance. Moreover, we display the numerical
result of this analytical relation and obtain the sufﬁcient
size of the training dataset.
•We conduct computer simulations to evaluate the perfor-
mance of ML-based channel estimation, where the linear
structure and deep neural network (DNN) are employed.
Through simulation experiments, the derived sufﬁcient
size of the training dataset is examined and our theoretical
analysis is validated.
•We discuss the design of ML-based channel estimation
for the situation where only limited training data is avail-
able. Since the linear learning module may be employed
in this situation, our analysis results can be used to
predict the performance of ML-based channel estimation
and help determine the input dimension of the learning
module.
The rest of this paper is organized as follows. Channel
models, the MMSE estimation, and the ML-based channel
estimation are introduced in Section II. The performance an al-
ysis on ML-based channel estimation is presented in Section
III. To validate our analysis, the performance of ML-based
channel estimation employing the linear structure and DNN i s
evaluated through simulations in Section IV and Section V,
respectively. The design considerations of ML-based chann el
estimation with limited training data are discussed in Sect ion
VI. Finally, the paper is concluded in Section VII.
Notation: We use boldface small letters and capital letters
to denote vectors and matrices. E[·],D[·], and/bardbl·/bardbl2repre-
sent the expectation, the variance, and the Euclidean norm,
respectively. The superscripts (·)∗,(·)T,(·)H,(·)−1denote the
conjugate of complex, the transpose, the Hermitian transpo se
of a complex vector or matrix, and the inversion, respective ly.
CN(·)represents the complex Gaussian distribution, while
χ2(κ)represents the chi-square distribution with dimension
ofκ. We denote the probability density function (PDF) and
cumulative probability function (CDF) of χ2(κ)aspχ2κ(·)and
Fχ2κ(·), respectively.
II. C HANNEL ESTIMATION
A. Channel Model and MMSE Estimation
In pilot-assisted channel estimation, known pilot signals are
transmitted and used for channel estimation. LS estimation is
usually adopted to estimate the channel responses as the ini tial
estimates and then these initial estimates can be improved w ith
many different methods [5].
The LS estimation simply divides the received signal values
by the pilot values [5]. We use ˆhpto represent a Np×1vector
that contains the LS estimates, where Npis the number ofpilot signals. ˆhpcan be modeled as the superposition of actual
channel responses and noise [5], [7], i.e.,
ˆhp=h+n, (1)
wherenis a white Gaussian noise vector with variance σ2.h
contains the true channel responses for pilot signals.
We denote ˆhsas aNp×1vector containing the improved
estimates based on ˆhpand use a function f(·)to represent a
certain channel estimation method [22], i.e.,
ˆhs=f/parenleftBig
ˆhp/parenrightBig
. (2)
We aim to analyze the performance of channel estimation in
this paper and the performance analysis is usually focused o n
a single channel response, e.g., in [25]. Therefore, we cons ider
only one estimate from the ﬁnal output of channel estimation .
We deﬁne ˆhsas an arbitrary element in ˆhsand investigate the
performance of ˆhs. The index of ˆhsis omitted for simplicity
since the position of ˆhsinˆhsis not concerned. Then, we have
ˆhs=f/parenleftBig
ˆhp/parenrightBig
. (3)
Note that the performance analysis on ˆhscan be used to
assess a channel estimation method represented by (2). Sinc e
ˆhsis an element of the vector ˆhs, the analysis can be focused
onˆhswhile investigating the estimation performance of ˆhsin
(2).
As a general representation, (3) can describe a wide range
of channel estimation methods. The design of a channel
estimation method is to pursue a low MSE and we denote
fopt(·)as the one that has the minimal MSE, i.e., the MMSE
estimation [26]. The analytical expression of fopt(·)depends
on the statistical model of the channel [9]. In this paper, we
consider two types of channel models: the stationary channe l
model and the quasi-stationary channel model.
The stationary channel which is subject to complex Gaus-
sian distribution is often assumed for conventional channe l
estimation methods [27]. Then, we have h∼ CN(0,Rhh)
and the MMSE estimation can be expressed as [5]
fopt/parenleftBig
ˆhp/parenrightBig
=rhsh/parenleftbig
Rhh+σ2I/parenrightbig−1ˆhp, (4)
wherehsis the actual channel response of ˆhs.rhshdenotes the
correlation vector between hsandh, i.e.,rhsh=E[hs(h)H].
Iis an identity matrix.
Channel estimation for the quasi-stationary channel has
been investigated in the recent literature, e.g., in [9]. Un der
a quasi-stationary channel, rhshandRhhare not ﬁxed but
depend on the given parameters δ.δmay describe, for
example, angles of propagation paths and the parameters
inδare assumed to be random variables [9]. Then, his
assumed to be conditionally Gaussian distributed, i.e. h|δ∼
CN/parenleftbig
0,Rhh|δ/parenrightbig
. Under this condition, the optimal estimation
is given by [9]
fopt/parenleftBig
ˆhp/parenrightBig
=E/bracketleftBig
h/vextendsingle/vextendsingle/vextendsingleˆhp/bracketrightBig
=E/bracketleftBig
E/bracketleftBig
h/vextendsingle/vextendsingle/vextendsingleˆhp,δ/bracketrightBig/vextendsingle/vextendsingle/vextendsingleˆhp/bracketrightBig
=E/bracketleftBig
fopt|δ/parenleftBig
ˆhp/parenrightBig/vextendsingle/vextendsingle/vextendsingleˆhp/bracketrightBig
,(5)
3
where
fopt|δ/parenleftBig
ˆhp/parenrightBig
=rhsh|δ/parenleftbig
Rhh|δ+σ2I/parenrightbig−1ˆhp.
It is difﬁcult to derive the exact expression of (5).
The analytical expression of fopt(·)or MMSE estimation
depends on the channel model. Therefore, when we develop a
channel estimation algorithm based on MMSE estimation, we
should investigate the channel model. Furthermore, when th e
channel model is complicated, the exact expression of MMSE
estimation is hard to obtain. As a result, the MMSE channel
estimation may sometimes be practically infeasible.
B. Machine Learning-based Channel Estimation
Channel estimation can be realized in a quite different
manner by leveraging machine learning. The procedure of
machine learning-based channel estimation is illustrated in
Fig. 1 [9]. The key component is the learning module, which
is employed to approximate the function in (3). Convolution
neural network (CNN) [22], recurrent neural network (RNN)
[21], the linear structure and etc. can be used as the learnin g
module. The linear structure directly connects the output w ith
the input and is the simplest learning module, which can only
ﬁt a linear function.
There are two phases in machine learning-based channel
estimation including the training phase and the deployment
phase. In the training phase, the parameters of the learning
module are optimized through reducing a loss function over a
data setT. To be speciﬁc, the data set Tcan be represented as
T={(ˆhp(1),hs(1))...(ˆhp(m),hs(m))...(ˆhp(M),hs(M))},
where(ˆhp(m),hs(m))denotes the mthpair of training data
inTandhs(m)is the label for the input ˆhp(m). Note that
we omit the index mfor simplicity if it is not needed. The
loss function is deﬁned as the square error of estimation, i. e.,
L(f(ˆhp),hs) =|f(ˆhp)−hs|2. In addition, we deﬁne LTas
the average loss function over the data set T, i.e.,
LT=1
M/summationdisplay
m/vextendsingle/vextendsingle/vextendsinglef/parenleftBig
ˆhp(m)/parenrightBig
−hs(m)/vextendsingle/vextendsingle/vextendsingle2
. (6)
We callLTthe training loss in the following. Through mini-
mizingLT, the learning module can be trained to approximate
a function that achieves good estimation performance. In th e
deployment phase, the initial estimates ˆhpinput the learning
module and then the learning module produces the estimate
ofhs.
The machine learning-based channel estimation does not
heavily rely on the channel model [9] and merely needs
a dataset for training. This is because the learning module
of machine learning-based channel estimation can be simply
regarded as a black box, which can directly perform channel
estimation after its parameters are optimized through trai ning.
It is not required to derive the explicit expression of chann el
estimation, unlike conventional methods. As a result, when
a complicated channel condition is considered, an effectiv e
estimator can still be learned in a data-driven manner.
However, the theoretical analysis on the performance of
machine learning-based channel estimation lacks in the rec ent
literature. If the sample size is inﬁnitely large and the tra iningTraining set Linear 
structure Deep Neural 
Netwerk 
Learning module Training phase 
Reduce 
training loss 
Deployment phase Calculate 
training loss 
ML 
sˆhs(2) hs(1) hpˆ(1) h
pˆ(2) h
pˆh
Fig. 1. Sketch diagram of machine learning based estimation .
loss approaches the expected square error, i.e., MSE, MMSE
estimation can be learned by training since the estimator th at
minimizes the training loss will be the one that minimizes
MSE. However, in practice, the sample size is a ﬁnite value
and the training loss is merely the sampled value of MSE.
Then, the training procedure can only guarantee that the
learned estimator minimizes the square error of training da ta.
If new data inputs the estimator, the MSE performance of
the output is unpredictable and only experimental evaluati on
has been provided for the performance of ML-based channel
estimation. Therefore, we analyze the MSE performance of
the ML-based channel estimation in this paper.
III. P ERFORMANCE ANALYSIS ON MACHINE
LEARNING -BASED CHANNEL ESTIMATION
A. Performance Analysis Using Hypothesis Testing
The MSE of channel estimation represented by f(·)is also
the expectation of loss function, as can be seen from the
equation below.
LE=E/bracketleftBig
L/parenleftBig
f/parenleftBig
ˆhp/parenrightBig
,hs/parenrightBig/bracketrightBig
=E/bracketleftbigg/vextendsingle/vextendsingle/vextendsinglef/parenleftBig
ˆhp/parenrightBig
−hs/vextendsingle/vextendsingle/vextendsingle2/bracketrightbigg
.(7)
It is difﬁcult to obtain the joint probability density funct ion
(PDF) of f(ˆhp)andhs, which depends on many channel
statistics, and thus the MSE of f(·)is intractable.
We denote f∗(·)as the learned function in ML-based
channel estimation. The learned estimation is normally not the
MMSE estimation fopt(·)as mentioned above, and there is a
certain loss in the MSE performance compared with fopt(·).
We denote the MSEs of fopt(·)andf∗(·)asLE1andLE2,
respectively. We use ∆LEto represent the MSE difference
betweenfopt(·)andf∗(·), i.e.,∆LE=LE2−LE1. Notice that
the MSE of f∗(·)is difﬁcult to calculate and one may concern
more about the MSE difference ∆LEthan the exact MSE LE2.
Since the MSE difference ∆LEcan show whether the ML-
based channel estimation approaches the optimal performan ce,
it can indicate the learning performance more clearly than t he
4
exact value of MSE LE2. We investigate the MSE difference
∆LEin this paper.
We employ hypothesis testing to analyze ∆LE[28]. Deﬁne
∆LE≥∆0
LEas hypothesis H0and∆LE<∆0
LEas
hypothesis H1. Set the conﬁdence level as 1−ε0. Then,
if the probability of the observed event that happens under
hypothesis H0is lower than ε0, i.e.,P(H0)≤ε0, we can
acceptH1and it is derived that the upper bound of the MSE
difference ∆LEis∆0
LE. More speciﬁcally, we believe that the
MSE difference between the MMSE estimation fopt(·)and the
learned estimation f∗(·)is no more than ∆0
LEat a conﬁdence
level of1−ε0. Since whether the condition P(H0)≤ε0holds
is unknown , further analysis on P(H0)is needed.
Denoteξ1as the training loss of fopt(·), i.e.,
ξ1=1
M/summationdisplay
m/vextendsingle/vextendsingle/vextendsinglefopt/parenleftBig
ˆhp(m)/parenrightBig
−hs(m)/vextendsingle/vextendsingle/vextendsingle2
.
Denoteξ2as the training loss of f∗(·), i.e.,
ξ2=1
M/summationdisplay
m/vextendsingle/vextendsingle/vextendsinglef∗/parenleftBig
ˆhp(m)/parenrightBig
−hs(m)/vextendsingle/vextendsingle/vextendsingle2
.
The learned estimation has the minimal training loss, i.e., ξ1≥
ξ2. We denote the probability that ξ1≥ξ2asε. Note that
P(H0)is the probability of ξ1≥ξ2under hypothesis H0.
We have P(H0) =εwhen∆LE≥∆0
LE. To simplify the
expression of ε, we need the following assumption.
Assumption 1. ξ1is independent of ξ2, i.e.,p(ξ1,ξ2) =
p1(ξ1)p2(ξ2), wherep1(ξ1)andp2(ξ2)are the PDFs of ξ1
andξ2, respectively.
If Assumption 1 does not hold, e.g., if fopt(·) =f∗(·)and
ξ1is highly correlated with ξ2, the actual value of P(H0)is
lower than its calculated value ε. To illustrate, when f∗(·)is
close tofopt(·), which may violate the independence between
ξ1andξ2, the MSE difference between f∗(·)andfopt(·)
is reduced, which contributes to H1. Then, the actual value
P(H0)decreases and thus is lower than the calculated value ε.
In the case, we can still accept H1at the same conﬁdence level.
Since the conﬁdence level can be regarded as the lower bound
ofP(H1), the actual probability of H1can be higher than
the conﬁdence level. In other words, the hypothesis testing
results derived under Assumption 1 apply for the situation
where Assumption 1 does not hold.
Under Assumption 1, εcan be expressed as
ε=/integraldisplay∞
0/integraldisplayx1
0p(x1,x2)dx2dx1
=/integraldisplay∞
0p1(x1)/integraldisplayx1
0p2(x2)dx2dx1
=/integraldisplay∞
0p1(x1)F2(x1)dx1,(8)
whereF2(x)is the cumulative probability function (CDF) of
ξ2, i.e.,F2(x) =/integraltextx
−∞p2(z)dz.
The value of εis dependent on ∆LE. Fig. 2 displays an
example of shapes for p1(x)andF2(x). With the increasing
of∆LE, the high value region of p1(x)will further move
to the near zero region of F2(x). According to (8), when the
multiplication of p1(x)andF2(x)is near zero, εwill be rather
( )1p x ( )2F x 
x
1E
1
ED
2E
2
E
Fig. 2. Sketch diagram for PDF of ξ1and CDF of ξ2.
small. Therefore, we can infer that εis negatively correlated
to∆LE.
We assume that the value of εisε0when∆LE= ∆0
LE.
SinceP(H0) =εfor∆LE≥∆0
LEandεis negatively
correlated to ∆LE,P(H0)has the maximum value when
∆LEachieves its minimum value ∆0
LE. Asε=ε0when
∆LE= ∆0
LE, the maximum value of P(H0)isε0. There-
fore, we have P(H0)≤ε0. It demonstrates the condition
P(H0)≤ε0in above hypothesis testing and thus proves that
compared with the optimal channel estimation, the MSE loss
of machine learning-based channel estimation is less than ∆0
LE
at a conﬁdence level of 1−ε0.
The above analysis actually demonstrates the validity of
ML-based channel estimation, which is the main challenge
when applying machine learning techniques. This is typical ly
examined by experiments, while we verify it from a theoretic al
perspective. The analysis shows the MSE upper bound of ML-
based channel estimation and the MSE is actually the expecte d
loss function, as mentioned above. The expected loss functi on
describes the performance of output for unseen data [29] and
thus indicates the applicability of machine learning when t he
input data is not from the training set.
From Fig. 2, the effect of the sample size on the perfor-
mance of ML-based channel estimation can be explained as
well. With the increasing of the sample size, the variances
ofξ1andξ2will be reduced, and then p1(x)will become
narrow and F2(x)will increase more sharply. As a result, the
overlapping part between p1(x)andF2(x)will be reduced
and the value of εwill be smaller. For the conﬁdence level
1−ε0, the corresponding upper bound ∆0
LEdecreases. There-
fore, the MSE of the learned estimation LE2will be closer to
the minimal MSE LE1.
B. Analytical Relation between Sample Size and Performance
To derive the analytical relation between the sample size an d
∆0
LE, the knowledge of the distributions of the training losses
ξ1andξ2are required. We make the following assumptions
to specify a distribution for the training loss.
Assumption 2. Output error is subject to complex Gaussian
distribution, i.e., (f(ˆhp)−hs)∼ CN(0,LE).LEis actually
the MSE of the estimation represented by f(ˆhp).
Assumption 3. Output errors are independent, i.e.,
(f(ˆhp(m1))−hs(m1))and(f(ˆhp(m2))−hs(m2))are in-
dependent when m1/ne}ationslash=m2.
Assumption 2 holds when the simplest learning module is
used, i.e., the linear structure. hsandˆhpare Gaussian random
5
variables as mentioned in Section II-A. The output error is t he
linear combination of those variables when the function f(·)
is a linear one. Thus, the output error is Gaussian as well [26 ].
Since the means of hsandˆhpare all zeros, the mean of the
output error is also zero. In addition, since E[f(ˆhp)−hs] = 0 ,
the variance of the output error equals E[|f(ˆhp)−hs|2], i.e.,
the MSE LEfor the channel estimation function f(·). Hence,
we have(f(ˆhp)−hs)∼ CN(0,LE).
Assumption 3 approximately holds when the output errors
are highly random [30]. To satisfy this condition, independ ent
training data should be provided and the number of parameter s
in the learning module should be small. The independence of
training data can be guaranteed by properly generating the
inputs and labels [9]. However, the number of parameters
in the learning module is related to the chosen learning
module and the use of a complex learning module may violate
Assumption 3. As an illustration, if a complex learning modu le
with a huge number of parameters is used, overﬁtting may
happen [29]. Then, the output errors may all be zeros and
thus highly correlated.
In summary, Assumption 2 and Assumption 3 approxi-
mately hold when the learning module is approximately linea r
and the input dimension is low since a low input dimension
can reduce the number of parameters in the learning module.
Lemma 1: Given assumptions 2 and 3, the normalized
training loss 2Mξ/LEis subject to chi-square distribution
χ2(2M), whereξis the training loss given in (6).
Proof. If assumption 2 holds, we have |f(ˆhp)−hs|2=
a2+b2, whereaandbare both Gaussian, i.e., a∼ N(0,LE/2)
andb∼ N(0,LE/2). Then, 2|f(ˆhp)−hs|2/LEcan be
represented as the superposition of two normalized Gaussia n
variables.
If assumption 3 holds, the normalized training loss
2Mξ/LEcan be regarded as the superposition of the squares
of2Mindependent nomalized Gaussian variables, which is
the formal description of the chi-square distribution χ2(2M).
Therefore, it is veriﬁed that the normalized training loss i s
subject to the chi-square distribution χ2(2M).
According to Lemma 1, 2Mξ1/LE1and2Mξ2/LE2are
both subject to the chi-square distribution χ2(2M). We denote
κ= 2Mas the degree of freedom in χ2(2M). Then, the PDF
ofξ1can be represented as
p1(x) =κ
LE1pχ2κ/parenleftbiggκx
LE1/parenrightbigg
, (9)
and the CDF of ξ2can be expressed as
F2(x) =Fχ2κ/parenleftbiggκx
LE2/parenrightbigg
. (10)Substituting (9) and (10) into (8) gives
ε=/integraldisplay∞
0p1(x1)F2(x1)dx1
=/integraldisplay∞
0κ
LE1pχ2κ/parenleftbiggκx1
LE1/parenrightbigg
Fχ2κ/parenleftbiggκx1
LE2/parenrightbigg
dx1
ς1=κx1
LE1=/integraldisplay∞
0pχ2κ(ς1)Fχ2κ/parenleftbiggLE1ς1
LE2/parenrightbigg
dς1
=/integraldisplay∞
0Fχ2κ/parenleftBigg
ς1
1+∆LE
LE1/parenrightBigg
pχ2κ(ς1)dς1.(11)
As can be seen from (11), εis determined by κ, which
is related to the sample size, the MSE difference ∆LEand
the minimal MSE LE1. We deﬁne α= ∆LE/LE1, whereα
can be regarded as the scaled MSE difference. We use αas
the performance metric for the ML-based channel estimation .
Then, there are only two variables left, i.e., κandα. After the
conﬁdence level 1−εis determined, a clear analytical relation
betweenαandκcan be derived.
It is intuitive that the structure of the learning module,
including the input dimension and the category of the learni ng
module (linear or non-linear), inﬂuences the learning perf or-
mance. However, the learning performance indicator αis only
determined by the training data size indicator κ. This is be-
cause the analytical relation between αandκis derived based
on Assumption 2 and Assumption 3. The two assumptions
actually exclude the effects of the structure of the learnin g
module from the analysis result since the two assumptions
require that the learning module should be approximately
linear and the input dimension should be low. We can see that
although the two assumptions limit the applicable scenario s of
the analysis, they indeed contribute to obtaining a clear re lation
between the learning performance and the sample size.
The value, 0.95, is usually considered to be an acceptable
conﬁdence level [28]. Therefore, we set ε= 0.05and plot the
curve ofαas a function of κin Fig. 3. The required training
data size can be obtained from this curve. As an example, we
consider that the learning performance is satisfactory whe n the
MSE difference of the learned estimator is lower than 10% of
the optimal MSE performance. Then, the required training da ta
size corresponds to α= 0.1on the curve. It can be seen that
whenκis above 1200, αis below 0.1. Therefore, it shows
that if the learning module is linear and its input dimension
is low, we can create a training dataset whose size is only
around 600 (the training data size M=κ/2) and a satisfactory
estimator can be learned based on the training dataset. Note
that the learning performance indicator αis an upper bound
for the scaled MSE difference. The above analysis result may
still hold when Assumption 2 and Assumption 3 are slightly
violated in practice. In addition, the sufﬁcient dataset si ze 600
is derived for α= 0.1. When other values of the learning
performance indicator αare chosen, the sufﬁcient dataset size
varies w.r.t α.
IV. P ERFORMANCE EVALUATION FOR ML- BASED
CHANNEL ESTIMATION EMPLOYING LINEAR STRUCTURE
In the previous section, we present the performance analysi s
of the ML-based channel estimation and a conclusion is
6
0 2000 4000 6000
Sample size indicator 0.060.10.14Scaled performance loss X 1202 
Y 0.1 
Fig. 3. Figure of scaled performance loss upper bound αvaried with sample
size indicator κwhenεis set to 0.05.
reached that 600 is a sufﬁcient sample size when the linear
learning module with a low input dimension is used. To verify
this conclusion, we conduct computer simulations to examin e
the performance of ML-based channel estimation in which the
linear learning module is employed. We use stationary chann el
models in simulations so that the optimal channel estimatio n
has a closed-form and the performance measure, i.e., scaled
MSE difference α, can be calculated.
A. System Model for Simulation
We consider an OFDM system with Nsubcarriers [5]. The
DC (direct current) carrier and a certain number of carriers
at the edges of the spectrum are null, and we denote Kas
the number of usable subcarriers in an OFDM symbol. CP
lengthNcpis set toN/4and assumed to be over the maximum
delay. The channel is assumed to be constant over one OFDM
symbol. Time and frequency synchronization are assumed to
be accurate as well.
We use an exponentially decaying power-delay proﬁle
(PDP), which can be expressed as
Γ(τ) =Ce−τ/τmax(12)
whereCis a normalization coefﬁcient and τmax is the maxi-
mum delay of the channel.
Referring to the framework in Fig. 1, the estimator can be
interpreted as Fig. 4. Note that the estimator is not exactly the
same as the framework in Fig. 1. The output of the estimator
has the same dimension as the input, while the framework in
Fig. 1 only gives a single estimate. However, when it comes
to the evaluation of estimation performance, the estimator
becomes consistent with the framework. We calculate the
average over the MSEs of the channel frequency responses
(CFRs) and the output of the estimator is treated as a single
estimate like the framework in Fig. 1. Therefore, the MSE
performance of the estimator in Fig. 4 can be used to examine
the analysis in Section III.Training set 
Learning 
algorithm 
1
f_LS ˆh
2
f_LS ˆh
1
f_LS ˆKh-
f_LS ˆKh1
f_ML ˆh
2
f_ML ˆh
1
f_ML ˆKh-
f_ML ˆKh
Fig. 4. Sketch diagram of ML-based channel estimation emplo ying the linear
structure.
The estimator in Fig. 4 is actually the realization of (2).
Since we use a linear learning module and the processing on
LS estimates is linear, the function fin (2) can be expressed
as the multiplication by a coefﬁcient matrix. Therefore, th e
mathematical expression of the estimator is given by
ˆhML
f=WˆhLS
f, (13)
whereWis aK×Kmatrix and contains the parameters of the
learning module. ˆhLS
fcorresponds to ˆhpin (2) and contains
the LS estimates of CFRs, i.e.,
ˆhLS
f=/bracketleftBig
ˆh1
fLS,...,ˆhK
fLS/bracketrightBigT
.
ˆhML
fcorresponds to ˆhsin (2) and contains the output of the
estimator, i.e.,
ˆhML
f=/bracketleftBig
ˆh1
fML,...,ˆhK
fML/bracketrightBigT
.
In Fig. 4, the training set is
T=/braceleftBig
...,/parenleftBig
ˆhLS
f(m),hf(m)/parenrightBig
,.../bracerightBig
,
wherehfcontains the actual values of the CFRs, which is the
label of training data.
The learning algorithm is to ﬁnd the W∗that minimizes
the training loss, which can be formulated as
W∗= arg
Wmin/summationdisplay
m/vextenddouble/vextenddouble/vextenddoubleWˆhLS
f(m)−hf(m)/vextenddouble/vextenddouble/vextenddouble2
2. (14)
The optimization problem of (14) has an analytical solution
[29]
W∗=H/parenleftBig
ˆHHˆH/parenrightBig−1ˆHH, (15)
whereˆH= [ˆhLS
f(1),...,ˆhLS
f(M)]is a matrix containing
the LS estimation of CFRs in the training set TandH
contains the corresponding true values of CFRs, i.e., H=
[hf(1),...,hf(M)]. As we use independent training data, the
column vectors in ˆHare not correlated. Therefore, (ˆHHˆH)is
full rank and invertible.
7
B. Numerical Results
In the simulation of the ML-based channel estimation, we
ﬁrst generate training data to optimize the parameter matri x
of the learning module and obtain W∗. We assume that the
correct channel statistics, i.e., the noise variance σ2and the
channel correlation function, are known. That is, given the
dataset size M, we can use the true channel model to generate
Mrealizations of channel vectors hfand the respective LS
estimates ˆhLS
f. We also assume that the channel realizations are
independent to support Assumption 3. After training, we use
W∗to perform channel estimation. The channel realizations
are assumed to be independent of those generated for trainin g
so that the estimation performance for whole new data can be
shown.
We simulate the LMMSE estimator in (4) as the opti-
mal estimation fopt(·). The MSE of the LMMSE estimator
corresponds to the minimal MSE LE1and the MSE of the
learned estimator corresponds to LE2. Then, the performance
measure of ML-based channel estimation α= ∆LE/LE1can
be calculated using LE1andLE2, where∆LE=LE2−LE1.
We ﬁrst conduct simulation experiments to verify the con-
clusion reached in Section III-B that 600 is a sufﬁcient data set
size when the learning module is linear and its input dimensi on
is low. In our simulations, the input dimension is the number
of usable subcarriers per symbol Kand we can adjust the
number of null subcarriers to control the input dimension.
We set the input dimension set as low values including 4, 8,
12. The discrete Fourier transform (DFT) size N, the dataset
sizeMand the maximum delay τmax are set to 16,600
and2, respectively. Fig. 5 compares the MSE performance
of ML-based channel estimation and the LMMSE channel
estimation. We can see that the performance of ML-based
channel estimation with the three input dimensions is close
to that of the LMMSE channel estimation at different signal-
to-noise ratios (SNRs). This simulation result veriﬁes tha t
a training data set of size 600 is indeed sufﬁcient for the
considered learning module. It also validates our theoreti cal
analysis using hypothesis testing and the derived analytic al
relation between the dataset size and performance.
Assumption 3 will not hold with the increasing input
dimension as mentioned in Section III-B. It can be inferred
that the scaled MSE difference αincreases as the input di-
mension increases. Since training data will be insufﬁcient with
the growing parameters in the learning module, the learning
performance will be degraded. Nevertheless, a training dat a set
of size 600 may still be sufﬁcient when the input dimension is
relatively high. As the performance measure αin Section III-B
is actually the upper bound for the scaled MSE difference,
the actual scaled MSE difference may be satisfactory when
Assumption 3 is slightly violated. We simulate the scaled MS E
difference αof ML-based channel estimation under different
values of the input dimension to examine its range. The scale d
MSE difference αis presented in Fig. 6 as a function of
the input dimension Kat SNRs of -10 dB, -5 dB, 0 dB,
10 dB, and 20 dB. It can be observed the curves are very
close under high SNRs and low SNRs, respectively, and only
have a little difference around the SNR of 0 dB. Furthermore,-10 0 10 20 
SNR(dB) 10 -2 10 -1 10 0MSE ML-based estimation ID=4 
LMMSE estimation ID=4 
ML-based estimation ID=8 
LMMSE estimation ID=8 
ML-based estimation ID=12 
LMMSE estimation ID=12 
Fig. 5. The MSE performance of ML-based channel estimation a nd LMMSE
channel estimation under different SNRs.
20 40 60 80 
Input dimension 00.040.10.16Scaled performance loss SNR = 20 dB 
SNR = 10 dB 
SNR = 0 dB 
SNR = -5 dB 
SNR = -10 dB 
X 60 
Y 0.09806 
Fig. 6. The scaled MSE difference of ML-based channel estima tion under
different input dimensions.
the scaled MSE difference αis even smaller at lower SNRs,
which indicates that the noise variance of the LS estimates
does not inﬂuence the learning performance. This agrees wit h
the result shown in (11), which implies that the noise level
is not a factor affecting the learning performance. Moreove r,
the scaled MSE difference αindeed grows with the increasing
of the input dimension K. As the curve for the SNR of 0 dB
has approximately the average performance, we investigate the
input dimension range based on this curve. It can be seen that
α <0.1with the input dimension lower than 60. We consider
that the learning performance is acceptable when the scaled
MSE difference α <0.1. Therefore, the input dimension can
increase to 60 with a training dataset of size 600.
When the input dimension is larger than 60, a training
dataset of size 600 is not sufﬁcient and more training data is
required. To investigate the required dataset size for ML-b ased
8
1000 2000 3000
Sample size 00.10.3Scaled performance loss 240 usable subcarriers 
180 usable subcarriers 
120 usable subcarriers 
X 1200 
Y 0.09057 X 1800 
Y 0.09109 X 2400 
Y 0.09173 
Fig. 7. The scaled MSE difference of ML-based channel estima tion under
different sample sizes.
channel estimation with a high input dimension, we set the
DFT size Nas256and simulate ML-based channel estimation
underK= 120,180,240, respectively. In Fig. 7, we display
the scaled MSE difference αwith respect to the sample size
M. As expected, the required sample size grows as the input
dimension increases. Furthermore, the required dataset si ze
is approximately in proportion to the input dimension. Base d
on this result, we can determine the sufﬁcient size of the
dataset for high input dimensions. However, further theore tical
analysis is required to verify the derived dataset size and i t is
left as future work.
V. P ERFORMANCE EVALUATION FOR ML- BASED
CHANNEL ESTIMATION EMPLOYING DNN
Deep learning (DL) has attracted much attention recently.
Therefore, in this section, we evaluate the performance of M L-
based channel estimation employing the deep neural network
(DNN) with the derived training dataset size 600. Note that t he
analysis in Section III-B may not hold for this situation. Th is
is because the DNN can approximate a non-linear function,
which violates Assumption 2, and commonly has a large
number of parameters, which violates Assumption 3.
A. System Model for Simulation
We simulate the same OFDM system as above but consider
the quasi-stationary scenario, where the maximum delay τmax
is assumed to be a random variable. As explained in Section
II, the closed-form of the optimal channel estimation is har d
to derive. However, it is the scenario where ML-based chan-
nel estimation shines. The estimator can be directly traine d
to approximate some complicated expression for the quasi-
stationary channel condition since DNN can be regarded as a
universal function approximator [20].
The structure of the estimator is illustrated in Fig. 8. DNN
works as the function fin (2), which maps the LS estimates
to the improved estimation results. The employed DNN hasTraining set 
Learning 
algorithm 
1
f_LS ˆh
f_LS ˆKh1
f_ML ˆh
f_ML ˆKh
Fig. 8. Sketch diagram of ML-based estimation employing the linear structure.
three layers, including one input layer, one hidden layer, a nd
one output layer. The numbers of neurons in each of the
three layers are 2K,4K,2K, respectively. Since a DNN
cannot get complex numbers as input and also cannot provide
them as output, we take the real part and imaginary part of
a complex number as two input variables and combine two
output variables to generate a complex number. Therefore,
the input and output dimensions are twice of the subcarriers ’
number. The activation function in input and output layers i s
linear, while the Sigmoid function is used in the hidden laye r.
B. Numerical Results
The frequency correlation function of channel responses
is the Fourier transform of the PDP. Since the maximum
delayτmax is a random variable, the channel correlation
function is unknown and the LMMSE channel estimation
cannot be performed. To cope with the uncertainty of the
channel correlation function, the robust estimator propos ed
in [31] can be applied. The robust estimator is actually the
LMMSE estimation using a uniform PDP based correlation
function. The maximum delay of the PDP uses the upper
bound of τmax, i.e., the possibly largest value of τmax. The
robust estimator is also called the robust LMMSE estimator
in this paper.
In simulations, the DFT size Nis set to64and the valid
subcarrier number Kis60. The maximum delay τmax is
drawn from a uniform distribution within [1,2,..,16]. Fig. 9
shows the MSE performance of ML-based channel estimation
and conventional channel estimation methods including LS
estimation and robust LMMSE estimator. When a large dataset
is provided for training, the ML-based estimator achieves
a signiﬁcant improvement over the LS estimator and out-
performs the robust LMMSE estimator. However, when the
sample size is reduced to 600, huge performance degradation is
observed in the ML-based channel estimation. It shows that t he
derived analytical relation between the training dataset s ize and
performance is not suitable for DL techniques since Assump-
tion 2 and Assumption 3 are violated when DL techniques are
used. To derive the sufﬁcient size of the training dataset fo r
ML-based channel estimation employing DL techniques, one
9
-10 -5 0 5 10 15 
SNR(dB) 10 -3 10 -2 10 -1 10 010 1MSE LS 
Robust LMMSE 
DNN with 600 sample pairs 
DNN with 60000 sample pairs 
Fig. 9. The MSE performance of LS estimator, robust LMMSE est imator,
and ML-based estimator under quasi-stationary channel wit h different SNRs.
may only need to reconsider the distribution of the training
loss since the performance analysis using hypothesis testi ng
in Section III-A still holds for DL techniques.
VI. D ESIGN OF ML- BASED CHANNEL ESTIMATION WITH
LIMITED TRAINING DATA
In the previous studies of ML-based channel estimation, it
is usually assumed that sufﬁcient training data, e.g., tens of
thousands of samples [9], are available. However, in com-
munications systems, generating training data may lead to
the reduction of system efﬁciency. Considering the system
efﬁciency, the available training data may be limited in som e
applications, e.g., only hundreds of training data. Theref ore,
we discuss the ML-based channel estimation with a small
training dataset in this section.
A linear learning module is preferable for ML-based chan-
nel estimation with limited training data compared to compl ex
neural networks. As seen in Fig. 9, although DNN has better
potential than the linear structure, it suffers from unacce ptable
performance degradation when the training data is limited.
Similarly, the input dimension should be relatively low sin ce
the learning module with a high input dimension requires a
large amount of training data as shown in Fig. 7. Therefore,
when the size of the training data is small, the ML-based
channel estimation may employ a linear learning module with
a low input dimension.
In this case, our theoretical results in Section III-B can be
used to make quantitative predictions for the learning perf or-
mance of ML-based channel estimation. Then, the learning
module becomes a white box because its performance is pre-
dictable given the size of the training dataset. Additional ly, our
analysis results can help design ML-based channel estimati on.
As an example, we consider the design of ML-based channel
estimation for an OFDM system with 512 subcarriers, in which
480 are valid, and only 600 sample pairs are available for
training. We choose the linear structure as the learning mod ule.As for the input dimension, considering the correlations be -
tween subcarriers, the potentially optimal performance ma y be
achieved by setting it as 480. However, 480is too large for the
sample size since the provided training data is merely enoug h
for a low input dimension according to our analysis in III-B.
Therefore, we partition the OFDM symbol into subsymbols
consisting of much fewer subcarriers as in [27] and ﬁlter
over these subsymbols. In this way, the input dimension can
be reduced. We consider that the learning performance is
acceptable when α≤0.1. Given that the sample size is
600, the input dimension can be raised to 60as shown in
Fig. 6. Therefore, we expect that 60 is a good choice for
the input dimension. When the input dimension is 60, the
learning performance is already satisfactory, which means that
the performance of the learned estimation is close to that
of the optimal estimation. If the input dimension is further
reduced, the performance of ML-based channel estimation
reduces due to the performance degradation of the optimal
channel estimation.
We simulate ML-based channel estimation employing the
linear structure under several symbol partition schemes. W e
assume a stationary channel and τmaxis set to 64. The results
are displayed in Fig. 10, where the MSE performance of the LS
estimator is also plotted as a baseline. Interestingly, we o bserve
that the scheme that partitions the symbols into subsymbols
containing 60 subcarriers each, i.e., the input dimension 6 0,
indeed performs the best. When the input dimension is 240
or 480, a signiﬁcant performance loss is observed due to the
lack of training data. The MSEs for the input dimensions 30,
60, and 120 are close and their MSE differences change w.r.t.
the SNR. At low SNRs, the input dimension 30 achieves the
best performance. This is because the MSE value is large
at low SNRs and the MSE of ML-based channel estimation
can be reduced signiﬁcantly by reducing the scaled MSE
difference α. The ML-based channel estimation with a lower
input dimension has smaller αand thus tends to have better
performance. However, when the MSE value is quite small
at high SNRs, the performance improvement brought by
reducing αmay be ignorable. Then, the ML-based channel
estimation with a high input dimension tends to have better
performance since it has a lower achievable MSE. Therefore,
the input dimension 120 shows the best performance and the
input dimension 30 performs the worst within the three input
dimensions at high SNRs. In contrast, the input dimension 60
achieves a good tradeoff between the scaled MSE difference α
and the achievable MSE and thus has a moderate performance
at all SNRs.
VII. C ONCLUSION
In this paper, we have presented the performance analysis
of ML-based channel estimation. The MSE upper bound of
ML-based channel estimation is investigated using hypothe sis
testing. Moreover, we propose the scaled MSE difference,
which divides the potentially minimal MSE with the differen ce
between the MSE of ML-based channel estimation and the
potentially minimal MSE, as the performance measure. An
analytical relation between this performance measure and t he
10
−10 −5 0 5 10 15 10 −3 10 −2 10 −1 10 010 1
SNR MSE LS 
480 subcarriers each group 
240 subcarriers each group 
120 subcarriers each group 
60 subcarriers each group 
30 subcarriers each group 
Fig. 10. The MSE performance of ML-based channel estimation under
different division schemes with different SNRs.
sample size is derived for ML-based channel estimation em-
ploying the linear learning module with a low input dimensio n.
We plot a curve for this analytical result and derive the size
of the training dataset for a given scaled MSE difference
0.1, which is veriﬁed through simulations. Our performance
analysis is also validated in the simulation experiments. W e
discuss the ML-based channel estimation for the scenario
where the available training data is limited and apply our
analysis results to support the design of the estimator.
Future work may reconsider the statistical model of hypoth-
esis testing and derive the theoretical performance evalua tion
for ML-based channel estimation employing other learning
structures, e.g., the DNN.
ACKNOWLEDGMENT
This work was supported in part by the National Science
Foundation of China (NSFC) (Grants 61931020).
REFERENCES
[1] M. K. Samimi and T. S. Rappaport, “3-D millimeter-wave st atistical
channel model for 5G wireless system design,” Trans. Microw. Theory
Techn. , vol. 64, no. 7, pp. 2207–2225, Jul. 2016.
[2] G. Matz and F. Hlawatsch, Wireless Communications Over Rapidly
Time-Varying Channels . Oxford: Academic Press, 2011.
[3] S. Y . Park and C. G. Kang, “Performance of pilot-assisted channel
estimation for OFDM system under time-varying multi-path r ayleigh
fading with frequency offset compensation,” in IEEE 51th Veh. Technol.
Conf. (VTC) , vol. 2, May 2000, pp. 1245–1249.
[4] V . Savaux and Y . Lou¨ et, “LMMSE channel estimation in OFD M context:
a review,” IET Signal Process. , vol. 11, no. 2, pp. 123–134, 2017.
[5] M. K. Ozdemir and H. Arslan, “Channel estimation for wire less OFDM
systems,” IEEE Commun. Surveys Tut. , vol. 9, no. 2, pp. 18–48, Second
2007.
[6] A. Alkhateeb, O. El Ayach, G. Leus, and R. W. Heath, “Chann el
estimation and hybrid precoding for millimeter wave cellul ar systems,”
IEEE J. Sel. Topics Signal Process. , vol. 8, no. 5, pp. 831–846, Oct.
2014.
[7] Y . G. Li, J. H. Winters, and N. R. Sollenberger, “MIMO-OFD M
for wireless communications: signal detection with enhanc ed channel
estimation,” IEEE Trans. Commun. , vol. 50, no. 9, pp. 1471–1477, Sep.
2002.
[8] J. J. van de Beek, O. Edfors, M. Sandell, S. K. Wilson, and P . O.
Borjesson, “On channel estimation in OFDM systems,” in IEEE 45th
Veh. Techn. Conf. (VTC) , vol. 2, Chicago, IL, Jul. 1995, pp. 815–819.[9] D. Neumann, T. Wiese, and W. Utschick, “Learning the MMSE channel
estimator,” IEEE Trans. Signal Process. , vol. 66, no. 11, pp. 2905–2917,
Jun. 2018.
[10] J. Kang, C. Chun, and I. Kim, “Deep-learning-based chan nel estimation
for wireless energy transfer,” IEEE Commun. Lett. , vol. 22, no. 11, pp.
2310–2313, Nov. 2018.
[11] X. Zhou and X. Wang, “Channel estimation for OFDM system s using
adaptive radial basis function networks,” IEEE Trans. Veh. Techn. ,
vol. 52, no. 1, pp. 48–59, 2003.
[12] L. Zhang and X. Zhang, “MIMO channel estimation and equa lization
using three-layer neural networks with feedback,” Tsinghua science and
technology , vol. 12, no. 6, pp. 658–662, 2007.
[13] A. Omri, R. Bouallegue, R. Hamila, and M. Hasna, “Channe l estimation
for LTE uplink system by perceptron neural network,” International
Journal of Wireless & Mobile Networks (IJWMN) , vol. 2, no. 3, pp.
155–165, 2010.
[14] E. Balevi and J. G. Andrews, “One-bit OFDM receivers via deep
learning,” IEEE Trans. Commun. , vol. 67, no. 6, pp. 4326–4336, 2019.
[15] M. Mehrabi, M. Mohammadkarimi, M. Ardakani, and Y . Jing , “Decision
directed channel estimation based on deep neural network k-step pre-
dictor for MIMO communications in 5G,” IEEE J. Sel. Areas Commun. ,
vol. 37, no. 11, pp. 2443–2456, 2019.
[16] Y . Liao, Y . Hua, and Y . Cai, “Deep learning based channel estimation
algorithm for fast time-varying MIMO-OFDM systems,” IEEE Commun.
Lett., vol. 24, no. 3, pp. 572–576, 2020.
[17] Y . Zhang, M. Alrabeiah, and A. Alkhateeb, “Deep learnin g for massive
MIMO with 1-bit ADCs: When more antennas need fewer pilots,” IEEE
Wireless Commun. Lett. , pp. 1–1, 2020.
[18] O. T. Demir and E. Bjornson, “Channel estimation in mass ive MIMO
under hardware non-linearities: Bayesian methods versus d eep learning,”
IEEE Open J. Commun. Society , vol. 1, pp. 109–124, 2020.
[19] M. Alrabeiah and A. Alkhateeb, “Deep learning for TDD an d FDD
massive MIMO: Mapping channels in space and frequency,” in 2019
53rd Asilomar Conf. Signals, Systems, Computers (CSSC) , 2019, pp.
1465–1470.
[20] Y . Yang, F. Gao, X. Ma, and S. Zhang, “Deep learning-base d channel
estimation for doubly selective fading channels,” IEEE Access , vol. 7,
pp. 36 579–36 589, 2019.
[21] Q. Bai, J. Wang, Y . Zhang, and J. Song, “Deep learning bas ed channel
estimation algorithm over time selective fading channels, ”IEEE Trans.
Cogn. Commun. Netw. , pp. 1–1, 2019.
[22] M. Soltani, V . Pourahmadi, A. Mirzaei, and H. Sheikhzad eh, “Deep
learning-based channel estimation,” IEEE Commun. Lett. , vol. 23, no. 4,
pp. 652–655, Apr. 2019.
[23] H. He, C.-K. Wen, S. Jin, and G. Y . Li, “Deep learning-bas ed channel
estimation for beamspace mmwave massive MIMO systems,” IEEE
Wireless Commun. Lett. , vol. 7, no. 5, pp. 852–855, 2018.
[24] J. Liu, K. Mei, X. Zhang, D. Ma, and J. Wei, “Online extrem e learning
machine-based channel estimation and equalization for OFD M systems,”
IEEE Commun. Lett. , vol. 23, no. 7, pp. 1276–1279, Jul. 2019.
[25] Jeongho Park, Jihyung Kim, Myonghee Park, Kyunbyoung K o,
Changeon Kang, and Daesik Hong, “Performance analysis of ch annel
estimation for ofdm systems with residual timing offset,” IEEE Transac-
tions on Wireless Communications , vol. 5, no. 7, pp. 1622–1625, 2006.
[26] S. M. Kay, Fundamentals of statistical signal processing . Prentice Hall
PTR, 1993.
[27] O. Edfors, M. Sandell, J. J. van de Beek, S. K. Wilson, and P. O. Bor-
jesson, “OFDM channel estimation by singular value decompo sition,”
IEEE Trans. Commun. , vol. 46, no. 7, pp. 931–939, Jul. 1998.
[28] E. L. Lehmann and J. P. Romano, Testing statistical hypotheses .
Springer Science & Business Media, 2006.
[29] I. Goodfellow, Y . Bengio, A. Courville, and Y . Bengio, Deep learning .
MIT press Cambridge, 2016.
[30] N. R. Draper and H. Smith, Applied regression analysis . John Wiley
& Sons, 1998.
[31] Y . Li, L. J. Cimini, and N. R. Sollenberger, “Robust chan nel estimation
for OFDM systems with rapid dispersive fading channels,” IEEE Trans.
Commun. , vol. 46, no. 7, pp. 902–915, Jul. 1998.
11
Kai Mei received his Master’s degree from Na-
tional University of Defense Technology in 2017.
Currently, he is studying at National University of
Defense Technology for a Ph.D. degree. He has been
a visiting Ph.D. student with the University of Oulu
in Finland from 2019 to 2020. His research interests
include synchronization and channel estimation in
OFDM systems and MIMO-OFDM systems, and
machine learning applications in wireless commu-
nications.
Jun Liu received the B.S. degree in optical informa-
tion science and technology from the South China
University of Technology (SCUT), in 2015, and
the M.E. degree in communications and information
engineering from the National University of Defense
Technology (NUDT), Changsha, China, in 2017,
where he is currently pursuing the Ph.D. degree
with the Department of Cognitive Communications.
He is currently a visiting Ph.D. student with the
University of Leeds. His current research interests
include machine learning with a focus on shal-
low neural networks applications, signal processing for br oadband wireless
communication systems, multiple antenna techniques, and w ireless channel
modeling.
Xiaochen Zhang received the B.S. degree from the
National University of Defense Technology, Chang-
sha, China, in 2018, where he is currently pursuing
the Ph.D. degree with the College of Electronic Sci-
ence and Engineering. His research interests include
resource allocation, multiaccess edge computing,
machine learning, and channel modeling.
Nandana Rajatheva received the B.Sc. (Hons.)
degree in electronics and telecommunication engi-
neering from the University of Moratuwa, Sri Lanka,
in 1987, ranking ﬁrst in the graduating class, and
the M.Sc. and Ph.D. degrees from the University
of Manitoba, Winnipeg, MB, Canada, in 1991 and
1995, respectively. He is currently a Professor with
the Centre for Wireless Communications, Univer-
sity of Oulu, Finland. He was a Canadian Com-
monwealth Scholar during the graduate studies in
Manitoba. He held a Professor/Associate Professor
positions with the University of Moratuwa and the Asian Inst itute of Tech-
nology, Thailand, from 1995 to 2010. He is currently leading the AI-driven
Air Interface design task in Hexa-X EU Project. He has coauth ored more than
200 refereed papers published in journals and in conference proceedings. His
research interests include physical layer in beyond 5G, mac hine learning for
PHY and MAC, sensing for factory automation and channel codi ng.
Jibo Wei received his BS degree and MS degree
from National University of Defense Technology,
Changsha, China, in 1989 and 1992, respectively,
and the PhD degree from Southeast University, Nan-
jing, China, in 1998, all in electronic engineering.
He is currently a professor of the Department of
Communication Engineering of NUDT. His research
interests include wireless network protocol and sig-
nal processing in communications, cooperative com-
munication, and cognitive network. He is the mem-
ber of the IEEE Communication Society and also
the member of the IEEE VTS. He is a Senior Member of China Insti tute
of Communications and a Senior Member of China Institute of E lectronics
respectively. He is also an editor of the journal of China Com munications.

PRIVATE MACHINE LEARNING VIA RANDOMISED RE-
SPONSE
David Barber
Department of Computer Science
University College London, UK
ABSTRACT
We introduce a general learning framework for private machine learning based on
randomised response. Our assumption is that all actors are potentially adversarial
and as such we trust only to release a single noisy version of an individual’s
datapoint. Our approach forms a consistent way to estimate the true underlying
machine learning model and we demonstrate this in the case of logistic regression.
1 P RIVATE MACHINE LEARNING
Our desire is to develop a strategy for machine learning driven by the requirement that private data
should be shared as little as possible and that no-one can be trusted with an individual’s data, neither
a data collector/aggregator, nor the machine learner that tries to ﬁt a model.
Randomised Response, see for example Warner (1965), is relevant in this context in which a datapoint
xnis replaced with a randomised ‘noisy’ version ~xn. A classical example is voting in an election in
which an individual voter votes for one of two candidates AorBand is asked to lie (with probability
p) about whom they voted for . This results in noisy data and estimating the fraction fAof voters that
voted for candidate Abased on this noisy data
~fA=1
NNX
n=1I(~xn=A) (1)
can give a potentially signiﬁcantly incorrect estimate. As Warner (1965) showed, since we know
the probabilistic mechanism that generated the noisy data, a better estimate of the fraction of voters
voting for candidate Ais given by
fA=~fA+p
1 2p(2)
In a machine learning context, the kind of scenario we envisage is that users may have labelled face
images as “happy" or “sad" on their mobile phones and the company MugTome wishes to train a
“happy/sad" face classiﬁer; however, users do not wish to send the raw face images to MugTome and
also wish to be able to plausibly deny which label they gave any training image. To preserve privacy,
each user will send to MugTome only a single corrupted datapoint — a single corrupted image and a
single corrupted label.
It is straightforward to extend our approach to deal with users sending multiple corrupted datapoints.
However, since MugTome will potentially then know which corrupted datapoints belong to each user,
they will have more information to help reveal the underlying clean datapoint. Since we assume we
cannot trust MugTome, MugTome may attempt to recover the underlying true datapoint. For example,
if a user sends three class labels c1;c2;c3,ci2f0;1g, then MugTome can have a good guess of the
underlying true class label by simple taking the majority class c=I(c1+c2+c3>2). Indeed, in
general, ifMcorrupted datapoints are independently generated for a user, then MugTome’s ability
to reveal the true class (or attribute) increases dramatically. For example, if MugTome know the
corruption mechanism p(cmjctrue)the posterior of the class is given by
p(ctruejc1;:::;cM)/p(ctrue)MY
m=1p(cmjctrue) (3)
1arXiv:2001.04942v2  [cs.LG]  24 Feb 2020
wherep(ctrue)is the prior belief on the true class. This posterior distribution concentrates exponen-
tially quickly (in M) around the true value ctrue. Similarly, if a pollster asks each voter three times
what they voted, then the questioner would have a very good idea of the true vote of each voter; to
protect the voter’s privacy, the voter would then have to trust that the pollster either does not pass on
any information that states that the three votes came from the same person or that the pollster doesn’t
attempt themselves to ﬁgure out what the voter voted for.
Similarly, in a medical setting in which a patient privately owns a datapoint, releasing Msynthetic
versions (corruptions) of that datapoint can compromise privacy if which synthetic datapoints belong
to each person is also known. To guarantee that privacy is retained would require patients to trust
people with their data, namely that any data aggregation process will remove their patient ID. However,
this is something out of the control of the patient and as such we do not consider generating multiple
synthetic datapoints (see for example Bindschaedler et al. (2017)) a ‘safe’ mechanism.
For these reasons, we wish to make a process in which an individual only reveals a single corrupted
datapoint; from that point onwards in the machine learning training process, no other trust in that
process is required. To motivate our general approach to private machine learning we discuss the
voting example in more detail in section(3). Connections to other forms of privacy preserving machine
learning are discussed in section(7). The justiﬁcation for our approach hinges on the properties of the
Spread Divergence, which we review in the following section.
2 S PREAD DIVERGENCE
Throughout we use the notation p(X=x)for a random variable Xin statex. However, to reduce
notational overhead, where unambiguous, we write simply p(x).
A divergence D(pjjq)(see, for example Dragomir (2005)) is a measure of the difference between two
distributions pandqwith the property
D(pjjq)0and D (pjjq) = 0,p=q (4)
An important class is the f-divergence, deﬁned as
Df(pjjq) =Eq(x)
fp(x)
q(x)
(5)
wheref(x)is a convex function with f(1) = 0 . A special case of an f-divergence is the well-known
Kullback-Leibler divergence KL(pjjq) =Ep(x)h
logp(x)
q(x)i
which is widely used to train models using
maximum likelihood. For the Spread Divergence, from q(x)andp(x)we deﬁne new distributions
~q(~x)and~p(~x)that have the same support. Using the notationR
xto denote integrationR
()dxfor
continuousx, andP
x2Xfor discretexwith domainX, we deﬁne a random variable ~xwith the same
domain asxand distributions
~p(~x) =Z
xp(~xjx)p(x);~q(~x) =Z
xp(~xjx)q(x) (6)
wherep(~xjx)‘spreads’ the mass of pandqsuch that ~p(~x)and~q(~x)have the same support. For
example, if we use a Gaussian p(~xjx) =N 
~xx;2
, then ~pand~qboth have support R. The spread
divergence has a requirement on the noise p(~xjx), namely that D(~pjj~q) = 0,p=q; that is, if
the divergence of the spreaded distributions is zero, then the original non-spreaded distribution will
match. As shown in Zhang et al. (2018) this is guaranteed for certain ‘spread noise’ distributions.
In particular, for continuous xand~xof the same dimension and injective function f, a sufﬁcient
condition for a valid spread noise p(~xjx) =K(~x f(x))is that the kernel K(x)has strictly positive
Fourier Transform. For discrete variables, a sufﬁcient condition is that p(~x=ijx=j) =Pijis that
Pij>0and the matrix Pis square and invertible.
Spread divergences have a natural connection to privacy preservation and Randomised Response
(Warner, 1965). The spread divergence suggests a general strategy to perform private machine
learning. We ﬁrst express the machine learning problem as minDf(p(X)jjp(X))for a speciﬁed
modelp(X). Then, given only noisy data ~X, we ﬁt the model by minDf
~p(~X)jj~p(~X)
. To
explain in more detail how this works, we ﬁrst describe randomised response in a classical voting
context and then justify how to generalise this to principled training of machine learning models
based on corrupted data.
2
3 A CLASSICAL VOTING EXAMPLE
There are two candidates in an election, candidate “one" and candidate “zero" and Alice would like
to know the fraction of voters that voted for candidate “one". We write the dataset of voting as a
collection of binary values fx1;:::;xNg,xn2f0;1g.
3.1 L EARNINGUSING CLEAN DATA
If we assume that Alice has full knowledge of which candidate each voter voted for, then clearly
Alice may simply count the fraction of people that voted for “one” and set
=1
NNX
n=1xn (7)
It will be useful to ﬁrst consider how to arrive at the same result from a modelling perspective. We
can consider an independent Bernoulli model
p(X1=x1;:::;XN=xN) =NY
n=1p(Xn=xn) (8)
where
p(X= 1) = (9)
so that
p(X=x) =x(1 )1 x(10)
We also construct an empirical data distribution that places mass only on the observed joint state,
namely
^p(X1;:::;XN) =NY
n=1(Xn;xn) (11)
where(x;x0)is the Kronecker delta function. Then
1
NKL(^p(X1;:::;XN)jjp(X1;:::;XN)) =LN() +const: (12)
where
LN() =1
NNX
n=1logp(Xn=xn) (13)
=1
NNX
n=1(xnlog+ (1 xn) log (1 )) (14)
and minimising KL(^pjjp)(or maximising LN()) with respect to recovers the fraction of votes
that are 1, equation(7). This shows how we can frame estimating the quantity from uncorrupted
private data as a divergence minimisation problem.
3.2 L EARNINGUSING CORRUPTED DATA
Returning to the privacy setting, Bob would also like to know the fraction of votes that are 1. However,
Alice does not want to send to Bob the raw data x1;:::;xNsince the votes of any individual should
not be explicitly revealed. To preserve privacy, Alice sends noisy data ~x1;:::; ~xNto Bob. In this
case we draw a single joint sample ~x1;:::; ~xNfrom the distribution
p(~X1;:::; ~XNjX1;:::;XN) =NY
n=1p(~XnjXn) (15)
3
where the ‘spread noise’ model is p(~Xn=ijXn=j) =Pij. Hence, ifxn= 0Alice draws a sample
~xn= 0with probability P00and~xn= 1with probability P10.
Given a sampled noisy dataset ~x1;:::; ~xNwe form an empirical spreaded data distribution
^p(~X1;:::; ~XN) =NY
n=1
~Xn;~xn
(16)
Similarly, the corrupted joint model is given by
~p(~X1;:::; ~XN) =NY
n=1~p(~Xn) (17)
where
~p(~X=j) =X
j2f0;1gp(~X=jjX=j)p(X=j) (18)
On receiving the noisy dataset ~x1;:::; ~xN, Bob can try to estimate by minimising
1
NKL
^p(~X1;:::; ~XN)jjp(~X1;:::; ~XN)
= 1
NNX
n=1log ~p(~xn) +const: (19)
with respect to . Equivalently, he may maximise the scaled spread log likelihood
~LN() =1
NNX
n=1log ~p(~xn) (20)
For this simple model, Bob can easily explicitly calculate
~p(~x= 1) =p(~x= 1jx= 1)p(x= 1)+p(~x= 1jx= 0)p(x= 0) =P11+P10(1 )(21)
Similarly, ~p(~x= 0) =P01+P00(1 ). In this case, equation(20) becomes
~f0log (P00(1 ) +P01) +~f1log (P10(1 ) +P11) (22)
where
~f1=1
NNX
n=1~xn (23)
Using ~f0+~f1= 1,P00+P10= 1,P01+P11= 1, the maximum of the spread log likelihood is at
=~f1+P10
1 P10 P01(24)
which forms Bob’s estimate of the underlying fraction of voters that voted for candidate “one”.
For example, if there were no noise P10=P01= 0, Bob would estimate =~f1, simply recovering
the fraction of votes that are 1 in the original data. In the limit of a large number of votes N!1
and true probability 0of a voter voting for candidate “one”, then ~f0tends toP00(1 0) +P010
and Bob’s estimate recovers the true underlying voting probability =0. Hence, even though Bob
only receives a corrupted set of votes, in the limit of a large number of votes, he can nevertheless
estimate the true fraction of people that voted for candidate “one”.
4 P RIVATE MACHINE LEARNING USING RANDOMISED RESPONSE
The above example suggests a general strategy to perform private machine learning:
1.Phrase problem as likelihood maximisation: We ﬁrst assume that a machine learning task
for private data x1;:::;xNcan be expressed as learning a data model p(X)by optimising
an objective
LN() =1
NNX
n=1logp(Xn=xn) (25)
4
2. Form a corrupted dataset: Draw a single joint sample ~x1;:::; ~xNfrom the distribution
p(~X1;:::; ~XNjX1;:::;XN) =NY
n=1p(~XnjXn) (26)
wherep(~XjX)is a deﬁned spread noise distribution and known by both the owner of the
private data and the receiver of the corrupted data. To do this, we go through each element
of the dataset xnand replace it with a corruption ~xnsampled from p(~Xn= ~xnjXn=xn).
3.Send data to learner: We then send to the learner the corrupted dataset ~x1;:::; ~xN, the
model to be learned p(X)and the corruption probability p(~XjX).
4.Estimatefrom corrupted data: Having received the corrupted data ~x1;:::; ~xN, the learner
ﬁtsby maximising the objective
~LN() =1
NNX
n=1log ~p(~xn) (27)
where
~p(~x) =Z
xp(~xjx)p(x) (28)
4.1 J USTIFICATION
If we assume that each element xnof the training data x1;:::;xNis identically and independently
sampled from a model p0(Xn=xn), then each corrupted observation ~xnis a sample from the same
distribution given by
~p0(~XN=yn) =Z
xp(~Xn=ynjX=x)p0(X=x) (29)
By the law of large numbers the objective equation(27) approaches its average over the data generating
mechanism
lim
N!1LN()a:s:  !Z
~x~p0(~X= ~x) log ~p(~X= ~x) (30)
and maximising the spread likelihood objective ~LN()becomes equivalent to minimising
KL
~p0(~X)jj~p(~X)
(31)
Provided that the spread noise is valid (see section(2)), then
KL
~p0(~X)jj~p(~X)
= 0)=0 (32)
for an identiﬁable model p. Thus
est= argmax
~LN() (33)
is a consistent estimator.
This means that (in the large data limit and assuming the training data is generated from the model),
even though we only train on corrupted data, we are optimising an objective ~LN()which has a
global minimum close to that of the objective on uncorrupted data LN(). Indeed, the estimator is
consistent in the sense that as the amount of training data increases, we will recover the true clean data
generating mechanism. Hence, provided that the corruption process is based on spread noise, then
we can still learn the model parameters even by training on only corrupted data. In our motivating
voting scenario in section(3), we saw explicitly that the estimate of the true underlying voting
fraction is consistent and indeed, this is a general property of our approach.
5
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0(a)pf= 0
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0 (b)pf= 0:001
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0 (c)pf= 0:002
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0 (d)pf= 0:003
0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0 (e)pf= 0:004
Figure 1: Training based on the reconstruction approach, section(4.3) for the model with binary
variablep(x= 1) =,p(x= 0) = 1 . In each case we plot along the x-axis the true 0from
0 to 1 and on the y-axis the value of that maximises J1(). In each plot we use a different ﬂip
probability. For a consistent estimator we would require that each plot is a straight x=yline, which
only occurs in the case of no noise, pf= 0.
4.2 T RAINING ON NOISE ONLY
A common approach in private machine learning is to form synthetic (noisy, corrupted) data and then
simply train the standard model on this noisy data — see for example Li et al. (2019). In our notation,
this would be equivalent to maximising the likelihood
L0
N()1
NNX
n=1logp(X= ~xn) (34)
As above, assuming that the training data is generated from an underlying model p0(Xn=xn), by
the law of large numbers,
lim
N!1L0
N()a:s:  !Z
~x~p0(~X= ~x) logp(X= ~x) (35)
In general, the optimum of this objective does not occur when =0and therefore training on noisy
data alone does not form a consistent estimator of the true underlying model.
We discuss learning with noisy labels more extensively in the context of logistic regression in
section(B) in which we show that provided the label ﬂip noise is not too high p0!1+p1!0<1, and
for zero mean isotropically Gaussian distributed inputs, maximum likelihood training with corrupted
class labels does form a consistent estimator. Hence, whilst one cannot guarantee that maximum
likelihood training of logistic regression on noisy data will result in a consistent estimator, there are
special situations in which this may work.
4.3 R ECONSTRUCTION APPROACH
A seemingly natural alternative to our method is to attempt to reconstruct the clean datapoint from
the noisy datapoint and use that within a standard learning framework. This approach would give an
objective
JN() =1
NNX
n=1Z
xnp(xnj~xn) logp(xn) (36)
Here we need to deﬁne a posterior distribution p(xnj~xn)to reconstruct the clean datapoint. Since the
learner only has knowledge of the prior p(x)it is natural to set
p(xnj~xn) =p(xnj~xn)p(~xnjxn)p(xn)R
xnp(~xnjxn)p(xn)(37)
By the law of large numbers JNconverges to its expectation with respect to the true data generating
mechanismp0(~x) =R
p(~xjx)p0(x), so that
lim
N!1JN()a:s:  !Z
x;~xp0(~x)p(xj~x) logp(x)J1() (38)
6
In general, the optimum of J1()is not at=0. To demonstrate this, we plot in ﬁgure(1) the
optimalfor a simple Bernoulli model for which we can calculate J1()exactly. As we see, for
all but zero ﬂip noise, pf= 0, the estimator does not correctly identify the underlying probabilty
generating mechanism. For this reason, we do not pursue this approach further.
4.4 O THER DIVERGENCES
An extension of the above is to learn by minimising other f-divergences
Df(~p(Y)jj^p(Yjy)) =E^p(Yjy)
f~p(Y)
^p(Yjy)
(39)
However, this generalisation to any f-divergence is harder to justify since the expectation of this
objective (by averaging over the noise realisations)Z
~p(y)Df(~p(Y)jj^p(Yjy)) (40)
will not in general give a divergence between spreaded distributions. This means that in the limit of a
large number of datapoints, it is not guaranteed to recover the true data generating process, except for
special choices of the f-divergence, such as the KL divergence. We leave a discussion of this for
future work.
5 P RIVATE LOGISTIC REGRESSION
As an application of the above framework to a standard machine learning model, we now discuss
how to form a private version of logistic regression.
Returning to our motivating example, users may have labelled face images as “happy" or “sad"
on their mobile phones and the company MugTome wishes to train a “happy/sad" face classiﬁer;
however, users do not wish to send the raw face images to MugTome and also wish to be able to
plausibly deny which label they gave any training image.
In this case we have a set of training data x1;:::;xN,xn2RDand corresponding binary class
labelsc1;:::;cN,cn2f0;1g. We wish to ﬁt a logistic regression model
p(cjx) =((2c 1)T
cx) (41)
where(x) = 1=(1 +e x)is the logistic function. We follow the general approach outlined in
section(4).
1. The model: For observation (x;c)and parameter 
p(c;x) =pc(cjx)px(x) (42)
wherepc(cjx)is the standard logistic regression model above and px(x)is a model of the
inputx. The training objective is
LN() =1
NNX
n=1logp(cn;xn) (43)
=1
NNX
n=1logpc(cnjxn) +1
NNX
n=1logpx(xn) (44)
We note that this is a separable objective for LN() =Lc
N(c) +Lx
N(x), in which the
logistic regression parameters care conditionally independent (conditioned on the training
data) of the input parameters x.
2.Form the corrupted dataset: We wish to send noisy data ~x1;:::; ~xN,~c1;:::; ~cNto the
learner. To do so we need to deﬁne a corruption model p(~c;~xjc;x). For simplicity, we
consider a corruption model of the form
p(~c;~xjc;x) =p(~cjc)p(~xjx) (45)
The corruption processes of p(~cjc)andp(~xjx)are problem speciﬁc; see the experiments
section(6) for some examples.
7
3.Send to learner corrupted data and model: The corrupted labels and inputs are sent to
the learner (~c1;~x1);:::; (~cN;~xN)along with the model pc(cjx),px(x)and corruption
processp(~cjc),p(~xjx).
4. Learn the model parameters : The spread log likelihood is
~L() =1
NNX
n=1log ~p(~cn;~xn) (46)
=1
NNX
n=1logZ
xn;cnp(~cnjcn)p(~xnjxn)pc(cnjxn)px(xn) (47)
Unfortunately, in all but special cases, the integral (for continuous x) or sum (for discrete
x) required to evaluate ~Lis not tractable and numerical approximation is required. For this
stage, there are many options available and we present below the approach taken in the
experiments.
Interestingly, we note that, unlike training on clean data, the objective ~L()is not separable
into a function of cplus a function of x, meaning that learning the class prediction
parametercis coupled with learning the input distribution parameter x.
5.1 I MPLEMENTATION
In general, the spread noise deﬁnes a distribution on a pair of spread variables p(~c;~xjc;x)and the
full joint distribution, including the original model is
p(~c;~x;c;x ) =p(~c;~xjc;x)pc(cjx)px(x) (48)
For continuous x, the spread likelihood is then obtained from
p(~c;~x) =X
cZ
xp(~c;~x;c;x ) (49)
In general, this sum/integral over xis intractable due to the high-dimensionality of x. We use a
standard approach to lower bound the log likelihood (for a single datapoint) by
logp(~c;~x) Eq(c;xj~c;~x)[logq(c;xj~c;~x)] +Eq(c;xj~c;~x)[logp(~c;~xjc;x)pc(cjx)px(x)]
(50)
whereqis a distribution chosen to make the bound tight, see for example Barber (2012). This allows
us to use an EM-style procedure in which we iterate between the two steps : (M-step) ﬁx qand
optimiseand (E-step) ﬁx and updateq.
1. Iteration kM-step: Update to increase the “energy”
k+1= argmax
E(;qk) (51)
where (for multiple datapoints)
E(;q)NX
n=1Eq(cn;xnj~cn;~xn)[logpc(cnjxn)]+NX
n=1Eq(xnj~cn;~xn)[logpx(xn)](52)
An advantage of this approach is that E(;q)is separable and we can update the class
prediction parameter cindependently of the input distribution parameter x.
In practice we will typically only do a partial optimisation (gradient ascent step) over to
guarantee an increase in the energy.
2.IterationkE-step: The bound is tightest when qis set to the posterior (see for example
Barber (2012)),
qk+1(c;xj~c;~x) =p(c;xj~c;~x) =p(~cjc)p(~xjx)p(cjx)p(x)
Z(~c;~x)(53)
wherep(cjx) =pkc(cjx),p(x) =pkx(cjx)and the normaliser is given by
Z(~c;~x)X
cZ
p(~cjc)p(~xjx)p(cjx)p(x)dx (54)
8
To implement the M-step, Equation(52) requires expectations of the form
X
cZ
p(c;xj~c;~x)f(x;c)dx (55)
for some function f(x;c). Assuming that the posterior will be reasonably peaked around the noisy
data we use sampling with an importance distribution
(c;xj~c;~x) =(cj~c)(xj~x) (56)
The expectation is then motivated by
X
cZ
xp(c;xj~c;~x)f(x;c) =X
cZ
x(cj~c)(xj~x)p(~cjc)p(~xjx)p(cjx)p(x)
(cj~c)(xj~x)Z(~c;~x)f(x;c) (57)
Choosing
(cj~c) =p(~cjc)
Z(~c);  (xj~x) =p(~xjx)p(x)
Z(~x)(58)
for normalising functions Z(~c),Z(~x)we then run a standard importance sampling approximation
(see section(A)). For a given noisy datapoint (~cn;~xn)we generate a set of Ssamplesc1
n;:::;cS
nfrom
(cj~cn)and samples x1
n;:::;xS
nfrom(xj~xn)and compute the importance weights
w(sjn) = 
(2cs
n 1)T
cxs
n
P
s((2csn 1)Tcxsn)(59)
The energy equation(52) separates into two independent terms (see section(A))
E(c;q)NX
n=1SX
s=1w(sjn) log 
(2cs
n 1)T
cxs
n
(60)
and
E(x;q)NX
n=1SX
s=1w(sjn) logpx(xs
n) (61)
Equation(60) is a weighted version of the standard logistic regression log likelihood, Lc(c)in
equation(44); similarly equation(61) is a weighted version of Lx(x). The advantage therefore is that,
given the importance samples, the learning procedure for requires only a minor modiﬁcation of the
standard maximum likelihood training procedure on clean data.
The full procedure is that we randomly initialise and then, for each datapoint n, draw samples and
accumulate the gradient across samples and datapoints. After doing a gradient ascent step in , we
update the importance distributions and repeat until convergence.
The Importance Sampling approximation is a convenient approach, motivated by the assumption
that corrupted datapoints will be close to their uncorrupted counterparts. Whilst we used a bound
as part of the approximation, this is not equivalent to using a parametric qdistribution; by sampling
we form a consistent estimator of the tightest possible lower bound. In other words, we are simply
using Importance Sampling to estimate the expectations required within a standard Expectation
Maximisation algorithm, see for example Barber (2012). We also tried learning a parametric q, similar
to standard variational approaches to log likelihood maximisation, but didn’t ﬁnd any improvement
on the Importance Sampling approach.
5.2 L EARNING THE PRIOR
If we have access to clean data, the optimal input model px(x)can be learned from maximising the
likelihoodLx(x). However, our general assumption is that we will never have access to clean data.
There may be situations in which the learner has a good model of px(x), without compromising
privacy (for example a publicly available dataset for a similar prediction problem might be available)
in which case it makes sense to set the prior to this known model.
9
In the absence of a suitable prior we can attempt to learn px(x)from the corrupted data by maximising
~L(). For simplicity we assume a factorised model and for a D-dimensional input vector x=
(x[1];:::;x [D])write
px(x) =DY
d=1p(x[d]jd) (62)
for a collection of learnable univariate distributions p(x[d]jd),d= 1;:::;D . Under this assumption,
and using the Importance Sampling approach in equation(61), this means that p(x[d]jd)can be
learned by maximising
Ex=NX
n=1SX
s=1w(sjn)DX
d=1logp(xn
s[d]jd) (63)
Since this is a separable objective, we can learn each p(xn
s[d]jd)independently.
For simplicity, we assume a discrete distribution for x[d]that contains Kstates (or bins). Then
Ex[d] =KX
k=1NX
n=1SX
s=1w(sjn)I(xn
s[d]2k) logp(kjd) (64)
where I(xn
s[d]2k)is 1 if the sample xn
s[d]is in thekthstate and 0 otherwise. Optimising with
respect top(kjd)we obtain
p(kjd) =PN
n=1PS
s=1w(sjn)I(xn
s[d]2k)
PK
k=1PN
n=1PS
s=1w(sjn)I(xns[d]2k)(65)
For the M-step of the algorithm we then make a gradient update for cand update the prior using
equation(65).
6 E XPERIMENTS
We implemented our approach in section(5) to train a logistic regression classiﬁer to distinguish
between the MNIST digits 7 and 9 based on noisy data (250 train and 900 test examples from each
class). We chose to train on a small dataset since this constitutes the most challenging scenario and
helps highlight potential differences between rival approaches. The MNIST images xhave pixesl
with 256 states and we used a discrete distribution to model x.
For our experiments we assume a corruption model p(~cjc)that ﬂips the label 0!1and1!0with
probabilitypfwith probability pf. We also assume here for simplicity assume a factorised input
corruption model p(~xjx) =QD
d=1p(~x[d]jx[d])in which with probability 1 pfand uniformly from
the other states of that pixel with probability pf.
In this case, computing the Importance Sampling distribution is straightforward since the posterior is
factorised over the image pixels. We considered three settings for the prior (required to compute the
Importance Sampling distribution) : (i) ﬂat prior, (ii) learned prior using EM, (iii) true factorised prior
based on computing the marginal distribution of each pixel on the training dataset. In the ‘true prior’
case we assumed that we know the true marginal distribution of each pixel p(x[d]jd)– in general,
this information would be private, but it is interesting to consider how much improvement is obtained
by knowing this quantity.
We compare the following approaches:
Log Reg Clean Data We trained logistic regression on clean data. This sets an upper limit on the
expected performance.
Log Reg on Noisy Data We trained a standard logistic regression model but using the corrupted
data. This forms a simple baseline comparison.
Spread Log Reg with Learned Prior We used our Spread Likelihood approach to learn the prior.
10
(a)
 (b)
 (c)
 (d)
 (e)
(f)
 (g)
 (h)
 (i)
 (j)
Figure 2: (a) An example MNIST “7” alongside its noisy examples (b) pf= 0:1, (c)pf= 0:2, (d)
pf= 0:3, (e)pf= 0:4) which is sent to Mugshot.com noise. Each pixel remains in state 1 pfand
is otherwise sampled uniformly from the available states of that pixel. The bottom row shows an
example of a clean “9” (f) and corruptions.
Spread Log with ‘True Prior’ In general our assumption is that the true prior will not be known
(since this requires users to release their private data to the prior learner). However, this
forms an interesting comparison and expected upper bound on the performance of the spread
approach.
Spread Log with Flat Prior In this case we used an informative, ﬂat prior on all pixel states.
We ran 10 experiments for each level of ﬂip noise pffrom 0:1;0:2;0:3;0:4and then tested the
prediction accuracy of the learned logistic classiﬁers on clean hold out data, see ﬁgure(3).
For all but small noise levels, the results show the superiority of the spread learning approach over
simply training on noisy data, consistent with our theory that training the standard model on noisy
data does not in general form a consistent estimator. The best performing spread approach is that
which uses the true prior – however, in general this true prior will not be available. For this experiment,
there appears to be little difference between using a ﬂat prior and a learned prior.
The performance of standard logistic regression training but using corrupted data is surprisingly
effective, at least at low noise levels. However the performance degrades quickly for higher noise
levels. A partial explanation for why logistic regression may give good results simply trained on
noisy data is given in section(B).
6.1 G AUSSIAN INPUT PRIORp(x)
We also demonstrate here training logistic regression treating the pixels as continuous. If we an
independent have (per pixel) a Gaussian prior
p(xi) =N 
xii;2
i
(66)
and independent Gaussian spread noise
p(~xijxi) =N 
~xixi;2
i
(67)
then using the Importance Sampling posterior is
(xij~xi) =N
ximean =bi
ai;var =1
ai
(68)
where
ai=1
2
i+1
2
i; bi=~xi
2
i+i
2
i(69)
We also used Gaussian spread noise to corrupt the images and train a binary classiﬁer to distinguish
between the MNIST digits 7 and 9 based on noisy data (4500 train and 900 test examples from
each class). For simplicity, we assumed factorised distributions with prior p(xi) =N 
xii;2
i
,
11
0.1 0.2 0.3 0.40.700.750.800.850.900.95
log reg
SD true prior
SD learn prior
SD flat prior
log reg noiseFigure 3: The test accuracy (on clean data) of the trained logistic regression models, averaged over 10
different randomly chosen training datasets of 500 datapoints. The x-axis is the corruption probability
pf. “log reg”: Standard logistic regression training on clean data; “SD true prior”: spread divergence
training approach with true prior; “SD learn prior”: spread approach with learned prior; “SD ﬂat
prior”: spread approach with ﬂat prior; “log reg noise”: standard logistic regression training but
trained on noisy data.
p(~xijxi) =N 
~xixi;2
i
. We chose spread ﬂip noise pf= 0:2for the class labels and uniform
spread noise with variance 2
i= 0:1; the priorp(x)was set to be quite uninformative with mean
i= 0and variance 2
i= 10 . This level of noise means that approximately 20% of the class labels
are incorrect in the data passed to MugTome and the associated image is signiﬁcantly blurred, see
ﬁgure(4). For standard logistic regression we found that for a learning rate of 0.2, 400 iterations gave
the best performance, with 95.5% train accuracy and 95.7% test accuracy. Using our Importance
Sampling scheme with S= 2samples per noisy datapoint, the trained when tested on clean images
had 94.4% test accuracy. This shows that despite the high level of class label and image noise,
MugTome are able to learn an effective classiﬁer, preserving the privacy of the users. The loss in test
and training accuracy, despite this high noise level is around a modest 1%. When using higher spread
noise with variance 2= 0:5, thelearned on the noisy data had a clean data test accuracy 93%,
which is also a modest decrease in accuracy for a signiﬁcant increase in privacy.
For future work it would be interesting to consider other forms of noise, for example downsampling
images. However, downsampling does not form an injective mapping and as such we cannot guarantee
that we can ﬁnd a consistent estimator for the underlying model.
7 D ISCUSSION
There are many forms of private machine learning. Some attempt to transform a datapoint xto a form
x0such that a protected attribute a(such as gender) cannot be recovered from x0, yet the prediction
(b)
 (c)
 (d)
 (e)
 (f)
Figure 4: (a) An example MNIST “7” alongside its noisy example which is sent to MugTome with
Gaussian noise variance (b) 2= 0:1and (c)2= 0:5; (d,e,f) similarly for an MNIST “9”.
12
of an output y(for example using p(yjx0)) is retained. For example this could be achieved by using a
loss function such as (see for example Li et al. (2019))
L(;; ) =X
n[Ly(yn;y(x0
n;)) La(an;a(x0
n;))] (70)
wherenis the data index, y(x0;)is a function that takes input x0and outputs a prediction y;a(x0;)
is a function that takes input x0and outputs an attribute prediction aandx0=f(x; )gives a
representation of the input; Ly,Laare loss functions. In this protected attribute setting, typically
some form of the clean dataset is required to learn the parameters ;; .
Another common form of private machine learning is based on differential privacy (Dwork & Roth,
2014), with the aim to make it difﬁcult to discern whether a datapoint xnwas used to train the
predictory(x;). That is, given a trained model, differential privacy attempts to restrict the ability to
differentiate whether any individual’s datum was used to train the model.
A closely related concept to randomised response is that of plausible deniability, namely privately
corrupting a datapoint xnsuch that no-one (except the datapoint provider) can conﬁdently state what
the original (private) value of xnis. Recently Bindschaedler et al. (2017) used this to create synthetic
datapoints, which were subsequently used with a standard machine learning training approach. The
authors showed that generating synthetic data ~xfrom a distribution p(~xjx)that takes dependency
amongst the elements of the vector xresults in better machine learning predictors than sampling
from a factorised distribution. In synthetic data generation approaches the assumption is that the
statistical characteristics are similar to the real data. However, care is required since if the generating
mechanism is powerful, it may generate data which is very similar to the private data.
In general these synthetic data generating approaches do not take into consideration when learning the
parameters of the machine learning model what that synthetic data generation mechanism is. This is
analogous to simply using the corrupted votes to directly estimate the fraction of voters that voted for
a candidate, equation(1), rather than using knowledge of the data generation approach, equation(2).
8 S UMMARY
We discussed a general privacy preserving mechanism based on random response in a datapoint is
replaced by a corrupted versions. We showed that, provided the corruption process is a valid spread
noise, then a maximum likelihood approach forms a consistent estimator. That is, even though the
model is only trained on corrupted, synthetic data, it is possible to recover the true underlying data
genering mechnanism on the clean data. We applied this approach to a simple logistic regression
model, showing that the approach can work well, even with high levels of noise. The approach is
readily applicable to a large class of much more complex models and other divergences.
ACKNOWLEDGEMENTS
I would like to thank Xijie Hou for useful discussions.
REFERENCES
D. Barber. Bayesian Reasoning and Machine Learning . Cambridge University Press, New York, NY ,
USA, 2012. ISBN 0521518148, 9780521518147.
Vincent Bindschaedler, Reza Shokri, and Carl A. Gunter. Plausible deniability for privacy-
preserving data synthesis. CoRR , abs/1708.07975, 2017. URL http://arxiv.org/abs/
1708.07975 .
S. S. Dragomir. Some general divergence measures for probability distributions. Acta Mathematica
Hungarica , 109(4):331–345, Nov 2005. ISSN 1588-2632. doi: 10.1007/s10474-005-0251-6.
C. Dwork and A. Roth. The Algorithmic Foundations of Differential Privacy. Found. Trends Theor.
Comput. Sci. , 9(3–4):211–407, 2014. ISSN 1551-305X. doi: 10.1561/0400000042.
Ang Li, Jiayi Guo, Huanrui Yang, and Yiran Chen. Deepobfuscator: Adversarial training framework
for privacy-preserving image classiﬁcation, 2019.
13
S. L. Warner. Randomised response: a survey technique for eliminating evasive answer bias. Journal
of the American Statistical Association , 60(309):63–69, 1965.
Mingtian Zhang, Peter Hayes, Tom Bird, Raza Habib, and David Barber. Spread divergences, 2018.
A P RIVACY PRESERVING LOGISTIC REGRESSION
The posterior is given by
p(c;xj~c;~x) =p(~cjc)p(~xjx)p(cjx)p(x)P
cR
xp(~cjc)p(~xjx)p(cjx)p(x)=p(~cjc)p(~xjx)p(cjx)p(x)
Z(~c;~x)(71)
For the learning, we need to take expectations
X
cZ
xp(c;xj~c;~x)f(x;c)s (72)
We use importance sampling to approximate this expectation
X
cZ
xp(c;xj~c;~x)f(x;c) =X
cZ
x(cj~c)(xj~x)p(c;xj~c;~x)
(cj~c)(xj~x)f(x;c) (73)
=X
cZ
x(cj~c)(xj~x)p(~cjc)p(~xjx)p(cjx)p(x)
(cj~c)(xj~x)Z(~c;~x)f(x;c) (74)
Choosing
(cj~c) =p(~cjc)P
cp(~cjc)=p(~cjc)
Z(~c);  (xj~x) =p(~xjx)p(x)R
xp(~xjx)p(x)=p(~xjx)p(x)
Z(~x)(75)
we have
X
cZ
xp(c;xj~c;~x)f(x;c) =Z(~c)Z(~x)X
cZ
x(cj~c)(xj~x)p(cjx)
Z(~c;~x)f(x;c) (76)
Here
Z(~c;~x) =X
cZ
x(cj~c)(xj~x)p(~cjc)p(~xjx)p(cjx)p(x)
(cj~c)(xj~x)(77)
=X
cZ
x(cj~c)(xj~x)p(~cjc)p(~xjx)p(cjx)p(x)
(cj~c)(xj~x)(78)
=Z(~c)Z(~x)X
cZ
x(cj~c)(xj~x)p(cjx) (79)
Putting this together and using the same samples to estimate the numerator and denominator expecta-
tions,
X
cZ
xp(c;xj~c;~x)f(x;c) =X
cZ
x(cj~c)(xj~x)p(cjx)P
cR
x(cj~c)(~xjx)p(cjx)f(x;c) (80)
X
sp(csjxs)P
sp(csjxs)f(xs;cs) (81)
=X
sw(s)f(xs;cs) (82)
for importance weight
w(s) =p(csjxs)P
sp(csjxs)(83)
14
For the logistic regression case, we have
f(x;c) = logp(cjx) = log 
(2c 1)Tx
(84)
where(x) = 1=(1 + exp( x)).
The variational lower bound then becomes
NX
n=1SX
s=1w(sjn) log 
(2cs
n 1)Txs
n
(85)
where, for a given noisy datapoint ~cn;~xnwe generate a set of Ssamplesc1
n;:::;cS
nfrom(cj~cn)
and samples x1
n;:::;xS
nfrom(xj~xn)
w(sjn) = 
(2cs
n 1)Txs
n
P
s((2csn 1)Txsn)(86)
B T RAINING ON NOISY DATA
A common approach in private machine learning is to train the standard model based on noisy alone,
corresponding to maximising L0
N(), equation(35). As we discussed, this does not in general give a
consistent estimator of the true underlying model. To show this, we consider a logistic regression
in which only the class labels care corrupted with probabilities p0!1p(~c= 1jc= 0) and in the
same state with p1!1p(~c= 1jc= 1) , leaving the inputs xuncorrupted. In this case
L0
N() =1
NNX
n=1I(~cn= 1) log 
T
cxn
+I(~cn= 0) log 
1 (T
cxn)
(87)
If we assume that the true labels are drawn from an underlying model
p(c= 1jx) = 
T
0x
(88)
then the probability of a corrupted label is given by
p(~c= 1jx) =p1!1 
T
0x
+p0!1 
1  
T
0x
(89)
and by the law of large numbers, L0
Ntends to
L0
1()Ep(x)
p(~c= 1jx) log(T
cx) + (1 p(~c= 1jx)) log 
1 (T
cx))
(90)
Taking the gradient wrt c, we obtain
g=Ep(x) 
p(~c= 1jx)(1 (T
cx))x (1 p(~c= 1jx))(T
cx)
x
(91)
A sufﬁcient condition for the gradient to be zero is
p(~c= 1jx)(1 (T
cx)) = (1 p(~c= 1jx))(T
cx) (92)
That is
p(~c= 1jx) =(T
cx) (93)
which is
p1!1 
T
0x
+p0!1 
1  
T
0x
=(T
cx) (94)
In general, equation(94) does not have a solution at c=0.
To understand when the objective equation(90) has an optimum, we assume the data is drawn from
p(x) =N(x;), then, deﬁning
z1=T
0x; z 2=T
cx (95)
Sincexis Gaussian distributed, zis also Gaussian distributed with
E[z1] =T
0;E[z2] =T (96)
15
E[z1z2] E[z1]E[z2] =T0 (97)
E
z2
1
 E[z1]2=T
00 (98)
E
z2
2
 E[z2]2=T (99)
We can then write the large data limit log likelihood as a two dimensional expectation
L0
1() =E[(p1!1(z1) +p0!1(1 (z1))) log(z2)]
+E[1 p1!1(z1) p0!1(1 (z1))) log (1 (z2)))] (100)
For simplicity, consider = 0, =s2I,T
00= 1,T= 1,T
0= cos(). It is straightforward
to show that in this case the gradient with respect to is zero when = 0, namely when =0.
However, in general, for non-isotropic data covariance , the gradient is non-zero at =0.
To derive the above result, we note that the covariance for zin this case is simply
Cs2
1 cos
cos 1
(101)
We now use the decomposition C=MMT, with Cholesky factor
M=s
1 0
cossin
(102)
Then drawing a sample from zis equivalent to zMforN (0;I). Deﬁning
(x) =p1!1(x) +p0!1(1 (x)) (103)
we can then write the expected log likelihood as a function of :
L0
1() =EN(0;I)[(Z1(1)) log(Z2(1;2)) + (1 (Z1(1))) log (1 (Z2(1;2)))]
(104)
where the functions are deﬁned as
Z1(1) =s1; Z 2(1;2) =s(1cos+2sin) (105)
Differentiating L0
1()with respect to , we obtain
sEN(0;I)[((Z1(1)) (Z2(1;2))) (2cos 1sin)] (106)
When= 0 we note that Z2(1;2)is independent of 2and that the above is therefore is zero.
Hence ~L0
1()has zero gradient at = 0.
It is straightforward to show that the second derivative of L0
1()(evaluated at = 0) is
sEN(10;1)[ 1(s1)] s2EN(10;1)[(s1)(1 (s1))] (107)
The second term in equation(107) above is clearly negative. Using integration by parts (and noting
that we may assume s>0), one may easily show that the ﬁrst term is also negative provided that
p1!1>p0!1.
Hence we arrive at the (perhaps surprising) result that for zero mean isotropic Gaussian distributed
input data, training on noisy data (~c;x)in which the class labels have been ﬂipped with some
probability, results in a consistent estimator for 0, provided the ﬂip noise is not too high, namely
p1!1>p0!1, or equivalently, p0!1+p1!0<1. This result holds even in the case of asymmetric
ﬂip noisep0!16=p1!0.
More generally, even if the data p(x)is not Gaussian distributed, from the Central Limit Theorem,
p(z)is likely to be close to Gaussian distributed for high dimensional inputs. Hence, for input data x
that is roughly isotropically distributed, we can expect that training using maximum likelihood for
16
any classiﬁer of the form p(c= 1jx) = 
Tx
will likely be close to recovering the true 0that
generated the data (in the limit of a large number of datapoints).
The above analysis considered only noise on the class label. If, independently of the class label we
add isotropic Gaussian noise to the observations, then the projection zwill still be isotropic Gaussian
distributed for Gaussian inputs p(x)and the above argument trivially extends to this case as well.
Hence, one can expect training (using standard logistic regression but with corrupted inputs and
ﬂipped labels) to be partially successful at recovering the true data generating process provided that
the input data is close to isotropically distributed, motivating a whitening pre-processing step of the
input data.
17

LONG -TERM PLANNING WITH DEEPREINFORCEMENT
LEARNING ON AUTONOMOUS DRONES
A P REPRINT
Ugurkan Ates
Department of Computer Science
Gebze Technical University
Istanbul, Turkey
ugurkanates97@gmail.com
July 14, 2020
ABSTRACT
In this paper, we study a long-term planning scenario that is based on drone racing competitions held
in real life. We conducted this experiment on a framework created for "Game of Drones: Drone
Racing Competition" at NeurIPS 2019. The racing environment was created using Microsoft’s
AirSim Drone Racing Lab. A reinforcement learning agent, a simulated quadrotor in our case, has
trained with the Policy Proximal Optimization(PPO) algorithm was able to successfully compete
against another simulated quadrotor that was running a classical path planning algorithm. Agent
observations consist of data from IMU sensors, GPS coordinates of drone obtained through simulation
and opponent drone GPS information. Using opponent drone GPS information during training helps
dealing with complex state spaces, serving as expert guidance allows for efﬁcient and stable training
process. All experiments performed in this paper can be found and reproduced with code at our
GitHub repository
Keywords Deep Reinforcement Learning Path PlanningMachine Learning Drone Racing
1 Introduction
Deep Learning methods are replacing traditional software methods in solving real-world problems. Cheap and easily
available computational power combined with labeled big datasets enabled deep learning algorithms to show their
full potential. AlexNet paper(2012; Krizhevsky et al.[ 9]) showed feeding sufﬁcient data into deep neural networks
successfully learned to extract representations better than handcrafted features which let the start an era known as the
rise of Deep Learning. Their great success in solving otherwise hard engineering problems such as object detection ,
voice recognition, chatbots, robotic manipulation and autonomous systems shown they can be applied to various ﬁelds
thanks to their generalisation capability.[16]
Path Planning(Motion Planning) is deﬁned as computing a continuous path from starting position S to destination
position D while avoiding any known obstacles in the way.[ 20] Whether it is in 2D or 3D geometry , any robotic system
then will able to follow the computed path to reach it’s destination. Real World robotic systems tend to use more
explainable and reproducible algorithms based on interval based search (A star or Dijkstra ) orsampling-based
algorithms. We wanted to show a reward based algorithm that depends on Markov Decision Process (MDP ) by trying
to maximize cumulative future rewards can also complete long term path planning tasks. Advantage of using this option
will allow autonomous robot(in our case simulated quadrotor) to create paths in non holonomic constraints which is
something current methods fails to achieve.[1][17]
Evaluation run test video can be seen at YouTubearXiv:2007.05694v1  [cs.LG]  11 Jul 2020
Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones A P REPRINT
Figure 1: An overview of simulated racing track.
Deep reinforcement learning (RL) has been demonstrated to effectively learn a wide range of reward driven skills,
including playing games (Mnih et al., 2013)[ 13],controlling robots (2017; Schulman et al., 2015b)[ 15] and navigating
complex environments (Lei et al., 2017).[10]
We wanted to show an autonomous quadrotor acting as an agent in a fully known MDP can successfully learn to plan
the optimized path on simulated drone racing track. Our agent was trained with PPO a very well-known once state of
art continuous control algorithm that is still used due ease to implement and reproducible with few hyperparameters
possible compared to other complex algorithms.
Our result shows after a fully trained agent could easily gain the ability to plan a long term planning scenario within
a simulated racing track which consists passing ten of checkpoints. We also deployed our trained agent into similar
conditions and it was able to generalize its path planning skills efﬁciently.
This shows promising results in the usage of drones in path planning scenarios as well as the deployment of these
trained policies into real life. Our sensor data is highly conﬁgurable and one can replace simulated sensors with actual
sensor data. With proper calibration, trained policies can be deployed into real life systems.
2 Preliminaries
Machine Learning Machine Learning is in most basic form can be described as acquiring domain knowledge with a
large number of examples of the desired objective.
These examples are called training set and fed into a learning algorithm of interest. Evaluated on test data they show
the success rate of the training process. Learning algorithms try to maximize the success rate to reduce the disparity
between predictions and actual outputs.[16][13]
Reinforcement Learning Reinforcement Learning is in the middle of supervised and unsupervised learning. Unlike
unsupervised learning methods we have some form guidance supervision system but this doesn’t affect the output of
our desired output predictions instead, it acts like a feedback signal that tries to steer our policy to an optimal one,
improving with each update to collect maximum cumulative reward.[7]
Deep Reinforcement Learning Deep Reinforcement Learning (DRL) has achieved great success on sequential task
problems involving high dimensional sensory inputs such as RGB images or actual physical sensor data on real life
robotic systems. Working on with highly dimensional input data there is some feature extraction layer that extracts
using neural networks. These abstracted features then later used on to approximate Q value. Deep Q Networks (DQN)
update policy regarding to Bellman expectation equation which includes an approximation of Q(state, action) with a
neural network. Another approach is the directly optimizing policy which results in Policy Gradient methods. These
methods result in more stable training but they fail to generalize into any complex problem.
Both critic and value use separate neural networks during training, Critic is updated with loss regarding bellman
expectation function just like DQN, while critic updates with gradient ascent to maximize its policy to take actions that
resulted in good rewards.
PPO is an advantage based actor-critic algorithm that tries to be conservative with policy updates. Using KL divergence
andclipped surrogate function it can update multiple steps of update in one trajectory without worrying about policy
2
Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones A P REPRINT
ﬂuctuating wildly[ 15]. TRPO another similar algorithm does this by enforcing a hard trust-region constrain t which
solved with second-order methods. PPO does this by with ﬁrst-order optimization its clipped objective. [6][7]
Path Planning The motion planning problem is often related to the subtask graph execution problem. Problem is
often mapped to a graph and thus becoming a graph search problem. The key concept behind the problem is to solve
ﬁnding an optimal path toward the target while avoiding obstacles in the way.[20][3]
3 Related Work
Hierarchical Reinforcement Learning In recent years Hierarchical Reinforcement Learning (HRL) has started to
show promising results, solving long horizon problems on environments with sparse or delayed rewards. We could
classify these algorithms into two type. First algorithm type focuses on high level policies ability to choose low level
actions which they are called selector methods Second type of algorithms rather focuses on determining high level
policies from low-level policies which are called subgoal based methods.
Both algorithm type can show enable transfer within different environments and can solve diverse tasks. Selector
algorithms mostly
Selector methods enable convenient skill transferring and can solve diverse tasks. They often require training of
high-level and low-level policies within different environments, where the low-level skills are pre-trained either by
proxy reward , by maximizing diversity , or in designed simple tasks.[11][21]
Subgoal-based methods are designed to solve sparse reward problems. A distance measure is required in order for
low-level policies to receive internal rewards according to its current state and the subgoal. Many algorithms simply use
Euclidean distance [8, 9] or cosine-distance as measurements.[18]
Multi Task Reinforcement Learning Multi-Task Learning (MTL) tries a more general approach when dealing with
a problem with multiple sub-tasks. Instead, it uses leveraging the power of Machine Learning to generalize upon each
task given properly chosen hyperparameters and given enough training time it learns other subtasks. This method also
increases the learning capacity of each subtask at once and improves the ability to learn several tasks. This is mostly
achieved due to similarities of subtasks that could be learned from features extracted in hidden layers could be more
efﬁciently exploited to learn other tasks and by a general representation of each subtask to learn.[19]
Multi-Task RL(MTRL) methods could pass these efﬁciency improvements made in the Deep Learning domain to the RL
domain by allowing multiple tasks to be learned by a single agent. The same domain example to this could be reaching
race-walking and running, a similar domain example could be a single pendulum or double pendulum balancing task.
[8]
Authors conducted an experiment to compare MTRL algorithms. Multi Deep Q Networks with separate replay
memories for each task learned ability to generalize on multiple tasks while having time complexity then Vanilla DQN.
They also constructed to actor-critic domain with Multi Deep Deterministic Policy Gradient extended their ability to
learn continuous action domains in the same vein, which they showed with pendulum task.
Path Planning with Reinforcement Learning There are many works of motion planning with DRL [ 1,10,2,3],.
Bae et al. compared Depth-ﬁrst methods with RL their results on experiments and shown a RL algorithm shown paths
generated by RL methods more easily executed by robots then Depth Methods, which generate a polygonal line paths
Paths generated by RL almost in all situations achieve a low orientation error when executed on the real robot. [2]
They also showed combined with local LIDAR information a RL based path planning was able to generalize into
wider applications and providing better generalization performance without getting trained for each unknown dynamic
environment.
The problem of multi robot path planning is motivated by many practical tasks because of its efﬁciency for performing
given missions. However, since each robot operates individually or cooperatively depending on the situation, the search
area of robot is increased. [3]
Reinforcement learning in the robot’s path planning algorithm is mainly focused on moving in a ﬁxed space where each
part is interactive. In most cases, existing path planning algorithms highly depend on the environment. [10]
Drones with Reinforcement Learning The works on Drones have long existed since the beginning of RL.[ 14,12,17]
. Some the early work done was involving dynamic programming algorithms to some complete basic tasks with RL
3
Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones A P REPRINT
Figure 2: General overview of communication between agent and simulation .
algorithm to show the capability of RL with drones but due lack of powerful function approximators such as neural
networks they couldn’t be used in complex real-life situations.
With Deep Learning revolution we have seen drones starting to use Deep Learning based object detection, image
recognition, etc. techniques to better navigate their paths and thus complete highly precision required tasks such as
delivering cargo[ 14] and creating HD map s of dangerous parts of the world. This, of course, comes with a caveat, since
DL methods are more compute hungry battery lives become a more apparent problem.
4 Long Term Planning withÂ ˘a Deep RL on Autonomous Drones
4.1 How does it works
We had access to modeled accurate representation of the actual drone racing track.[ 12] The track is highly customizable
thanks to AirSim APIs,(see Section 4.1) this helped to easily modify parts of it in towards to achieve domain random-
ization . We spawned our training agent in different locations during the training process for learning to recover it’s the
path when it went off during the evaluation process.
Our long term task is the complete racing track which consists of ten(10) mini-tasks to ﬁnishing faster than competitor
drone. Our agent should try to avoid corners of gate checkpoints, hitting too many corners would distract our planning
and lead us into an unrecoverable state.
Agent gets rewards signals for each step taken in the environment. Reward engineering is the process of creating
appropriate rewards for learning agents to collect during training.[ 4,5] Our technical details regarding details of
explained in here.(See Section 5). Completing all subtasks and ﬁnishing race deﬁned as one episode of MDP
process.PPO algorithm we used here can update its policy multiple times with one trajectory. We used competitor
drone equipped the same physical appearance and aerodynamics to collect guidance data via API calls. A simple path
planning algorithm has been used for competitor drone. Using an expert-guided approach allows our agent to not stuck
in local optima and away from the target goal. Especially with high dimensional state problems its very hard to train
agents from scratch. Exploration in the environment mostly supplied with expert guidance and hand-designed rewards.
Using a function approximator like neural networks we achieved near-real human operator performance with enough
training time. We have explained our observations and actions in detail.(see Section 4.1.2 )
After 2.5 million steps taken in simulations, our agent started partially complete ten gate checkpoints while hitting
some of the obstacles and stuck in local optima. Using the same network we have trained 4 million steps more training
with the same seed. While continuing trained networks isn’t much common in Deep RL domain we had to due limited
computational power we experimented with. After starting of each continued training process agent had to adapt itself
to new trajectories thus felt a performance drops were expected.
4.1.1 AirSim Drone Racing Lab Framework
The goal of our AirSim Drone Racing Lab simulation framework is to help bridge the gap between simulation and reality
by utilizing high ﬁdelity graphics and physics simulation.[ 12] It uses Epic’s Unreal Engine 4 for graphic rendering
4
Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones A P REPRINT
pipeline while physics engine is deployed into developed by themselves. The main goal of it enabling to train different
machine learning algorithms with the ability to create unique scenarios and conditions with given high level APIs to
developers. It features the ability to use their ﬂight controller as well as replacing the default controller with something
more sophisticated. It also features ability to specify inertial sensors which would enable more realistic training sessions.
Drone racing involves multiple competitor agents and due nature can also be modeled as multi-agent problem. We have
used an already available basic path planning algorithm implemented inside the simulation environment which helped
us with training and evaluation processes. It is also fairly easy to import 3D models such as drone gates, obstacles
to randomize domain. We can achieve most of the racing info with API calls which let us access monitor score and
progress as well as time penalties.
It is also fairly simple to get camera data located on the top and behind each drone available in the simulation. Ground
truth data can be extracted from simulation which would ease the process of data labeling in an Imitation Learning
approach.
4.1.2 Proximal Policy Optimization
Proximal Policy Optimization is model free on-policy actor critic algorithm introduced by Open AI (Schulman et al.
2017)[15] that tries to uses clipped surrogate function as an optimization goal.
With it’s easy to implement nature and requirement to adjust fewer parameters with comparable results to other on-policy
algorithms,PPO has become one of the most frequently used continuous control problems.
max
ET[(atjst)
old(atjst)]At(st;at) (1)
HereAtstands for generalized advantage function. (Schulman et al. 2015b)[15]
"hyperparameter is used for ratio of clipping between 1-", 1+"depending on whether advantage is positive or negative.
It is widely believed that the key innovation of PPO responsible for its improved performance over the baseline of
TRPO is the ratio clipping mechanism over KL divergence.
4.1.3 Observations and Actions
The RL-agent needs to get all relevant information about the current state of the environment to be able to fulﬁll the
task successfully. The following listing provides the raw data that has been identiﬁed as relevant. In this work, we
have used quadrotors IMU (Inertial Measurement Uni t) sensors which contains information of linear velocity,angular
velocity, gyroscope data for which contains pitch(x-axis), roll (y-axis), and yaw (z-axis). The gyroscope has no initial
frame of reference (like gravity), you can combine its data with data from an accelerometer to measure the angular
position. We also used GPS data we were able to collect from APIs of AirSim. These provided a rich understanding of
the environment thanks to realistic continuous state inputs.
We also use our competitor drone equipped with a classical path planning algorithm also provide us its location which
acts a guidance mechanism during training.
We have opted to use already implemented low-level drone PID controller - Since our focus was on path planning rather
than improving on PID controller, though this also reminds a point for improvement in the future.
From our tests, we have seen need for clipping due to inefﬁcient learning so we have used clipped versions of each
sensor input data and outputs from our neural nets. Our neural network outputs speed difference in terms of x,y,z
coordinates each step given to controller API which then allows our drone to balance and improve on.
4.1.4 Reward Engineering
Reward engineering is a highly active research topic in the RL domain and researchers have come up with different
ideas related to it. Inverse RL tries to approximate reward function from expert demonstrations shown some success
in small toy tasks but has yet to generalize for complex tasks. Handwritten reward engineering techniques remains a
common practice when designing a RL system though problems such as low-sparse dilemma remain to be solved. In
our experiment, we have handcrafted rewards and which was the most time-consuming part of this project.
Our agent had to pass the checkpoint at gates without hitting collapses in 3D geometric space . We had to create a
feedback signal mechanism so it wouldn’t receive negative rewards for not getting the next gate after passing a gate.
5
Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones A P REPRINT
Figure 3: A timer starts to incentive drone to reach next gate checkpoint.
We created a timer function that would keep track of each gate time and decide rewards for that gate given expert
demonstrations by our competitor drone.
Each training episode spawned at a different but constricted random location which had 2.0 -3.5 meters into actual gate
checkpoints, this allowed for more domain randomization .
We feed agents with positive reward signals based on proportional to their distance its current target gate. Distance
between each was approximately is 10-15 meters, and we had given extra positive rewards when the agent was in close
range of 3 meters to target as a positive incentive. When an agent was 0.5 meters close to its current target gate we
started calling our distance calculation function to properly calculate if our agent passes inside the gate.
Our calculation was based on L2 distance of the middle point of each gate proportional to drone orientation. We give a
huge positive reward at the passing of each gate and start timer alongside with it. Until timer for each game runs out our
agent has to leave its current area and to move towards the next target gate. We have given penalty rewards to our agent
that stuck with the old target gate. Choosing the right values for this situation required a lot of trial and errors to ﬁnd the
proper ﬁt. We terminated episodes if the agent was going too far from its target destination. Any attempt on going too
far from gate checkpoint that wasn’t detected by us was crashing simulation and we could detect crashes in simulation
thus allowed reset environment for next training run.We also kept track of time and number of hits collisions to gate
corners for negative penalty signals.
5 Experiment
5.0.1 Implementation Details
Our neural network has 3layered MLP outputting speed coordinates (x,y,z) respectively that should be inputted
controller of quadrotor to increase or decrease the speed. We thought about using a recurrent neural network for
allowing better memorisation but at ﬁrst tests it didnt seem to improve much so we dropped for a more simplistic
network. Our neural network had hidden size of 256along with starting learning rate 1e-4. Our trajectories were 2048
steps and mini-batch consisted of 256transitions each time.
We clipped neural network output to between -1 and 1. In our experiment, this shows stability and convergence unlike
ﬁrst the approach we tried resulted have failed results.
We also normalized every state value and rewards for an efﬁcient training process. We run the simulation at arbitrary
640x480 resolution at 60 frames per second with all graphical effects in low, shadows disabled. Our training
conducted in cloud GPU providers and we downloaded model weights regularly to see the running model in the local
simulation. Unfortunately, Unreal Engine API didn’t allow us to record video on server machines.
We have created a mechanism to stop training and continue as efﬁciently possible with minimal effect on pre-train drops
Communication with our model and simulation was made possible via TCP communication built inside the simulation ,
we have also extended some of its abilities for remote monitoring of data packages.
6
Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones A P REPRINT
Figure 4: Episodic Reward - Time Steps during training.
5.0.2 Evaluation and Results
Our goal was to show teach a RL trained agent to complete ten gate checkpoints track given time limits, minimizing
hitting obstacles and being able to recover and continue the optimal path from given limits.In order to achieve this we
have hand crafted a unique solution customized for drone racing environment which can be extended to similar domains.
In our early tests we have seen bad effects of no clipping which led stuck in various local optima positions. In order to
eliminate this issue we have normalized our state and action values , this way any in case of getting away from optimal
path would result in big negative penalty but it was still recoverable due advantage policy update nature of algorithm.
We have recorded a video of the drone during the evaluation run and published our results as reproducible with network
weights to test and improve upon.
6 Conclusion and Future Work
In this experiment, we have shown a fully trained reinforcement learning agent with using AirSim Drone Racing
simulation can achieve successful long term planning using continuous state values and continuous actions.
Deep Reinforcement Learning algorithms coupled with strong function approximators can achieve great results at path
planning tasks as well as generalization of said tasks. Spawning agents from different starting positions achieve tries to
achieve domain randomization for agents which then allows recovery from hard states. This has shown with deploying
agents on different spawn points and seeing their ability to recover and continue at an optimal path. This work can be
extended to real-life drone racing situations due to the similarity between
We believe a properly calculated IMU data of actual drone can allow us to deploy trained policy into real life without
having a hard time adapting. Deploying trained simulation policies can allow training for a lot of interesting planning
scenarios due to the ease of training agents in the simulation. One example could feature planning and delivery tasks
for a drone that carries packages in real life.
7 References and Source Code
The documentation and source code for project may be found at
https://github.com/ugurkanates/NeurIRS2019DroneChallengeRL
7
Long-Term Planning with Deep Reinforcement Learning on Autonomous Drones A P REPRINT
References
[1]Dennis Barrios Aranibar and Pablo Javier Alsina. Reinforcement Learning-Based Path Planning for Autonomous
Robots. semanticschool , 2019.
[2]Hyansu Bae, Kim Gidong, Jonguk Kim, Dianwei Qian, and Sukgyu Lee. Multi-robot path planning method using
reinforcement learning. Applied Sciences , 9:3057, 07 2019.
[3]Kuanqi Cai, Chaoqun Wang, Jiyu Cheng, Clarence W. De Silva, and Max Q.-H. Meng. Mobile Robot Path
Planning in Dynamic Environments: A Survey. arXiv:2006.14195 [cs] , June 2020. arXiv: 2006.14195.
[4] Daniel Dewey. Reinforcement learning and the reward engineering principle. aaai spring , 2014.
[5]Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills
without a reward function. arxiv , 2018.
[6]Daniel Graves, Nhat M. Nguyen, Kimia Hassanzadeh, and Jun Jin. Learning predictive representations in
autonomous driving to improve deep reinforcement learning. arxiv , 2018.
[7]Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. Deep Reinforce-
ment Learning that Matters. arXiv:1709.06560 [cs, stat] , January 2019. arXiv: 1709.06560.
[8]Lianwen Jin, Zhaoyang Yang, Kathryn Merrick, and Hussein Abbass. Multi-Task Deep Reinforcement Learning
for Continuous Action Control. ijcai, pages 3301–3307, 2017.
[9]Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural
networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information
Processing Systems 25 , pages 1097–1105. Curran Associates, Inc., 2012.
[10] Xiaoyun Lei, Zhian Zhang, and Peifang Dong. Dynamic Path Planning of Unknown Environment Based on Deep
Reinforcement Learning. Journal of Robotics , 2018:e5781591, September 2018.
[11] Siyuan Li, Rui Wang, Minxue Tang, and Chongjie Zhang. Hierarchical Reinforcement Learning with Advantage-
Based Auxiliary Rewards. arXiv:1910.04450 [cs] , October 2019. arXiv: 1910.04450.
[12] Ratnesh Madaan, Nicholas Gyde, Sai Vemprala, Matthew Brown, Keiko Nagami, Tim Taubner, Eric Cristofalo,
Davide Scaramuzza, Mac Schwager, and Ashish Kapoor. AirSim Drone Racing Lab. arXiv:2003.05654 [cs] ,
March 2020. arXiv: 2003.05654.
[13] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
Martin Riedmiller. Playing Atari with Deep Reinforcement Learning. arXiv:1312.5602 [cs] , December 2013.
arXiv: 1312.5602.
[14] Guillem MuÃ ´soz, Cristina Barrado, Ender Ã ˘Getin, and Esther Salami. Deep reinforcement learning for drone
delivery. Drones , 3(3), 2019.
[15] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy Optimization
Algorithms. arXiv:1707.06347 [cs] , August 2017. arXiv: 1707.06347.
[16] Osvaldo Simeone. A Very Brief Introduction to Machine Learning With Applications to Communication Systems.
arXiv:1808.02342 [cs, math] , November 2018. arXiv: 1808.02342.
[17] Satinder Singh, Andrew G. Barto Roderic A. Grupen, Roderic A. Grupen, and Christopher I. Connolly. Robust
Reinforcement Learning in Motion Planning. CiteSeer , 1995.
[18] Sungryull Sohn, Junhyuk Oh, and Honglak Lee. Hierarchical Reinforcement Learning for Zero-shot Generalization
with Subtask Dependencies. arXiv:1807.07665 [cs, stat] , May 2019. arXiv: 1807.07665.
[19] Tung-Long Vuong, Do-Van Nguyen, Tai-Long Nguyen, Cong-Minh Bui, Hai-DangKieu, Viet-Cuong Ta, Quoc-
Long Tran, and Thanh-Ha Le. Sharing experience in multitask reinforcement learning. IJCAI-19 , 2020.
[20] wikipedia. Motion planning, June 2020. Page Version ID: 962698400.
[21] Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor
Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. arxiv , 2018.
8

Challenges of Evolving Sequential to Parallel Code:  
An Exploratory Review 
 
Anne Meade, Jim Buck ley, J. J. Collins 
Lero—The Irish Software Engineering Research Centre 
University of Limerick, Ireland 
{anne.meade, jim.buckley, j.j.collins}@ul.ie  
ABSTRACT  
Large legacy systems that have been in use for several decades 
need to evolve in order to take  advantage of new technological 
advances. One such development is the emergence of multi-core 
processors and parallel platforms.  However, the evolution of code 
written for single-core platforms into code that can take advantage of multi-core technology is challenging. The aim of 
this research is to explore the challenges that parallel 
programmers face in the evolution of existing software to exploit multicore and parallel architectures. A review of the current 
literature was conducted and ten frequently reported challenges 
were identified. It is important to raise awareness of potential challenges that practitioners may face when evolving sequential 
code to exploit multicore platforms in order to be better prepared 
for future evolution. The research community can use these 
results to develop a research agenda in order to design and 
develop solutions to address these challenges. 
Categories and Subject Descriptors  
D.1.3 [ Programming Techniques ]: Concurrent Programming – 
Parallel programming.  
General Terms  
Documentation, Performance, Human Factors 
Keywords  
Challenges, concurrency, evoluti on, multicore, parallel, program  
1. INTRODUCTION 
Lehman’s first law of software evolution states that a program 
must undergo continual change or become less useful [1]. New 
requirements can drive this change such as a request for a new 
feature or a need to fix the software [2]. Another important 
motivation for software evolution is to exploit new technological 
advances [3]. An emerging tec hnological advancement is the 
move from single processor architecture to multicore processors [4]. This is particularly important for large legacy software 
systems that are too expensive to be developed from scratch, such 
as specialised scientific systems or large business systems. The focus of this research is the evolutionary process of transforming 
sequential programs into parallel  implementations in order to 
meet the increasing need for performance improvements in a multicore environment.  The concept of parallel programming is not novel [5] and 
has a strong track record in both the academic and commercial 
sector. However, the availability  of parallel hardware through the 
affordable and continuously evolving de facto standard multicore-based architectures, generates a new requirement in the field of 
parallelization. That is, support fo r the evolution of the code from 
a serial to a parallel paradigm. This  particular type of evolution is 
challenging and is often referred to  as one of the key constituents 
with respect to the next software engineering crisis [6].  
This paper reviews current literature exploring the 
challenges posed when migrati ng from sequential to parallel 
deployment infrastructures. Un derstanding these challenges will 
aid the process of evolving existi ng software to exploit multicore 
and parallel architectures fully. The scope of this research does 
not allow for consideration of so ftware once it is parallelized. 
This paper is structured as follows: Section 2 provides 
background and outlines the necessity for parallel programming 
drawing from the history of processors. Section 3 presents the 
research design. Section 4 presen ts the results of the literature 
review. The findings are discu ssed in Section 5. Section 6 
concludes this paper and presents an outlook to future work.  
2. BACKGROUND AND MOTIVATION 
As computer software evolves, faster rates of execution are 
expected. In 1965, Gordon Moore [7] predicted that the number 
of transistors on a chip would roughly double every 18 months 
into the near future. This predic tion, now referred to as Moore’s 
law, has borne fruit as Intel chips reached 2GHz in 2001 and 
consequently, software has expe rienced exponential hikes in 
performance. In keeping with Moore’s law, clock speeds should have continued to increase a nd have surpassed 15GHz in 2010. 
However, this has not been the case [8]. In Figure 2-1, the blue dots represent the clock frequency for Intel x86 processors from 
1970-2010. The increase in frequency came to a halt circa 2005 
as it became infeasible to cool the heat generated from the amount 
of transistors that could fit on a single chip [9]. Clock speed increases can no longer support so ftware performance gains so as 
an alternative, CPU manufacturers have started to increase the 
number of processor cores; this is represented by the red dots in 
Figure 2-1. In order to sustain ever-increasing computing 
performance, developers are faced  with the task of evolving 
systems that run on single processors to alternatively utilise multiple processors [8]. 
Microsoft software architect and chair of the C++ standards 
committee Herb Sutter claimed the "free lunch was over" for 
programmers riding the wave of exponential growth in processor 
performance predicted by Moore [5]. Sutter emphasised the need to change development practice and incorporate parallelization. 
However, the task of paralleli zation is not an easy feat as 
Massingill et al. [10] emphasise: 
Creating parallel software is difficult, time-consuming and 
error-prone. It is especially challenging for legacy applications  
 
 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not made or distributed for profit or commercial advantage and that 
copies bear this notice and the full citation on the first page. To copy 
otherwise, or republish, to post on servers or to redistribute to lists, 
requires prior specific permission and/or a fee. 
IWPSE-EVOL’11 , September 5-6, 2011, Szeged, Hungary.  
Copyright 2011 ACM 978-1-4503- 0848-9/11/09...$10.00. 
 
1

.  
Figure 2-1 Intel processor range ([9]) 
The future is paved with high performance environments and so 
the challenges posed by evolving sequential codebases to parallel 
must be overcome [11]. 
3. RESEARCH DESIGN 
3.1 Research question 
It is important to have a clear understanding of the challenges that 
programmers face in the task of evolving legacy systems to 
exploit multicore technologies. This can be useful for the research 
community to develop a research agenda to overcome these 
challenges and for practitioners to be better prepared for potential 
obstacles they may encounter. To the best of the researchers’ 
knowledge, there has been no system atic identifica tion of these 
challenges. Therefore, the research question is:  
What challenges are reported in the literature facing 
practitioners who are concerned with evolving legacy systems 
to exploit multicore environments? 
 
We discuss the research design  to address this question in 
subsection 3.2 and present th e results in Section 4.  
3.2 Literature review 
In order to address the research question, we conducted an 
exploratory literature review. This preliminary study, while not 
based on the systematic method as proposed by Kitchenham [12], 
surveys recent publications in order to identify and synthesize 
commonly reported challenges of evolving code from serial to parallel. 
3.2.1 Study selection 
In order to identify the literature reviewed in this paper, a search 
was conducted using Google Scholar as the search engine. The 
search was designed where primary fields such as Multicore  and 
Parallelization  were identified in order to base the literature in 
the correct field, while secondary fields such as Challenges  and 
Software  were selected to scope the literature more specifically. 
Based on title relevance, an initial set of 4237 articles were 
selected and further refined to 114 based on abstract relevance. 
Finally 15 papers were chosen. Fu rther details and results of the 
selection process are accessible online [13]. A number of challenges were found to recur repeatedly in the literature and 
through iterative refinement ten distinctive classifications were 
identified, which encompassed all the recorded challenges. The 
results of the literature review are presented in Section 4. 
4. CHALLENGES IN PARALLEL 
COMPUTING: A REVIEW OF THE 
LITERATURE 
The literature review returned a set of ten challenges that 
developers face when evolvi ng systems from sequential to 
parallel. The results are presented in Table 4-1 where references to the literature are listed, identifying where each challenge was 
reported. The frequency of the occurrence of each challenge is 
enumerated. A grounded approach was adopted where four 
categories were derived, bottom up,  through identification of the 
individual challenges mentioned in  the papers and derivation of 
common themes between these ch allenges. The four categories 
are as follows: 
 Developer : Challenges involving human analytical 
factors. These challenges are focussed on the developer’s 
knowledge and capacity to reason 
 Development Methods and Tools: Challenges related to tool support or methods used by the developer  
 Application Specific : Challenges related to the specific 
software system that is being parallelized 
 Constraints : Challenges surrounding limitations enforced 
on the developer during the parallelization process 
Figure 4-1 shows how the four categories of challenges are related to one another and toge ther model the evolution of a 
sequential legacy system into a parallel system. The developer 
performs and uses development methods and tools, to parallelize 
a legacy application. There are various constraints that together 
form a boundary in which this  evolution takes place. The 
remainder of this section discusse s the different challenges, which 
are structured according to the categories outlined.  
 
Figure 4-1. Relationship of categories for evolving 
applications from sequential to parallel 
4.1 Category: Developer  
4.1.1 Parallel Comprehension - C1 
Parallel code is more difficult to understand than sequential code. 
Seven papers reported this challenge (see Table 4-1). A sequential 
program has one thread of control, data structures and memory 
management; understanding the sequence of events is 
straightforward. Parallel programs add the additional complexity 
of multiple threads of control [14]. Sutter et al. [11] claim humans 
are “quickly overwhelmed” by this complexity. When a 
programmer studies the text of a program, the flow of execution 
is not always evident. The programmer must imagine the 
interplay of parallel activities.  Asanovic et al. [8] claim “not 
every programmer is able to understand the nitty gritty of concurrent software”. Sections of code cannot be independently 
reasoned about in a parallel program,  as state is frequently shared 
and interactions between threads are not always obvious [11].  
4.1.2 Lack of Training – C2 
Programmers are not receiving trai ning in the field of parallel 
programming. Six papers reported this challenge. Students are 
graduating from universities with software engineering 
qualifications without adequate  experience in concurrency 
techniques [15]. These graduates as employees, must take it upon themselves to acquire this knowle dge [16]. The future need from 
industry for parallel expertise will not be met unless educational institutes adapt and make courses available [17]. As mentioned in 
C1, humans do not naturally le nd themselves to thinking 
 ApplicationDevelopment 
Methods & 
ToolsUsed to
parallelizePerforms 
and uses
DeveloperConstraints
2
Table 4-1. Challenges facing parallel programmers 
concurrently, therefore it is important to provide this grounding at 
an early stage. Education and training are cited as the best 
approach to tackle the problem of parallel programming in the 
long term [20]. Goth suggests initial emphasis should be on 
higher-level parallel concepts rather than parallel languages [17]. 
4.1.3 Lack of Domain Knowledge – C3 
Parallel programmers lack th e domain knowledge necessary to 
fully parallelize an application. Three papers reported this 
challenge. Domain expertise is essential in order to parallelize 
large applications; commercial compilers are limited in this area 
and have only had success when para llelizing simplistic code [21]. 
Often, the only way to successfully complete parallelization and 
assure its effectiveness is to use explicit parallelism, programmers 
explicitly state where the concurre ncy can occur, which involves 
both domain and programming expert ise [11, 22]. As a parallel 
programmer’s domain is computing and not that of the 
application, lack of in-dep th knowledge of the system’s 
algorithms can lead to incorrect results when converting to 
parallel.  
4.2 Category: Development Methods & Tools  
4.2.1 Testing – C4 
Testing parallel programs is challe nging. Six papers reported this 
challenge. A range of threads is  spawned when running a program 
in parallel. Testing all the po ssible configurations including 
running order of these threads is a complex task. When evolving a 
test suite to run in a parallel environment, the suite must exercise 
“a range of thread counts and a more diverse range of hardware 
configurations” [14]. A program is deterministic if it always does 
the same thing on the same input, no matter how the instructions 
are scheduled on the multicore computer and it is 
nondeterministic if its behaviour mi ght vary from run to run. The 
flow of control in a parallel program is nondeterministic so a program that passes a suite of te sts when run serially, may not do 
so when run in parallel [14]. Verifying the correctness of these problems has been described in the literature as a ‘nightmare’ 
[15]. All possible interleaving in the program must be proven to 
produce the correct results [11]. Programmers must ensure 
deadlocks, conflicts, race conditi ons and other issues do not occur 
regardless of running order of th reads. Parallel programming is reported as much more tedious and error prone than serial 
programming [23] therefore comprehensive testing is crucial.  
4.2.2 Lack of Tools – C5 
There are insufficient tools available to aid parallel programmers. 
Four papers reported this chal lenge. Tools are required for all 
phases of parallelization from modi fying serial code to execution 
setup. The literature reports the need for better tools to 
systematically find defects and bottlenecks and also to aid testing, 
debugging and tuning of parallel programs [11]. Tools currently 
available do not help in the optimization of code when parallelizing, as specific hardware  constraints need to be known to 
programmers. This lack of tool s causes problems for programmers 
from previous generations reliant on compilers [14]. Sutter et al. 
[11] maintain today’s concurrent tools are at the level sequential 
tools were at the beginning of the structured programming era. 
Dig [21] claims there is a lack of tooling to support the steps in 
between refactoring and performan ce tuning such as profiling. 
After locating performance bottle necks, the programmer requires 
assistance to determine the most  efficient refact oring method.  
4.2.3 Complex Debugging – C6 
Debugging a parallel program is a challenge. Three papers 
reported this challenge.  As mentioned in 4.2.1, flow of control is 
nondeterministic in a parall el program making debugging a 
difficult task [14]. The scheduling of threads is the responsibility 
of the system so the programmer cannot predict the execution path 
of the program. For example, de velopers wanting to correct a 
concurrency problem such as a deadlock may not be able to 
reproduce the flow of control. Typical methods of debugging 
using breakpoints will not work for a program running in parallel, 
as the behaviour in one execution may vary from the behaviour in 
another making bug fixing and maintenance a difficult task [4, 
11]. Better logging facilities that keep a record of messages being 
passed may aid this debugging process [11].  
4.3 Category: Application Specific  
4.3.1 Data dependencies – C7 
Managing data dependencies when  writing parallel code is 
challenging. Three papers repo rted this challenge. A data 
dependence results from multiple use of the same location(s) in storage by different tasks, for example one statement produces a Category ID Challenge Description Freq. Reported in 
Developer 
 C1 Parallel 
Comprehension Parallel code is more di fficult to understand than 
sequential code 7 
 [5, 8, 10, 11, 14, 18, 19] 
C2 
 Lack of Training Programmers are not receiving training in the field 
of parallel programming 6 [14-17, 19, 20] 
C3 Lack of Domain 
Knowledge Parallel programmers lack the domain knowledge 
necessary to fully parallelize an application 3 [11, 21, 22]  
Development 
Methods 
& Tools 
 C4 Testing Testing parallel programs is challenging 6 [10, 11, 14, 15, 18, 23] 
C5 Lack of Tools There are insufficient tools available to aid parallel 
programmers 4 [4, 11, 14, 21] 
C6 Complex Debugging Debugging a parallel pr ogram is a challenge 3 [4, 11, 14]  
Application 
Specific C7 
 Data Dependencies Managing data de pendencies when writing parallel 
code is challenging 3 [11, 14, 18] 
 
Constraints 
 C8 
 Interdependence of 
Abstraction Levels Maintaining consistency across all levels of abstraction is challenging 3 [4, 11, 14] 
C9 Hardware Variance The connec tion configuration of cores on a 
processor may vary widely and affect the software 2 [14, 15]  
C10 Time Limitations Rewriting sequential legacy programs is time 
consuming 2 [10, 18] 
      
3
result and saves it in a location, and a second statem ent references 
that result by looking up that location. To perform work 
effectively, each calculation needs to have access to data that it 
needs. If this data (for example, the result) is being processed on 
another core, as is commonly the case if the code is parallelized, the synchronisation of these parts should not take more processing 
than what is saved by adding pa rallelism into the code [18]. 
Incorrect management of data de pendencies can typically lead to 
inefficient results [14, 18].  
4.4 Category: Constraints  
4.4.1 Interdependence of abstraction levels – C8 
Maintaining consistency across all levels of abstraction is 
challenging. Three papers reported this challenge. When 
parallelizing an application, there is a dependency between 
programming languages, compilers,  libraries, middleware and 
operating systems. These parts cannot be viewed in isolation 
without considering all others. If the system is viewed at a higher 
level of abstraction in isolation,  lower level guarantees may be 
ignored leading to problems [4]. Si milarly if the sy stem is viewed 
at a lower level of abstraction synchronization defects can occur, 
as developers may not be aware of  the behaviour at a higher level. 
Concurrency should be expressed sy stematically at all levels of 
abstraction so that a system can run efficiently in accordance with 
the architecture of the mach ine it is running on [11].  
4.4.2 Hardware variance – C9 
The connection configuration of cores on a processor may vary 
widely and sometimes affect th e software. Two papers reported 
this challenge. The architecture of computers varies hugely. Intel 
alone, has shipped 10 different multicore processors in the period 
of 6 years (2004-2010) [15]. If the parallel program is optimal 
when the thread count matches the number of cores on the 
machine, or the memory model is being utilised by the 
programmer (for example data locality to cache size), then these 
need to be carefully recalibrated when porting the software to 
another machine [14, 15]. Reliance on hardware and the 
subsequent variance of hardware can raise many problems when 
code is ported to a different  parallel environment.  
4.4.3 Time limitation – C10 
Rewriting sequential legacy prog rams is time consuming. Two 
papers reported this challenge. Often legacy programs are 
complex and have been written by many programmers over a long 
time period. Programmers often a ssigned to parallelize legacy 
codebases are not the programmers who wrote the code initially. 
A lot of time needs to be allocated to transform these codebases 
from sequential to parallel, as the programmers are unfamiliar 
with the code base [18]. Typi cally these programmers will not 
have the luxury of investing a lot of time to understand every detail due to this time-lag and so must carry out the work without 
having full knowledge of the applic ation [10]. This can lead to 
errors and failed parallelization efforts. 
5. DISCUSSION 
The challenges categorised in Section 4, will help inform 
practitioners of the problems they may encounter when evolving a 
sequential codebase to a high performance envi ronment. The 
following subsections, suggest poten tial research areas, with a 
focus on the challenges identified in this paper. 
5.1 Category: Developer  
Software patterns are used to help the developer to reason about 
code and learn from the experiences of others [9]. Research is currently being carried out in th e area of parallel design pattern 
languages. Pattern Language for Parallel Computing  (PLPP) [10] 
was developed in order to embody a development methodology 
for parallel programmers in a st ructured set of patterns. This 
project has since merged with Our Pattern Language (OPL) [24] 
and is an exploratory project hoping to create a sufficient design 
pattern language for parallel pr ogrammers. The effectiveness of 
using OPL as a framework for providing solutions to the challenges reported in this paper w ould be a beneficial study.   
5.2 Category: Development Methods & Tools  
Development methods and tool s, performed and used by 
developers drive the parallelizati on process. Case studies reported 
by Pankratius [4] investigating programming approaches such as 
Transactional Memory, OpenMP a nd Pthreads, highlight the need 
for more extensive tool choice to support parallel programmers. 
Some tools are becoming available (e.g. Cilk ++ from Cilk Arts, 
which allows parallel programs written in C++ to retain serial 
semantics), however these are not in widespread use [17]. 
Systematic defect detection to ols would be very welcome in 
parallel programming to find all possible execution paths in a program [11]. If such a tool existed and could effectively 
reproduce complex errors, the chal lenge of testing and debugging 
parallel applications (C4 and C6) would be greatly reduced.  
While the literature does acknowledge that the goal of 
parallelization is increased performance [4], only one paper 
explicitly lists performance optimisation as a challenge in the 
evolutionary process [14]. As this research reported frequently 
occurring challenges (more than once), performance related 
optimisation could not be included. However, the researchers are 
aware that as memory hierarchies are radically different between 
platforms as highlighted in C9 , optimisation is a challenging 
process; a more systematic review of the literature may have led to this conclusion. In a sequential code scenario, the compiler 
takes the responsibility for optimisation, however, in a parallel 
context the programmer is charge d with this responsibility [14]. 
Often, error-prone experiment ation is used for performance 
tuning; there is a distinct need for tools to aid this process. 
5.3 Category: Application Specific  
The software system is the unit of evolution within the 
parallelization process. Locks are used to enforce data dependence 
(C7) in a parallelized software system. However, locks can often lead to race conditions, prio rity inversion or deadlock. 
Transactional memory is an alternative approach to using locks. It 
is based on the concept of database  transactions whereby a list of 
operations must all be completed before the transaction can be 
deemed as finished. If the operation is interrupted, all the 
operations involved are cancelled. The transactional memory 
approach allows a parallel program to be broken down into blocks 
and worked on in an all-or-nothing basis. This field of research is 
on-going, as a tool is yet to emer ge that caters for finding all 
possible dependencies, eliminat ing the need for locks [25]. 
5.4 Category: Constraints  
Developers must be aware of  the constraints that bound the 
parallelization proce ss. Rajan [15] proposes a solution in his 
concurrent design pattern framework to the challenge of hardware variance; each pattern exposes c oncurrency opportunities in code 
yet does not enforce concrete mapping to threads/locks etc. This enables the runtime environment to choose the mapping between 
the program and the platform disconnecting the dependency on 
the hardware. Rajan expresses the need for further research in this 
area. Awareness of levels of abstraction when parallelizing code is 
4
crucial as relying on the perspective of only one level within a 
system may result in inconsistences elsewhere, potentially leading 
to inefficient code. Tools providing appropriate abstractions of a 
system during parallelization, w ould be very beneficial [11].   
6. CONCLUSIONS AND FUTURE WORK 
This paper consolidates findings from selected literature 
identifying challenges encountered in the process of parallelizing 
sequential codebases. Challenges are reported relating to the 
developer who evolves the sequenti al code into parallel format, 
the methods and tools performed a nd used by the developer, the 
application that is undergoing evolution and the constraints 
bounding the evolutionary process. As  this is a preliminary study 
the following work is planned in the future: 
1. Conduct a thorough search of the literature based on the 
guidelines for conducting a Systematic Literature Review 
(SLR), as presented by Kitchenh am [12] in order to elaborate 
on the challenges found and also to explore the abstractions 
of use within the parallelization process. 
2. Conduct a case study involving parallel programmers in order to investigate if the challenges are systematically 
encountered in a real world environment. 
The findings in this study show that there are serious, unresolved 
challenges in evolving from the sequential to parallel paradigm. 
These challenges are unlikely to be overcome quickly or easily, 
and if increased computing power continues to depend on the best 
use of available cores, then it is probable that we are going to have 
to prepare for continuous evoluti on to avail of the performance 
opportunities presented by ever-inc reasing core counts. As Sutter 
says, “the free lunch is over”.  
Acknowledgements 
This work was supported, in pa rt, by Science Foundation Ireland 
grant 10/CE/I1855 to Lero – the Irish Software Engineering 
Research Centre (www.lero.ie). Ma ny thanks also to Deva Kumar 
Deeptimahanti, Klaas-Jan Stol and Lorcan Coyle for valuable and helpful suggestions.  
7. REFERENCES 
[1] Lehman, M., Programs, life cycles, and laws of software 
evolution.  Proceedings of the IEEE, 1980. 
[2] Tonella, P., M. Torchiano, B. Du Bois, and T. Systä, 
Empirical studies in reverse en gineering: state of the art and 
future trends.  Empirical Software Engineering, 2007. 12(5): 
p. 551-571. 
[3] Coyle, L., M. Hinchey, B. Nuseibeh, J. Fiadeiro, x0E, and 
Luiz, Guest Editors' Introduction: Evolving Critical Systems.  
Computer, 2010. 43(5): p. 28-33. 
[4] Pankratius, V., Software Engineering in the Era of 
Parallelism , in Emerging research directions in computer 
science: contributions from th e young informatics faculty in 
Karlsruhe 2010. 
[5] Sutter, H., The free lunch is over: A fundamental turn toward 
concurrency in software , in Dr. Dobb’s Journal 2005. p. 202-
210. 
[6] Vandierendonck, H. and T. Mens, Averting the Next Software 
Crisis.  Computer, 2011. 44(4): p. 88-90. 
[7] Moore, G.E., Cramming More Components Onto Integrated 
Circuits.  Proceedings of the IEEE, 1998. 86(1): p. 82-85. 
 [8] Asanovic, K., R. Bodik, and J. Demmel, A view of the 
parallel computing landscape , in Communications of the 
ACM 2009. 
[9] Catanzaro, B. and K. Keutzer, Parallel computing with 
patterns and frameworks , in XRDS: Crossroads, The ACM 
Magazine for Students 2010. p. 22-27. 
[10] Massingill, B.L., T.G. Matt son, and B.A. Sanders, 
Reengineering for Parallelism: an entry point into PLPP for 
legacy applications.  Concurrency and Co mputation: Practice 
and Experience, 2007. 19(4): p. 503-529. 
[11] Sutter, H. and J. Larus, Software and the concurrency 
revolution , in Queue 2005. p. 54-62. 
[12] Kitchenham, B., Guidelines for performing systematic 
literature reviews in software engineering , in 
Engineering 2007. 
[13] IWPSE-E VOL Study Selection Details . 2011; Available from: 
http://staff.lero.ie/amead e/iwpse-evol/study.html. 
[14] Wen-mei, H., K. Keutze r, and T.G. Mattson, The 
Concurrency Challenge.  Design & Test of Computers, IEEE, 
2008. 25(4): p. 312-320. 
[15] Rajan, H., Building scalable softwa re systems in the 
multicore era , in Proceedings of the FSE/SDP workshop on 
Future of software engineering research 2010. 
[16] Eccles, R. and D.A. Stacey, Understanding the parallel 
programmer , in High-Performance Computing in an 
Advanced Collaborative Environment, 2006. HPCS 2006. 
20th International Symposium on2006. p. 12. 
[17] Goth, G., Entering a parallel universe , in Communications of 
the ACM 2009. p. 15-17. 
[18] Geer, D., For programmers, multicore chips mean multiple 
challenges , in Computer 2007. 
[19] Scott, L.R., T. Clark, and B. Bagheri, Education and 
research challenges in parallel computing , in Computational 
Science–ICCS 2005 2005. p. 44-51. 
[20] Sangani, K., Computing-Two good to be true-Multicore 
microprocessors may be great for doing lots of things at once, but what about doing  one thing more quickly? , in 
Engineering & Technology 2007. p. 40-43. 
[21] Dig, D., A Refactoring Approach to Parallelism.  Software, 
IEEE, 2011. 28(1): p. 17-22. 
[22] Chalabine, M. and C. Kessler. A Survey of Reasoning in 
Parallelization . in Software Engineeri ng, Artificial 
Intelligence, Networking, and Parallel/Distributed 
Computing, 2007. SNPD 2007. Eighth ACIS International 
Conference on . 2007. 
[23] Marowka, A., Parallel computing on any desktop , in 
Communications of the ACM 2007. 
[24] Keutzer, K., B. Massingill, T. Mattson, and B. Sanders, A 
Design Pattern Language for Engineering (Parallel) 
Software: Merging the PLPP and OPL Projects , in 
ParaPLoP Š10: 2nd Annual Conference on Parallel Programming Patterns 2010. 
[25] Ramadan, H.E., C.
J. Rossbach, and E. Witchel, Dependence-
aware transactional memory for increased concurrency , in 
Proceedings of the 41st annual IEEE/ACM International 
Symposium on Microarchitecture 2008, IEEE Computer 
Society. p. 246-257.  
5

PyKale: Knowledge-Aware Machine Learning from Multiple
Sources in Python
Haiping Lu
The University of Sheffield
Sheffield, United Kingdom
h.lu@sheffield.ac.ukXianyuan Liu
The University of Sheffield
Sheffield, United Kingdom
xianyuan.liu@sheffield.ac.ukRobert Turner
The University of Sheffield
Sheffield, United Kingdom
r.d.turner@sheffield.ac.uk
Peizhen Bai
The University of Sheffield
Sheffield, United Kingdom
PBai2@sheffield.ac.ukRaivo E Koot
The University of Sheffield
Sheffield, United Kingdom
rekoot1@sheffield.ac.ukShuo Zhou
The University of Sheffield
Sheffield, United Kingdom
SZhou20@sheffield.ac.uk
Mustafa Chasmai
Indian Institute of Technology, Delhi
New Delhi, India
cs1190341@iitd.ac.inLawrence Schobs
The University of Sheffield
Sheffield, United Kingdom
laschobs1@sheffield.ac.uk
ABSTRACT
Machine learning is a general-purpose technology holding promises
for many interdisciplinary research problems. However, significant
barriers exist in crossing disciplinary boundaries when most ma-
chine learning tools are developed in different areas separately.
We present Pykale – a Python library for knowledge-aware ma-
chine learning on graphs, images, texts, and videos to enable and
accelerate interdisciplinary research. We formulate new green ma-
chine learning guidelines based on standard software engineering
practices and propose a novel pipeline -based application program-
ming interface (API). PyKale focuses on leveraging knowledge from
multiple sources for accurate and interpretable prediction, thus
supporting multimodal learning and transfer learning (particularly
domain adaptation) with latest deep learning and dimensionality
reduction models. We build PyKale on PyTorch and leverage the
rich PyTorch ecosystem. Our pipeline-based API design enforces
standardization and minimalism, embracing green machine learning
concepts via reducing repetitions and redundancy, reusing existing
resources, and recycling learning models across areas. We demon-
strate its interdisciplinary nature via examples in bioinformatics,
knowledge graph, image/video recognition, and medical imaging.
KEYWORDS
machine learning, deep learning, domain adaptation, multimodal
learning, transfer learning
1 INTRODUCTION
Machine learning is the cornerstone technology for artificial in-
telligence (AI), driving many advances in our everyday lives and
industrial sectors. AI research becomes more and more interdisci-
plinary as many problems rely on expertise from various domains.
We have also witnessed many machine learning models transverse
different research areas and disciplines. For example, the success of
convolutional neural networks (CNNs) [ 26] has spread from com-
puter vision to graph analysis via graph convolutional networks
(GCN) [ 23] and medical imaging via U-net [ 49], and transformers[59] developed in natural language processing (NLP) have become
a hot topic in solving vision tasks [6, 10, 60].
With rapid development and growing interests in machine learn-
ing, many researchers hope to solve real-world interdisciplinary
problems using machine learning. However, even with the popular-
ity of open-source software and high-level scripting language such
as Python, navigating the abundant choices and variety of machine
learning software is not trivial. Researchers often run into barriers
when adapting a machine learning tool for a new task of their inter-
est. Solving complex real-world problems in practice often involve
analyzing multiple sources of data, e.g., multiple modalities, multi-
ple domains, and multiple knowledge bases. Most machine learning
software packages are developed with a specific domain of applica-
tion in mind. While popular generic packages such as PyTorch and
TensorFlow support multiple domains and are not tailored for a
specific domain, their focus on generic frameworks makes them in-
adequate to directly support interdisciplinary research where both
flexible configurations and high-level integration are important.
In this paper, we propose PyKale , an open-source Python library
to enable and accelerate interdisciplinary research via knowledge-
aware multimodal learning and transfer learning on graphs, images,
texts, and videos. It aims to fill the gaps between rich data sources,
abundant machine learning libraries, and eager interdisciplinary
researchers, with a focus on leveraging knowledge from multiple
sources for accurate and interpretable prediction. To the best of our
knowledge, this is the first publicly available Python library that
considers both multimodal learning and transfer learning under a
common framework of learning from multiple sources. It will make
latest machine learning tools more accessible and accelerate the
development of such tools. The name of the library consists of Py
forPython, and Kale forKnowledge- aware learning.
PyKale proposes a novel pipeline-based application program-
ming interface (API) to enforce standardization and minimalism, as
shown in Figure 1. It advocates our newly formulated green ma-
chine learning concepts of reducing repetitions and redundancy
(Fig. 2(a)), reusing existing resources (Fig. 2(b)), and recycling learn-
ing models across areas (Fig. 2(c)) by building on standard softwarearXiv:2106.09756v1  [cs.LG]  17 Jun 2021
Lu, et al.
Load Preprocess Embed Predict Evaluate Interpret
(a) Pipeline-based API
Load digitsStandardize
ImageLearn CNN
featuresPredict 
digit classCompute 
accuracyVisualize 
pa�erns
(b) Digit classification pipeline
Load BindingDBChem Chars 
�SequenceDrug/Target 
EmbeddingPredict 
BindingConcord 
IndexInterpret �
AstraZeneca
(c) Drug target interaction prediction pipeline
Figure 1: The proposed pipeline-based API in PyKale and two real-world examples.
(a) Reduce
 (b) Reuse
 (c) Recycle
 (d) The PyKale logo
Figure 2: Green machine learning concepts in PyKale.
engineering practices, extending them, and tailoring the philoso-
phies to machine learning. We include examples in bioinformatics,
knowledge graph, image/video recognition, and medical imaging
in PyKale. This library was motivated by needs in healthcare appli-
cations and thus considers healthcare as a primary domain of usage.
PyKale is largely built on PyTorch and leverages many packages
in the PyTorch ecosystem,1with the aim to become part of it. Our
logo in Fig. 2(d) reflects the above characteristics, using an icon of
simplified kale leaves.
Specifically, the main contributions are fourfold:
•We introduce the PyKale library for knowledge-aware ma-
chine learning. It focuses on leveraging knowledge from
multiple sources for accurate and interpretable prediction to
enable and accelerate interdisciplinary research.
•We propose a pipeline-based API for a standardized and
minimal design to help break interdisciplinary barriers. We
advocate green machine learning concepts of reduce, reuse,
and recycle in such a design.
•We demonstrate the usage of PyKale on real-world examples
from multiple disciplines including bioinformatics, knowl-
edge graph, image/video recognition, and medical imaging.
•We provide many community-engaging features including a
detailed documentation, a project board to show the progress
and road map, and GitHub discussions open to all users.
The rest of this paper is organized as follows. We first review
the state of the art open source software packages that are related
1https://pytorch.org/ecosystem/to PyKale in Section 2. Then we discuss the design principles and
API structure of PyKale in Section 3. Next, we describe the usage
of PyKale in Section 4 and show example use cases from different
applications in Section 5. Finally, we show the openness of our
package and discuss its limitations and future developments in
Section 6, with conclusions drawn in Section 7.
The PyKale library is publicly available at https://github.com/
pykale/pykale with accompanying data (mainly for testing at the
moment) at https://github.com/pykale/data under an MIT license.
PyKale can be installed from the Python Package Index (PyPI)
viapip install pykale . PyKale documentation is hosted at
https://pykale.readthedocs.io. The primary targeted users are re-
searchers and practitioners who have experience in Python and
PyTorch programming and need to apply or develop machine learn-
ing systems taking data from multiple sources for prediction tasks,
particularly in interdisciplinary areas such as healthcare. This paper
refers to release version 0.1.0rc2 .
2 RELATED WORK
As an open source project, we have learned from numerous libraries
in the public domain to build ours. Here, we can only briefly men-
tion several that have been particularly influential or relevant. In
particular, we focus on those PyTorch-based libraries that we have
frequently studied in our development, regretfully omitting many
libraries, such as those based on TensorFlow.
2.1 PyTorch ecosystem
PyTorch is a popular open source machine learning library, par-
ticularly for computer vision and NLP applications. The PyTorch
ecosystem has a rich collection of tools and libraries for the devel-
opment of advanced machine learning and AI systems. PyKale aims
to fill the gap within the PyTorch ecosystem to support more in-
terdisciplinary research based on multiple data sources. Therefore,
we make extensive usage of existing libraries from the PyTorch
ecosystem to reduce duplicated implementation.
2.1.1 PyTorch Lightning. PyTorch Lightning is a popular deep
learning framework providing a high-level interface for PyTorch. By
PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python
removing boilerplate code, it simplifies the development of research
code and improves the reproducibility, flexibility, and readability of
the resulting models [ 11]. The goal of PyKale shares some similarity
with PyTorch Lightning, but with a different focus on supporting
interdisciplinary research. We have lots of inspirations from the
design of PyTorch Lightning.
2.1.2 Other PyTorch libraries. PyKale depends on some other li-
braries from the PyTorch ecosystem including TensorLy for tensor
analysis [ 24], TorchVision for computer vision [ 37], and PyTorch
Geometric for graph analysis [ 13]. We also learned from MONAI for
medical image analysis [ 36], GPyTorch for Gaussian processes [ 16],
Kornia for computer vision [ 48], and TorchIO for medical imaging
preprocessing [ 45]. These libraries have focuses different from ours
on interdisciplinary research and multiple data sources.
The above libraries are listed as references at the end of our
contributing guideline page.2
2.2 Multimodal/transfer learning libraries
To the best of our knowledge, no other libraries in the PyTorch
ecosystem have the same pipeline-based API design as PyKale.
Several PyTorch-based libraries on multimodal learning or transfer
learning are summarized below.
2.2.1 MultiModal Framework (MMF). MMF [ 52] is the only library
in the current PyTorch ecosystem focusing on multimodal learning
of vision and language data in applications such as visual question
answering, image captioning, visual dialog, hate detection and
other vision and language tasks. PyKale differs from MMF not
only in the API design, but also in the scientific fields covered
and interdisciplinarity. PyKale aims to support interdisciplinary
research such as medical imaging and drug discovery, and includes
examples in these areas in the current release.
2.2.2 Transfer-Learning-Library. The Transfer-Learning-Library
[21] is a library on transfer learning, providing domain adaptation
and fine tuning algorithms for computer vision applications. PyKale
differs not only in the API design but also in the multiple modalities
of data supported, including also graphs and texts, as well as in
interdisciplinarity.
2.2.3 Cornac. Cornac [ 50] is a library for multimodal recommender
systems, leveraging auxiliary data (e.g., item descriptive text and
image, social network, etc). PyKale differs from Cornac not only
in the API design but also in the more diverse machine learning
models and applications supported, not limited to recommender
systems.
2.2.4 ADA. ADA [ 56], with a full name “ (Yet) Another Domain
Adaptation library ”, is an excellent package built on PyTorch Light-
ning for unsupervised and semi-supervised domain adaptation.
We refactored ADA and made many changes for adaption to our
pipeline-based API. We include a docstring at the top of each mod-
ule adapted from ADA to indicate the source at ADA. Beyond a new
API design, PyKale extends ADA substantially to support video
domain adaptation and also supports non-vision data for interdisci-
plinary research.
2https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.mdThere are some other smaller libraries on multimodal or trans-
fer learning that are more narrowly focused than the above, such
as Multimodal-Toolkit [ 17]. PyKale frames multimodal learning
and transfer learning under one roof of knowledge-aware machine
learning from multiple sources with a unified pipeline-based API,
aiming to support interdisciplinary research rather than just popu-
lar vision or language tasks.
3 PYKALE DESIGN
3.1 Green machine learning
Green machine learning (andgreen AI ) is a scarcely used term
referring to energy-efficient computing [ 5,15,27]. Here, we pro-
pose a new green machine learning perspective for machine learning
software development by formulating the 3Rguiding principles
below. We build these principles on standard software engineer-
ing practices by extending them and tailoring the philosophies to
machine learning:
•Reduce repetition and redundancy
–Refactor code to standardize workflow and enforce styles,
e.g., we refactored the deep drug-target binding affinity
(DeepDTA) [64] into our PyKale pipeline (Fig. 1(c)).
–Identify and remove duplicated functionalities, e.g., con-
struct data loading API for popular dataset to share among
different projects.
•Reuse existing resources
–Reuse the same machine learning pipeline for different
data and applications, such as using the same multilinear
principal component analysis (MPCA) pipeline for gait
[35], brain [53], and heart [55].
–Reuse existing libraries (e.g., those in the PyTorch ecosys-
tem, such as PyTorch Geometric) for available functionali-
ties rather than implementing them again.
•Recycle learning models across areas
–Identify commonalities between applications, e.g., the simi-
larity between commercial recommender systems (predict-
ing user-item interactions) and drug discovery (predicting
drug-target interactions).
–Recycle models for one application to another, e.g. from
recommender systems [2] to drug discovery [61].
Although the above is largely based on standard software engi-
neering practices, this new formulation offers a new perspective to
focus on core principles of standardization and minimalism. It has
guided us to design a unique pipeline-based API to unify workflow,
break barriers between areas and applications, and cross boundaries
to fuse existing ideas and nurture new ideas.
3.2 Pipeline-based API
Inspired by the convenience of machine learning pipelines in ma-
chine learning library like Spark MLlib [ 38] and scikit-learn [ 44], we
design PyKale with a pipeline-based API as shown in Fig. 1(a). This
design has six key steps and embodies our green machine learning
principles above by organizing code along a standardized machine
learning pipeline to identify commonalities, reduce redundancy,
and minimize cognitive overhead.
Lu, et al.
1# Load digits from multiple sources [digits_dann_lightn/main.py]
2from kale.loaddata.digits_access import DigitDataset
3from kale.loaddata.multi_domain import MultiDomainDatasets
4
5source, target, _ = DigitDataset.get_source_target(
6 DigitDataset("MNIST"), DigitDataset("USPS"), data_path)
7dataset = MultiDomainDatasets(source, target))
8
9# Preprocess digits [kale/loaddata/digits_access.py]
10import kale.prepdata.image_transform as image_transform
11
12self._transform = image_transform.get_transform(transform_kind)
13def get_train(self):
14 return MNIST(data_path, train=True, transform=self._transform)
15
16# Embed digit representations [digits_dann_lightn/model.py]
17from kale.embed.image_cnn import SmallCNNFeature
18
19# Predict digit class and domain [digits_dann_lightn/model.py]
20from kale.predict.class_domain_nets import ClassNetSmallImage,
21 DomainNetSmallImage
22
23# Build domain adaption pipeline [digits_dann_lightn/model.py]
24import kale.pipeline.domain_adapter as domain_adapter
25
26model = domain_adapter.create_dann_based(method="DANN",
27 dataset=dataset,
28 feature_extractor= SmallCNNFeature(),
29 task_classifier=ClassNetSmallImage(),
30 critic=DomainNetSmallImage(),
31 **train_params)
32
33# Utility functions [digits_dann_lightn/main.py]
34from kale.utils.csv_logger import setup_logger
35from kale.utils.seed import set_seed
Code 1: Code snippets from the source code
for the digits domain adaptation example at
pykale/examples/digits_dann_lightn/main.py to demon-
strate the unified pipeline-based API, simplified for
inclusion here.
In the following, we explain our unified API by starting with
what the input and output are. We provide Python code snippets to
help this explanation in Code 1, mainly using the domain adaptation
for digit classification example with a pipeline in Fig. 1(b).3Figure
1(c) shows another pipeline for drug discovery. More code snippets
are in Section 5.
3.2.1 Load. Thekale.loaddata module mainly takes source paths
(local or online) as the input and constructs dataloaders for datasets
as the output. Its primary function is to load data for input to the
machine learning system/pipeline. See line 2–7 of Code 1 for an
example of loading digit images from multiple sources and line
8–13 of Code 4 for an example of loading drug and targets data.
3.2.2 Preprocess. The kale.prepdata module takes the loaded
raw input data as input and preprocesses (transforms) them into a
suitable representation for the following machine learning modules.
3https://github.com/pykale/pykale/tree/main/examples/digits_dann_lightnPreprocessing steps include data normalization, augmentation, and
other transformations of data representation not involving machine
learning. Its submodules are typically imported in kale.loaddata .
See line 10–14 of Code 1 for an example of standardizing digit
images with predefined transforms.
3.2.3 Embed. The kale.embed module takes preprocessed, nor-
malized data representations to learn new representations in a new
space as the output. It includes dimensionality reduction algorithms
(feature extraction and feature selection), such as MPCA [ 35] and
CNNs. They can be viewed as encoders or embedding functions that
learn suitable representations from data. This is a machine learning
module. See line 17 of Code 1 for an example of (importing) a CNN
feature extractor.
3.2.4 Predict. The kale.predict module takes the learned (or
preprocessed, if skipping kale.embed ) representations to predict a
desired target value as the output. Thus, this module provides pre-
diction functions or decoders that learn a mapping from the input
representation to a target prediction. This is also a machine learning
module. See line 20–21 of Code 1 for an example of (importing)
digit and domain classifiers.
3.2.5 Evaluate. Thekale.evaluate module evaluates the predic-
tion performance using some metrics. We reuse metrics from other
libraries (e.g., sklearn.metrics in line 4 of Code 5) and only im-
plement metrics not commonly available, such as the Concordance
Index (CI) [ 1] for measuring the proportion of concordant pairs.
See line 19 of Code 4 for its example usage.
3.2.6 Interpret. Thekale.interpret module aims to provide func-
tions for interpretation of the learned features, the model, or the
prediction results/outputs, e.g., via further analysis or visualization,
and we only implement functions not commonly available. This
module has implemented functions for selecting and visualizing
weights from a trained model. See line 16–20 of Code 5 for an
example of visualizing weights of a linear model for interpretation.
3.2.7 Pipeline. Thekale.pipeline module provides mature, off-
the-shelf machine learning pipelines for “plug-in usage”. Its submod-
ules typically specify a machine learning workflow by combining
several other modules. See line 24–31 of Code 1 for an example of
calling a domain adaptation pipeline.
3.2.8 Utilities. Thekale.utils module provides common utility
functions, such as setting random seeds, logging results, or down-
loading data. See line 34–35 of Code 1 for examples of importing
the seed-setting and csv-logging submodules.
3.3 Machine learning models
Machine learning models in PyKale can be categorized into four
main (possibly overlapping) groups.
3.3.1 Multimodal learning. To support learning from data of multi-
ple modalities, we need to first support learning from each individ-
ual modality. Thanks to the rich PyTorch ecosystem, we can build
upon other libraries to have machine learning models supporting
graphs, images, texts, and videos, primarily using PyTorch Dataload-
ers. The only missing major modality is audio but we have ongoing
effort to include it in the near future, building upon torchaudio .
PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python
Learning from heterogeneous data sources and data integration
can be viewed as multimodal learning as well. To this end, PyKale
has built a DeepDTA [ 42] pipeline kale.pipeline.deep_dti that
learns from drug and target data, the chemical representation of
which can be transformed into sequence or vector representations.
PyKale also implemented our recent Graph information propaga-
tion Network (GripNet) [ 61]kale.embed.gripnet for link pre-
diction and data integration on heterogeneous knowledge graphs.
PyKale has an example drug_gripnet to show the model usage
on public bioinformatics knowledge graph with drug and protein’s
information.
3.3.2 Transfer learning. In transfer learning, PyKale currently fo-
cuses on domain adaptation [ 3,43]. We largely inherited the ex-
cellent, modular architecture from ADA [ 56], covering many im-
portant semi-supervised and unsupervised domain adaptation al-
gorithms, such as domain-adversarial neural networks (DANN)
[14], conditional adversarial domain adaptation networks (CDAN)
[33], deep adaptation networks (DAN) [ 32] and joint adaptation
networks (JAN) [ 34], and Wasserstein distance guided represen-
tation learning (WDGRL) [ 51]. These algorithms are applicable to
all modalities with appropriate representations. PyKale currently
has two pipelines for domain adaptation: domain_adapter and
video_domain_adapter inkale.pipeline .
3.3.3 Deep learning. PyKale builds deep neural networks (DNNs)
upon the PyTorch API. Current implementations include CNNs [ 26]
/ 3D CNNs [ 7,57], GCNs [ 23], and attention-based networks such
as transformers [ 59] and squeeze and excitation networks [ 18] (see
more in Section 5). We use TorchVision [ 37], PyTorch Geometric
[13], and PyTorch Lightning [11] in our implementation.
3.3.4 Dimensionality reduction. PyKale has built a Python ver-
sion of the MPCA algorithm [ 35] atkale.embed.mpca , as well as
an MPCA-based pipeline at kale.pipeline.mpca_trainer , using
both the scikit-Learn library [ 44] and the TensorLy library [ 24]. This
pipeline has been successfully used for interpretable prediction in
gait recognition from video sequences [ 35], cardiovascular disease
diagnosis [ 55] and prognosis [ 58] using cardiac magnetic resonance
imaging (MRI), and brain state classification using functional MRI
(fMRI). We are further building into PyKale other advanced tensor-
based algorithms such as regularized Multilinear Regression and
Selection (Remurs) [ 53] and sparse tubal-regularized multilinear
regression (Sturm) [29].
3.4 Software engineering
The PyKale team includes machine learning researchers and Re-
search Software Engineers (RSEs). We have adopted good software
engineering practices in a research context, often based on other
libraries, particularly those in the PyTorch ecosystem.
3.4.1 Version control and collaboration. We use git for version con-
trol and GitHub for collaborative working. The PyKale repository
stipulates a license (MIT) and contributing guidelines.4These en-
able reuse of the software and sustainability of the project through
4https://github.com/pykale/pykale/blob/main/.github/CONTRIBUTING.mdcommunity contributions. This is a platform for long term availabil-
ity of the resource and lays the foundation for community mainte-
nance over an indefinite period.
3.4.2 Documentation. We use “docstrings” to embed documenta-
tion within the source code to maximize synchronicity between
code and documentation. Sphinx5is used to automatically build
these (along with additional information in “reStructuredText” for-
mat) into html docs. Documentation is published via readthedocs.
com and is kept up to date by Continuous Integration (CI). This
is useful for keeping users and developers up-to-date with new
features and bug fixes in a sustainable way. Detailed installation
instructions are included.6
3.4.3 Tests and continuous integration. We use the PyTest frame-
work and currently have 88% test coverage. The test suite can be
run locally, and also runs automatically on GitHub and must pass
for code to be merged into the main branch. This ensures that new
features do not create unintended side-effects. CI is implemented
using GitHub workflows/actions.7Our CI checks include static anal-
ysis, pre-commit checks (e.g., maximum file size), documentation
building (via Read the Docs), changelog update, project assignment
for issues and pull requests, PyPI release of packages, PyTest tests
on multiple platforms and multiple python versions, and Codecov
code coverage report. This ensures that the version in the main
branch is always the most up to date working version, and meets
our standards of functionality and coding style.
To maintain a small repository size (currently less than 1MB),
we store test data in a separate repository at https://github.com/
pykale/data, which can be downloaded via download_file_by_url
inkale.utils.download automatically.
4 PYKALE USAGE
Interdisciplinary research is a complex subject to support and care
has to be taken to lower the barriers to entry. PyKale includes
examples and tutorials to help users’ exploration.
4.1 Usage of pipeline-based API in examples
PyKale examples are highly standardized. Each example typically
has three essential modules ( main.py ,config.py ,model.py ), one
optional directory ( configs ), and possibly other modules ( trainer.py ):
•main.py is the main module to be run, showing the main
workflow.
•config.py is the configuration module that sets up the data,
prediction problem, hyper-parameters, etc. The settings in
this module are the default configuration.
•configs is the directory to place customized configurations
for individual runs. We use .yaml files (see Section 4.3) for
this purpose.
•model.py is the model module to define the machine learn-
ing model and configure its training parameters.
•trainer.py is the trainer module to define the training and
testing workflow. This module is only needed when NOT
using PyTorch Lightning.
5https://www.sphinx-doc.org/
6https://pykale.readthedocs.io/en/latest/installation.html
7https://github.com/pykale/pykale/tree/main/.github/workflows
Lu, et al.
4.2 Building new modules or projects
Users can build new modules or projects following the steps below.
•Step 1 - Examples: Choose one of the examples of the users’
interest (e.g., most relevant to the users’ project) to
–browse through the configuration, main, and model mod-
ules,
–download the data if needed, and
–run the example following instructions in the example’s
README.
•Step 2a - New model: To develop new machine learning
models under PyKale,
–define the blocks in the users’ pipeline to figure out what
the methods are for data loading, preprocessing data, em-
bedding (encoder/representation), prediction (decoder),
evaluation, and interpretation, and
–modify existing pipelines with the users’ customized blocks
or build a new pipeline with pykale blocks and blocks
from other libraries.
•Step 2b - New applications: To develop new applications
using PyKale,
–clarify the input data and the prediction target to find
matching functionalities in pykale (request if not found),
and
–tailor data loading, preprocessing, and evaluation (and
interpretation if needed) to the users’ application.
4.3 YAML configuration
PyKale examples configure a machine learning system using YAML
[4]. This is inspired by the usage of YAML in the GitHub pack-
age for the Isometric Network (ISONet) [ 47],8with our adapted
version illustrated in Code 2. As modern machine learning sys-
tems typically have many settings to configure, specifying many/all
settings in command line or Python modules becomes difficult to
manage and read. Using YAML greatly improves the readability
and reproducibility, and makes configuration changes much easier,
via a default configuration specified in config.py (top of Code 2)
and customized configurations specified in a respective .yaml file
(bottom of Code 2), which will be merged to overwrite the default
setting at run time.
4.4 Notebook tutorials with Binder and Colab
We have eight real-world examples of PyKale usage.9However,
tutorials without the need of any installation are important for new
users to get familiar with the PyKale workflow and API. For these
we must scale-back on real-world datasets due to the computational
resources needed, as these lead to long runtimes, unsuitable for
interactive learning. Therefore, we are simplifying our examples
into Jupyter notebook tutorials so that each tutorial takes min-
utes instead of hours to run. This will strike a balance between
computational requirements and runtime, without resorting to toy
examples.
8https://github.com/HaozhiQi/ISONet
9https://github.com/pykale/pykale/tree/main/examples1# The file config.py that defines the default configuration
2from yacs.config import CfgNode asCN
3
4# Config definition
5_C = CN()
6
7# Dataset
8_C.DATASET = CN()
9_C.DATASET.ROOT = "../data"
10_C.DATASET.NAME = "CIFAR10"
11
12# Solver
13_C.SOLVER = CN()
14_C.SOLVER.SEED = 2020
15_C.SOLVER.BASE_LR = 0.05
16_C.SOLVER.TRAIN_BATCH_SIZE = 128
17_C.SOLVER.MAX_EPOCHS = 100
18
19# ISONet configs
20_C.ISON = CN()
21_C.ISON.DEPTH = 34
22
23# Misc options
24_C.OUTPUT_DIR = "./outputs"
1# Customization in a .yaml file
2SOLVER :
3BASE_LR : 0.01
4MAX_EPOCHS : 10 # For quick testing
5ISON :
6DEPTH : 38
Code 2: Code snippets to demonstrate the usage of
YAML to configure machine learning systems in PyKale,
from pykale/examples/cifar_isonet/config.py , which is
adapted from https://github.com/HaozhiQi/ISONet.
To bring further convenience, we set up cloud-based services
with both Binder10and Google Colaboratory (Colab)11for our note-
book tutorials so that any users can run PyKale tutorials without
the need of any installation. The first such tutorial has been re-
leased,12with screenshots in Figure 3. More such tutorials are in
development.
5 USE CASES: PYKALE EXAMPLES
PyKale currently has example applications from three areas below:
•Image/video recognition: classification of images (objects,
digits) or videos (actions in first-person videos);
•Bioinformatics/graph analysis: prediction of links between
entities in knowledge graphs (BindingDB, BioSNAP-Decagon);
•Medical imaging: disease diagnosis from cardiac MRIs.
The above examples deal with graphs, images, and videos. Our
current APIs support text processing (e.g., for NLP tasks) but an
example in this area is still in development. We are also conducting
research in integrating audio features in action recognition so an
10https://mybinder.org/
11https://colab.research.google.com/
12https://github.com/pykale/pykale/blob/main/examples/digits_dann_lightn/tutorial.
ipynb
PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python
(a) Binder
(b) Google Colab
Figure 3: PyKale digits domain adaptation example on cloud-
based services.
example involving audio data is a future task as well. Examples in
computer vision applications such as image and video recognition
are a good start for most users due to the popularity of vision
applications and a low barrier to entry (e.g., no need for specific
domain knowledge as in drug discovery). Models first developed in
computer vision can be reused or recycled for other applications.
The data used in PyKale examples are real-world data frequently
used in research papers. Subsequently, it may take quite some time
to finish running these examples. For quick running and demonstra-
tion of the workflow, tutorials (Section 4.4) are simplified examples
that serve as a better starting point. The following subsections
give an overview of the data and algorithms used in the example
machine learning systems in PyKale.
5.1 CIFAR and digits classification
Small-image datasets are good for building examples of real-world
relevance. PyKale has three such examples: two on the CIFAR
datasets [ 25] and one on digits datasets including MNIST [ 9,28],
modified MNIST [ 14], USPS [ 20], and SVHN [ 40].cifar_isonet
is the first example in PyKale refactoring the ISONet code https:
//github.com/HaozhiQi/ISONet on CIFAR10 and CIFAR100 into the
PyKale API. cifar_cnntransformer is another example on CIFAR
showing the simple application of a mixed CNN and transformer
model for vision tasks. Code 1 has shown code snippets of digits
classification via domain adaptation using MNIST as the source do-
main and USPS as the target domain, in the digits_dann_lightn
example.
Examples on these popular datasets could help users familiar
with them make easy connections between the PyKale API and
what they are already familiar with.1# Transform and dataset for multi-modal video data
2from kale.prepdata.video_transform import get_transform
3from kale.loaddata.video_datasets import BasicVideoDataset
4
5transform = get_transform(transform_kind, image_modality)
6dataset = BasicVideoDataset(data_path, train_list,
7 imagefile_template="frame_{:010d}.jpg"
8 ifimage_modality in["rgb"]
9 else "flow_{}_{:010d}.jpg",
10 transform=transform,
11 image_modality=image_modality)
12
13# Action video feature extractor
14from kale.embed.video_feature_extractor import
15 get_video_feat_extractor
16feature_extractor = get_video_feat_extractor("I3D", image_modality,
17 attention, num_classes)
Code 3: Code snippets to demonstrate the example on action
recognition via domain adaptation.
5.2 Action recognition via domain adaptation
5.2.1 Data. For this action recognition example, we constructed
three first-person vision datasets, ADL 𝑠𝑚𝑎𝑙𝑙 , GTEA-KITCHEN and
EPIC 𝑐𝑣𝑝𝑟 20, with 222/454/10094 action videos respectively. We se-
lected and reorganized three videos from ADL dataset [ 46] as three
domains of ADL 𝑠𝑚𝑎𝑙𝑙 , re-annotated GTEA [ 12,30] and KITCHEN
[8] datasets to build GTEA-KITCHEN, and adopted the public
dataset EPIC 𝑐𝑣𝑝𝑟 20[39]. We will provide instructions on how to
construct these datasets from the source at https://github.com/
pykale/data/tree/main/video_data/video_test_data (in progress).
Each dataset has two modalities: RGB and optical flow.
5.2.2 Algorithms. For video feature extraction, we built two state-
of-the-art action recognition algorithms, I3D [ 7] and 3D ResNet
[57], into PyKale. For domain adaptation, we extended the domain
adaptation framework for images (digits, adapted from [ 56]) to
videos. We followed the same pipeline as digits classification while
providing additional specific functions for action videos and multi-
modal data. As shown in Code 3, the image modality parameter
can be set to choose the proper data transform and loader for dif-
ferent modalities: RGB, optical flow, and joint, and all video feature
extractors are accessed via a unified interface.
5.3 Drug-target interaction prediction
5.3.1 Data. Predicting the binding affinity between drug com-
pounds and target proteins is fundamental for drug discovery and
drug repurposing. This example uses three public datasets (for three
metrics Ki, Kd and IC50) from BindingDB [ 31] containing 52,284,
375,032, and 991,486 interaction pairs respectively, accessed via
the Therapeutics Data Commons (TDC) platform [ 19]. Given the
amino acid sequences of targets and SMILES (Simplified Molecular
Input Line Entry System) strings of drug compounds, the task is
to predict drug-target binding affinity. Following [ 22], the affinity
metrics are transformed into the logarithm form for more stable
training and validation.
Lu, et al.
1from kale.loaddata.tdc_datasets import BindingDBDataset
2from kale.embed.seq_nn import CNNEncoder
3from kale.predict.decode import MLPDecoder
4from kale.pipeline.deep_dti import DeepDTATrainer
5import pytorch_lightning as pl
6
7# Load training, validation, and test data
8train_dataset = BindingDBDataset(name="Kd", split="train")
9val_dataset = BindingDBDataset(name="Kd", split="valid")
10test_dataset = BindingDBDataset(name="Kd", split="valid")
11train_loader = DataLoader(dataset=train_dataset, batch_size=64)
12val_loader = DataLoader(dataset=val_dataset, batch_size=64)
13test_loader = DataLoader(dataset=test_dataset, batch_size=64)
14
15# Initialize DeepDTA with encoder and decoder
16drug_encoder, target_encoder = CNNEncoder(), CNNEncoder()
17decoder = MLPDecoder()
18model = DeepDTATrainer(drug_encoder, target_encoder, decoder,
19 ci_metric=True)
20
21# Train the model under the PyTorch Lightning framework
22trainer = pl.Trainer(max_epochs=100)
23trainer.fit(model, train_dataloader=train_loader,
24 val_dataloaders=val_loader)
25trainer.test(test_dataloaders=test_loader)
Code 4: Demonstration code for drug-target interaction pre-
diction with DeepDTA.
5.3.2 Algorithms. For this drug-target interaction prediction prob-
lem, we built DeepDTA [ 42] into PyKale, a typical CNN-based model
with encoder-decoder architecture. The drug SMILES string and
target amino acid sequence are encoded by their independent CNNs,
and then a multilayer perceptron is used to decode the affinity from
the drug-target combined encoding. We refactored DeepDTA into
the PyKale API structure with separate modules for load data, pre-
process data, encode (embed) and decode (predict). These refactored
modules are flexible and can be reused across different applications.
Code 4 illustrates the usage of the DeepDTA model as implemented
in PyKale.
5.4 Polypharmacy side effect prediction via
knowledge graph link prediction
5.4.1 Data. Polypharmacy uses drug combination to treat com-
plex diseases, which may cause side effects. Predicting such side
effects can be formulated as a link prediction problem on knowl-
edge graphs of drugs and proteins [ 41,61,62]. This example uses
the public BioSNAP-Decagon dataset [ 63] with 6,075,428 edges,
1,100 different edge labels, and three types of edges: drug-drug
interaction, protein-protein interaction and drug-target interaction.
The drug-drug interactions model polypharmacy side effects.
5.4.2 Algorithms. We built our recent GripNet [ 61] into PyKale for
this example. GripNet is a subgraph network framework for multi-
relational link prediction on heterogeneous graphs via segregated
node representation learning on “supervertices” and “superedges”.
PyKale implements APIs to support these advanced concepts for
node embedding and link prediction on heterogeneous graphs.1from kale.pipeline.mpca_trainer import MPCATrainer
2from kale.interpret import model_weights
3from sklearn.model_selection import train_test_split
4from sklearn.metrics import accuracy_score, roc_auc_score
5
6# Assume data x and labels y are loaded and preprocessed
7trainer = MPCATrainer(mpca_params={"return_vector": True})
8x_train, x_test, y_train, y_test = train_test_split(x, y,
9 test_size=0.2)
10trainer.fit(x_train, y_train)
11print ("Accuracy:", accuracy_score(y_test, trainer.predict(x_test))
12print ("AUC:", roc_auc_score(y_test,
13 trainer.decision_function(x_test))
14
15# Visualize model coefficients (weights) for interpretation
16weights = trainer.mpca.inverse_transform(trainer.clf.coef_)
17 - trainer.mpca.mean_
18top_weights = model_weights.select_top_weight(weights)
19model_weights.plot_weights(top_weights[0][0],
20 background_img=x[0][0])
Code 5: MPCA-based pipeline for predicting pulmonary ar-
terial hypertension with cardiac MRI.
5.5 Cardiac MRI classification
5.5.1 Data. The data used to build this example are a subset of
the dataset used in [ 55], which consists of cardiac MRI (CMRI)
sequences acquired from patients with pulmonary arterial hyper-
tension and health controls. They are not yet in the public domain
but we are exploring release options. The CMRI sequences are stan-
dardized using methods in kale.prepdata.image_transform .
5.5.2 Algorithms. This example uses the MPCATrainer pipeline in
kale.pipeline.mpca_trainer , which implemented the machine
learning pipeline used in [ 35,54,55] in the scikit-learn style. The
three key steps are MPCA dimensionality reduction, feature selec-
tion, and classification (SVM, Linear SVM, or logisitic regression),
where the feature selection and classification algorithms reuse re-
spective APIs in scikit-learn [ 44]. Code 5 shows the steps for MPCA-
based prediction and interpretation on cardiac MRI.
6 PYKALE OPENNESS AND PLAN
6.1 License and community engagement
PyKale is publicly available at https://github.com/pykale/pykale
under an MIT license, which is a simple permissive license with
minimal restrictions. The PyKale GitHub repository has active dis-
cussion board at https://github.com/pykale/pykale/discussions and
project board at https://github.com/pykale/pykale/projects to in-
teract with users and make the development process and plan
transparent to all users. Complete documentation is hosted at
https://pykale.readthedocs.io/ with multiple versions available, gen-
erated automatically from https://github.com/pykale/pykale/tree/
main/docs. We have provided tutorials and examples as well as de-
tailed contributing guidelines at https://github.com/pykale/pykale/
blob/main/.github/CONTRIBUTING.md and change logs at https://
github.com/pykale/pykale/blob/main/.github/CHANGELOG.md.
We also released a 12-minute YouTube video at https://youtu.
be/i5BYdMfbpMQ to briefly explain the motivation and principles
PyKale: Knowledge-Aware Machine Learning from Multiple Sources in Python
behind PyKale. This paper is another effort to reach out to the wider
research community to share this resource and get feedback for
further improvements.
6.2 Limitations and future development
PyKale is an open-source project started in June 2020, with the first
PyPI release in January 2021. It was motivated by the growing needs
for machine learning systems that can deal with multiple sources
of data, particularly in interdisciplinary areas such as healthcare.
For example, clinicians often need to make use of a combination
of medical images (e.g., X-rays, CTs, MRIs), biological data (gene,
DNA, RNA), and electronic health record for decision making. Our
focus on multimodal learning and transfer learning has defined a
challenging scope, while holding the promises to break barriers in
interdisciplinary research.
To date, PyKale has built APIs supporting machine learning
from graphs, images, texts, and videos, with four mature pipelines
implemented. Nevertheless, we do not have an example on text
data yet, and we have not built APIs for audio yet. Developing
projects involving multiple data sources takes considerably longer
time than developing those involving a single data source. The
current version of PyKale has two examples on multimodal learning
involving heterogeneous drug and target data and two examples on
domain adaptation (transfer learning) for images and videos. These
examples laid solid foundations for us to grow our research in these
areas and build more advanced examples in future development.
In addition, our tests currently have a coverage of 88%. We need
further improvements for a higher coverage and more rigorous
tests.
7 CONCLUSIONS
In this paper, we have introduced PyKale , a Python library for
knowledge-aware machine learning from multiple sources, par-
ticularly from multiple modalities for multimodal learning and
from multiple domains for transfer learning. This library was mo-
tivated by needs in healthcare applications (hence the acronym
kale, a healthy vegetable) and aims to enable and accelerate inter-
disciplinary research. Building on standard software engineering
practices, we proposed a new green machine learning perspective
to advocate reducing repetitions and redundancy, reusing existing
resources, and recycling learning models across areas. Following
such principles, we designed our API to be pipeline-based to unify
the workflow and increase the flexibility. This design can help to
break barriers between different areas or applications and facilitate
the fusion and nurture of ideas across discipline boundaries.
The goal of PyKale is to facilitate interdisciplinary, knowledge-
aware machine learning research for graphs, images, texts, and
videos. It will make it easier to bring machine learning models
developed in one area to the other, and integrate data from multiple
sources for prediction tasks in interdisciplinary areas. Its focus on
leveraging knowledge from multiple sources also helps accurate and
interpretable prediction. To demonstrate such potential, we have
shown example applications including bioinformatics, knowledge
graph, image/video recognition, and medical imaging on real-world
datasets.ACKNOWLEDGMENTS
The development of PyKale is partially supported by the Innova-
tor Awards: Digital Technologies from the Wellcome Trust (grant
215799/Z/19/Z). We thank the support and contributions from David
Jones, Will Furnass, and other members of the Research Software
Engineering (RSE) team headed by Paul Richmond. We also thank
early users of our library and their helpful feedback.
REFERENCES
[1]Kartik Ahuja and Mihaela van der Schaar. 2019. Joint Concordance Index. In Pro-
ceedings of the 2019 53rd Asilomar Conference on Signals, Systems, and Computers .
2206–2213.
[2]Peizhen Bai, Yan Ge, Fangling Liu, and Haiping Lu. 2019. Joint interaction
with context operation for collaborative filtering. Pattern Recognition 88 (2019),
729–738.
[3]Shai Ben-David, John Blitzer, Koby Crammer, Fernando Pereira, et al .2007. Anal-
ysis of representations for domain adaptation. In Proceedings of the Advances in
Neural Information Processing Systems . 137–144.
[4]Oren Ben-Kiki, Clark Evans, and Brian Ingerson. 2009. Yaml ain’t markup lan-
guage (yaml ™) version 1.1. Working Draft 2008-05 11 (2009).
[5]Antonio Candelieri, Riccardo Perego, and Francesco Archetti. 2021. Green ma-
chine learning via augmented Gaussian processes and multi-information source
optimization. Soft Computing (2021), 1–13.
[6]Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with trans-
formers. In Proceedings of the European Conference on Computer Vision . 213–229.
[7]Joao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new
model and the kinetics dataset. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition . 6299–6308.
[8]Fernando De la Torre, Jessica Hodgins, Adam Bargteil, Xavier Martin, Justin
Macey, Alex Collado, and Pep Beltran. 2009. Guide to the carnegie mellon
university multimodal activity (cmu-mmac) database. (2009).
[9]Li Deng. 2012. The mnist database of handwritten digit images for machine
learning research. IEEE Signal Processing Magazine 29, 6 (2012), 141–142.
[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, and et al. 2021. An
Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In
Proceedings of the International Conference on Learning Representations .
[11] William A Falcon and et al. 2019. PyTorch Lightning. GitHub. 3 (2019). https:
//github.com/PyTorchLightning/pytorch-lightning
[12] Alireza Fathi, Xiaofeng Ren, and James M Rehg. 2011. Learning to recognize
objects in egocentric activities. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition . 3281–3288.
[13] Matthias Fey and Jan E. Lenssen. 2019. Fast Graph Representation Learning with
PyTorch Geometric. In ICLR Workshop on Representation Learning on Graphs and
Manifolds .
[14] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
Larochelle, François Laviolette, Mario Marchand, and Victor Lempitsky. 2016.
Domain-adversarial training of neural networks. Journal of Machine Learning
Research 17, 1 (2016), 2096–2030.
[15] Eva García Martín. 2017. Energy efficiency in machine learning: A position paper.
InProceedings of the 30th Annual Workshop of the Swedish Artificial Intelligence
Society , Vol. 137. 68–72.
[16] Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G
Wilson. 2018. GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference
with GPU Acceleration. In Proceedings of the Advances in Neural Information
Processing Systems , Vol. 31. 7587–7597.
[17] Georgian. 2020. Multimodal-Toolkit. GitHub (2020). https://github.com/georgian-
io/Multimodal-Toolkit
[18] Jie Hu, Li Shen, and Gang Sun. 2018. Squeeze-and-excitation networks. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition . 7132–
7141.
[19] Kexin Huang, Tianfan Fu, Wenhao Gao, and et al. 2021. Therapeutics Data
Commons: Machine Learning Datasets and Tasks for Therapeutics. arXiv preprint
arXiv:2102.09548 (2021).
[20] Jonathan Hull. 1994. A database for handwritten text recognition research. IEEE
Transactions on Pattern Analysis and Machine Intelligence 16, 5 (1994), 550–554.
[21] Junguang Jiang, Bo Fu, and Mingsheng Long. 2020. Transfer-Learning-library.
GitHub (2020). https://github.com/thuml/Transfer-Learning-Library
[22] Mostafa Karimi, Di Wu, Zhangyang Wang, and Yang Shen. 2019. DeepAffinity: in-
terpretable deep learning of compound–protein affinity through unified recurrent
and convolutional neural networks. Bioinformatics 35, 18 (2019), 3329–3338.
[23] Thomas Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph
Convolutional Networks. In Proceedings of the 5th International Conference on
Learning Representations .
Lu, et al.
[24] Jean Kossaifi, Yannis Panagakis, Anima Anandkumar, and Maja Pantic. 2019.
TensorLy: Tensor Learning in Python. Journal of Machine Learning Research 20,
26 (2019), 1–6.
[25] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. 2010. Cifar-10 (canadian
institute for advanced research). 5 (2010).
[26] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classifi-
cation with deep convolutional neural networks. In Proceedings of the Advances
in Neural Information Processing Systems . 1097–1105.
[27] Sun Yuan Kung. 2014. Kernel methods and machine learning . Cambridge Univer-
sity Press.
[28] Yann LeCun. 1998. The MNIST database of handwritten digits. http://yann. lecun.
com/exdb/mnist/ (1998).
[29] Wenwen Li, Jian Lou, Shuo Zhou, and Haiping Lu. 2019. Sturm: Sparse tubal-
regularized multilinear regression for fmri. In Proceedings of the International
Workshop on Machine Learning in Medical Imaging . 256–264.
[30] Yin Li, Zhefan Ye, and James M Rehg. 2015. Delving into egocentric actions. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition .
287–295.
[31] Tiqing Liu, Yuhmei Lin, Xin Wen, R. Jorissen, and M. Gilson. 2007. BindingDB: a
web-accessible database of experimentally determined protein–ligand binding
affinities. Nucleic Acids Research 35 (2007), D198 – D201.
[32] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. 2015. Learn-
ing transferable features with deep adaptation networks. In Proceedings of the
International Conference on Machine Learning . 97–105.
[33] Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, and Michael I Jordan. 2018.
Conditional Adversarial Domain Adaptation. In Proceedings of the Advances in
Neural Information Processing Systems , Vol. 31.
[34] Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. 2017. Deep
transfer learning with joint adaptation networks. In Proceedings of the Interna-
tional Conference on Machine Learning . 2208–2217.
[35] Haiping Lu, Konstantinos N Plataniotis, and Anastasios N Venetsanopoulos.
2008. MPCA: Multilinear principal component analysis of tensor objects. IEEE
Transactions on Neural Networks 19, 1 (2008), 18–39.
[36] Nic Ma, Wenqi Li, and Richard Brown. 2021. Project-MONAI/MONAI: 0.5.3 . https:
//doi.org/10.5281/zenodo.4891800
[37] Sébastien Marcel and Yann Rodriguez. 2010. Torchvision the Machine-Vision
Package of Torch. In Proceedings of the 18th ACM International Conference on
Multimedia . 1485–1488.
[38] Xiangrui Meng, Joseph Bradley, Burak Yavuz, Evan Sparks, Shivaram Venkatara-
man, Davies Liu, Jeremy Freeman, DB Tsai, Manish Amde, Sean Owen, et al .2016.
Mllib: Machine learning in apache spark. Journal of Machine Learning Research
17, 1 (2016), 1235–1241.
[39] Jonathan Munro and Dima Damen. 2020. Multi-modal domain adaptation for
fine-grained action recognition. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition . 122–132.
[40] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and An-
drew Y. Ng. 2011. Reading Digits in Natural Images with Unsupervised Feature
Learning. NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning
(2011).
[41] Vít Nováček and Sameh K Mohamed. 2020. Predicting Polypharmacy Side-effects
Using Knowledge Graph Embeddings.. In Proceedings of the AMIA Joint Summits
on Translational Science . 449–458.
[42] Hakime Öztürk, E. Olmez, and Arzucan Özgür. 2018. DeepDTA: deep drug–target
binding affinity prediction. Bioinformatics 34 (2018), i821 – i829.
[43] Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. 2010. Domain
adaptation via transfer component analysis. IEEE Transactions on Neural Networks
22, 2 (2010), 199–210.
[44] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M.
Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cour-
napeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine
Learning in Python. Journal of Machine Learning Research 12 (2011), 2825–2830.
[45] Fernando Pérez-García, Rachel Sparks, and Sebastien Ourselin. 2020. TorchIO:
a Python library for efficient loading, preprocessing, augmentation and patch-
based sampling of medical images in deep learning. (2020). http://arxiv.org/abs/
2003.04696
[46] Hamed Pirsiavash and Deva Ramanan. 2012. Detecting activities of daily living
in first-person camera views. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition . 2847–2854.
[47] Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. 2020. Deep
isometric learning for visual recognition. In Proceedings of the International
Conference on Machine Learning . 7824–7835.
[48] Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee, and Gary Bradski.
2020. Kornia: an open source differentiable computer vision library for pytorch.
InProceedings of the IEEE Winter Conference on Applications of Computer Vision .
3674–3683.
[49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional
networks for biomedical image segmentation. In Proceedings of the InternationalConference on Medical Image Computing and Computer-Assisted Intervention . 234–
241.
[50] Aghiles Salah, Quoc-Tuan Truong, and Hady W Lauw. 2020. Cornac: A Com-
parative Framework for Multimodal Recommender Systems. Journal of Machine
Learning Research 21, 95 (2020), 1–5.
[51] Jian Shen, Yanru Qu, Weinan Zhang, and Yong Yu. 2018. Wasserstein distance
guided representation learning for domain adaptation. In Proceedings of the AAAI
Conference on Artificial Intelligence , Vol. 32.
[52] Amanpreet Singh, Vedanuj Goswami, Vivek Natarajan, Yu Jiang, Xinlei Chen,
Meet Shah, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2020. MMF: A
multimodal framework for vision and language research. https://github.com/
facebookresearch/mmf.
[53] Xiaonan Song and Haiping Lu. 2017. Multilinear regression for embedded feature
selection with application to fmri analysis. In Proceedings of the AAAI Conference
on Artificial Intelligence , Vol. 31.
[54] Xiaonan Song, Lingnan Meng, Qiquan Shi, and Haiping Lu. 2015. Learning
tensor-based features for whole-brain fMRI classification. In Proceedings of the
International Conference on Medical Image Computing and Computer-Assisted
Intervention . 613–620.
[55] Andrew J Swift, Haiping Lu, Johanna Uthoff, Pankaj Garg, Marcella Cogliano,
Jonathan Taylor, Peter Metherall, Shuo Zhou, Christopher S Johns, Samer Al-
abed, et al .2021. A machine learning cardiac magnetic resonance approach to
extract disease features and automate pulmonary arterial hypertension diagnosis.
European Heart Journal-Cardiovascular Imaging 22, 2 (2021), 236–245.
[56] Anne-Marie Tousch and Christophe Renaudin. 2020. (Yet) Another Domain
Adaptation library. https://github.com/criteo-research/pytorch-ada
[57] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar
Paluri. 2018. A closer look at spatiotemporal convolutions for action recognition.
InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition .
6450–6459.
[58] Johanna Uthoff, Samer Alabed, Andrew J Swift, and Haiping Lu. 2020. Geodesi-
cally Smoothed Tensor Features for Pulmonary Hypertension Prognosis Using
the Heart and Surrounding Tissues. In Proceedings of the International Conference
on Medical Image Computing and Computer-Assisted Intervention . 253–262.
[59] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
Gomez, Ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In
Proceedings of the Advances in Neural Information Processing Systems . 6000–6010.
[60] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local
neural networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition . 7794–7803.
[61] Hao Xu, Shengqi Sang, Peizhen Bai, Laurence Yang, and Haiping Lu. 2020. Grip-
Net: Graph Information Propagation on Supergraph for Heterogeneous Graphs.
arXiv:2010.15914 [cs.LG]
[62] M. Zitnik, Monica Agrawal, and J. Leskovec. 2018. Modeling polypharmacy side
effects with graph convolutional networks. Bioinformatics 34 (2018), i457 – i466.
[63] Marinka Zitnik, Rok Sosič, Sagar Maheshwari, and Jure Leskovec. 2018. BioSNAP
Datasets: Stanford Biomedical Network Dataset Collection.
[64] Hakime Öztürk, Arzucan Özgür, and Elif Ozkirimli. 2018. DeepDTA: deep
drug–target binding affinity prediction. Bioinformatics 34, 17 (2018), i821–i829.

FLRA: A Reference Architecture for
Federated Learning Systems
Sin Kit Lo1;2[0000 0002 9156 3225], Qinghua Lu1;2[0000 0002 7783 5183],
Hye-Young Paik2[0000 0003 4425 7388], and Liming Zhu1;2[0000 0001 5839 3765]
1Data61, CSIRO, Sydney, Australia
2University of New South Wales, Sydney, Australia
Abstract. Federated learning is an emerging machine learning paradigm
that enables multiple devices to train models locally and formulate a
global model, without sharing the clients' local data. A federated learn-
ing system can be viewed as a large-scale distributed system, involving
dierent components and stakeholders with diverse requirements and
constraints. Hence, developing a federated learning system requires both
software system design thinking and machine learning knowledge. Al-
though much eort has been put into federated learning from the machine
learning perspectives, our previous systematic literature review on the
area shows that there is a distinct lack of considerations for software ar-
chitecture design for federated learning. In this paper, we propose FLRA,
a reference architecture for federated learning systems, which provides
a template design for federated learning-based solutions. The proposed
FLRA reference architecture is based on an extensive review of existing
patterns of federated learning systems found in the literature and exist-
ing industrial implementation. The FLRA reference architecture consists
of a pool of architectural patterns that could address the frequently re-
curring design problems in federated learning architectures. The FLRA
reference architecture can serve as a design guideline to assist architects
and developers with practical solutions for their problems, which can be
further customised.
Keywords: Software architecture ·Reference architecture ·Federated
learning ·Pattern ·Software engineering ·Machine learning ·Articial
Intelligence.
1 Introduction
The ever-growing use of industrial-scale IoT platforms and smart devices con-
tribute to the exponential growth in data dimensions [23], which, in turn, em-
powers the research and applications in AI and machine learning. However, the
development of AI and machine learning also signicantly elevates data privacy
concerns, and General Data Protection Regulation (GDPR)3stipulates a range
of data protection measures with which many of these systems must comply.
3https://gdpr-info.eu/arXiv:2106.11570v1  [cs.LG]  22 Jun 2021
2 SK. Lo et al.
Local
model
trainingModel
aggregation
Local
model
trainingLocal
model
training
Task script Local
modelGlobal
modelClientSystem
ownerClient
deviceClient
deviceClient
deviceCentral
Server
Fig. 1: Federated Learning Overview [25].
This is a particular challenge in machine learning systems as the data that is
ready for model training is often insucient and they frequently suer from
\data hungriness issues". As data privacy is now one of the most important eth-
ical principles of machine learning systems [17], there needs to be a solution that
can deliver sucient amount of data for training while the privacy of the data
owners is respected.
To tackle this challenge, Google proposed federated learning [28] in 2016.
Federated learning is a variation of distributed machine learning techniques that
enables model training on a highly distributed client devices network. The key
feature of federated learning is the training of models using the data collected
locally, without transferring the data out of the client devices. A global model
is initialised on a central server and broadcast to the participating client devices
for local training. The locally trained model parameters are then collected by
the central server and aggregated to update global model parameters. The global
model parameters are broadcast again for the next training round. Each local
training round usually takes a step in the gradient descent process. Fig. 1 presents
an overview of the federated learning process.
A federated learning system can be viewed as a large-scale distributed sys-
tem, involving dierent components and stakeholders with diverse requirements
and constraints. Hence, developing a federated learning system requires both
software system design thinking and machine learning knowledge [25]. Further,
despite having various reference architectures for machine learning, big data,
industrial IoT, and edge computing systems, to the best of our knowledge, there
is still no reference architecture for an end-to-end federated learning system.
Based on ndings in several federated learning reviews [24,18], the application
of federated learning is still limited and immature, with only certain stages of
an end-to-end federated learning architecture are extensively studied, leaving
FLRA: A Reference Architecture for Federated Learning Systems 3
many unlled gaps for architecture and pipeline development. In contrast, many
reusable solutions and components were proposed to solve the dierent chal-
lenges of federated learning systems and this motivates the design of a general
federated learning system reference architecture. Therefore, this paper presents
a pattern-oriented reference architecture that serves as an architecture design
guideline and to facilitate the end-to-end development and operations of feder-
ated learning systems, while taking dierent quality attributes and constraints
into considerations. This work provides the following contributions:
{A pattern-oriented federated learning reference architecture named FLRA,
generated from the ndings of a systematic literature review (SLR) and min-
ing of industrial best practices on machine learning system implementations.
{A pool of patterns associated with the dierent components of the FLRA
reference architecture that target to address the recurring design problems
in federated learning architectures.
The structure of the paper is as follows. Section 2 introduces the methodology
for the reference architecture design, followed by the presentation of the reference
architecture in Section 3. Section 4 presents the related work. Section 5 presents
the discussions of this work and nally concludes this paper.
2 Methodology
We have employed parts of an empirically-grounded design methodology [11] to
design the federated learning reference architecture. Firstly, the design and de-
velopment of this reference architecture are based on empirical evidence collected
through our systematic literature review on 231 federated learning academic lit-
erature from a software engineering perspective [24]. The review is conducted
based on Kitchenham's guideline [19] with which we designed a comprehensive
protocol for the review's initial paper search, paper screening, quality assess-
ments, data extractions, analyses, and synthesis. We have also adopted the soft-
ware development practices of machine learning systems in [35] to describe the
software development lifecycle (SDLC) for federated learning. Using the stages
of this lifeycycle as a guide, we formulated our research questions as: (1) Back-
ground understanding; (2) Requirement analysis; (3) Architecture design; and
(4) Implementation & evaluation. One major nding of the SLR is that federated
learning research and applications are still highly immature, and certain stages
of an end-to-end federated learning architecture still lack extensive studies [24].
However, we have also identied many solutions and components proposed to
solve the dierent challenges of federated learning systems, which can be reused
and adapted. This motivates the design of a federated learning system reference
architecture.
Based on the ndings, we specically adopted the qualitative methods in
empirical studies of software architecture [34] to develop and conrm the theory
for the reference architecture design. The proposition is generated based on syn-
theses and validations of the dierent recurring customers and business needs of
4 SK. Lo et al.
Requirements
& patterns
Architectural
components Existing ML
pipelines &
architecturesReference
architectureExploration
and
analysisMining
MiningProven
conceptsV isionQualitative
content
analysis
Cross
case
analysisCross
checking
SupportEmpirical
evidenceConfirmation
of theoryGeneration
of theory FL simulation
frameworks
FL SLR
Body of
empirical
evidence
Fig. 2: Methodology for federated learning reference architecture design.
the federated learning systems, in addition to the collections and analyses of the
reusable patterns to address these architectural needs. We then conducted stud-
ies on some of the best practices in centralised and distributed machine learning
systems to cover some of the components that are not covered in the federated
learning studies. The main processes are the: (1) generation of theory and (2)
conrmation of theory . The architecture design methodology is illustrated in
Fig. 2.
2.1 Generation of theory
The generation of the initial design of the reference architecture theory is per-
formed in this stage. Since there is no standard reference architecture for feder-
ated learning yet, we generated the theory by referring to the architecture of a
machine learning system. Here, we adopted cross-case analysis [34] as the the-
ory generation method, which is an analysis method that compares two dierent
cases based on some attributes and examines their similarities and dierences.
We performed a cross-case analysis on the pipeline design of conventional ma-
chine learning and federated learning systems. Here, we reviewed several machine
learning architectures proposed by well-known companies, such as Google4, Mi-
crosoft5, and Amazon6, specically on their machine learning pipeline designs.
Furthermore, based on our previous project implementation experience, we de-
ned a general federated learning pipeline based on the standards proposed by
these industry players that covers job creation, data collection, data preprocessing
4https://cloud.google.com/architecture/mlops-continuous-delivery-and-au
tomation-pipelines-in-machine-learning
5https://docs.microsoft.com/en-us/azure/machine-learning/concept-model-
management-and-deployment
6https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.
html
FLRA: A Reference Architecture for Federated Learning Systems 5
(cleaning, labeling, augmentation, etc.), model training, model evaluation, model
deployment , and model monitoring stage. Since federated learning was rst in-
troduced by Google, the pipeline components analysis and mining are performed
heavily on the federated learning standards proposed by Google researchers
in [28,5,18], and the frameworks for federated learning system benchmark and
simulation, such as Tensorow Federated (TFF)7, LEAF8, and FedML9. From
the ndings, we were able to conclude that the data collection is fairly similar
whereas data preprocessing, model training, model evaluation, model deployment ,
and model monitoring stages for federated learning systems are dierent from
machine learning pipelines. Especially for the model training stage, the feder-
ated learning pipelines encapsulate model broadcast, local model training, model
upload and collection , and model aggregation operation under this single stage.
Furthermore, the iterative interaction between multiple client devices with one
central server is the key design consideration of the federated learning architec-
ture, and therefore, most academic work extensively studied the model training
stage and proposed many solutions which can be adapted as reusable compo-
nents or patterns to address dierent requirements.
Besides observing the pipeline design and the components, we performed
qualitative content analyses on existing machine learning and federated learn-
ing systems proposed by industrial practitioners and academics to extract re-
quirements, reusable patterns, and components for the design of the reference
architecture. In the SLR, a series of system quality attributes are dened based
on ISO/IEC 25010 System and Software Quality model10and ISO/IEC 2501211
Data Quality model to record the dierent challenges of a federated learning
system addressed by researchers. The empirical evidence associated with each
quality attribute and business need is analysed and validated as the support
for the design proposition of the reference architecture. After the generation of
theory for the hypothesis of the reference architecture, we designed the reference
architecture according to the theory.
2.2 Conrmation of theory
In this stage, we conrmed and veried the viability and applicability of the
reference architecture proposed. Since this reference architecture is built from
scratch based on the patterns and requirements collected through qualitative
analyses, we evaluated the architecture by building a convincing body of evi-
dence to support the reference architecture, which is dierent from conventional
evaluation approaches. We employed the qualitative validation method known
astriangulation [34]. The basic idea is to gather dierent types of evidence to
support a proposition. The evidence might come from dierent sources, be col-
lected using dierent methods, and in our case, the evidence is from the SLR
7https://www.tensorflow.org/federated
8https://github.com/TalwalkarLab/leaf
9https://fedml.ai/
10https://iso25000.com/index.php/en/iso-25000-standards/iso-25010
11https://iso25000.com/index.php/en/iso-25000-standards/iso-25012
6 SK. Lo et al.
and the industrial implementations of machine learning systems from renowned
institutions and companies, and our previous implementation experience.
We have reviewed these evidence based on the SLDC lifecycle of machine
learning systems we developed for the SLR to identify the adoptions of the dif-
ferent reusable patterns or components, in addition to the basic machine learning
pipeline components that are mentioned in these evidence. These mentions and
adoptions are collected to prove applicability of the instantiated components in
the federated learning reference architecture. In short, the triangulation process
justied that the reference architecture is applicable as it is supported by various
empirical evidence we collected and analysed.
Data collectorClient selector
Asynchronous
aggregator
Global model
evaluatorModel deployer
Deployment
selectorModel
aggregatorModel trainer
Data 
Pre-processorClient device Central server
Model
replacement
triggerModel monitorDatabase of
Local Data
ComponentSecure
aggregator
Pattern Data
repository
LegendsIncentive
registry
Message
compressorModels co-
versioning
registryHierarchical
aggregatorHeterogeneous
data handlerMulti-task
model
trainer
Decentralised
aggregator
SensorDecision-makerClient clusterJob creator
Client registryDatabase of
global models
Local model
evaluatorUsersNew
dataDatabase of
local data
Model
performance
feedback
UsersModel/
device
userComponents
interactionExternal
response
Fig. 3: FLRA: a reference architecture of federated learning systems.
3 FLRA Reference Architecture
In this section, we present FLRA, a pattern-oriented reference architecture for
federated learning systems. Fig. 3 illustrates the overall reference architecture. A
base version of a federated learning system consists of two main participants: (1)
central server and (2) client devices. A central server initialises a machine learn-
ing job and coordinates the federated training process, whereas client devices
perform model training using local data and computation resources.
FLRA: A Reference Architecture for Federated Learning Systems 7
Underneath the two participants, there are two types of components: (1)
Mandatory components and (2) optional components. The mandatory com-
ponents provide the basic functions required by a federated machine learning
pipeline. To fulll the dierent software quality requirements and design con-
straints in federated learning systems, we collected and dened a set of patterns
based on our SLR results and the mining of some existing federated learning
simulation frameworks. Each pattern is embedded as optional components to
facilitate the architecture design.
We summarised all the mandatory and optional components of the refer-
ence architecture and briey highlighted the functionalities and responsibility of
each component in Table 1. The table presents the details of each component
associated with the federated learning pipeline stages.
3.1 Job creation
The federated learning process starts with the creation of a model training job
(including initial model and training congurations) via job creator on the
central server. Within the job creator component, three optional components
could be considered are: client registry ,client cluster ,client selector . In a
federated learning system, client devices may be owned by dierent parties, con-
stantly connect and drop out from the system. Hence, it is challenging to keep
track of all the participating client devices including dropout devices and dis-
honest devices. This is dierent from distributed or centralised machine learning
systems in which both clients and the server are typically owned and managed by
a single party [27]. A client registry is required to maintain all the information
of the client devices that are registered, (e.g., ID, resource information, num-
ber of participating rounds, local model performance, etc.) Both IBM Federated
Learning Framework12and doc.ai13adopted client registry in their design to im-
prove maintainability and reliability of the system since the system can manage
the devices eectively and quickly identify the problematic ones via the client
registry component. FedML which is a federated learning benchmarking and
simulation framework has also explicitly covered the client manager module in
their framework that serves the same purpose as the client registry . However,
the system may sacrice client data privacy due to the recording of the device
information on the central server.
The non-IID14data characteristics of local raw data and the data-sharing re-
striction translates to model performance challenge [18,28,42,21]. When the data
from client devices are non-IID, the global models aggregated is less generalised
to the entire data. To improve the generalisation of the global model and speed
up model convergence, a client cluster component can be added to cluster the
12https://github.com/IBM/federated-learning-lib
13https://doc.ai/
14Non-Identically and Independently Distribution: Highly-skewed and personalised
data distribution that vary heavily between dierent clients and aects the model
performance and generalisation [33].
8 SK. Lo et al.
Table 1: Components of the federated learning reference architecture
Stages Types Components Responsibility
Job creationMandatory Job creator Initialises training job and global model
OptionalClient registry Improves system's maintainability andre-
liability by maintaining client's information
Client cluster Tackles statistical heterogeneity &sys-
tem heterogeneity by grouping clients with
similar data distribution or resources before
aggregation
Client selector Improves model & system's performance
by selecting high performance client devices
Data
collection
&
preprocessingMandatoryData collector Collects raw data through sensors or smart
devices deployed
Data preprocessor Preprocesses raw data
Optional Heterogeneous
Data HandlerTackles statistical heterogeneity through
data augmentation methods
Model
trainingMandatoryModel trainer Trains local model
Local model
evaluatorEvaluates local model performance after each
local training round
Model aggregator Aggregates local models to produce new
global model
OptionalMulti-task
model trainerImproves model performance (personalisa-
tion) by adopting multi-task training meth-
ods
Message
compressorImproves communication eciency
through message size reduction to reduce
bandwidth consumption
Secure aggregator Improves data privacy &system security
through dierent secure multiparty computa-
tion protocols
Asynchronous
aggregatorImproves system performance by reducing
aggregation pending time of late client up-
dates
Decentralised
aggregatorImproves system reliability through the re-
moval of single-point-of-failure
Hierarchical
aggregatorImproves system performance & tackle
statistical heterogeneity &system het-
erogeneity by aggregating models from sim-
ilar clients before global aggregation
Model co-versioning
registryImproves system's accountability by
recording the local models associated to each
global models to track clients' performances
Model
deploymentMandatoryModel deployer Deploys completely-trained-models
Decision maker Decides model deployment
OptionalDeployment
selectorImproves model performance (personalisa-
tion) through suitable model users selection
according to data or applications
Incentive registry Increases clients' motivatability
Model
monitoringMandatory Model monitor Monitors model's data inference performance
Optional Model replacement
triggerMaintains system & model performance
by replacing outdated models due to perfor-
mance degrades
FLRA: A Reference Architecture for Federated Learning Systems 9
client devices into groups according to their data distribution, gradient loss, and
feature similarities. This design has been used in Google's IFCA algorithm15,
TiFL system[8], and Massachusetts General Hospital's patient system[13]. The
side eect of client cluster is the extra computation cost caused by client
relationship quantication.
The central servers interacts with a massive number of client devices that
are both system heterogeneous and statistically heterogeneous. The magnitude
of client devices number is also several times larger than that of the distributed
machine learning systems [18,24]. To increase the model and system performance,
client devices can be selected every round with predened criteria (e.g., resource,
data, or performance) via client selector component. This has been integrated
into Google's FedAvg [28] algorithm and IBM's Helios [39].
3.2 Data collection & preprocessing
Each client device gathers data using dierent sensors through the data col-
lector component and process the data (i.e., feature extraction, data cleaning,
labeling, augmentation, etc.) locally through the data preprocessor compo-
nent, due to the data-sharing constraint. This is dierent from centralised or
distributed machine learning systems in which the non-IID data characteristic
is negligible since the data collected on client devices are usually shued and
processed on the central server. Thus, within the data preprocessor , an op-
tional component heterogeneous data handler is adopted to deal with the
non-IID and skewed data distribution issue through data augmentation tech-
niques. The known uses of the component include Astraea16, FAug scheme [14]
and Federated Distillation (FD) method [2].
3.3 Model training
Local model training. Once the client receives the job from the central server,
themodel trainer component performs model training based on congured hy-
perparameters (number of epochs, learning rate, etc.). In the standard federated
learning training process proposed by McMahan in [28], only model parameters
(i.e., weight/gradient) are mentioned to be sent from the central server, whereas
in this reference architecture, the models include not only the model parame-
ters but also the hyperparameters. For multi-task machine learning scenarios, a
multi-task model trainer component can be chosen to train task-related mod-
els to improve model performance and learning eciency. Multitask Learning is
a machine learning approach to transfer and share knowledge through training
of individual models. It improves model generalisation by using the domain in-
formation contained in the parameters of related tasks as an inductive bias. It
does this by learning tasks in parallel while using a shared representation; what
is learned for each task can help other tasks be learned better [7]. In federated
15https://github.com/felisat/clustered-federated-learning
16https://github.com/mtang724/Self-Balancing-Federated-Learning
10 SK. Lo et al.
learning scenarios, this technique is particularly relevant when faced with non-
IID data which can produce personalised model that may outperform the best
possible shared global model [18]. This best practice solution is identied based
on Google's MultiModel17architecture, and Microsoft's MT-DNN18.
Model evaluation. Thelocal model evaluator component measures the
performance of the local model and uploads the model to the model aggrega-
toron the central server if the performance requirement is met. In distributed
machine learning systems, the performance evaluation on client devices is not
conducted locally, and only the aggregated server model is evaluated. However,
for federated learning systems, local model performance evaluation is required
for system operations such as client selection, model co-versioning, contributions
calculation, incentive provision, client clustering, etc.
Model uploading. The trained local model parameters or gradients are up-
loaded to the central server for model aggregation. Unlike centralised machine
learning systems that performs model training in a central server or distributed
machine learning systems that deals with fairly small amount of client nodes,
the cost for transmitting model parameters or gradients between the bandwidth-
limited client devices and central server is high when the system scales up [18,24].
Amessage compressor component can be added to improve communica-
tion eciency. The embedded pattern are extracted from Google Sketched Up-
date [20], and IBM PruneFL [16].
Model aggregation. Themodel aggregator formulates the new global model
based on the received local models. There are four types of aggregator-related
optional components within the model aggregator component: secure ag-
gregator ,asynchronous aggregator ,decentralised aggregator , and hier-
archical aggregator . Asecure aggregator component prevents adversarial
parties from accessing the models during model exchanges through multiparty
computation protocols, such as dierential privacy or cryptographic techniques.
These techniques provide security proof to guarantee that each party knows only
its input and output. For centralised and distributed machine learning settings
that practice centralised system orchestration, communication security between
clients and server is not the main concern. In contrast, for federated learning
settings, this best practices are used in SecAgg [6], HybridAlpha [38], and Ten-
sorFlow Privacy Library19.Asynchronous aggregator is identied from ASO-
fed [9], AFSGD-VP [12], and FedAsync [37]. The asynchronous aggregator
component enables the global model aggregation to be conducted asynchronously
whenever a local model update arrives. Similar technique have been adopted in
17https://ai.googleblog.com/2017/06/multimodel-multi-task-machine-learni
ng.html
18https://github.com/microsoft/MT-DNN
19https://github.com/tensorflow/privacy/
FLRA: A Reference Architecture for Federated Learning Systems 11
distributed machine learning approaches such as iHadoop [10] and it is proven
that this can eectively reduce the overall training time. The conventional de-
sign of a federated learning system that relies on a central server to orchestrate
the learning process might lead to a single point of failure. A decentralise ag-
gregator performs model exchanges and aggregation in decentralised manner
to improve system reliability. The known uses of decentralised aggregator
include BrainTorrent [31] and FedPGA [15]. Blockchain can be employed as
a decentralised solution for federated learning systems. In distributed machine
learning systems, p2p network topology is employed to in MapReduce [27] to
resolve the single-point-of-failure threat on parameter servers. A hierarchical
aggregator component can be selected to improve system eciency by adding
an intermediate edge layer to aggregate the model updates from related client
devices partially before performing the nal global aggregation. This pattern has
been adopted by HierFAVG [22], Astraea, and HFL [1].
In addition to aggregator-related optional components, a model co-versioning
registry component can be embedded within the model aggregator compo-
nent to map all the local models and their corresponding global models. This
enables the model provernance and improves system accountability. The model
co-versioning registry pattern is summarised and adopted from the version
control methods in DVC20, Replicate.ai21, and Pachyderm22.
3.4 Model deployment
After the aggregation, the global model evaluator assesses the performance of
the global model. One example is TensorFlow Extended (TFX)23that provides
a model validator function to assess the federated learning model performance.
If the global model performs well, the model deployer component deploys the
global model to the client device for decision-making through the decision-
maker component. For instance, TensorFlow lite24prepares the nal validated
model for deployment to the client devices for data inference. Within the model
deployer component, there are two optional components for selection: deploy-
ment selector andincentive registry . The deployment selector compo-
nent examines the client devices and selects clients to receive the global model
based on their data characteristics or applications. The deployment selector
pattern has been applied in Azure Machine Learning25, Amazon SageMaker26,
and Google Cloud27to improve model performance. The incentive registry com-
20https://dvc.org
21https://replicate.ai
22https://www.pachyderm.com
23https://www.tensorflow.org/tfx
24https://www.tensorflow.org/lite
25https://docs.microsoft.com/en-us/azure/machine-learning/concept-model-
management-and-deployment
26https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.
html
27https://cloud.google.com/ai-platform/prediction/docs/deploying-models
12 SK. Lo et al.
ponent maintains all the client devices' incentives based on their contributions
and agreed rates to motivate clients to contribute to the training. Blockchain
has been leveraged in FLChain [3] and DeepChain [36] to build a incentive
registry .
3.5 Model monitoring
After the deployment of models for the actual data inference, a model monitor
keeps track of the model performance continuously. If the performance degrades
below a predened threshold value, the model replacement trigger compo-
nent noties the model trainer for local ne-tuning or sends an alert to the job
creator for a new model generation. The model replacement trigger pattern
is identied based on the known uses including Microsoft Azure Machine Learn-
ing Designer28, Amazon SageMaker29, Alibaba Machine Learning Platform30.
4 Related Work
The most widely mentioned denition of a reference architecture is dened by
Bass et al. [4] as \a reference model mapped onto software elements (that cooper-
atively implement the functionality dened in the reference model) and the data
ow between them. Whereas a reference model divides the functionality, a refer-
ence architecture is the mapping of that functionality onto a system decomposi-
tion." Nakagawa et al. collected a series of denitions of reference architectures
by various researchers and summarised them as follows: \the reference architec-
ture encompasses the knowledge about how to design system architectures of a
given application domain. It must address the business rules, architectural styles
(sometimes also dened as architectural patterns that address quality attributes
in the reference architecture), best practices of software development (architec-
tural decisions, domain constraints, legislation, and standards), and the software
elements that support the development of systems for that domain [29]."
Reference architectures for machine learning applications and big data analy-
sis were researched comprehensively. For instance, P a akk onen and Pakkala pro-
posed a reference architecture of big data systems for machine learning in an
edge computing environment [30]. IBM AI Infrastructure Reference Architecture
is proposed to be used as a reference by data scientists and IT professionals who
are dening, deploying, and integrating AI solutions into an organization [26].
Reference architectures for edge computing systems are also widely studied.
For example, H2020 FAR-Edge-project, Edge Computing Reference Architecture
2.0, Intel-SAP Reference Architecture, IBM Edge computing reference architec-
ture, and Industrial Internet Reference Architecture (IIRA) are proposed by
practitioners to support the development of multi-tenant edge systems.
28https://azure.microsoft.com/en-au/services/machine-learning/designer
29https://aws.amazon.com/sagemaker
30https://www.alibabacloud.com/product/machine-learning
FLRA: A Reference Architecture for Federated Learning Systems 13
There are existing works proposed to support federated learning system and
architecture design. For instance, Google was the earliest to introduce a sys-
tem design approach for federated learning [5]. A scalable production system for
federated learning in the domain of mobile devices, based on TensorFlow de-
scribed from a high-level perspective. A collection of architectural patterns for
the design of federated learning systems are summarised and presented by [25].
There are also many architectures and adoptions of federated learning systems
proposed by researchers for diverse applications. For instance, Zhang et al. [40]
proposed a blockchain-based federated learning architecture for industrial IoT to
improve client motivatability through an incentive mechanism. Samarakoon et
al. [32] have adopted federated learning to improve reliability and communica-
tion latency for vehicle-to-vehicle networks. Another real-world federated learn-
ing adoption by Zhang et al. [41] is a dynamic fusion-based federated learning
approach for medical diagnostic image analysis to detect COVID-19 infections.
We observed that there have been multiple studies on federated learning from
dierent aspects and their design methods are highly diverse and isolated which
makes their proposals challenging to be reproduced.
Motivated by the previous works mentioned above, we intend to ll the re-
search gap by putting forward an end-to-end reference architecture for federated
learning systems development and deployment which has been distinctly lacking
in the current state-of-the-art.
5 Discussion & Conclusion
A reference architecture can be served as a standard guideline for system design-
ers and developers for quick selection of best practice solutions for their problems,
which can be further customised as required. To the best of our knowledge, there
is still no reference architecture proposed for an end-to-end federated learning
system while many reusable components and patterns have been proposed. Thus,
in this paper, we proposed FLRA, a pattern-oriented reference architecture for
federated learning system design to increase the real-world adoption of federated
learning.
To design the reference architecture, we developed an empirically-grounded
qualitative analysis method as the basis of design theory generation. The em-
pirical evidence to support the reference architecture design is a collection of
ndings (requirements, patterns, and components) gathered and dened by our
previous systematic literature review on federated learning and well-known in-
dustry practices of machine learning systems.
After developing the reference architecture, we compared it with existing ma-
chine learning architectures of Google, Amazon, Microsoft, and IBM to examine
its applicability. The key dierences between centralised or distributed machine
learning with federated learning are the non-IIDness of training data, varia-
tion in the data partitioning (e.g., vertical, horizontal, and transfer federated
learning) and device partitioning (e.g., cross-device, cross-silo), the ownership
and security requirements of dierent client devices, the system heterogeneity,
14 SK. Lo et al.
and the participation of client nodes. The proposed FLRA architecture adopted
many reusable machine learning and federated learning patterns while maintain-
ing most of the mandatory machine learning pipeline components. This ensures
that the reference architecture is generalised to support the basic model training
tasks in the real world.
While there are dierent constraints when developing a federated learning
system for dierent applications and settings, the possible trade-os and the
pattern solutions to these challenges are discussed comprehensively. The conr-
mation of theory justied the applicability of FLRA and the patterns associated
with the support of empirical evidence collected. Hence, the FLRA proposed
is applicable in the real world for a general, end-to-end development of feder-
ated learning systems. Our future work will focus on developing an architecture
decision model for federated learning system design. We will also work on the
architecture design for trust in federated learning systems.
References
1. Abad, M.S.H., Ozfatura, E., GUndUz, D., Ercetin, O.: Hierarchical federated learn-
ing across heterogeneous cellular networks. In: ICASSP 2020 - 2020 IEEE Inter-
national Conference on Acoustics, Speech and Signal Processing (ICASSP). pp.
8866{8870 (2020)
2. Ahn, J., Simeone, O., Kang, J.: Wireless federated distillation for distributed edge
learning with heterogeneous data. In: 2019 IEEE 30th Annual International Sym-
posium on Personal, Indoor and Mobile Radio Communications (PIMRC). pp. 1{6
(2019)
3. Bao, X., Su, C., Xiong, Y., Huang, W., Hu, Y.: Flchain: A blockchain for auditable
federated learning with trust and incentive. In: 2019 5th International Conference
on Big Data Computing and Communications (BIGCOM). pp. 151{159 (2019)
4. Bass, L., Clements, P., Kazman, R.: Software architecture in practice. Addison-
Wesley Professional (2003)
5. Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V.,
Kiddon, C., Kone cn y, J., Mazzocchi, S., McMahan, H.B., et al.: Towards federated
learning at scale: System design. arXiv preprint arXiv:1902.01046 (2019)
6. Bonawitz, K., Ivanov, V., Kreuter, B., Marcedone, A., McMahan, H.B., Patel, S.,
Ramage, D., Segal, A., Seth, K.: Practical secure aggregation for privacy-preserving
machine learning. Association for Computing Machinery, New York, NY, USA
(2017)
7. Caruana, R.: Multitask Learning, pp. 95{133. Springer US, Boston, MA (1998)
8. Chai, Z., Ali, A., Zawad, S., Truex, S., Anwar, A., Baracaldo, N., Zhou, Y., Lud-
wig, H., Yan, F., Cheng, Y.: Ti: A tier-based federated learning system. In: Pro-
ceedings of the 29th International Symposium on High-Performance Parallel and
Distributed Computing. p. 125{136. HPDC '20, Association for Computing Ma-
chinery, New York, NY, USA (2020)
9. Chen, Y., Ning, Y., Slawski, M., Rangwala, H.: Asynchronous online federated
learning for edge devices with non-iid data. In: 2020 IEEE International Conference
on Big Data (Big Data). pp. 15{24. IEEE (2020)
10. Elnikety, E., Elsayed, T., Ramadan, H.E.: ihadoop: Asynchronous iterations for
mapreduce. In: 2011 IEEE Third International Conference on Cloud Computing
Technology and Science. pp. 81{90 (2011)
FLRA: A Reference Architecture for Federated Learning Systems 15
11. Galster, M., Avgeriou, P.: Empirically-grounded reference architectures: A pro-
posal. In: Proceedings of the Joint ACM SIGSOFT Conference { QoSA and ACM
SIGSOFT Symposium { ISARCS on Quality of Software Architectures { QoSA
and Architecting Critical Systems { ISARCS. p. 153{158. QoSA-ISARCS '11, As-
sociation for Computing Machinery, New York, NY, USA (2011)
12. Gu, B., Xu, A., Huo, Z., Deng, C., Huang, H.: Privacy-preserving asynchronous fed-
erated learning algorithms for multi-party vertically collaborative learning. arXiv
preprint arXiv:2008.06233 (2020)
13. Huang, L., Shea, A.L., Qian, H., Masurkar, A., Deng, H., Liu, D.: Patient clustering
improves eciency of federated machine learning to predict mortality and hospi-
tal stay time using distributed electronic medical records. Journal of Biomedical
Informatics 99, 103291 (2019)
14. Jeong, E., Oh, S., Kim, H., Park, J., Bennis, M., Kim, S.L.: Communication-
ecient on-device machine learning: Federated distillation and augmentation under
non-iid private data. arXiv preprint arXiv:1811.11479 (2018)
15. Jiang, J., Hu, L.: Decentralised federated learning with adaptive partial gradient
aggregation. CAAI Transactions on Intelligence Technology 5(3), 230{236 (2020)
16. Jiang, Y., Wang, S., Valls, V., Ko, B.J., Lee, W.H., Leung, K.K., Tassiulas, L.:
Model pruning enables ecient federated learning on edge devices. arXiv preprint
arXiv:1909.12326 (2019)
17. Jobin, A., Ienca, M., Vayena, E.: The global landscape of AI ethics guidelines.
Nature Machine Intelligence 1(9), 389{399 (2019)
18. Kairouz, P., McMahan, H.B., Avent, B., Bellet, A., Bennis, M., Bhagoji, A.N.,
Bonawitz, K., Charles, Z., Cormode, G., Cummings, R., et al.: Advances and open
problems in federated learning. arXiv preprint arXiv:1912.04977 (2019)
19. Kitchenham, B., Brereton, O.P., Budgen, D., Turner, M., Bailey, J., Linkman,
S.: Systematic literature reviews in software engineering{a systematic literature
review. Information and software technology 51(1), 7{15 (2009)
20. Kone cn y, J., McMahan, H.B., Yu, F.X., Richt arik, P., Suresh, A.T., Bacon, D.: Fed-
erated learning: Strategies for improving communication eciency. arXiv preprint
arXiv:1610.05492 (2016)
21. Li, X., Huang, K., Yang, W., Wang, S., Zhang, Z.: On the convergence of fedavg
on non-iid data. arXiv preprint arXiv:1907.02189 (2019)
22. Liu, L., Zhang, J., Song, S.H., Letaief, K.B.: Client-edge-cloud hierarchical feder-
ated learning. In: ICC 2020 - 2020 IEEE International Conference on Communi-
cations (ICC). pp. 1{6 (2020)
23. Lo, S.K., Liew, C.S., Tey, K.S., Mekhilef, S.: An interoperable component-based
architecture for data-driven iot system. Sensors 19(20) (2019)
24. Lo, S.K., Lu, Q., Wang, C., Paik, H.Y., Zhu, L.: A systematic literature review
on federated machine learning: From a software engineering perspective. ACM
Comput. Surv. 54(5) (May 2021)
25. Lo, S.K., Lu, Q., Zhu, L., Paik, H.Y., Xu, X., Wang, C.: Architectural patterns for
the design of federated learning systems. arXiv preprint arXiv:2101.02373 (2021)
26. Lui, K., Karmiol, J.: AI Infrastructure Reference Architecture. IBM Systems
(2018), https://www.ibm.com/downloads/cas/W1JQBNJV
27. Marozzo, F., Talia, D., Truno, P.: P2p-mapreduce: Parallel data processing in
dynamic cloud environments. Journal of Computer and System Sciences 78(5),
1382{1402 (2012), jCSS Special Issue: Cloud Computing 2011
28. McMahan, B., Moore, E., Ramage, D., Hampson, S., y Arcas, B.A.:
Communication-ecient learning of deep networks from decentralized data. In:
Articial Intelligence and Statistics. pp. 1273{1282. PMLR (2017)
16 SK. Lo et al.
29. Nakagawa, E.Y., Oliveira Antonino, P., Becker, M.: Reference architecture and
product line architecture: A subtle but critical dierence. In: Crnkovic, I., Gruhn,
V., Book, M. (eds.) Software Architecture. pp. 207{211. Springer Berlin Heidelberg,
Berlin, Heidelberg (2011)
30. P a akk onen, P., Pakkala, D.: Extending reference architecture of big data systems
towards machine learning in edge computing environments. Journal of Big Data
7, 1{29 (2020)
31. Roy, A.G., Siddiqui, S., P olsterl, S., Navab, N., Wachinger, C.: Braintorrent:
A peer-to-peer environment for decentralized federated learning. arXiv preprint
arXiv:1905.06731 (2019)
32. Samarakoon, S., Bennis, M., Saad, W., Debbah, M.: Federated learning for ultra-
reliable low-latency v2v communications. In: 2018 IEEE Global Communications
Conference (GLOBECOM). pp. 1{7 (2018)
33. Sattler, F., Wiedemann, S., M uller, K.R., Samek, W.: Robust and communication-
ecient federated learning from non-i.i.d. data. IEEE Transactions on Neural Net-
works and Learning Systems 31(9), 3400{3413 (2020)
34. Seaman, C.: Qualitative methods in empirical studies of software engineering. IEEE
Transactions on Software Engineering 25(4), 557{572 (1999)
35. Wan, Z., Xia, X., Lo, D., Murphy, G.C.: How does machine learning change soft-
ware development practices? IEEE Transactions on Software Engineering pp. 1{1
(2019)
36. Weng, J., Weng, J., Zhang, J., Li, M., Zhang, Y., Luo, W.: Deepchain: Auditable
and privacy-preserving deep learning with blockchain-based incentive. IEEE Trans-
actions on Dependable and Secure Computing pp. 1{1 (2019)
37. Xie, C., Koyejo, S., Gupta, I.: Asynchronous federated optimization. arXiv preprint
arXiv:1903.03934 (2019)
38. Xu, R., Baracaldo, N., Zhou, Y., Anwar, A., Ludwig, H.: Hybridalpha: An e-
cient approach for privacy-preserving federated learning. In: Proceedings of the
12th ACM Workshop on Articial Intelligence and Security. p. 13{23. AISec'19,
Association for Computing Machinery, New York, NY, USA (2019)
39. Xu, Z., Yu, F., Xiong, J., Chen, X.: Helios: Heterogeneity-aware federated learning
with dynamically balanced collaboration. arXiv preprint arXiv:1912.01684 (2019)
40. Zhang, W., Lu, Q., Yu, Q., Li, Z., Liu, Y., Lo, S.K., Chen, S., Xu, X., Zhu, L.:
Blockchain-based federated learning for device failure detection in industrial iot.
IEEE Internet of Things Journal 8(7), 5926{5937 (2021)
41. Zhang, W., Zhou, T., Lu, Q., Wang, X., Zhu, C., Sun, H., Wang, Z., Lo, S.K.,
Wang, F.Y.: Dynamic fusion-based federated learning for covid-19 detection. IEEE
Internet of Things Journal pp. 1{1 (2021)
42. Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., Chandra, V.: Federated learning
with non-iid data. arXiv preprint arXiv:1806.00582 (2018)

Robustness Analysis of Deep Learning
Frameworks on Mobile Platforms
Amin Eslami Abyane and Hadi Hemmati
Department of Electrical and Software Engineering, University of Calgary, Canada
famin.eslamiabyane, hadi.hemmati g@ucalgary.ca
Abstract. With the recent increase in the computational power of mod-
ern mobile devices, machine learning-based heavy tasks such as face de-
tection and speech recognition are now integral parts of such devices.
This requires frameworks to execute machine learning models (e.g., Deep
Neural Networks) on mobile devices. Although there exist studies on the
accuracy and performance of these frameworks, the quality of on-device
deep learning frameworks, in terms of their robustness, has not been
systematically studied yet. In this paper, we empirically compare two
on-device deep learning frameworks with three adversarial attacks on
three dierent model architectures. We also use both the quantized and
unquantized variants for each architecture.The results show that, in gen-
eral, neither of the deep learning frameworks is better than the other
in terms of robustness, and there is not a signicant dierence between
the PC and mobile frameworks either. However, in cases like Boundary
attack, mobile version is more robust than PC. In addition, quantization
improves robustness in all cases when moving from PC to mobile.
Keywords: Robustness ·On-device learning ·Deep learning frameworks.
1 Introduction
In recent years, advancements in hardware resources and demands from many
application domains have led to the growth and success of deep learning (DL)
approaches. Consequently, several deep learning frameworks, such as TensorFlow
[14] and PyTorch [29], have been introduced to improve DL developers produc-
tivity. These powerful frameworks have gained massive success and popularity
in academia and industry and are being used on a large scale every day. Among
the many application domains of DL systems, one particular domain that has
seen much interest is applying DL techniques on mobile devices (i.e., on-device
learning). In general, mobile devices' collected or observed data are potentially
of great interest for many DL applications such as speech recognition, face detec-
tion, and next-word prediction. There are two generic solutions to utilize these
data. The rst approach is to send the data from the mobile devices to a server
to run the DL task (training or testing) and return the results. This approach
has some signicant drawbacks. The rst one is that in this communication with
the server, the user's privacy might be threatened, and the second one is thatarXiv:2109.09869v1  [cs.LG]  20 Sep 2021
2 A. Eslami Abyane, H. Hemmati
we are adding a network overhead and delay to the system. This overhead might
get very noticeable if the task is frequent, such as image classication using a
camera to process lots of images in a second.
The second approach is called on-device machine learning, which does not
have the privacy concerns and is the context of this paper. In an on-device
machine learning approach, the inference is made on the user's device, and no
network communication is required. However, the mobile device's computational
power (no matter how powerful the mobile device is) is much less than a server
or even a regular GPU-based PC, which is an obstacle for training in this fash-
ion. To help DL models inference possible on a mobile device, libraries such as
TensorFlow Lite[9] and PyTorch Mobile[2] have been proposed.
From the software engineering perspective, an important quality aspect of
DL-based software systems is their robustness, which is usually tested and an-
alyzed against adversarial attacks [16], [33], [18]. Robustness is especially sig-
nicant in mobile apps given the amount of personal information that can be
exploited from cell phones, if an adversary gets access to the app. For instance,
DL-based face detection is a standard access control measure for cell phones
these days. Suppose an adversarial attack on the underlying DL model can mis-
classify a specic image created by the adversary as the trusted class. In that
case, the attacker gets access to the mobile device.
There have been some limited studies in the recent literature that evaluate
DL frameworks both on PC and mobile [24], [20], [27], but only in terms of their
accuracy (eectiveness) and performance (eciency). This has motivated us to
conduct this study with a software testing and analysis lens, on mobile and PC
DL frameworks in terms of their robustness.
We study two main on-device DL frameworks, TensorFlow Lite and PyTorch
Mobile, from Google and Facebook, respectively. We use image classication as
a common DL task to evaluate the robustness of the frameworks. Our controlled
experiment is designed to study the eect of the models, the adversarial attacks,
the quantization process [25], and the framework on robustness. We compare
two deep learning frameworks (TensorFlow and PyTorch) with three adversarial
attacks (both white-box and black-box) on three dierent model architectures,
both quantized and unquantized. This results in 36 congurations on mobile
devices and 18 congurations on PC, as our comparison baseline.
The results show that neither of the mobile deep learning frameworks is
better than the other in terms of robustness, and the robustness depends on the
model type and other factors, which is the case on PC as well. Moreover, there
is no signicant dierence in robustness between PC and mobile frameworks
either. However, cases like the Boundary attack on PyTorch, we see that the
mobile version is signicantly more robust than the PC version (12.5% decrease
in attack success rate). Finally, we see that quantization improves the robustness
of both TensorFlow and PyTorch on all models and attacks when moving from
PC to mobile (with median improvements between 2.4% to 23.8%, per attack
type). Note that all data and scripts are available for replication purposes1.
1https://github.com/aminesi/robustness-on-device
Robustness Analysis of Deep Learning Frameworks on Mobile Platforms 3
2 Background
2.1 Deep Neural Network (DNN)
A DNN is an articial neural network consisting of many layers, and each layer
has multiple neurons. Each neuron performs a simple task, takes an input, and
gives an output based on a function. A simple combination of these layers is often
called multi-layer perceptron (MLP). However, DNNs are not limited to MLPs.
One of the most popular kinds of neural networks is called Convolutional Neural
Networks (CNNs). A convolutional layer is typically used in tasks that work with
images. A convolutional layer's objective is to extract features from a picture and
reduce the problem's dimensionality. Another group of neural networks is called
Recurrent Neural Networks (RNNs). This type of network has units that act as
memory. Thus they are often used in tasks that deal with language and speech.
Like a human being, DNN learns patterns in the training phase and can be used
for the designed task. DNNs are extremely powerful and are widely used for
image classication, face recognition, and speech recognition.
In the image classication domain, which is a common application domain of
DNNs and is the domain of our experiment, some very well-known models have
proven to be very eective:
MobileNetV2 [31]: This model is specially designed for mobile devices and
is more light-weight than the other models. It contains 32 convolutional lters
and 19 layers of residual bottleneck. The overall size of this model is around 14
MB. This model takes images of size 224 ×224 as input for classication.
ResNet50 [21]: This is one of the most inuential models in the image clas-
sication domain. It is much heavier than MobileNetV2 (it is close to 100 MB),
but it is more accurate than MobileNetV2.
InceptionV3 [34]: Much like ResNet50, this model is another complex model
(close to 100 MB). It consists of inception blocks; Unlike the other two models,
which used 224 ×224 images, this one takes images of size 299 ×299, which is one
reason it is more complex than the previous ones.
2.2 Robustness and Adversarial Attacks
DNNs are known to be sensitive to corner case data (e.g., data items that are
close to decision boundaries). That means it is possible that a slight change in
the input can result in a corner case sample where the DNN will not perform
accurately (e.g., the item is misclassied if the task is classication). Suppose
this slight change of input is deliberate to fool the model. In that case, it is called
an adversarial attack, and the robustness of a DNN model is the extent that the
model can defend itself from such attacks (i.e., still generate correct outputs).
Adversarial attacks were rst introduced in the image processing tasks, where
images are easily manipulable, and the tasks (e.g., classication) are pretty sen-
sitive. However, these attacks have gone beyond the image domain and are now
being studied in other learning tasks, such as text and audio analysis domains.
4 A. Eslami Abyane, H. Hemmati
Adversarial attacks can be categorized from several perspectives. Most com-
monly, they are categorized into two groups, based on their level of access to the
model details, which are white-box and black-box attacks. White-box attacks
require knowledge about the internals of the models they are attacking. For in-
stance, some attacks need the models' gradient after the backpropagation step
to generate adversarial samples, whereas black-box attacks do not need such in-
formation and are model-agnostic. Another standard categorization of attacks is
grouping them into targeted and untargeted attacks. Targeted attacks try to fool
the model into misclassifying data into a particular class. In contrast, untargeted
attacks just try to force the model to misclassify, no matter the wrong output.
Since most of the popular and well-known attacks are dened as untargeted [19],
[26], [28], in this study we focus on the following untargeted attacks both from
white-box and black-box categories.
Fast Gradient Sign Method (FGSM) [19] is perhaps the most famous
adversarial attack amongst these attacks. FGSM is a gradient-based white-box
attack, and it works by adding perturbations into the image following the formula
presented in equation 1. Where x and y are the input image and label respec-
tively,represents model parameters, ris gradient, J is the loss function, and
is the amount of perturbation.
adv=x+:sign (r(J(;x;y ))) (1)
Basic Iterative Method (BIM) [26] is an extension of FGSM attack, so
it is also a white-box attack. As its name suggests, it is iterative, and it does the
FGSM attack multiple times and updates the input in each iteration.
Boundary Attack [15] is a decision-based (it only uses the nal decision of
model to create samples) black-box attack. It starts from an adversarial point,
and in each step makes a move randomly orthogonal to the image and a move
towards the image. Since it uses a small step size to get closest to the image
(while staying on the adversarial side), it requires many iterations.
2.3 DL Frameworks
Neural networks use complex mathematical equations that need to be imple-
mented in Libraries like TensorFlow and PyTorch that provide a set of simple
Application Programming Interfaces (APIs) for the machine learning developers.
Both these frameworks are implemented in python language for their high-level
APIs, and they use C++ for their low-level implementation to gain higher speeds.
With the advances in on-device learning, these frameworks are now coming
with mobile versions, making programming DL on mobile devices easier.
TensorFlow's mobile variant called \TensorFlow Lite" is a cross-platform
(Android, iOS, and edge devices) library and supports languages such as Java
and Swift, which is based on a cross-platform serialization library called Flat-
Buers. In addition, it supports multiple quantization congurations and dier-
ent hardware such as CPU and GPU with various options, and on Android, it
supports Android Neural Networks API (NNAPI).
Robustness Analysis of Deep Learning Frameworks on Mobile Platforms 5
\PyTorch Mobile" is the other on-device DL library, which is similar to Ten-
sorFlow Lite in terms of functionalities. It works on the same platforms that
TensorFlow Lite does and supports quantization but is less exible in this as-
pect. At the time of doing this experiment, PyTorch Mobile only supports CPU
without any additional options (e.g., thread count).
However, like any other software program, these implementations are not
awless. There are also quite many dierent design choices and implementa-
tion dierences between various frameworks. Especially, given the hardware and
operating systems dierences, there might be many variations in terms of eec-
tiveness, eciency, and robustness of the same model implemented on PC vs.
mobile in dierent frameworks.
2.4 Quantization
Quantization is the process of compressing a DNN implementation to speed
up the model execution, at the cost of its precision [8]. As we know, DNN
models' implementations in DL frameworks include many matrices/tensors and
thus many matrix/tensor operations. The motivation behind quantization is to
reduce the complexity of matrix operations to be able to run more operations
with fewer resources [35]. Typically, all DNN model parameters, like weights and
activations, use a 32-bit oating-point precision. Since mobile devices have fewer
resources than PCs, the quantization idea has been proposed to slightly reduce
the model's precision to make models smaller and infer faster. The quantization
target may be an 8-bit integer, or 16-bit oating-point, or any other precision
for the numerical data types.
Integer quantization is explored in [25]. In this work, they introduce a formula
for quantization to 8-bit integer, which is shown in equation 2. Key parameters
here are zero point and scale, which should be selected in a way that every
possible normal value can be mapped to an 8-bit xed-point value.
normalvalue = (int8value zeropoint )scale (2)
Although TensorFlow currently supports multiple precisions, PyTorch only
supports an 8-bit integer at the moment. There are dierent ways of quantiza-
tion, and we will briey describe them in this section.
Post-training dynamic range quantization: This type of quantization
[5], as the name suggests, is done after the model is trained. A quantizer quantizes
all statically dened parameters, like weights using a formula like equation 2.
So in this approach, the activations are intact. According to [5], this approach
works best for language models. The name dynamic is used since the activation
values are quantized on the y when the model is running.
Post-training static range quantization: This quantization approach [5],
like the previous one, is applied on a trained model. The dierence is that activa-
tions are pre quantized as well, making the model more compact and faster. To
nd the best quantization, the quantizer should nd the best scale and zero point
6 A. Eslami Abyane, H. Hemmati
to ensure successful mapping to the target (e.g., int8), which is not possible for
something that is not statically dened like activations.
The solution is to calibrate the model with some image samples in the quanti-
zation step so that quantizer can see various possible dynamic values (activation
values in this case) and calculate appropriate scale and zero points for them.
This approach is more appropriate for image models compared to the previous
one as suggested by the literature [5].
Quantization aware training: This approach [5] is distinct from the other
two in that it tries to learn the eect of quantization in the training phase. It is
a more robust approach but costs more.
3 Experiments
Our objective is to quantitatively and systematically evaluate the robustness of
DL learning frameworks on mobile. To address this objective, we answer the
following research questions:
{ RQ1: How robust DL frameworks are on mobile? This RQ aims to
compare TensorFlow Lite and PyTorch Mobile when running on mobile by
assessing their robustness against well-known adversarial attacks.
{ RQ2: How does mobile DL frameworks' robustness compare to
their PC equivalent? In this RQ, we compare the robustness results of
DL frameworks on PC vs. mobile platforms.
{ RQ3: What is the eect of quantization on the robustness of mod-
els? In this RQ, we will study the quantization's eect by repeating the
experiment designed for RQ1, but this time with the quantized models.
3.1 Experiment Design
Models under study and the datasets: Image classication is one of the
main application domains of DL models these days. Especially in the mobile
application domain, some use cases such as access control, as discussed earlier,
are critical and can be heavily dependant on the trustworthiness of the under-
lying classication models. Therefore, in this study, we focus on image classi-
cation and use the three well-known image classication models explained in
Section 2.1: MobileNetV2, ResNet50, and InceptionV3. These models have been
selected to cover various models in terms of power and complexity (MobileNetV2:
light-weight, ResNet50: resource-demanding, and InceptionV3: the heaviest).
For TensorFlow models, we use pre-trained models on ImageNet from Keras
applications [6]. PyTorch models are all from torchvision [11] models packages
(for the sake of quantization, models are selected from quantization packages as
regular models are not quantizable).
For quantization, we use the second approach explained in Section 2.4, post-
training static range quantization, which is a decent t for image data and still
not very costly. For calibration of the models in the quantization mode, we use
1,000 random samples from our dataset.
Robustness Analysis of Deep Learning Frameworks on Mobile Platforms 7
(a) Original
 (b) FGSM
 (c) BIM
 (d) Boundary
Fig. 1. A sample generated for TensorFlow MobileNetV2 model.
We use ImageNet [3] as our main dataset. ImageNet is one of the biggest and
most well-known datasets in the image domain, and it uses WordNet [13] for
its label hierarchy. It consists of 1,281,167 training images and 50,000 validation
images with a total of 1,000 image classes. It is around 150 gigabytes in size.
Since our robustness analysis heavily depends on the ground truth for clas-
sication, we need to make sure that the original test set samples are correctly
classied. By doing this, we ensure that if an adversarial sample is misclassied,
it is due to perturbations and not intrinsic model errors.
To achieve this goal, we use the intersection of correctly classied validation
samples by all the models on all of our frameworks. Then we choose 3,000 samples
from these, randomly, to ensure we have enough unbiased samples.
Model deployment and inference: The procedure to deploy and test our
models on mobile frameworks is as follows:
1. Create a trained model on PC (by creating a new model or using a pre-
trained model or ne-tune a pre-trained models with transfer learning)
2. Convert the models into their mobile variant, using TensorFlow Lite and
PyTorch Mobile (optionally quantize the model (only in RQ2)).
3. Load the model and samples into memory and run the inference on mobile
to calculate the robustness.
Recall that this procedure is divided between PC (model training) and mobile
(model inference), since mobile devices alone are not powerful enough to do heavy
tasks such as training an extremely resource-intensive neural network.
Adversarial Attacks: As discussed in Section 2.2, we use the following three
famous untargeted attacks in this study: FGSM, BIM, and Boundary attack. We
utilize a python package for creating adversarial attacks called Foolbox [30]. For
FGSM and BIM attacks, we use an of 0.005. For BIM attack, we choose ten
iterations which is the default value in Foolbox. For Boundary attack, both
orthogonal and towards steps are set to 0.01 (which are again the default values
in Foolbox), and the number of steps is set to 5000. A small number of steps
results in a very perturbed image, and a huge one is highly time-consuming and
does not always result in a better sample.
To generate adversarial samples, we follow these steps that produces visually
acceptable images (see Fig.1):
1. Preprocess our carefully selected samples according to the model require-
ments, as follows: First Resize the image's smallest dimension to 256 (299 in
8 A. Eslami Abyane, H. Hemmati
Table 1. Input image normalization in preprocessing. (values are according to image
channels (B, G, R) for ResNet50 in TF and (R, G, B) for other congurations)
Framework model mean ( ) std ( )
TensorFlowMobileNetV2 (127.5, 127.5, 127.5) (127.5, 127.5, 127.5)
ResNet50 (103.939, 116.779, 123.68) (1, 1, 1)
InceptionV3 (127.5, 127.5, 127.5) (127.5, 127.5, 127.5)
PyTorchMobileNetV2 (0.485, 0.456, 0.406) (0.229, 0.224, 0.225)
ResNet50 (0.485, 0.456, 0.406) (0.229, 0.224, 0.225)
InceptionV3 (0.485, 0.456, 0.406) (0.229, 0.224, 0.225)
case of InceptionV3) (This is the choice of both TensorFlow, and PyTorch
[12], [4]). Then center crop the resized image according to input size of the
model. Afterwards for PyTorch, divide image pixel values by 255 to map the
values to [0, 1] range (TensorFlow uses (0, 255) range). For ResNet50 only
in TensorFlow, change image format from RGB to BGR. Finally normalize
the image channels based on the mean and standard deviation, in Table 1,
which is from TensorFlow and PyTorch documentations [7], [12].
2. Pass the model and samples to an attacker to generate adversarial samples.
3. Convert adversarial samples to PNG images to use in mobile devices.
Note that we do not generate the adversarial samples on mobile for two
reasons: (a) the computation intensity of this task, and (b) models on mobile do
not provide essential information for white-box attacks. Another point is that
we convert images to PNG format, which is a lossless format. This format is
crucial since we want our image to be the same as one on PC to be able to
get reliable results. Also, note that for the Boundary attack, we may need to
rerun the algorithm several times until all adversarial samples are successfully
generated (in our case, we had to rerun three times).
Evaluation metrics: To evaluate robustness, we use success rate, which
measures the proportion of samples that could successfully fool the models.
Since our originally selected test set samples are all classied correctly on all
congurations on PC, the success rate of an attack has an inverse correlation
with the model's robustness.
In RQ2, to better assess the dierences between results, when comparing
success rates of quantized vs unquantized models, we run a non-parametric sta-
tistical signicant test (Wilcoxon signed-rank) and report the eect size measure
(Vargha and Delaney A Measure), as well.
Execution environment We run our PC DL frameworks on a node from
the Compute Canada cluster with 32 gigabytes of RAM, an Nvidia V100 GPU,
and an Intel Gold 6148 Skylake @ 2.4 GHz CPU. We use a physical device
for our mobile device, an HTC U11 with a Qualcomm Snapdragon 835 chipset
and 6 gigabytes of RAM, running Android 9. We did not use multiple devices
since model robustness is independent of device type, given that implementations
are using the same library. It is worth noting that we do not use an emulator
Robustness Analysis of Deep Learning Frameworks on Mobile Platforms 9
as our mobile system. We started the project by testing TensorFlow with an
emulator, but surprisingly, we found that quantized models ran slower than
regular models in the emulator. It turned out that TensorFlow's quantized kernel
is only optimized for mobile CPU. Thus quantized versions ran poorly on an
emulator. Therefore, we used a real device, as discussed before. We developed a
prototype mobile app (on both frameworks) that takes conguration and images
as input and calculates the success rate .
3.2 Results and Discussions
In this section, we present and discuss the results of RQ1 to RQ3.
RQ1 results (TensorFlow Lite vs. PyTorch Mobile robustness on
Mobile platforms): Fig.2 reports the results for this RQ. The rst observa-
tion is that in MobileNetV2 and ResNet50 models, TensorFlow Lite was more
robust against FGSM and BIM attacks (Fig.2a and Fig.2b). However, in the
Boundary attack, PyTorch Mobile was more robust. Also, Fig.2c shows that, for
InceptionV3, PyTorch Mobile is more robust against FGSM and BIM attacks.
However, TensorFlow Lite is more robustness against the Boundary attack.
Thus, in mobile DL frameworks, the robustness depends on congurations,
and no learning framework among TensorFlow Lite and PyTorch Mobile domi-
nates the other one in terms of robustness.
It can also be seen that the more complex the model gets, the more robust it
will be against adversarial samples. For instance, In TensorFlow Lite, the success
rate of the FGSM attack on MobileNetV2, ResNet50, and InceptionV3 is 77%,
74.27%, and 55.8%, respectively.
Furthermore, both FGSM and BIM attacks are much more eective than the
Boundary attack. In addition, BIM attack always performs better than FGSM
as it tries to improve an FGSM sample iteratively.
Moreover, we can see that Boundary attack is less successful than FGSM
and BIM in all cases with a very low success rate. This seems contradictory
with the denition of Boundary attack, which was supposed to generate samples
always on the adversarial side. In other words, the success rate should have
always been 100%. The reason for lower success rates is that images are in 8-
bit unsigned integer (UINT8) format, but the neural networks work with 32-bit
oating-points (FP32). Therefore, when a sample is generated, it is in FP32
format and is always adversarial. However, the reduction of precision to UINT8,
in conversion to the image format, makes some samples cross the boundary, and
the success rate signicantly drops.
Besides the attack success rate, another important factor is performance. Ta-
ble 2 shows the inference time for the mobile platform when running unquantized
models. As the table shows in terms of performance (run-time cost), TensorFlow
Lite is much faster in all congurations (the slower framework is highlighted in
the table) for regular models. This might be because PyTorch Mobile uses fewer
threads as the number of workers cannot be set on PyTorch.
Answer to RQ1: Neither PyTorch Mobile nor TensorFlow Lite is signi-
cantly more robust than the other, in all cases. The choice of a more robustness
10 A. Eslami Abyane, H. Hemmati
(a) MobileNetV2
 (b) ResNet50
 (c) InceptionV3
Fig. 2. Success rate of adversarial attacks on mobile device
Table 2. Inference time in the mobile device for the entire test set (3,000 samples).
The bold cells represent the framework with faster inferences.
Inference time (s)
Model Attack TensorFlow Lite PyTorch Mobile
MobileNetV2FGSM 157 276
BIM 174 301
Boundary 187 302
ResNet50FGSM 1003 1583
BIM 1036 1504
Boundary 1093 1460
InceptionV3FGSM 1617 1774
BIM 1626 1841
Boundary 1516 1820
mobile framework depends on the model architecture and the attack itself. In
terms of performance, however, TensorFlow Lite is consistently faster!
RQ2 results (TensorFlow and PyTorch robustness on PC vs. Mo-
bile): To answer this RQ, we start by analyzing robustness on PC DL frame-
works as our baseline. Fig. 3 report the success rates of three adversarial attacks
(FGSM, BIM, and Boundary) on three models (MobileNetV2, ReNet50, and
InceptionV3) over two PC frameworks (TensorFlow and PyTorch).
As it can be seen in Fig.3a and Fig.3b on MobileNetV2 and ResNet50 archi-
tectures TensorFlow is more robust against white-box attacks, while on Bound-
ary attack which is black-box, PyTorch is more robust. However, in InceptionV3
(Fig.3c), it is the exact opposite, and TensorFlow is more robust against the
black-box attack. Also, PyTorch is more robust against white-box attacks. These
patterns were seen in our experiments on the mobile device (RQ1), as well.
Table 3 reports adversarial sample generation cost on PC. As it can be seen,
the Boundary attack takes a signicantly longer time to nish, with a much lower
success rate in the end. While white-box attacks nish in minutes, the Boundary
attack takes a couple of hours to complete, even in a high-end system, such as
Compute Canada cluster. This perfectly illustrates why it is almost impossible to
create samples using black-box techniques on mobile devices. Moreover, white-
Robustness Analysis of Deep Learning Frameworks on Mobile Platforms 11
(a) MobileNetV2
 (b) ResNet50
 (c) InceptionV3
Fig. 3. Success rate of adversarial attacks on PC
Table 3. Adversarial generation time for the entire test sets (3,000 samples) reported
in hours:minutes:seconds.
Generation time (s)
Model Framework FGSM BIM Boundary
MobileNetV2TensorFlow 00:02:20 00:04:13 01:01:46
PyTorch 00:03:35 00:04:16 01:26:04
ResNet50TensorFlow 00:02:01 00:03:38 04:31:17
PyTorch 00:02:30 00:05:37 04:56:24
InceptionV3TensorFlow 00:03:06 00:03:45 05:54:14
PyTorch 00:01:36 00:05:18 07:25:36
box attacks need gradient, which is unavailable on mobile frameworks at the
moment. Thus they too cannot be run on mobile. Consequently, currently, there
is no easy way to generate adversarial samples on mobile devices. Finally, we
can see that TensorFlow is slightly faster than PyTorch, in most cases.
To better compare mobile and PC platforms, Fig.4 reports the same raw
data as Figures 2 and 3, but grouped by platforms. As Fig.4a, 4b, and 4c show,
robustness is almost the same in all cases except on the Boundary attack on
InceptionV3, where we see a slight increase in robustness on mobile. We also see
similar patterns in Fig.4d, 4e, and 4f, between mobile and PC for PyTorch where
the robustness on mobile is either the same or very close to the PC version in all
congurations. The exception is the Boundary attack on MobileNetV2, where
we see a sudden drop in success rate in PyTorch Mobile. This shows PyTorch's
mobile version of MobileNetV2 is more robust against the Boundary attack.
Answer to RQ2: In most cases, switching platforms between PC and mobile
does not change the robustness drastically. This means that the implementation
of models on both hardware and languages perform similarly, and they are almost
equivalent. However, in some cases, like on PyTorch when using Boundary attack
on MobileNetV2, we might get much higher robustness on mobile platforms.
RQ3 results (the quantization eect): To answer this RQ, we report the
results in Fig.5. The results show that the attacks lose their initial eectiveness
in all cases and their success rate decreases. This shows that quantization can
increase the robustness of the model against attacks. In some cases, robustness
increases slightly, whereas in a case like a Boundary attack (which is trying to
12 A. Eslami Abyane, H. Hemmati
(a) TensorFlow, MobileNetV2
 (b) TensorFlow, ResNet50
 (c) TensorFlow, InceptionV3
(d) PyTorch, MobileNetV2
 (e) PyTorch, ResNet50
 (f) PyTorch, InceptionV3
Fig. 4. Dierent attacks' success rates on dierent platforms.
create samples closest to the boundary), the slightest eect like quantization can
massively improve the robustness. In other words, these attacks are very depen-
dent on the model's specications. If model parameters change (as quantization
does), the attack will not be as eective as it was.
The median decrease of success rates per attack is 3.55% (FGSM), 2.43%
(BIM), and 23.77% (Boundary), with a minimum of 0.47% (for BIM om PyTorch-
ResNet50) and a maximum of 37.5% (For Boundary on PyTorch-InceptionV3).
This dierence between the unquantized and quantized models' robustness is
statistically signicant with a p-value less than 0.001 when running a non-
parametric statistical signicant test (Paired Wilcoxon Signed-Rank test), with
the eect size measure (Paired Vargha and Delaney A Measure) is 0.608.
In terms of model performance, as Table 4 shows, quantization closes the
model inference time gap between the two frameworks, and in some models
like MobileNetV2, PyTorch Mobile even runs faster than TensorFlow Lite. As
expected, the speedup after quantization is signicant, and it goes up to 2.87
times in some cases (e.g., MobileNetV2 in PyTorch).
Answer to RQ3: In addition to the speed and size reduction that quantiza-
tion provides, it can be a very low-cost and straightforward defense mechanism
against adversarial attacks. Quantization increases the robustness with a median
up to 37.5% for some attacks (Boundary) over a distribution of three models and
two DL frameworks per attack.
3.3 Threats to validity
In terms of the construct validity, the key to our success rate measure is knowing
the ground truth. However, we are only relying on the original classication
models to come up with the ground truth. That is, there might be cases where
Robustness Analysis of Deep Learning Frameworks on Mobile Platforms 13
(a) TensorFlow, MobileNetV2
 (b) TensorFlow, ResNet50
 (c) TensorFlow, InceptionV3
(d) PyTorch, MobileNetV2
 (e) PyTorch, ResNet50
 (f) PyTorch, InceptionV3
Fig. 5. Eect of quantization on success rate
the model is misclassifying, but the adversarial sample is correctly classied.
As discussed, we take the intersection of the three models' correctly classied
samples to reduce the probability of having a misclassied ground truth.
In terms of the internal validity, our study design is pretty simple, with evalu-
ating the robustness of models using attacks success rate. We are using predened
classication models and existing libraries to create attacks. Therefore, we are
not introducing confounding factors in our implementation or design.
In terms of conclusion validity and to address the randomness of the re-
sults, we ran a non-parametric paired statistical signicant test (Paired Wilcoxon
Signed-Rank test) and reported the eect size measure (Paired Vargha and De-
laney A Measure) when comparing the success rates in RQ3. In RQ1 and RQ2,
the results were mainly the same when comparing frameworks and platforms.
Therefore, our conclusions were the dierences are practically insignicant any-
ways, so there was no need to run any statistical signicance tests.
Finally, regarding the external validity and generalizability of the study, one
potential validity threat is having limited datasets and models, which results in
biased conclusions. We mitigate this threat by choosing one of the most exten-
sive datasets in the image domain. Furthermore, we used three state-of-the-art
models with dierent complexities to help generalize our results. Finally, we used
both white-box and black-box attacks, and all the attacks were some of the best
in their categories. However, still, all of these datasets and models are from the
domain of the image. Thus the results might not be generalizable to other do-
mains such as natural language processing. In addition, we only used Android as
our mobile platform. Therefore, the results might not be representative of iOS.
14 A. Eslami Abyane, H. Hemmati
Table 4. Eect of quantization on mobile inference time
Inference time (s)
Model Attack Framework Regular Quantized
MobileNetV2FGSMTensorFlow Lite 157 133
PyTorch Mobile 276 96
BIMTensorFlow Lite 174 128
PyTorch Mobile 301 105
BoundaryTensorFlow Lite 187 132
PyTorch Mobile 302 112
ResNet50FGSMTensorFlow Lite 1003 534
PyTorch Mobile 1583 616
BIMTensorFlow Lite 1036 531
PyTorch Mobile 1504 622
BoundaryTensorFlow Lite 1093 559
PyTorch Mobile 1460 554
InceptionV3FGSMTensorFlow Lite 1617 798
PyTorch Mobile 1774 942
BIMTensorFlow Lite 1626 804
PyTorch Mobile 1841 982
BoundaryTensorFlow Lite 1516 808
PyTorch Mobile 1820 993
4 Related Work
Luo et al. [27] made a comparison for classiers between dierent mobile frame-
works like TensorFlow Lite PyTorch Mobile and Cae 2 [1], which is now part of
PyTorch. They used many models such as ResNet50, InceptionV3, DenseNet121
[22], and compared all the models on all the mentioned frameworks. They also
compared the neural inference power of dierent mobile devices. Some of the
results were as follows: none of the platforms had a noticeable advantage in all
cases, TensorFlow Lite had a much faster model loading time compared to the
others, the same AI model on the dierent platform had dierent accuracy, and
Android Neural Networks API (NNAPI) did not constantly improve the infer-
ence time. This study was mainly focused on accuracy and performance but did
not have any robustness assessments, which our study covers.
Ignatov et al. [24] made a benchmark consisting of multiple tests such as
image recognition, face recognition, image deblurring, image semantic segmen-
tation, image enhancement, and memory limitations. Then they compared the
performance of DNN models on dierent mobile phones. This paper's main idea
was to measure the power of the CPU chipset; thus, there was no other compar-
ison in this work. They only used TensorFlow Lite as the DL framework, and
there was no comprehensive study on the impact of dierent models.
Guo et al. [20] presented a study on PyTorch Mobile, TensorFlow Lite and
TensorFlow.js[10], CNTK[32], MXnet[17]. The paper made a comparison on PC
Robustness Analysis of Deep Learning Frameworks on Mobile Platforms 15
and found that PyTorch and MXnet were more vulnerable in adversarial attacks.
It compared browsers using TensorFlow.js with PC for MNIST and CIFAR-10
datasets using dierent models and found that TensorFlow.js suered from high
memory usage and had meaningful lower accuracy ResNet model. Android de-
vices were faster in small models in inference time, whereas IOS devices were
better at large models. It used TensorFlow Lite to compare Android and iOS
devices with PC and found similar accuracy to PC. Finally, they found that
quantization did not aect accuracy much, and it made inference faster on An-
droid devices. Although there was some robustness analysis in this work, the
models were very simple and unrealistic, and evaluation was only on PC.
Huang et al. [23] made some interesting experiments on the robustness of
models on Android devices. They used TensorFlow Lite as the framework for
their study. Their approach had some key points. They extracted TensorFlow
Lite models from the Google Play store. Then based on some criteria, They found
similar pre-trained PC models available online and implemented the attacks on
similar models. Their results showed that their approach was more eective in
fooling the models than blind attacks (attacks without knowing the model).
However, this study was only focused on attacking a specic model on mobile,
and It did not look at dierent frameworks and the eect of platforms.
Given the related work, we see a gap in the literature for assessing the ro-
bustness of mobile DL frameworks, which our study covers.
5 Acknowledgement
This work was enabled in part by support from WestGrid (www.westgrid.ca)
and Compute Canada (www.computecanada.ca) and the Natural Sciences and
Engineering Research Council of Canada [RGPIN/04552-2020].
6 Conclusion and Future works
In this paper, we conduct a comprehensive study on deep learning mobile frame-
works' robustness with dierent congurations. We compare the two major mo-
bile frameworks (TensorFlow Lite and PyTorch Mobile), using 18 congurations
(36 congurations considering quantization): two frameworks, three models, and
three adversarial attack techniques. Our results show that frameworks are not
necessarily superior in terms of robustness on the mobile platform, and the more
robust framework varies by model architecture and attack type. Furthermore,
changing the platform to mobile usually does not aect robustness but in some
cases results in a slight increase in robustness which is not signicant. However,
we also show that quantization is a very eective approach in reducing the cost
of model inference and making it more robust toward attacks in DL frameworks,
consistently improving the robustness of Mobile DL frameworks (even up to
37.5% improvement when compared to regular models). In the future, we plan
to extend this study to other application domains (such as textual data) and
study other frameworks and platforms such as TensorFlow.js and iOS devices.
16 A. Eslami Abyane, H. Hemmati
References
1. Cae2 | a new lightweight, modular, and scalable deep learning framework.
https://cae2.ai/
2. Home | pytorch. https://pytorch.org/mobile/home/
3. Imagenet. http://www.image-net.org/
4. Inception v3 | pytorch. https://pytorch.org/hub/pytorch vision inception v3/
5. Introduction to quantization on pytorch | pytorch.
https://pytorch.org/blog/introduction-to-quantization-on-pytorch/
6. Keras applications. https://keras.io/api/applications/
7. keras-applications/imagenet utils.py at 1.0.8. https://github.com/keras-
team/keras-applications/blob/1.0.8/keras applications/imagenet utils.py
8. Model optimization | tensorow lite.
https://www.tensorow.org/lite/performance/model optimization
9. Tensorow lite | ml for mobile and edge devices. https://www.tensorow.org/lite
10. Tensorow.js | machine learning for javascript developers.
https://www.tensorow.org/js
11. torchvision | pytorch 1.7.0 documentation.
https://pytorch.org/docs/stable/torchvision/index.html
12. torchvision.models | torchvision master documentation.
https://pytorch.org/vision/stable/models.html
13. Wordnet | a lexical database for english. https://wordnet.princeton.edu/
14. Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghe-
mawat, S., Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore,
S., Murray, D.G., Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M.,
Yu, Y., Zheng, X.: Tensorow: A system for large-scale machine learning. In: Pro-
ceedings of the 12th USENIX Conference on Operating Systems Design and Im-
plementation. p. 265{283. OSDI'16, USENIX Association, USA (2016)
15. Brendel, W., Rauber, J., Bethge, M.: Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models (2018)
16. Carlini, N., Wagner, D.: Towards evaluating the robustness of neural networks.
In: 2017 IEEE Symposium on Security and Privacy (SP). pp. 39{57 (2017).
https://doi.org/10.1109/SP.2017.49
17. Chen, T., Li, M., Li, Y., Lin, M., Wang, N., Wang, M., Xiao, T., Xu, B., Zhang,
C., Zhang, Z.: Mxnet: A exible and ecient machine learning library for hetero-
geneous distributed systems (2015)
18. Fawzi, A., Moosavi-Dezfooli, S.M., Frossard, P.: Robustness of classiers: From ad-
versarial to random noise. In: Proceedings of the 30th International Conference on
Neural Information Processing Systems. p. 1632{1640. NIPS'16, Curran Associates
Inc., Red Hook, NY, USA (2016)
19. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. In: Bengio, Y., LeCun, Y. (eds.) 3rd International Conference on Learn-
ing Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference
Track Proceedings (2015), http://arxiv.org/abs/1412.6572
20. Guo, Q., Chen, S., Xie, X., Ma, L., Hu, Q., Liu, H., Liu, Y.,
Zhao, J., Li, X.: An empirical study towards characterizing deep learn-
ing development and deployment across dierent frameworks and plat-
forms. In: 34th IEEE/ACM International Conference on Automated Soft-
ware Engineering, ASE 2019, San Diego, CA, USA, November 11-15,
2019. pp. 810{822. IEEE (2019). https://doi.org/10.1109/ASE.2019.00080,
https://doi.org/10.1109/ASE.2019.00080
Robustness Analysis of Deep Learning Frameworks on Mobile Platforms 17
21. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recog-
nition. In: 2016 IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016. pp. 770{
778. IEEE Computer Society (2016). https://doi.org/10.1109/CVPR.2016.90,
https://doi.org/10.1109/CVPR.2016.90
22. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely connected
convolutional networks (2018)
23. Huang, Y., Hu, H., Chen, C.: Robustness of on-device models: Adversarial attack
to deep learning models on android apps (2021)
24. Ignatov, A., Timofte, R., Chou, W., Wang, K., Wu, M., Hartley, T., Gool, L.V.:
Ai benchmark: Running deep neural networks on android smartphones. In: Leal-
Taix e, L., Roth, S. (eds.) Computer Vision { ECCV 2018 Workshops. pp. 288{314.
Springer International Publishing, Cham (2019)
25. Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A., Adam, H.,
Kalenichenko, D.: Quantization and training of neural networks for ecient integer-
arithmetic-only inference (2017)
26. Kurakin, A., Goodfellow, I., Bengio, S.: Adversarial examples in the physical world
(2017)
27. Luo, C., He, X., Zhan, J., Wang, L., Gao, W., Dai, J.: Comparison and bench-
marking of ai models and frameworks on mobile devices (2020)
28. Moosavi-Dezfooli, S.M., Fawzi, A., Frossard, P.: Deepfool: A simple and ac-
curate method to fool deep neural networks. In: 2016 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR). pp. 2574{2582 (2016).
https://doi.org/10.1109/CVPR.2016.282
29. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic dierentiation in pytorch (2017)
30. Rauber, J., Zimmermann, R., Bethge, M., Brendel, W.: Foolbox native: Fast ad-
versarial attacks to benchmark the robustness of machine learning models in py-
torch, tensorow, and jax. Journal of Open Source Software 5(53), 2607 (2020).
https://doi.org/10.21105/joss.02607
31. Sandler, M., Howard, A.G., Zhu, M., Zhmoginov, A., Chen, L.: Mobilenetv2:
Inverted residuals and linear bottlenecks. In: 2018 IEEE Conference on Com-
puter Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT,
USA, June 18-22, 2018. pp. 4510{4520. IEEE Computer Society (2018).
https://doi.org/10.1109/CVPR.2018.00474
32. Seide, F., Agarwal, A.: CNTK: microsoft's open-source deep-learning toolkit. In:
Krishnapuram, B., Shah, M., Smola, A.J., Aggarwal, C.C., Shen, D., Rastogi,
R. (eds.) Proceedings of the 22nd ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17,
2016. p. 2135. ACM (2016). https://doi.org/10.1145/2939672.2945397
33. Su, J., Vargas, D.V., Sakurai, K.: One pixel attack for fooling deep neural net-
works. IEEE Transactions on Evolutionary Computation 23(5), 828{841 (2019).
https://doi.org/10.1109/TEVC.2019.2890858
34. Szegedy, C., Vanhoucke, V., Ioe, S., Shlens, J., Wojna, Z.: Rethinking
the inception architecture for computer vision. In: 2016 IEEE Conference
on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV,
USA, June 27-30, 2016. pp. 2818{2826. IEEE Computer Society (2016).
https://doi.org/10.1109/CVPR.2016.308
35. Wu, H., Judd, P., Zhang, X., Isaev, M., Micikevicius, P.: Integer quantization for
deep learning inference: Principles and empirical evaluation (2020)

Evaluating Code Readability and Legibility:
An Examination of Human-centric Studies
Delano Oliveira, Reydne Bruno, Fernanda Madeiraly, Fernando Castor
Federal University of Pernambuco, Recife, Brazil, dho@cin.ufpe.br, reydne.bruno@gmail.com, castor@cin.ufpe.br
yKTH Royal Institute of Technology, Stockholm, Sweden, fer.madeiral@gmail.com
Abstract —Reading code is an essential activity in software
maintenance and evolution. Several studies with human subjects
have investigated how different factors, such as the employed
programming constructs and naming conventions, can impact
code readability, i.e., what makes a program easier or harder to
read and apprehend by developers, and code legibility, i.e., what
inﬂuences the ease of identifying elements of a program. These
studies evaluate readability and legibility by means of different
comprehension tasks and response variables. In this paper, we
examine these tasks and variables in studies that compare
programming constructs, coding idioms, naming conventions, and
formatting guidelines, e.g., recursive vs. iterative code. To that
end, we have conducted a systematic literature review where we
found 54 relevant papers. Most of these studies evaluate code
readability and legibility by measuring the correctness of the
subjects’ results (83.3%) or simply asking their opinions (55.6%).
Some studies (16.7%) rely exclusively on the latter variable.
There are still few studies that monitor subjects’ physical signs,
such as brain activation regions (5%). Moreover, our study shows
that some variables are multi-faceted. For instance, correctness
can be measured as the ability to predict the output of a program,
answer questions about its behavior, or recall parts of it. These
results make it clear that different evaluation approaches require
different competencies from subjects, e.g., tracing the program vs.
summarizing its goal vs. memorizing its text. To assist researchers
in the design of new studies and improve our comprehension of
existing ones, we model program comprehension as a learning
activity by adapting a preexisting learning taxonomy. This
adaptation indicates that some competencies, e.g., tracing, are
often exercised in these evaluations whereas others, e.g., relating
similar code snippets, are rarely targeted.
Index Terms —Code readability, code legibility, code under-
standability, code understanding, program comprehension
I. I NTRODUCTION
Understanding a program usually requires reading code. A
program might be easier or harder to read depending on its
readability , i.e., what makes a program easier or harder to
read and apprehend by developers, and legibility , i.e., what
inﬂuences the ease of identifying elements of a program.
Different factors can inﬂuence code readability and legibility,
such as which constructs are employed [1], [2], how the code
is formatted with whitespaces [3]–[5], and identiﬁer naming
conventions [6]–[15]. Researchers have conducted empirical
studies to investigate several of those factors, where different
but functionally equivalent ways of writing code are compared,
e.g., recursive vs. iterative code [16] and abbreviated vs. word
identiﬁer names [15]. These studies involve asking subjects to
perform one or more tasks related to source code and assessing
their understanding or the effort involved in the tasks.There are systematic literature reviews about program com-
prehension studies [17], [18]. To the best of our knowledge,
however, no previous work has investigated how code readabil-
ity and legibility are evaluated in studies comparing different
ways of writing code, in particular, what kinds of tasks these
studies conduct and what response variables they employ. An-
alyzing these tasks and variables can help researchers identify
limitations in the evaluations of previous studies, gauge their
comprehensiveness, and improve our understanding of what
said tasks and variables evaluate.
In this paper, we investigate how code readability and
legibility are evaluated in studies that aim to identify ways
of writing code that are more readable or legible than others.
Our goal is to study what types of tasks are performed by
human subjects in those studies, what cognitive skills are
required from them, and what response variables those studies
employ. To achieve this goal, we carried out a systematic
literature review, which started with 2,843 documents, where
54 papers are relevant for our research—these papers are the
primary studies of our study. Our results show that 40 primary
studies involve asking subjects to provide information about a
program, e.g., to predict its output or the values of its variables,
or to provide a high level description of its functioning. In
addition, 30 studies involve asking subjects to provide personal
opinion, and 15 studies involve asking subjects to act on code.
Most studies employ combinations of these types of tasks.
When considering response variables, the most common
approach is to verify if the subjects are able to correctly
provide information about the code or act on it. Correctness
is the response variable of choice in 45 studies. Moreover,
27 studies measure the time required to perform tasks as an
imprecise proxy to the effort required by these tasks. Besides,
mirroring the aforementioned tasks where subjects are required
to provide their personal opinions, 30 studies employ opinion
as a response variable. Additionally, 9 of the 54 studies use
opinion as the sole response variable. Furthermore, studies
evaluating legibility tend to employ opinion as a response
variable proportionally more often than studies focusing on
readability. Also, there are still relatively few studies that mon-
itor subjects’ physical signs, such as brain activation regions
(5%) [19]–[21]. The analysis of the response variables reveals
that they are multi-faceted. For example, correctness can be
measured as the ability to predict the output of a program,
answer questions about its general behavior, precisely recall
speciﬁc parts of it, among other things.arXiv:2110.00785v1  [cs.SE]  2 Oct 2021
To assist researchers in the design of new studies and
improve our comprehension of existing ones, we adapted
an existing learning taxonomy to the context of program
comprehension. This adapted taxonomy indicates that some
competencies, e.g., tracing, are often exercised in the studies
whereas others, e.g., relating similar code snippets, are rarely
targeted. The taxonomy also indicates that 37% of the primary
studies have a narrow focus, requiring a single cognitive skill
from the subjects, even though program comprehension is a
complex activity. Additionally, the taxonomy highlights the
tendency of the studies evaluating readability and legibility
to focus on comprehension tasks that do not contextualize
the information that is apprehended. In software development
practice, program comprehension is often associated with other
tasks such as reengineering an existing program or extending
it. Existing studies rarely go this far.
II. C ODE READABILITY AND LEGIBILITY
In software engineering, the terms readability, legibility, un-
derstandability, and comprehensibility have overlapping mean-
ings. For example, Buse and Weimer [22] deﬁne “ readability
as a human judgment of how easy a text is to understand ”.
In a similar vein, Almeida et al. [23] afﬁrm that “ legibility is
fundamental to code maintenance; if source code is written
in a complex way, understanding it will require much more
effort ”. In addition, Lin and Wu [24] state that “ “Software
understandability ” determines whether a system can be un-
derstood by other individuals easily, or whether artifacts of
one system can be easily understood by other individuals ”.
Xia et al. [25] treat comprehension and understanding as
synonyms, expressing that “ Program comprehension (aka.,
program understanding, or source code comprehension) is a
process where developers actively acquire knowledge about a
software system by exploring and searching software artifacts,
and reading relevant source code and/or documentation ”.
In linguistics, the concept of text comprehension is similar
to program comprehension in software engineering. Gough
and Tunmer [26] state that “ comprehension (not reading com-
prehension, but rather linguistic comprehension) is the process
by which given lexical (i.e., word) information, sentences and
discourses are interpreted ”. However, Hoover and Gough [27]
further elaborate on that deﬁnition and claim that “ decoding
and linguistic comprehension are separate components of
reading skill ”. This claim highlights the existence of two
separate processes during text comprehension: (i) decoding the
words/symbols and (ii) interpreting them and sentences formed
by them. DuBay [28] separates these two processes and deﬁnes
them as legibility , which concerns typeface, layout, and other
aspects related to the identiﬁcation of elements in text, and
readability , that is, what makes some texts easier to read than
others. In a similar vein, for Tekif [29], legibility studies are
mainly concerned with typographic and layout factors while
readability studies concentrate on the linguistic factors.
These two perspectives also apply to programs. We can ﬁnd
both the visual characteristics and linguistic factors in source
code, although with inconsistent terminology. For example,Daka et al. [30] afﬁrm that “ the visual appearance of code in
general is referred to as its readability ”. The authors clearly
refer to legibility (in the design/linguistics sense) but employ
the term “readability” possibly because it is more often used
in the software engineering literature.
Based on the differences between the terms “readability”
and “legibility” that are well-established in other areas such as
linguistics [28], design [31], human-computer interaction [32],
and education [29], we believe that the two terms should have
clear, distinct, albeit related, meanings also in the area of soft-
ware engineering. On the one hand, the structural and semantic
characteristics of the source code of a program that affect the
ability of developers to understand it while reading the code,
e.g., programming constructs, coding idioms, and meaningful
identiﬁers, impact its readability . On the other hand, the
visual characteristics of the source code of a program, which
affect the ability of developers to identify the elements of the
code while reading it, such as line breaks, spacing, alignment,
indentation, blank lines, identiﬁer capitalization, impact its
legibility . Hereafter, we employ these two terms according
to these informal deﬁnitions.
III. M ETHODOLOGY
Our goal is to investigate how human-centric studies evalu-
ate whether a certain way of writing code is more readable
or legible than another functionally equivalent one. More
speciﬁcally, we investigate what tasks are performed by human
subjects and how their performance is evaluated in empirical
studies aiming to evaluate readability and legibility. We focus
on studies that directly compare two or more different ways of
writing code and have a focus on low-level source code ele-
ments, without accounting for tools, documentation, or higher
level issues (details are further presented in this section). We
address two research questions in this paper:
RQ1 What are the tasks performed by human subjects in
empirical studies?
RQ2 What are the response variables of these studies?
To answer our research questions, we conducted a system-
atic literature review, which was designed following the guide-
lines proposed by Kitchenham et al. [33]. Figure 1 presents
the roadmap of our review including all steps we followed.
First, we performed the selection of studies. We started with
a manual search for papers to further be used as seed papers,
so that a search string could be deﬁned, and automatic search
could be performed on search engines (Section III-A). We
retrieved 2,843 documents with the automatic search, which
passed through 1) a triage for study exclusion (Section III-B),
2) an initial study selection where inclusion criteria were
applied (Section III-C), and 3) a ﬁnal study selection where
we evaluated the quality of the studies based on a number
of criteria (Section III-D). Then, the selected 54 papers were
analyzed (Section III-E). We extracted data from the papers
and synthesized it to answer our research questions. We detail
these steps in the following sections. It is worth mentioning
that we did not leverage systematic review tools, like Parsifal
[34], because we were not aware of them at the time.
A. Search Strategy
Our search strategy is composed of three parts: a manual
search to gather seed studies, the deﬁnition of a generic search
string, and the automatic search in search engines. First, we
performed the manual search for seed studies aiming to ﬁnd
terms for the deﬁnition of a generic search string. For that, we
chose the following top-tier software engineering conferences:
ICSE, FSE, MSR, ASE, ISSTA, OOPSLA, ICSME, ICPC, and
SANER. Then, the ﬁrst two authors of this paper looked at the
papers published in these conferences in the last three years
and a half (from 2016 to June 2019), and selected the ones that
would help us answer our research questions. We also included
one paper from TSE, which we already knew is relevant to our
research. This process resulted in 13 seed papers.
The title and keywords of the seed papers were analyzed,
and then we extracted the general terms related to our research
questions. We chose general terms as a conservative way to
gather as many papers as possible that can ﬁt within the scope
of our study; otherwise, we would delimit our set of studies
based on speciﬁc topics. We used the resulting terms to build
the following search string:
Title (ANY (terms ))OR Keywords (ANY (terms )),
where terms =f“code comprehension”, “code understandability”,
“code understanding”, “code readability”,
“program comprehension”, “program understandability”,
“program understanding”, “program readability”,
“programmer experience” g
We did not include terms with “legibility” in the search
string. Most of the papers with this word in the title or key-
words are related to linguistics or computational linguistics. In
these ﬁelds, researchers use this term with a different meaning
than what would be expected in a software engineering paper.
Using it in our search string would drastically increase the
number of false positives. Also, we did not include terms with
“software”, e.g., “software readability”, because software is
broader than source code and program.
Finally, we performed an automatic search for studies using
our generic search string adapted for three search engines:
ACM Digital Library [35], IEEE Explore [36], and Scopus
[37]. We retrieved 1,926, 729, and 1,909 documents, respec-
tively, in September 2, 2019. Since a given document might
be retrieved from more than one engine, we uniﬁed the output
of the engines to eliminate duplicates, which resulted in 2,843
unique documents. The 13 seed papers were returned by the
automatic search.
B. Triage (Study Exclusion)
The 2,843 documents retrieved with our automatic search
passed through a triage process so that we could discard clearly
irrelevant documents. We ﬁrst deﬁned ﬁve exclusion criteria:
EC1: The study is outside the scope of this study. It is not
primarily related to source code comprehension, readability,
or legibility, does not involve any comparison of different
ways of writing code, neither direct nor indirect, or is
clearly irrelevant to our research questions. For instance,
Search 
string 
54 (49+5) 
studies 523 (518+5) 
studies 
ACM 
IEEE 
Scopus 
Search string 
definition 
Manual 
search 13 seed 
studies 
Triage (study 
exclusion) 
Initial selection 
(study inclusion) 
Final selection 
(study quality 
assessment) 
Data 
extraction and 
synthesis 
Answers to 
research 
questions 
Search 
2,843 unique documents 1,926 
729
1,909 
54 (49+5) studies 
5 newly 
identified studies Fig. 1: Systematic literature review roadmap.
we exclude studies focusing on high-level design, docu-
mentation, and dependencies (higher-level issues).
EC2: The study is not a full paper (e.g., MSc dissertations,
PhD theses, course completion monographs, short papers)
or is not written in English: not considering these types of
documents in systematic reviews is a common practice [33].
As a rule of thumb, we consider that full papers must be at
least 5 pages long.
EC3: The study is about readability metrics without an
experimental evaluation.
EC4: The study is about program comprehension aids, such
as visualizations or other forms of analysis or sensory aids
(e.g., graphs, trace-based execution, code summarization,
speciﬁcation mining, reverse engineering).
EC5: The study focuses on accessibility, e.g., targets indi-
viduals with visual impairments or neurodiverse developers.
Each of the 2,843 documents was analyzed by an author
of this paper, who checked the title and abstract of the
document, and in some cases the methodology, against the
exclusion criteria. The documents that do not meet any of the
exclusion criteria were directly accepted to enter the next step
(described in the next section). The documents that meet at
least one exclusion criterion passed through a second round
in the triage process, where each document was analyzed by
a different author. In the end of the two rounds, we discarded
all documents that were annotated with at least one exclusion
criterion in both rounds. We followed this two-round process
to mitigate the threat of discarding potentially relevant studies
in the triage. Because only rejected papers in the ﬁrst round
were analyzed in the second round, i.e., raters in the second
round knew that the documents had been marked for exclusion,
no inter-rater agreement analysis was conducted. We ended up
with 523 documents.
C. Initial Selection (Study Inclusion)
After discarding clearly irrelevant documents in the triage
step, we applied the following inclusion criteria in the 523
papers to build our initial set of papers:
IC1 (Scope): The study must be primarily related to the
topics of code comprehension, readability, legibility, or
hard-to-understand code.
IC2 (Methodology): The study must be or contain at least
one empirical study, such as controlled experiment, quasi-
experiment, case study, or survey involving human subjects .
IC3 (Comparison): The study must compare alternative
programming constructs, coding idioms, or coding styles
in terms of code readability or legibility .
IC4 (Granularity): The study must target ﬁne-grained pro-
gram elements and low-level/limited-scope programming
activities. Not design or comments, but implementation.
The application of the inclusion criteria to a paper often
requires reading not only the title and abstract as in the
triage step, but also sections of introduction, methodology,
and conclusion. If a given paper violates at least one inclusion
criterion, the paper is annotated with “not acceptable”. When
there are doubts about the inclusion of a paper, the paper
is annotated with “maybe” for further discussion. We also
performed this step in two rounds, but differently from the
triage, all papers in this step were independently analyzed
by two different authors. We calculated the Kappa coefﬁcient
[38] for assessing the agreement between the two analyzes.
We found k= 0:323, which is considered a fair agreement
strength [33]. In the end of this step, the papers annotated with
“acceptable” in both rounds were directly selected, and papers
annotated with “not acceptable” in both rounds were rejected.
All the other cases were discussed by the four authors in live
sessions to reach consensus. We ended up with 54 papers.
D. Study Quality Assessment
After the search, exclusion, and inclusion of papers, the
remaining 54 papers passed through a ﬁnal selection step,
aiming to assess their quality. In this step, we aim to identify
low-quality papers for removal. To do so, we elaborated nine
questions that were answered for each paper. We adapted
these questions from the work of Keele [39]. There were
three questions about study design, e.g., “are the aims clearly
stated?”; four questions about analysis, e.g., “are the data
collection methods adequately described?”; and two questions
about rigor, e.g., “do the researchers explain the threats to the
study validity?”. There are three possible answers for each
question: yes (1), partially (0.5), and no (0). The sum of
the answers for a given paper is its score. The maximum is,
therefore, 9. If a paper scores 0.5 or better in all questions,
its overall score is 4.5 or more. Thus, we deﬁned that a paper
should score at least 4.5 to be kept in our list of papers.
Each paper was assessed by one of the authors. At the begin-
ning of this step, each author selected one paper, performed the
quality assessment, and justiﬁed to the other authors the given
score for each question in a live discussion. This procedure
allows us to align our understanding of the questions and
avoid misleading assessments. The scores of the studies were:
min = 5,median = 8,max = 9. Since the minimum score
for keeping a paper is 4.5 and no paper scored less than 5, no
studies were removed because of bad quality.
Inclusion of additional studies. Five relevant studies were not
captured by our automatic search because our search string did
not cover them, and they were unknown for us when we built
the set of seed papers. We found some of these studies when
we were reading already included studies in the ﬁnal selectionstep—those studies were cited as “related works” to the ones
included in our list. The ﬁve studies were discussed by all
the authors in a live session and, after reaching agreement
about their pertinence, they were subjected to exclusion and
inclusion criteria and quality evaluation.
Deprecated studies. We identiﬁed some papers that we refer
to as deprecated . A paper is deprecated if it was extended
by another paper that we selected. For instance, the work of
Buse and Weimer published in 2008 [40] was extended in a
subsequent paper [22]. In this case, we consider the former to
be deprecated and only take the latter into account.
E. Data Analysis
In this step of the study, we analyzed a total of 54 papers.
Initially, we read all the papers in full and extracted from each
one the data necessary to answer our research questions. This
activity was carried out through a questionnaire that had to be
ﬁlled in for each paper. For this extraction, the papers were
divided equally among the researchers, and periodic meetings
were held to discuss the extracted data. At this stage we seek
to extract information about the characteristics of the studies,
for example, whether they pertain to readability or legibility,
the evaluation method, the tasks the subjects were required to
perform, and information about the results.
After extracting the data, we analyzed it so as to address
the two research questions. For the ﬁrst research question,
we collected all the tasks extracted from the 54 studies. The
ﬁrst two authors examined these tasks together, identifying
commonalities and grouping them together. All these tasks
were then subject to discussion among all the authors. After we
reached a consolidated list of tasks performed by the subjects
of these studies, we organized them in three large groups,
considering what is required from the subjects: (i) to provide
information about the code; (ii) to act on the code; and (iii)
to provide a personal opinion about the code.
For the second research question, we adopted a similar pro-
cedure. Initially, we collected the extracted response variables.
Since they were very diverse, we created groups that include
multiple response variables with similar characteristics. For
example, response variables related to correctness include the
predicted output of a program, a recalled part of it, or a
general description of its function. At the end, we elicited
ﬁve categories of response variables in the studies. The initial
analysis of the data was conducted by the ﬁrst two authors
and later the results were reﬁned with the collaboration of all
the authors.
Data availability. Raw data, such as the list of the 2,843 docu-
ments returned by our automatic search, and additional detail
about our methodology, such as the questionnaires we used
for study quality assessment and data extraction, are available
at https://github.com/reydne/code-comprehension-review.
IV. R ESULTS
In this section we attempt to answer our two research
questions, based on the obtained data.
A. Tasks Performed by Human Subjects (RQ1)
The essential code comprehension task is code reading.
By construction, all the selected studies have at least one
reading task where the subject is required to read a code
snippet, a set of snippets, or even large, complete programs.
Since all the studies compare two or more ways of writing
code, the subjects often have multiple code reading tasks. In
addition, the subjects are also expected to comprehend the
code. However, there are different ways to measure subject
comprehension performance. Subjects are asked to provide
information about the code, act on the code, or provide
personal opinion. In most studies, more than one kind of task
was employed. Table I summarizes the identiﬁed tasks.
A large portion of the primary studies, 40 out of 54, required
subjects to provide information about the code . For example,
Benander et al. [16] asked the subjects to explain using
free-form text what a code snippet does, right after having
read it. Blinman et al. [7] asked the subjects to choose the
best description for a code snippet among multiple options.
Explaining what the code does is not an objective method
to evaluate comprehension because someone has to judge the
answer. We found 18 studies that employed this task.
The subjects of 27 studies were asked to answer questions
about characteristics of the code. In some studies, the subjects
were asked to predict the behavior of a source code just by
looking at it. For example, Gopstein et al. [1] and Ajami and
Feitelson [2] presented subjects with multiple code snippets
and asked them to guess the outputs of these snippets. Dolado
et al. [41] asked the subjects to answer a set of questions about
expression results, ﬁnal values of variables, and how many
times loops were executed. In other studies, the subjects were
asked questions about higher-level issues related to source
code. For example, Scalabrino et al. [42] asked the subjects
if they recognize an element of a speciﬁc domain or about
the purpose of using an external component in the snippet
(e.g., JDBC APIs). Similarly, Binkley et al. [9] inquired the
subjects about the kind of application or the industry where a
speciﬁc line of code might be found. In addition, some studies
required subjects to localize code elements of interest. For
example, Binkley et al. [43] asked subjects to ﬁnd identiﬁers
in a snippet, marking each code line that has that identiﬁer.
Ceccato et al. [10] asked the subjects to pinpoint the part of
the code implementing a speciﬁc functionality.
Some studies made the assumption that code that is easy to
understand is also easy to memorize. Therefore, they attempted
to measure how much subjects remember the code. For exam-
ple, Love [44] asked subjects to memorize a program for three
minutes and rewrite the program as accurately as possible in
the next four minutes. Lawrie et al. [8] ﬁrst presented a code
snippet to the subjects. Then, in a second step, they listed
six possible identiﬁers and the subjects had to select the ones
that they recalled appearing in the code. Overall, seven studies
asked the subjects to remember the code.
On the other hand, some studies required the subjects to
act on the code . In ten studies subjects were asked to ﬁndTABLE I: Task types and their corresponding studies. A study
may involve more than one type of task.
Task type Studies
provide information about the code (40 studies)
explain what the code does 18 studies: [7], [8], [12], [13], [16], [21], [43]–[45],
[47]–[55]
answer questions about code
characteristics27 studies: [1], [2], [5], [6], [9], [10], [19], [20],
[41]–[43], [46], [47], [50], [51], [54]–[65]
remember (part of) the code 7 studies: [8], [9], [43], [44], [48], [52], [53]
act on the code (15 studies)
ﬁnd and ﬁx bugs in the code 10 studies: [11], [13]–[15], [19], [21], [45], [46],
[66], [67]
modify the code 8 studies: [10], [13], [45], [46], [56], [62], [66], [67]
write code 3 studies: [61], [62], [68]
provide personal opinion (30 studies)
opinion about the code (read-
ability or legibility)23 studies: [3]–[5], [10], [12], [13], [22], [45], [49],
[55], [59], [61], [62], [64], [65], [67]–[74]
answer if understood the code 4 studies: [42], [55], [59], [60]
rate conﬁdence in her answer 3 studies: [8], [20], [53]
rate the task difﬁculty 7 studies: [5], [10], [20], [21], [45], [49], [51]
and ﬁx bugs in the code. Scanniello et al. [11] asked subjects
to do so in two programs with different identiﬁer styles. In
eight other studies the subjects were asked to modify the code
of a working program, i.e., without the need to ﬁx bugs. For
example, Jbara and Feitelson [45] asked subjects to implement
a new feature in a program seen in a previous task. Likewise,
Schulze et al. [46] requested that subjects modify and delete
annotated code with speciﬁc preprocessor directives, which
also requires understanding the respective source code.
In a few studies, subjects were asked to write code from a
description. Writing code per se is not a comprehension task,
but it may be associated to a comprehension task. For example,
Wiese et al. [61] ﬁrst asked subjects to write a function that
returns true if the input is 7 and false otherwise so that they
could identify what code style (novice, expert, or mixed) the
subjects preferred when coding. Afterwards, they asked the
subjects to choose the most readable among three versions of
a function, each one with a different code style. One of the
goals of this study was to determine if the subjects write code
in the same style that they ﬁnd more readable.
Lastly, in 30 studies subjects were asked to give their
personal opinion . In nine studies the subjects were inquired
about their personal preferences or gut feeling without any
additional task. For example, Buse et al. [22] asked them to
rate (from 1 to 5) how legible or readable a code snippet is.
Similarly, Arab [3] asked subjects to classify the legibility of
three presentation schemes of Pascal code in descending order.
In the study of Santos and Gerosa [73], the subjects chose the
most readable between two functionally equivalent snippets.
In other studies, 21 in total, the subjects were asked about
their personal opinion while they performed other tasks. For
example, O’Neal et al. [55] ﬁrst asked subjects to read a code
snippet, and then to state if they understood the snippet and to
provide a description of its functionality. Similarly, Lawrie et
al. [8] asked subjects to provide a free-form written description
of the purpose of a function and to rate their conﬁdence in
their description. In addition, subjects were asked to rate the
difﬁculty of the comprehension tasks they had to perform in
some studies, e.g., in the study of Fakhoury et al. [21].
Summary: To assess code readability and legibility, re-
searchers conduct studies where subjects are asked to pro-
vide information about the code, act on the code, or give
their personal opinion. We found out that 16.7% of the
studies only ask the subjects to provide personal opinion.
Moreover, although most of the analyzed studies require
subjects to provide information about source code, that
information varies widely in nature. Subjects may be asked
to predict the output of programs, identify code elements,
or explain high-level functionality.
B. Response Variables (RQ2)
Depending on the goals, methodology, and subjects of a
study, response variables vary. This section presents the results
obtained in the analysis of the response variables used in the
selected studies. Since there is considerable diversity of re-
sponse variables, we have organized them into ﬁve categories.
Figure 2 presents the frequency of all response variables. The
numbers do not add up to 54, which is the overall number
of analyzed studies, because most studies employed more
than one response variable. In Table II, we present a detailed
synthesis of the identiﬁed response variables.
The performance of subjects was measured in some studies
in terms of whether they were able to correctly provide infor-
mation about programs just by looking at the source code. The
response variables may pertain to code structure, semantics,
use of algorithms, or program behavior, for instance. We
aggregated response variables like these into a category called
correctness . On the one hand, correctness can be objectively
determined. For example, Bauer et al. [5] measured the sub-
jects’ code understanding by asking them to ﬁll in a ques-
tionnaire with multiple-choice questions referring to program
output. Gopstein et al. [1] and Ajami and Feitelson [2] asked
the subjects to predict the outputs of the execution of short
programs. On the other hand, correctness was subjectively
determined in some studies, where there was some margin to
interpret if the results produced by a subject were correct. For
example, Blinman et al. [7] evaluated if the subjects’ textual
description of a program was correct. Similarly, Love [44]
asked subjects to write a textual description of a program and
scored it based on a subjective evaluation. Overall, correctness
response variables were employed by 83.3% of the studies.
The second most often employed response variable, present
in 30 studies, is the subjects’ personal opinion. What is
common to all these studies is the use of the preferences
and gut feeling of the subjects, instead of the results of what
they do, to assess readability and legibility. We grouped these
response variables in a category called opinion . Scalabrino
et al. [42], for example, asked the subjects to state whether
they understood a code snippet or not. Santos and Gerosa [73]
presented pairs of functionally equivalent snippets to the sub-
jects and asked them to choose which one they think is more
readable or legible. Steﬁk and Gellenbeck [72] requested that
subjects rate lists of words/symbols associated to programming
constructs (e.g., conditionals and loops) based on how intuitive
Fig. 2: Frequency of response variables.
TABLE II: Response variables and their corresponding studies.
Category and type Sub-type Studies
Correctness (45 studies)
Objective Binary 37 studies: [1], [2], [5]–[11], [14], [15], [19]–
[21], [41]–[45], [47], [51]–[67]
Scale 3 studies: [13], [46], [68]
Subjective Binary 11 studies: [12], [16], [21], [47], [48], [50]–
[53], [61], [62]
Rate 7 studies: [8], [13], [43]–[45], [49], [54]
Opinion (30 studies)
Personal preference Rate 9 studies: [22], [64], [65], [68]–[72], [74]
Choice 6 studies: [4], [12], [13], [61], [62], [73]
Ranking 1 study: [3]
On understandability Binary 2 studies: [42], [60]
Rate 2 studies: [55], [59]
Professional opinion Acceptability 2 studies: [67], [74]
Rate answer conﬁdence Rate 3 studies: [8], [20], [53]
Rate task difﬁculty Ranking 1 study: [5]
Rate 6 studies: [10], [20], [21], [45], [49], [51]
Time (27 studies)
Time to complete task 20 studies: [2], [5], [7], [10], [11], [13]–[16],
[41], [43], [45], [46], [49], [55], [57], [64]–[67]
Time reading code 5 studies: [12], [21], [42], [43], [47]
Time per question 4 studies: [9], [47], [56], [58]
Number of attempts 2 studies: [58], [67]
Visual Metrics (6 studies)
Eye tracking 4 studies: [5], [21], [43], [49]
Letterboxing 2 studies: [14], [15]
Brain Metrics (3 studies)
fMRI 1 study: [19]
fNIRS 1 study: [21]
EEG 1 study: [20]
they think they are. Lawrie et al. [8] asked the subjects to rate
their conﬁdence in their understanding of the code.
Thetime that subjects spent to perform tasks was measured
in multiple studies. This response variable category is the third
most often used in the analyzed studies, with 27 instances.
There is variety in the way the studies measured time. For
Ajami and Feitelson [2], “time is measured from displaying
the code until the subject presses the button to indicate he
is done” . Hofmeister et al. [15] computed the time subjects
spent looking at speciﬁc parts of a program. Geffen et al. [56]
measured the response time for each question when subjects
answer a multiple-choice questionnaire about the code. Instead
of directly measuring time, Malaquias et al. [67] counted the
attempts to ﬁx syntactic and semantic errors in the code.
In some studies, information about the process of the task
was collected instead of its outcomes. In particular, multiple
studies employed special equipment to track what the subjects
see, and employed visual metrics as response variables. Six
of the analyzed studies employed some form of eye tracking.
For example, Blinkley et al. [43] computed the visual attention,
measured as the amount of time during which a subject is look-
ing at a particular area of the screen. Bauer et al. [5] computed
three visual metrics: ﬁxation duration, ﬁxation rate, i.e., the
number of ﬁxations per second, and saccadic amplitude, i.e.,
the spatial length of a saccade, that is, the transition between
two ﬁxations. Hofmeister et al. [15] employed a software tool
that limits the subjects’ view of the code to a few lines at a
time. This frame can be shifted up and down using the arrow
keys to reveal different parts of the code. This approach is
called “letterboxing”. Hofmeister et al. [15] called each code
frame an area of interest (AOI), and measured the time subjects
spent on an AOI, ﬁrst-pass reading times, and AOI visits.
Recently, some researchers have resorted to leveraging brain
monitoring tools to understand what happens in the brain of
the subjects during program comprehension. In total, three
of the analyzed studies employed response variables based
on brain monitoring. Siegmund et al. [19] used functional
magnetic resonance imaging (fMRI) to measure brain activity
by detecting changes associated with blood ﬂow. Fakhoury
et al. [21] employed functional near-infrared spectroscopy
(fNIRS) to measure brain activity through the hemodynamic
response within physical structures of the brain. Yeh et al. [20]
leveraged electroencephalography (EEG) to monitor the elec-
trical activity of the brain. These response variables were
grouped into the brain metrics category.
The analyzed studies differ in the response variables they
employed, depending on whether they aimed to assess read-
ability, legibility, or both. As shown in Figure 2, readability
studies leveraged all the response variable categories whereas
no legibility study leveraged brain metrics. This is not surpris-
ing, considering the much lower number of legibility studies.
In addition, a high proportion of legibility studies (86%)
employed opinion response variables. This is not the case for
studies only about readability (43%) or about both (65%).
Most studies, 38 in total, employed more than one variable
to validate their hypotheses. The response variables time and
correctness are the ones that most often appear together (27
studies), followed by opinion and correctness (21 studies). In
a different manner, 16 studies employed only one response
variable, and correctness and opinion were the response vari-
ables employed in isolation. Among these, 9 studies (16.7%)
employed only personal opinion. The use of this response
variable category in isolation is a clear threat to the validity
of these studies. Furthermore, no study used time as the only
response variable. This makes sense since, even though time
is often used as a proxy for effort, it is not signiﬁcant by itself.
Summary: There are ﬁve categories of response variables.
Correctness is the most widely used, being employed in
83.3% of the analyzed studies. Time and opinion are also
often employed (50% and 55.6% of the studies, respec-
tively). A signiﬁcant number of studies used the variable
time and/or opinion associated with correctness. On the
other hand, 30% of the studies employed a single response
variable. The readability and readability+legibility studies
used all the response variable categories while almost all
the legibility studies used the opinion response variable.V. P ROGRAM COMPREHENSION AS A LEARNING ACTIVITY
The empirical studies analyzed in this work involve a wide
range of tasks to be performed by their subjects (see Sec-
tion IV-A). For example, they may ask subjects to memorize
a program, follow its execution step by step, answer questions
about it, or write a high-level explanation of what it does.
All these tasks are conducted with the goal of evaluating
readability and legibility. However, they demand different
cognitive skills from the subjects and, as a consequence,
evaluate different aspects of readability and legibility. We
attempt to shed a light on this topic by analyzing the cognitive
skill requirements associated with each kind of task.
According to the Merriam-Webster Thesaurus, to learn
something is “go gain an understanding of” it. We follow
this deﬁnition by treating the problem of program compre-
hension (or understanding) as a learning problem. In this
section we propose an adaptation of the learning taxonomy
devised by Fuller and colleagues [75] to the context of
program comprehension. This taxonomy is itself an adaptation
of Bloom’s taxonomy of educational objectives [76] to the
context of Software Development. Fuller et al. [75] state
that“learning taxonomies [..] describe the learning stages at
which a learner is operating for a certain topic. ” A learning
taxonomy supports educators in establishing intended learning
outcomes for courses and evaluate the students’ success in
meeting these goals. According to Biggs [77] and Fuller et
al. [75], learning taxonomies can help with “understanding
about understanding” and “communicating about understand-
ing”. Based on this description, they seem to be a good ﬁt
to help researchers better understand studies about program
understanding and communicate about them. Our central idea
is to use the elements deﬁned by the taxonomy of Fuller
et al. [75], with some adaptions, to identify and name the
cognitive skills required by different tasks employed by code
readability and legibility studies.
A. A Learning Taxonomy
Bloom’s taxonomy [76] consists of three hierarchical mod-
els aiming to classify educational learning objectives in terms
of complexity and speciﬁcity. In the context of this work, the
cognitive domain of this taxonomy is the most relevant. It
recognizes that learning is a multi-faceted process that requires
multiple skills with different levels of complexity and which
build upon each other. This taxonomy has been later revised by
Anderson et al. [78]. The most visible aspect of the revised
taxonomy is a list of six activities that deﬁne progressively
more sophisticated levels of learning: remember, understand,
apply, analyze, evaluate, and create.
Fuller et al. [75] proposed an adaptation of the revised ver-
sion of Bloom’s taxonomy for the area of Computer Science,
with a particular emphasis on software development. They
developed a set of activities that build upon Bloom’s revised
taxonomy and organized them in a model that emphasizes
that some activities in software development involve acting
on knowledge, instead of just learning. We leverage this tax-
onomy and apply it in the context of program comprehension.
TABLE III: Learning activities extended from Fuller et al. [75]—Inspect and Memorize are not in the original taxonomy.
Opinion is not included because it is not directly related to learning.
Activity Description Example
Adapt Modify a solution for other domains/ranges. This activity is about the modiﬁ-
cation of a solution to ﬁt in a given context.Remove preprocessor directives to reduce variability in a family of systems [46].
Analyze Probe the [time] complexity of a solution. Identify the function where the program spends more time running.
Apply Use a solution as a component in a larger problem. Apply is about changing a
context so that an existing solution ﬁts in it.Reuse of off-the-shelf components.
Debug Both detect and correct ﬂaws in a design. Given a program, identify faults and ﬁx them [11].
Design Devise a solution structure. The input to this activity is a problem speciﬁcation. Given a problem speciﬁcation, devise a solution satisfying that speciﬁcation.
Implement Put into lowest level, as in coding a solution, given a completed design. Write code using the given examples according to a speciﬁcation [68].
Model Illustrate or create an abstraction of a solution. The input is a design. Given a solution, construct a UML model representing it.
Present Explain a solution to others. Read a program and then write a description of what it does and how [48].
Recognize Base knowledge, vocabulary of the domain. In this activity, the subject must
identify a concept or code structure obtained before the task to be performed.“The sorting algorithm (lines 46-62) can best be described as: (A) bubble sort (B)
selection sort (C) heap sort (D) string sort (E) partition exchange. sort” [63].
Refactor Redesign a solution (as for optimization). The goal is to modify non-functional
properties of a program or, at a larger scale, reengineer it.Rewrite a function so as to avoid using conditional expressions.
Relate Understand a solution in context of others. This activity is about identifying
distinctions and similarities, pros and cons of different solutions.Choose one out of three high-level descriptions that best describe the function of
a previously studied application [7].
Trace Desk-check a solution. Simulate program execution while looking at its code. Consider the fragment “x=++y”: what is the ﬁnal value of x if y is -10? [41].
Inspect* Examine code to ﬁnd or understand ﬁne-grain static elements. Inspect is similar
to Analyze, but it happens at compile time instead of run time.“All variables in this program are global.” [true/false] [51].
Memorize* Memorize the code in order to reconstruct it later, partially or as a whole. Given a program, memorize it in 3 minutes and then reconstruct it in 4 [44].
Table III presents the activities (i.e., cognitive skills) devised
by Fuller et al. [75]. Also, we introduce two activities (marked
with “*”) that stem directly from tasks performed by subjects
in some of our primary studies and require skills that are not
covered by the original set of activities. Table III also presents
examples of the activities, which are either extracted from
tasks from our primary studies or general examples when no
task involved that activity. In the next section we leverage these
activities to gain a better understanding of the tasks conducted
by subjects in our primary studies.
B. Mapping Tasks to Learning Activities
We analyzed the tasks that subjects performed in the pri-
mary studies (Section IV-A) and identiﬁed which activities
(Table III) they required. A task can require subjects to conduct
one or more activities. For example, Avidan and Feitelson
[12] asked subjects to explain what a function does. This is
an instance of the Present activity. Miara et al. [51] applied
a questionnaire that asked subjects to identify speciﬁc code
elements and also inquired them about the results of expression
evaluations. In this case, both Inspect and Trace activities
were executed. Besides the activities in Table III, we also
considered Giving an Opinion (hereafter, “Opinion”), which is
not part of the taxonomy because it is not a learning activity.
We considered Opinion to reﬂect the intrinsically biased and
unreliable nature of tasks that ask subjects to provide their
opinion. Previous work [79], [80] has shown that evidence in
software engineering studies often contradicts opinions. This
kind of analysis falls outside the scope of this paper.
Figure 3 presents a confusion matrix to show the fre-
quency of studies in which tasks and learning activities co-
occur. For instance, there are six studies that involves the
“Remember the code” task in the context of the Memorize
learning activity. The matrix shows that there is a direct
correspondence between some tasks and activities. For ex-
ample, all the instances of the “Find and ﬁx bugs in the
code” task involve the Debug activity, and all the tasks that
require subjects to provide an opinion are connected to the
Explain what the code doesAnswer questions about codeRemember the code
Find and fix bugs in the codeModify the codeWrite code
Opinion about the code
Answer if understood the code Rate confidence in her answerRate the task difficultyAdapt
Debug
Implement
Inspect
Memorize
Present
Recognize
Relate
Trace
Opinion*0 0 0 0 1 0 0 0 0 0
0 0 0 10 0 0 0 0 0 1
0 0 1 0 7 3 0 0 0 1
0 8 0 0 3 1 0 0 0 2
0 0 6 0 0 0 0 0 0 0
16 0 1 0 0 0 0 0 2 4
0 3 0 0 0 0 0 0 0 0
2 1 0 0 0 0 0 0 0 0
0 21 0 0 6 0 0 0 1 3
0 0 0 0 0 0 23 4 3 7Fig. 3: Confusion matrix of tasks (columns) and learning
activities (rows).
Opinion activity. In addition, some tasks may be connected to
various activities. For instance, “Modify the code” may require
subjects to Implement, Trace, Inspect, or Adapt the code to
be modiﬁed. This makes sense; to modify a program, one
may have to understand its static elements and its behavior, as
well as adapt code elements to be reused. Another example is
“Answer questions about code”, which often requires subjects
to Trace and Inspect code. Furthermore, “Explain what the
code does” is usually related to Present. Notwithstanding, in
two studies [7], [55] subjects were presented with multiple de-
scriptions for the same code snippet and asked which one is the
most appropriate. This requires the subject to Relate different
programs and descriptions. Finally, there are some nonintuitive
relationships between tasks and activities. Chaudhary [48]
asked the subjects to reconstruct a program after spending
some time looking at it. This is a “Remember the code” task
where the subjects have to explain what the program does
by writing a similar program. It is a mix of Present and
Implement. In another example, Lawrie et al. [8] asked the
subjects to describe what a code snippet does and rate their
conﬁdence in their answers. Rating one’s conﬁdence in an
answer is about Opinion but it also must be associated with
some other activity, since some answer must have been given
in the ﬁrst place. In this case, the activity is to Present the code
snippet, besides giving an Opinion. A similar phenomenon can
be observed for studies where subjects were asked to “Rate
the task difﬁculty” they had performed, e.g., [21], [49].
Figure 4 shows the frequency of the activities required
in the analyzed studies, separating readability and legibility
studies, as well as those targeting both attributes. Opinion
was required by more studies than any learning activity. The
two most widely used learning activities require that subjects
extract information from programs. The most widely used one,
Trace, was required in 25 studies, where subjects were asked to
predict the output value of a program or the value of a variable.
Coming in second, the Present activity was required in 16
studies, where usually subjects were asked to explain what
the program does. It is also common for subjects to be asked
to Inspect the code (11 studies), so as to determine, e.g., how
many loops are there in the program. Subjects were required
to Implement or Debug code in 10 studies each. Figure 4
also highlights that studies focusing solely on legibility and
on readability and legibility combined require subjects to give
Opinion more than any learning activity. Even considering
the small number of legibility studies, this may be a hint
that researchers have yet to ﬁgure out the effective ways to
meaningfully assess legibility-related attributes.
Figure 5 highlights how many studies use combinations of
two activities. More than one activity was employed by 34 of
the studies, which combine mainly the most popular activities,
i.e., Trace, Present, and Inspect. The numbers in the diagonal
indicate how many studies required their subjects to use a
single activity. In total, 20 studies employed a single activity.
This number amounts to 37% of our primary studies. The use
of a single learning activity in an evaluation of readability or
legibility suggests a narrow focus, since these attributes are
multi-faceted, as highlighted by Table III.
C. A Two-Dimensional Model for Program Comprehension
Besides the list of activities, Fuller et al. [75] proposed
a taxonomy adapted from the revised version of Bloom’s
taxonomy [78]. It is represented by two semi-independent di-
mensions, Producing and Interpreting. Each dimension deﬁnes
hierarchical linear levels where a deeper level requires the
competencies from the previous ones. Producing has three
levels (None, Apply, and Create) and Interpreting has four
(Remember, Understand, Analyze, and Evaluate). Figure 6
represents this two-dimensional model. According to Fuller et
al. [75], a level appearing more to the right of the ﬁgure along
the Interpreting dimension (x-axis), e.g., Evaluate, requires
more competencies than one appearing more to the left, e.g.,
Remember. The same applies to the levels appearing nearer
the top along the Producing dimension (y-axis).
Fig. 4: Frequency of learning activities and subject opinion.
Adapt Debug
ImplementInspectMemorizePresentRecognizeRelate Trace
Opinion*Adapt
Debug
Implement
Inspect
Memorize
Present
Recognize
Relate
Trace
Opinion*0
1 3
0 4 0
1 4 4 0
0 0 0 1 0
0 3 3 3 5 1
0 0 0 2 1 0 0
0 1 0 1 0 0 0 1
1 4 6 7 0 6 2 2 6
0 4 7 5 2 8 1 1 12 9
Fig. 5: Co-occurrence of learning activities and subject
opinion—the main diagonal represents the number of studies
that an activity or opinion is used alone.
The activities in Table III were positioned in Figure 6
in conformance to their required competencies (we did not
consider Opinion as it is not part of the learning taxonomy).
We complemented the two-dimensional model by including
the two activities that we introduced, Inspect and Memorize.
Figure 6 is a heatmap that presents how frequently each
competence was required by the studies analyzed in this work.
Dark regions are associated to more frequent activities.
The Interpreting dimension indicates that most activities
employed in studies evaluating readability and legibility occur
at the Understand level, followed by Analyze and, to a lesser
extent, Remember. The higher competence level of the Inter-
preting dimension, Evaluate, is almost never used. Considering
the Producing dimension, where the ability to design and build
a new product, e.g., a program, is evaluated, we notice that
None is the most representative, followed by Apply. This is to
be expected since most of the studies focus on comprehension-
related activities. However, program comprehension is often
required for work that involves the Apply and Create levels.
Nevertheless, these levels are rarely tackled by our primary
studies. This reinforces the point raised in Section V-B, that
many of these studies have a narrow evaluation focus.
We built three additional heatmaps to emphasize the differ-
ences between readability and legibility studies when consider-
ing the two-dimensional model. Figure 7 presents the heatmaps
for readability studies, legibility studies, and both studies in
sequence. Figure 7a is very similar to Figure 6. In contrast,
Figure 7b shows that legibility studies are concentrated in the
Understand level, where Trace is the main activity. Finally,
Figure 7c presents the heatmap for studies that tackle both
readability and legibility together. Albeit similar to Figure 6,
the area in the intersection between the Analyze level of the
Interpreting dimension and the Apply level of the Producing
dimension is darker in this ﬁgure. This reﬂects the proportion-
ally higher number of studies that employ the Debug activity.
VI. T HREATS TO VALIDITY
Construct validity . Our study was built on the selected
primary studies, which stem from the search and selection
processes. The search for studies relies on a search string,
which was deﬁned based on the seed papers. We chose only
conferences to search for seed papers. A paper published
in a journal is often an extension of a conference paper
and, in Computer Science, the latest research is published
in conferences. Furthermore, we focused on conferences only
as a means to build our search string. Journal papers were
considered in the actual review. Additionally, we only used
three search engines for our automatic search. Other engines,
such as Springer and Google Scholar, could return different
results. However, the majority of our seed studies were indexed
by ACM and IEEE, and then we used Scopus to expand
our search. Moreover, while searching on the ACM digital
library, we used the Guide to the Computing Literature , which
retrieves resources from other publishers, such as Springer.
Finally, we avoided Google Scholar because it returned more
than 17 thousand documents for our search string.
Internal validity . This study was conducted by four re-
searchers. We understand that this could pose a threat to its
internal validity since each researcher has a certain knowledge
and way of conducting her research activities. However, each
researcher conducted her activities according to the established
protocol, and periodic discussions were conducted between all
researchers. Another threat to validity is the value of Cohen’s
Kappa in the study inclusion step ( k= 0:323), which is
considered fair. This value stems from the use of three possible
evaluations (“acceptable”, “not acceptable”, and “maybe”) in
that step. However, we employed “maybe” to avoid having to
choose between “acceptable” and “not acceptable” when we
had doubts about the inclusion of a paper—all papers marked
with at least one “maybe” were discussed between all authors.
Moreover, a few primary studies do not report in detail the
tasks the subjects performed. Because of that, we might have
misclassiﬁed these studies when mapping studies to task types.
External validity . Our study focuses on studies that report on
comparisons of alternative ways of writing code, considering
low-level aspects of the code. Our ﬁndings might not apply to
other works that evaluate code readability or legibility.
Fig. 6: Learning activities and their frequency.
(a) Readability
 (b) Legibility
 (c) Both
Fig. 7: Learning activities by readability and legibility. This
ﬁgure decomposes Figure 6 to show spatial disposition.
Conclusion validity . According to Kitchenham et al. [33],
conclusion validity is concerned with how reliably we can
draw conclusions about the relationship between a treatment
and the outcomes of an empirical study. In a systematic review,
that relates to the data synthesis and how well this supports
the conclusions of the review. The threats of our study related
to it are therefore presented as internal validity.
VII. C ONCLUSION
We presented a systematic literature review on how code
readability and legibility are evaluated in human-centric stud-
ies that compare different ways of writing equivalent code. Our
goal was to investigate what tasks are performed by subjects
and what response variables are employed in those studies,
as well as what cognitive skills are required by those tasks.
We presented comprehensive classiﬁcations for both tasks and
response variables. Moreover, we adapted a learning taxonomy
to program comprehension, mapping the tasks identiﬁed in
the literature review to the cognitive skills that comprise
the taxonomy. This study highlighted limitations of primary
studies: (i) 37% of them exercised a single cognitive skill; (ii)
16.7% only employed personal opinion as a response variable;
and (iii) few studies evaluated readability and legibility in
a way that simulates real-world scenarios, where program
comprehension is part of a more complex task, requiring
higher-level cognitive skills. We also introduced a separation
between code readability and legibility. This separation is
common in other areas such as linguistics, but, to the best
of our knowledge, we are the ﬁrst to propose it in the context
of software engineering. We are performing a systematic
literature review focusing on the different ways of writing code
and which ones improve or hinder readability and legibility.
Acknowledgements. We thank the anonymous reviewers for
their valuable feedback on this paper. This research was par-
tially funded by CNPq/Brazil (304755/2014-1, 406308/2016-
0, 465614/2014-0), and FACEPE/Brazil (APQ-0839-1.03/14,
IBPG-0690-1.03/19, 0388-1.03/14, 0592-1.03/15).
REFERENCES
[1] D. Gopstein, J. Iannacone, Y . Yan, L. DeLong, Y . Zhuang, M. K.-C.
Yeh, and J. Cappos, “Understanding Misunderstandings in Source
Code,” in Proceedings of the 11th Joint Meeting on Foundations of
Software Engineering (ESEC/FSE ’17) . New York, NY , USA: ACM,
2017, pp. 129–139.
[2] S. Ajami, Y . Woodbridge, and D. G. Feitelson, “Syntax, predicates,
idioms – what really affects code complexity?” Empirical Software
Engineering , vol. 24, no. 1, pp. 287–328, Feb. 2019.
[3] M. Arab, “Enhancing Program Comprehension: Formatting and
Documenting,” ACM SIGPLAN Notices , vol. 27, no. 2, pp. 37–46, Feb.
1992.
[4] X. Wang, L. Pollock, and K. Vijay-Shanker, “Automatic Segmentation
of Method Code into Meaningful Blocks: Design and Evaluation,”
Journal of Software: Evolution and Process , vol. 26, no. 1, pp. 27–49,
2014.
[5] J. Bauer, J. Siegmund, N. Peitek, J. C. Hofmeister, and S. Apel,
“Indentation: Simply a Matter of Style or Support for Program
Comprehension?” in Proceedings of the 27th International Conference
on Program Comprehension (ICPC ’19) . Piscataway, NJ, USA: IEEE
Press, 2019, pp. 154–164.
[6] B. E. Teasley, “The effects of naming style and expertise on program
comprehension,” International Journal of Human-Computer Studies ,
vol. 40, no. 5, pp. 757–770, May 1994.
[7] S. Blinman and A. Cockburn, “Program Comprehension: Investigating
the Effects of Naming Style and Documentation,” in Proceedings of the
6th Australasian User Interface Conference (AUIC ’05) . Newcastle,
Australia: ACS, 2005, pp. 73–78.
[8] D. Lawrie, C. Morrell, H. Feild, and D. Binkley, “Effective identiﬁer
names for comprehension and memory,” Innovations in Systems and
Software Engineering , vol. 3, no. 4, pp. 303–318, Dec. 2007.
[9] D. Binkley, D. Lawrie, S. Maex, and C. Morrell, “Identiﬁer length
and limited programmer memory,” Science of Computer Programming ,
vol. 74, no. 7, pp. 430–445, May 2009.
[10] M. Ceccato, M. Di Penta, J. Nagra, P. Falcarin, F. Ricca, M. Torchiano,
and P. Tonella, “The Effectiveness of Source Code Obfuscation: an Ex-
perimental Assessment,” in Proceedings of the IEEE 17th International
Conference on Program Comprehension (ICPC ’09) . IEEE, 2009, pp.
178–187.
[11] G. Scanniello and M. Risi, “Dealing with Faults in Source Code:
Abbreviated vs. Full-Word Identiﬁer Names,” in Proceedings of the
2013 IEEE International Conference on Software Maintenance (ICSM
’13). USA: IEEE Computer Society, 2013, pp. 190–199.
[12] E. Avidan and D. G. Feitelson, “Effects of Variable Names on
Comprehension: An Empirical Study,” in Proceedings of the 25th
International Conference on Program Comprehension (ICPC ’17) .
Piscataway, NJ, USA: IEEE Press, 2017, pp. 55–65.
[13] G. Beniamini, S. Gingichashvili, A. K. Orbach, and D. G. Feitelson,
“Meaningful Identiﬁer Names: The Case of Single-Letter Variables,”
inProceedings of the 25th International Conference on Program
Comprehension (ICPC ’17) . Piscataway, NJ, USA: IEEE Press, 2017,
pp. 45–54.
[14] A. Schankin, A. Berger, D. V . Holt, J. C. Hofmeister, T. Riedel,
and M. Beigl, “Descriptive Compound Identiﬁer Names Improve
Source Code Comprehension,” in Proceedings of the 26th International
Conference on Program Comprehension (ICPC ’18) . New York, NY ,
USA: ACM, 2018, pp. 31–40.
[15] J. C. Hofmeister, J. Siegmund, and D. V . Holt, “Shorter Identiﬁer
Names Take Longer to Comprehend,” Empirical Software Engineering ,
vol. 24, no. 1, pp. 417–443, Feb. 2019.
[16] A. C. Benander, B. A. Benander, and H. Pu, “Recursion vs. Iteration:
An Empirical Study of Comprehension,” Journal of Systems and
Software , vol. 32, no. 1, pp. 73–82, 1996.
[17] J. Siegmund, “Program comprehension: Past, present, and future,” in
Proceedings of the IEEE 23rd International Conference on Software
Analysis, Evolution, and Reengineering (SANER ’16) , vol. 5, 2016, pp.
13–20.
[18] I. Schr ¨oter, J. Kr ¨uger, J. Siegmund, and T. Leich, “Comprehending
studies on program comprehension,” in Proceedings of the IEEE/ACM
25th International Conference on Program Comprehension (ICPC ’17) .
IEEE, 2017, pp. 308–311.
[19] J. Siegmund, N. Peitek, C. Parnin, S. Apel, J. Hofmeister, C. K ¨astner,
A. Begel, A. Bethmann, and A. Brechmann, “Measuring NeuralEfﬁciency of Program Comprehension,” in Proceedings of the 11th
Joint Meeting on Foundations of Software Engineering (ESEC/FSE
’17). New York, NY , USA: ACM, 2017, pp. 140–150.
[20] M. K.-C. Yeh, D. Gopstein, Y . Yan, and Y . Zhuang, “Detecting and
Comparing Brain Activity in Short Program Comprehension Using
EEG,” in Proceedings of the 2017 IEEE Frontiers in Education
Conference (FIE ’17) . Los Alamitos, CA, USA: IEEE Computer
Society, 2017, pp. 1–5.
[21] S. Fakhoury, D. Roy, Y . Ma, V . Arnaoudova, and O. Adesope,
“Measuring the impact of lexical and structural inconsistencies on
developers’ cognitive load during bug localization,” Empirical Software
Engineering , pp. 1–39, Aug. 2019.
[22] R. P. L. Buse and W. R. Weimer, “Learning a Metric for Code
Readability,” IEEE Transactions on Software Engineering , vol. 36,
no. 4, pp. 546–558, Jul. 2010.
[23] J. R. de Almeida, J. B. Camargo, B. A. Basseto, and S. M. Paz, “Best
practices in code inspection for safety-critical software,” IEEE software ,
vol. 20, no. 3, pp. 56–63, 2003.
[24] J.-C. Lin and K.-C. Wu, “Evaluation of Software Understandability
Based On Fuzzy Matrix,” in Proceedings of the 2008 IEEE International
Conference on Fuzzy Systems (IEEE World Congress on Computational
Intelligence) , 2008, pp. 887–892.
[25] X. Xia, L. Bao, D. Lo, Z. Xing, A. E. Hassan, and S. Li, “Measuring
Program Comprehension: A Large-Scale Field Study with Profession-
als,” IEEE Transactions on Software Engineering , vol. 44, no. 10, pp.
951–976, 2018.
[26] P. B. Gough and W. E. Tunmer, “Decoding, Reading, and Reading
Disability,” Remedial and special education , vol. 7, no. 1, pp. 6–10,
1986.
[27] W. A. Hoover and P. B. Gough, “The simple view of reading,” Reading
and writing , vol. 2, no. 2, pp. 127–160, 1990.
[28] W. H. DuBay, “The principles of readability.” Online Submission , 2004.
[29] C. Tekﬁ, “Readability Formulas: An Overview,” Journal of documenta-
tion, vol. 43, pp. 261–273, 1987.
[30] E. Daka, J. Campos, G. Fraser, J. Dorn, and W. Weimer, “Modeling
Readability to Improve Unit Tests,” in Proceedings of the 10th Joint
Meeting on Foundations of Software Engineering (ESEC/FSE ’15) .
New York, NY , USA: Association for Computing Machinery, 2015, pp.
107–118.
[31] I. Strizver, Type Rules: The designer’s guide to professional typography .
John Wiley & Sons, 2013.
[32] S. Zufﬁ, C. Brambilla, G. Beretta, and P. Scala, “Human computer inter-
action: Legibility and contrast,” in Proceedings of the 14th International
Conference on Image Analysis and Processing (ICIAP ’07) . IEEE,
2007, pp. 241–246.
[33] B. A. Kitchenham, D. Budgen, and P. Brereton, Evidence-Based Soft-
ware Engineering and Systematic Reviews . Chapman & Hall/CRC,
2015.
[34] Parsifal , https://parsif.al/.
[35] ACM Digital Library , http://dl.acm.org/.
[36] IEEE Explore , http://ieeexplore.ieee.org/.
[37] Scopus , http://www.scopus.com/.
[38] J. Cohen, “A Coefﬁcient of Agreement for Nominal Scales,”
Educational and Psychological Measurement , vol. 20, no. 1, pp. 37–46,
1960.
[39] S. Keele et al. , “Guidelines for performing systematic literature reviews
in software engineering,” Technical report, Ver. 2.3 EBSE Technical
Report. EBSE, Tech. Rep., 2007.
[40] R. P. Buse and W. R. Weimer, “A Metric for Software Readability,” in
Proceedings of the 2008 International Symposium on Software Testing
and Analysis (ISSTA ’08) . New York, NY , USA: ACM, 2008, pp.
121–130.
[41] J. J. Dolado, M. Harman, M. C. Otero, and L. Hu, “An Empirical
Investigation of the Inﬂuence of a Type of Side Effects on Program
Comprehension,” IEEE Transactions on Software Engineering , vol. 29,
no. 7, pp. 665–670, Jul. 2003.
[42] S. Scalabrino, G. Bavota, C. Vendome, M. Linares-V ´asquez, D. Poshy-
vanyk, and R. Oliveto, “Automatically Assessing Code Understandabil-
ity,” IEEE Transactions on Software Engineering , pp. 1–1, 2019.
[43] D. Binkley, M. Davis, D. Lawrie, J. I. Maletic, C. Morrell, and
B. Sharif, “The impact of identiﬁer style on effort and comprehension,”
Empirical Software Engineering , vol. 18, no. 2, pp. 219–276, Apr.
2013.
[44] T. Love, “An Experimental Investigation of the Effect of Program
Structure on Program Understanding,” ACM SIGOPS Operating Systems
Review – Proceedings of an ACM conference on Language design for
reliable software , vol. 11, no. 2, pp. 105–113, Mar. 1977.
[45] A. Jbara and D. G. Feitelson, “On the Effect of Code Regularity on
Comprehension,” in Proceedings of the 22nd International Conference
on Program Comprehension (ICPC ’14) . New York, NY , USA: ACM,
2014, pp. 189–200.
[46] S. Schulze, J. Liebig, J. Siegmund, and S. Apel, “Does the Discipline
of Preprocessor Annotations Matter? A Controlled Experiment,” in
Proceedings of the 12th International Conference on Generative
Programming: Concepts & Experiences (GPCE ’13) . New York, NY ,
USA: ACM, 2013, pp. 65–74.
[47] J. P. Boysen and R. F. Keller, “Measuring Computer Program
Comprehension,” in Proceedings of the 11th Technical Symposium on
Computer Science Education (SIGCSE ’80) . New York, NY , USA:
ACM, 1980, pp. 92–102.
[48] B. D. Chaudhary and H. V . Sahasrabuddhe, “Meaningfulness as a
Factor of Program Complexity,” in Proceedings of the ACM 1980
Annual Conference (ACM ’80) . New York, NY , USA: ACM, 1980,
pp. 457–466.
[49] A. Jbara and D. G. Feitelson, “How programmers read regular
code: a controlled experiment using eye tracking,” Empirical Software
Engineering , vol. 22, no. 3, pp. 1440–1477, Jun. 2017.
[50] N. Kasto and J. Whalley, “Measuring the difﬁculty of code
comprehension tasks using software metrics,” in Proceedings of the
15th Australasian Computing Education Conference - Volume 136
(ACE ’13) . Darlinghurst, Australia, Australia: Australian Computer
Society, Inc., 2013, pp. 59–65.
[51] R. J. Miara, J. A. Musselman, J. A. Navarro, and B. Shneiderman,
“Program Indentation and Comprehensibility,” Communications of the
ACM , vol. 26, no. 11, pp. 861–867, Nov. 1983.
[52] S. Wiedenbeck, “Beacons in computer program comprehension,”
International Journal of Man-Machine Studies , vol. 25, no. 6, pp.
697–709, Dec. 1986.
[53] ——, “The initial stage of program comprehension,” International
Journal of Man-Machine Studies , vol. 35, no. 4, pp. 517–540, Nov.
1991.
[54] S. N. Woodﬁeld, H. E. Dunsmore, and V . Y . Shen, “The Effect
of Modularization and Comments on Program Comprehension,”
inProceedings of the 5th International Conference on Software
Engineering (ICSE ’81) . Piscataway, NJ, USA: IEEE Press, 1981, pp.
215–223.
[55] M. B. O’Neal and W. R. Edwards, “Complexity Measures for
Rule-Based Programs,” IEEE Transactions on Knowledge and Data
Engineering , vol. 6, no. 5, pp. 669–680, Oct. 1994.
[56] Y . Geffen and S. Maoz, “On Method Ordering,” in Proceedings of the
24th International Conference on Program Comprehension (ICPC ’16) .
IEEE, 2016, pp. 1–10.
[57] E. R. Iselin, “Conditional statements, looping constructs, and program
comprehension: an experimental study,” International Journal of
Man-Machine Studies , vol. 28, no. 1, pp. 45–66, Jan. 1988.
[58] M. Ma ´ckowiak, J. Nawrocki, and M. Ochodek, “On Some End-User
Programming Constructs and Their Understandability,” Journal of
Systems and Software , vol. 142, pp. 206–222, 2018.
[59] F. Sykes, R. T. Tillman, and B. Shneiderman, “The Effect of
Scope Delimiters on Program Comprehension,” Software: Practice and
Experience , vol. 13, no. 9, pp. 817–824, 1983.
[60] A. Trockman, K. Cates, M. Mozina, T. Nguyen, C. K ¨astner, and
B. Vasilescu, ““Automatically Assessing Code Understandability”
Reanalyzed: Combined Metrics Matter,” in Proceedings of the 15th
International Conference on Mining Software Repositories (MSR ’18) .
New York, NY , USA: ACM, 2018, pp. 314–318.
[61] E. S. Wiese, A. N. Rafferty, and A. Fox, “Linking Code Readability,
Structure, and Comprehension among Novices: It’s Complicated,”
inProceedings of the 41st International Conference on Software
Engineering: Software Engineering Education and Training (ICSE-
SEET ’19) . Piscataway, NJ, USA: IEEE Press, 2019, pp. 84–94.
[62] E. S. Wiese, A. N. Rafferty, D. M. Kopta, and J. M. Anderson,
“Replicating Novices’ Struggles with Coding Style,” in Proceedings ofthe 27th International Conference on Program Comprehension (ICPC
’19). Piscataway, NJ, USA: IEEE Press, 2019, pp. 13–18.
[63] T. Tenny, “Program Readability: Procedures Versus Comments,” IEEE
Transactions on Software Engineering , vol. 14, no. 9, pp. 1271–1279,
Sep. 1988.
[64] P. W. Oman and C. R. Cook, “A Paradigm for Programming Style
Research,” ACM SIGPLAN Notices , vol. 23, no. 12, pp. 69–78, Dec.
1988.
[65] ——, “Typographic Style is More than Cosmetic,” Communications of
the ACM , vol. 33, no. 5, pp. 506–520, May 1990.
[66] S. Kleinschmager, R. Robbes, A. Steﬁk, S. Hanenberg, and E. Tanter,
“Do Static Type Systems Improve the Maintainability of Software
Systems? An Empirical Study,” in Proceedings of the 20th IEEE
International Conference on Program Comprehension (ICPC ’12) , 2012,
pp. 153–162.
[67] R. Malaquias, M. Ribeiro, R. Bonif ´acio, E. Monteiro, F. Medeiros,
A. Garcia, and R. Gheyi, “The Discipline of Preprocessor-Based
Annotations Does #ifdef TAG n’t #endif Matter,” in Proceedings of
the 25th International Conference on Program Comprehension (ICPC
’17). Piscataway, NJ, USA: IEEE Press, 2017, pp. 297–307.
[68] A. Steﬁk and S. Siebert, “An Empirical Investigation into Programming
Language Syntax,” ACM Transactions on Computing Education
(TOCE) , vol. 13, no. 4, pp. 19:1–19:40, Nov. 2013.
[69] V . Arnaoudova, M. Di Penta, and G. Antoniol, “Linguistic Antipatterns:
What They Are and How Developers Perceive Them,” Empirical
Software Engineering , vol. 21, no. 1, pp. 104–158, Feb. 2016.
[70] D. Posnett, A. Hindle, and P. Devanbu, “A Simpler Model of Software
Readability,” in Proceedings of the 8th Working Conference on Mining
Software Repositories (MSR ’11) . New York, NY , USA: Association
for Computing Machinery, 2011, pp. 73–82.
[71] S. Scalabrino, M. Linares-V ´asquez, R. Oliveto, and D. Poshyvanyk,
“A Comprehensive Model for Code Readability,” Journal of Software:
Evolution and Process , vol. 30, no. 6, p. e1958, 2018.
[72] A. Steﬁk and E. Gellenbeck, “Empirical studies on programming
language stimuli,” Software Quality Journal , vol. 19, no. 1, pp. 65–99,
Mar. 2011.
[73] R. M. a. dos Santos and M. A. Gerosa, “Impacts of Coding Practices
on Readability,” in Proceedings of the 26th Conference on Program
Comprehension (ICPC ’18) . New York, NY , USA: ACM, 2018, pp.
277–285.
[74] F. Medeiros, G. Lima, G. Amaral, S. Apel, C. K ¨astner, M. Ribeiro,
and R. Gheyi, “An investigation of misunderstanding code patterns
in C open-source software projects,” Empirical Software Engineering ,
vol. 24, no. 4, pp. 1693–1726, Aug. 2019.
[75] U. Fuller, C. G. Johnson, T. Ahoniemi, D. Cukierman, I. Hern ´an-Losada,
J. Jackova, E. Lahtinen, T. L. Lewis, D. M. Thompson, C. Riedesel,
and E. Thompson, “Developing a Computer Science-speciﬁc Learning
Taxonomy,” ACM SIGCSE Bulletin , vol. 39, no. 4, pp. 152–170, Dec.
2007.
[76] B. Bloom, M. Engelhart, E. Furst, W. H. Hill, and D. R. Krathwohl,
Taxonomy of educational objectives: The classiﬁcation of educational
goals. Handbook I: Cognitive domain . New York: David McKay
Company, 1956.
[77] J. Biggs, Teaching for quality learning at university . Buckingham:
Open University Press, 1999.
[78] L. Anderson, B. Bloom, D. Krathwohl, P. Airasian, K. Cruikshank,
R. Mayer, P. Pintrich, J. Raths, and M. Wittrock, A Taxonomy for
Learning, Teaching, and Assessing: A Revision of Bloom’s Taxonomy
of Educational Objectives . Longman, 2001.
[79] C. J. Rossbach, O. S. Hofmann, and E. Witchel, “Is transactional
programming actually easier?” in Proceedings of the 15th ACM
SIGPLAN Symposium on Principles and Practice of Parallel
Programming (PPoPP ’10) . New York, NY , USA: Association
for Computing Machinery, 2010, pp. 47–56.
[80] F. Castor, J. a. P. Oliveira, and A. L. Santos, “Software transactional
memory vs. locking in a functional language: A controlled experiment,”
inProceedings of the Compilation of the Co-Located Workshops on
DSM’11, TMC’11, AGERE! 2011, AOOPES’11, NEAT’11, & VMIL’11 .
Association for Computing Machinery, 2011, pp. 117–122.

arXiv:2112.10086v1  [cs.IT]  19 Dec 20211
Heterogeneous Transformer: A Scale Adaptable
Neural Network Architecture for Device Activity
Detection
Yang Li, Zhilin Chen, Yunqi Wang, Chenyang Yang, and Yik-Chu ng Wu
Abstract —To support the modern machine-type communica-
tions, a crucial task during the random access phase is devic e
activity detection, which is to detect the active devices fr om a
large number of potential devices based on the received sign al
at the access point. By utilizing the statistical propertie s of the
channel, state-of-the-art covariance based methods have b een
demonstrated to achieve better activity detection perform ance
than compressed sensing based methods. However, covarianc e
based methods require to solve a high dimensional nonconvex
optimization problem by updating the estimate of the activi ty
status of each device sequentially. Since the number of up-
dates is proportional to the device number, the computation al
complexity and delay make the iterative updates difﬁcult fo r
real-time implementation especially when the device numbe r
scales up. Inspired by the success of deep learning for real-
time inference, this paper proposes a learning based method
with a customized heterogeneous transformer architecture for
device activity detection. By adopting an attention mechan ism
in the architecture design, the proposed method is able to
extract the relevance between device pilots and received si gnal,
is permutation equivariant with respect to devices, and is s cale
adaptable to different numbers of devices. Simulation resu lts
demonstrate that the proposed method achieves better activ ity
detection performance with much shorter computation time t han
state-of-the-art covariance approach, and generalizes we ll to
different numbers of devices, BS-antennas, and different s ignal-
to-noise ratios.
Index Terms —Activity detection, attention mechanism, deep
learning, Internet-of-Things (IoT), machine-type commun ications
(MTC).
I. I NTRODUCTION
To meet the dramatically increasing demand for wireless
connectivity of Internet-of-Things (IoT), machine-type c om-
munications (MTC) have been recognized as a new paradigm
in the ﬁfth-generation and beyond wireless systems. Differ ent
from the traditional human-to-human communications, MTC
scenarios commonly involve a large number of IoT devices
connecting to the network, but only a small portion of the
devices are active at any given time due to the sporadic
trafﬁcs [1]–[3]. In activity detection, each device is assi gned
Y . Li is with Shenzhen Research Institute of Big Data, Shenzh en, 518172,
China (e-mail: liyang@sribd.cn).
Z. Chen is with The Edward S. Rogers Sr. Department of Electri cal and
Computer Engineering, University of Toronto, Toronto, ON M 5S 3G4, Canada
(e-mail: zchen@comm.utoronto.ca).
Y . Wang and Y .-C. Wu are with the Department of Electrical and Elec-
tronic Engineering, The University of Hong Kong, Hong Kong ( e-mail:
{yunqi9@connect, ycwu@eee }.hku.hk).
C. Yang is with the School of Electronics and Information Eng ineering,
Beihang University, Beijing, 100191, China (e-mail: cyyan g@buaa.edu.cn).a unique pilot sequence and the base station (BS) detects
which pilot sequences are received in the random access
phase [4], [5]. However, the pilot sequences for device acti vity
detection have to be nonorthogonal, due to the large number
of devices but limited coherence time . The nonorthogonality
of the pilot sequences inevitably induces interference amo ng
different devices, and hence complicates the task of device
activity detection in MTC.
To tackle the problem of device activity detection with
nonorthogonal pilot sequences, two major approaches have
been proposed in the literature. The ﬁrst approach identiﬁe s
the active devices through joint device activity detection and
channel estimation using compressed sensing based methods
[6]–[18]. Speciﬁcally, [6], [7] proposed approximate mess age
passing (AMP) based algorithms to jointly recover the devic e
activity and the instantaneous channel state information. Fur-
thermore, AMP was extended to include data detection [8], [9 ]
and to multi-cell systems [10]–[12], respectively. In addi tion
to AMP, other compressed sensing based methods, such as
Bayesian sparse recovery [13], [14] and regularization bas ed
sparse optimization [15]–[18] have also been investigated for
joint device activity detection and channel estimation.
Different from the compressed sensing based methods,
the second approach utilizes the statistical properties of the
channel without the need of estimating the instantaneous
channel state information. This approach is referred to as t he
covariance based methods, since they are based on the sample
covariance matrix of the received signal [19]–[23]. The co-
variance based methods have recently drawn a lot of attentio n
due to the superiority of activity detection performance. I n
particular, the analytical results in [24], [25] show that t he
required pilot sequence length of the covariance based meth ods
for reliable activity detection is much shorter than that of the
compressed sensing based methods.
While the covariance based methods outperform the com-
pressed sensing based methods due to the advantage of uti-
lizing the statistical properties of the channel, the covar iance
approach requires to solve a high dimensional nonconvex
optimization problem [19]–[23], where the estimate of the
activity status of each device is updated sequentially usin g
the coordinate descent method. The sequential nature of the
coordinate descent method implies that the number of update s
is proportional to the total number of devices. Consequentl y,
the resulting computational complexity and delay make it
unsuitable for real-time implementation, especially when the
device number is very large.
2
Recently, deep learning has been exploited to avoid the high
computational cost caused by iterative algorithms [26]–[2 9].
Instead of solving each optimization problem case-by-case ,
deep learning utilizes neural networks to represent a mappi ng
function from many problem instances to the corresponding
solutions based on a large number of training samples. Once
the mapping function is obtained, the neural network can
infer the solution of any new problem in a real-time manner.
Moreover, thanks to the universal approximation property o f
neural networks [30], deep learning also has the opportunit y
to learn a better solution than the conventional model-base d
methods for complex problems [31]–[34].
The potentials of deep learning in computational efﬁciency
and solution quality motivate us to study a learning based
method for tackling the high dimensional nonconvex problem
in device activity detection. For this purpose, we interpre t
the activity detection as a classiﬁcation problem and desig n
a customized neural network architecture for representing the
mapping function from the received signal and device pilots
to the corresponding activity status. While generic multi-
layer perceptrons (MLPs) have been widely used for function
representations, they are not tailored to the activity dete ction
problem due to the lack of some key properties. In particular ,
•To detect the device activities, the BS should perceive whic h
device pilots are received from the received signal. There-
fore, it is beneﬁcial to incorporate a computation mechanis m
into the neural network for evaluating the relevance betwee n
the received signal and device pilots. However, generic
MLPs do not generate such an attribute.
•The device activity detection has an inherent permutation
equivariance property with respect to devices. To be speciﬁ c,
if the indices of any two devices are exchanged, the neural
network should output a corresponding permutation. Incor-
porating permutation equivariance into the neural network
architecture can reduce the parameter space and also avoid
a large number of unnecessary permuted training samples
[28], [29], [31]. Unfortunately, the architecture of gener ic
MLPs cannot guarantee the permutation equivariance prop-
erty.
•As the device number scales up, it is highly expected that the
neural network is generalizable to larger numbers of device s
than the setting in the training procedure. Nevertheless,
generic MLPs are designed for a pre-deﬁned problem size
with ﬁxed input and output dimensions, and thus the well-
trained MLPs are no longer applicable to a different number
of devices. Recent works have applied deep learning for
joint activity detection and pilot design [35] and for joint
activity detection and channel estimation [36] , respectively.
However, using the neural networks of [35], [36], the device
pilots can only be either optimized for a pre-deﬁned device
number [35] or ﬁxed as a given matrix [36], and thus the
well-trained neural networks can neither generalize to a
different device number nor toa different set of device
pilots.
To incorporate the properties mentioned above into the neu-
ral network, this paper proposes a heterogeneous transform er
architecture, which is inspired by the recent successes of t hetransformer model in natural language processing (NLP) [37 ].
In particular, transformer is built on an attention mechani sm,
which can extract the relevance among different words withi n
a sentence. Based on the relevance extraction, transformer can
decide which parts of the source sentence to pay attention to .
We observe that the relevance extraction is appealing to the
activity detection problem, since the BS should perceive wh ich
device pilots contribute to the received signal by evaluati ng the
relevance between the received signal and device pilots.
Yet different from the NLP tasks where different words
belong to the same class of features, the received signal and
device pilots in the activity detection problem have differ ent
physical meanings, and hence should be processed different ly.
This observation motivates us to design a heterogeneous
transformer architecture. Speciﬁcally, instead of using a single
set of parameters to process all the inputs as in the standard
transformer, we use two different sets of parameters, one to
process the representations corresponding to the device pi lots,
and the other set for the received signal.
The overall deep neural network consists of an initial em-
bedding layer, multiple heterogeneous transformer encodi ng
layers, and a decoding layer. The initial embedding layer
takes the received signal and device pilots as the inputs
and produces the initial embeddings. The initial embedding s
are further processed through the encoding layers, where th e
heterogeneous attention mechanism is applied to extract th e
relevance among the received signal and device pilots. Fina lly,
the decoding layer decides the activity status of each devic e
based on the extracted relevance.
The main contributions of this work are summarized as
follows.
1) We provide a novel perspective on how device activity
detection can be formulated as a classiﬁcation problem with
the received signal and device pilots as inputs. By con-
structing a training data set of received signals and device s
pilots with ground-truth labels, we propose a deep learning
approach to mimic the optimal mapping function from the
received signal and device pilots to device activities, whi ch
can potentially achieve better detection performance than
state-of-the-art covariance approach. Moreover, instead of
iteratively solving an optimization problem case-by-case ,
the proposed learning based method can infer the solution
of any new problem in a real-time manner due to the
computationally cheap inference.
2) We further propose a customized heterogeneous trans-
former architecture based on the attention mechanism,
which learns the device activities from the relevance be-
tween the received signal and device pilots. Moreover, by
sharing the parameters for producing the representations o f
different device pilots, the proposed heterogeneous trans -
former is permutation equivariant with respect to devices,
and the dimensions of parameters that require to be op-
timized during the training procedure are independent of
the number of devices. This scale adaptability makes the
proposed architecture generalizable to different numbers of
devices.
3) Simulation results show that the proposed learning based
method using heterogeneous transformer achieves better
3
activity detection performance with much shorter compu-
tation time than state-of-the-art covariance approach. Th e
proposed method also generalizes well to different numbers
of devices, BS-antennas, and different signal-to-noise ra tios
(SNRs).
The remainder of this paper is organized as follows. System
model and existing approaches are introduced in Section II. A
novel deep learning perspective on device activity detecti on is
proposed in Section III. A heterogeneous transformer archi tec-
ture is designed in Section IV . Simulation results are provi ded
in Section V . Finally, Section VI concludes the paper.
Throughout this paper, scalars, vectors, and matrices are d e-
noted by lower-case letters, lower-case bold letters, and u pper-
case bold letters, respectively. The real and complex domai ns
are denoted by RandC, respectively. We denote the transpose,
conjugate transpose, inverse, real part, and imaginary par t of a
vector/matrix by (·)T,(·)H,(·)−1,ℜ(·), andℑ(·), respectively.
TheN×Nidentity matrix and the length- Nall-one vector are
denoted as INand1N, respectively. The trace, determinant,
and the column vectorization of a matrix are represented as
Tr(·),| · |, and vec (·), respectively. The notation ⊙denotes
the element-wise product, I(·)denotes the indicator function,
ReLu(·)denotes the function max(·,0), andCN(·,·)denotes
the complex Gaussian distribution.
II. S YSTEM MODEL AND EXISTING APPROACHES
A. System Model
Consider an uplink multiple-input multiple-output (MIMO)
system with one M-antenna BS and Nsingle-antenna IoT
devices. We adopt a block-fading channel model, where the
channel from each device to the BS remains unchanged within
each coherence block. Let√gnhndenote the channel from
then-th device to the BS, where√gnandhn∈CMare the
large-scale and small-scale Rayleigh fading components, r e-
spectively. Due to the sporadic trafﬁcs of MTC, only K≪N
devices are active in each coherence block. If the n-th device
is active, we denote the activity status as an= 1 (otherwise,
an= 0).
To detect the activities of the IoT devices at the BS, we
assign each device a unique pilot sequence sn∈CLp, where
Lpis the length of the pilot sequence. Device ntransmits
the pilot sequence snwith transmit power pnif it is active.
Assuming that the transmission from different devices are
synchronous, we can model the received signal at the BS as
Y=N/summationdisplay
n=1sn√pngnanhT
n+W=SG1
2AH+W, (1)
whereS/defines[s1,...,sN]∈CLp×N,G/defines
diag{p1g1,...,p NgN},A/defines diag{a1,...,a N},
H/defines[h1,...,hN]T∈CN×M, andW∈CLp×Mis
the Gaussian noise at the BS.
This paper aims to detect the activity status {an}N
n=1based
on the received signal Yat the BS. Since the IoT devices are
stationary in many practical deployment scenarios, the lar ge-
scale fading components can be obtained in advance and hence
assumed to be known [21]–[23]. In order to reduce the channelgain variations among different devices, the transmit powe r
of each device can be controlled based on the large-scale
channel gain [8]. This is especially beneﬁcial to the devices
with relatively weak channel gains.
B. Existing Approaches
Existing approaches for device activity detection can be
roughly divided into two categories: compressed sensing ba sed
methods and covariance based methods.
1) Compressed Sensing Based Methods: Due to the sporadic
trafﬁcs of MTC, the activity status and the instantaneous
small-scale fading channels can be jointly estimated by sol v-
ing a compressed sensing problem. Speciﬁcally, by denoting
B/definesSG1
2∈CLp×NandX/definesAH∈CN×M, according
to (1), the activity status can be obtained by recovering the
row-sparse matrix XfromY=BX+W. However, since
a large amount of instantaneous channel state information
requires to be estimated simultaneously, the activity dete ction
performance of compressed sensing based methods cannot
compete with that of covariance based methods.
2) Covariance Based Methods: When the BS is equipped
with multiple antennas, by utilizing the statistical prope rties
of the channel, covariance based methods can estimate the
activity status without estimating the instantaneous chan -
nels. Speciﬁcally, the covariance approach treats the smal l-
scale fading channel matrix Hand the noise matrix W
as complex Gaussian random variables. Each column of H
andWare assumed to follow independent and identically
distributed (i.i.d.) CN(0,IN)andCN(0,σ2ILp), whereσ2
is the noise variance. Let ymdenote the m-th column of
the received signal Y, which follows i.i.d. CN(0,Σ)with
Σ=E/bracketleftbig
ymyH
m/bracketrightbig
=SGASH+σ2ILp. Consequently, the
activity status {an}N
n=1can be detected by maximizing the
likelihood function [21]–[23]
p(Y;{an}N
n=1) =M/productdisplay
m=1p(ym;{an}N
n=1)
=1
|πΣ|Mexp/parenleftbig
−Tr/parenleftbig
Σ−1YYH/parenrightbig/parenrightbig
,(2)
which is equivalent to the following combinatorial optimiz a-
tion problem
min
{an}N
n=1log|Σ|+1
MTr/parenleftbig
Σ−1YYH/parenrightbig
, (3a)
s.t.an∈ {0,1},∀n= 1,...,N. (3b)
The covariance approach ﬁrst relaxes the binary constraint (3b)
asan∈[0,1], and then applies the coordinate descent method
that iteratively updates each anfor solving the relaxed prob-
lem. However, since the coordinate descent method requires
to update each ansequentially, the total iteration number
is proportional to the number of devices N, which induces
tremendous computational complexity and delay, especiall y
when the device number is massive. Moreover, due to the non-
convexity of the cost function (3a), the covariance approac h
can only obtain a stationary point of the relaxed problem.
Different from the conventional optimization based algo-
rithms which involve a lot of iterations, deep learning base d
4
methods can provide a real-time solution by computationally
cheap operations [26]–[29]. In the following two sections,
we propose a deep learning based method for device activity
detection. The proposed method consists of interpreting de vice
activity detection as a classiﬁcation problem (Section III ), and
a customized neural network architecture (Section IV).
III. A N OVEL DEEPLEARNING PERSPECTIVE
A. Device Activity Detection as Classiﬁcation Problem
In this paper, we strive to learn the the activity status
{an}N
n=1without estimating the instantaneous channel H.
However, instead of learning to optimize the problem (3)
whose optimal solution is difﬁcult to obtain, we learn the
activity status {an}N
n=1directly from the ground-truth training
labels based on the received signal model (1). We can see
from (1) that the received signal Yis actually a weighted
sum of the active device pilots SG1
2A=BA. To ﬁnd out
which columns of Bcontribute to Y, we need to build a
computation mechanism to evaluate the relevance between Y
andB. Moreover, since each an∈ {0,1}is a discrete variable,
we can view the activity detection as a classiﬁcation proble m,
i.e., classifying each anas0or1based on YandB.
Speciﬁcally, we construct a training data set Dforsuper-
vised learning , where the i-th training sample is composed of /parenleftbigg
Y(i),B(i),/braceleftBig
˜a(i)
n/bracerightBigN
n=1/parenrightbigg
, and˜a(i)
nis the ground-truth label of
then-th device’s activity status. The received signal Y(i)is
constructed by substituting a given pair of/parenleftbigg
B(i),/braceleftBig
˜a(i)
n/bracerightBigN
n=1/parenrightbigg
into (1), where B(i)is determined by a set of pilot sequences,
large-scale fading gains, and transmit powers,/braceleftBig
˜a(i)
n/bracerightBigN
n=1can be generated from Bernoulli distribution, and the small -
scale fading channel Hand the noise Ware sampled from
complex Gaussian distributions. Although HandWare used
to construct the received signal in the training samples, th ey
themselves are not explicitly included in the training samp les,
since{an}N
n=1should be detected based on YandBwithout
knowing HandW.
Using the training data set D, we learn a classiﬁer to infer
the active probability of each device PnfromYandB. Let
f:CLp×M×CLp×N→[0,1]Ndenote the mapping function
from(Y,B)top/defines[P1,...,P N]T. We strive to optimize the
mapping function f(·,·)such that the difference between the
output of the mapping function/braceleftBig
P(i)
n/bracerightBigN
n=1and the ground-
truth label/braceleftBig
˜a(i)
n/bracerightBigN
n=1is as close as possible. For this purpose,
we adopt cross entropy [38] for measuring the discrepancy
between/braceleftBig
P(i)
n/bracerightBigN
n=1and/braceleftBig
˜a(i)
n/bracerightBigN
n=1, and learn the classiﬁer
by minimizing the following cross entropy based loss functi on:
min
f(·,·)|D|/summationdisplay
i=1/parenleftBigg
2
NN/summationdisplay
n=1/parenleftBigg
N−K
N˜a(i)
nlogP(i)
n
+K
N(1−˜a(i)
n)log/parenleftBig
1−P(i)
n/parenrightBig/parenrightBigg/parenrightBigg
. (4)Notice that when the number of active devices is equal to
that of inactive devices, i.e., K=N−K, the loss function
(4) will reduce to the standard binary cross-entropy loss [3 8],
which is widely used for balanced classiﬁcation in machine
learning. However, due to the sporadic trafﬁcs of MTC, the
number of active devices is commonly much less than that
of inactive devices, i..e., K≪N−K, and thus the standard
binary cross-entropy loss will induce overﬁtting to the ina ctive
class. In order to avoid overﬁtting, we put a much larger
weight(N−K)/Non the loss corresponding to the sporadic
active devices while setting a smaller weight K/N on the loss
corresponding to the more common inactive devices in (4).
B. Parametrization by Neural Network
To solve problem (4), we train a neural network (the detailed
architecture is given in Section IV) for parameterizing the
mapping function f(·,·).During the training procedure, the
neural network learns to adjust its parameters for minimizi ng
the loss function (4), so that the neural network can mimic
the optimal mapping function from the received signal Y
and the scaled pilot matrix Bto the active probability p.
In particular, the neural network parameters can be updated
via gradient based methods, e.g., the Adam algorithm [39],
where the gradients can be automatically computed in any
deep learning framework, e.g., Pytorch [40]. After trainin g,
by inputting any YandBinto the neural network, we
can compute the corresponding output pvia computationally
cheap feed-forward operations. Once pis obtained, we can
use Bernoulli sampling to obtain the activity status of each
device. Alternatively, we can adopt a threshold ξto determine
the activity status as an=I(Pn> ξ).
Notice that the training data set is constructed based on
the received signal model (1), where the ground-truth label s
of the device activities are given. This allows the neural
network to mimic the optimal mapping function f(·,·)directly
from the ground-truth labels [35], [36]. Therefore, there i s
an opportunity to achieve better detection performance tha n
state-of-the-art covariance based methods that only obtai n a
stationary point of the relaxed problems [21]–[23]. Moreov er,
after the training procedure, the neural networks can infer the
solution of any new problem in a real-time manner due to the
computationally cheap inference.
C. Limitations of Generic MLPs
The remaining task is the neural network architecture de-
sign for representing the mapping function f(·,·). Although
generic MLPs have been widely used for function represen-
tations, they are not tailored to the activity detection pro blem
mainly due to three reasons. First, the active probability o f
each device Pnshould be learned based on the the relevance
between the received signal Yand the scaled pilot matrix B.
However, generic MLPs simply concatenate YandBas a
single input, and hence it is difﬁcult to extract the relevance
between YandB. Second, when any two columns of B
are exchanged and Yis unchanged, f(·,·)should output a
corresponding permutation of the original p.Nevertheless,
generic MLPs cannot guarantee the permutation equivarianc e
5
for the activity detection problem. Last but not the least, a s
the number of devices scales up, it is highly expected that th e
neural network is scale adaptable and generalizable to larg er
numbers of devices than the setting in the training procedur e.
Unfortunately, as the input and output dimensions of generi c
MLPs are ﬁxed, they are designed for a pre-deﬁned problem
size. Once the number of devices Nhas changed, the well-
trained MLPs are no longer applicable.
IV. P ROPOSED HETEROGENEOUS TRANSFORMER FOR
REPRESENTING f(·,·)
In this section, we propose a customized neural network
architecture for representing the mapping function f(·,·).
Instead of directly applying generic MLPs, we strive to
incorporate properties of the activity detection problem i nto
the neural network architecture. In particular, the propos ed
architecture is capable of extracting the relevance betwee n the
inputsYandB, is permutation equivariant with respect to
devices, and is scale adaptable to different numbers of devi ces.
Before presenting the proposed architecture, we ﬁrst brieﬂ y
review the basic idea of the transformer model.
A. Transformer Model
Transformer was originally designed for NLP tasks such as
machine translation. It adopts an encoder-decoder archite cture.
The encoder converts the input into a hidden representation
by a sequence of encoding layers, while the decoder produces
the output from the hidden representation by a sequence of
decoding layers. Both the encoder and decoder of transforme r
are built on the attention mechanism [37], which extracts th e
relevance among different input components. In the context of
NLP, the attention mechanism allows the transformer model
to learn from the relevance among different words within a
sentence. Since both the encoder and decoder have a similar
structure, we only review the architecture of the encoder as
follows.
The transformer encoder consists of several sequential en-
coding layers, where each encoding layer extracts the rel-
evance among the input components. The generated output
of each encoding layer is then passed to the next encoding
layer as the input. Speciﬁcally, each encoding layer mainly
consists of two blocks: a multi-head attention (MHA) block
that extracts the relevance among different input componen ts,
and a component-wise feed-forward (FF) block for additiona l
processing. Each block further adopts a skip-connection [4 1],
which adds an identity mapping to bypass the gradient ex-
ploding or vanishing problem for ease of optimization, and a
normalization step [42], which re-scales the hidden repres en-
tations to deal with the internal covariate shift in collect ive
optimization of multiple correlated features.
For better understanding, the l-th encoding layer is illus-
trated in Fig. 1, where the inputs/braceleftBig
x[l−1]
n/bracerightBigN
n=1are passed to the
MHA block and the component-wise FF block successively.
The most important module in Fig. 1 is the MHA block,
which evaluates the relevance of every pair of components
in/braceleftBig
x[l−1]
n/bracerightBigN
n=1by scoring how well they match in multiple
Fig. 1. The l-th encoding layer of the transformer model.
attention spaces. By combining all the matching results fro m
different attention spaces, each layer output is able to cap ture
the relevance among the input components. In the context of
NLP, this relevance information reﬂects the importance of e ach
source word and intuitively decides which parts of the sourc e
sentence to pay attention to.
B. Architecture of Proposed Heterogeneous Transformer
The relevance extraction of transformer is appealing for
representing the mapping function f(·,·), as the device ac-
tivity should be detected based on the relevance between the
received signal Yand the scaled pilot matrix B. However,
different from the NLP tasks where different words belong to
the same class of features, the inputs YandBin the activity
detection problem have different physical meanings, and he nce
should be processed differently. This observation motivat es
us to design a heterogeneous transformer architecture for
representing f(·,·).
In particular, the proposed heterogenous transformer is co m-
posed of an initial embedding layer, Lencoding layers, and a
decoding layer, where we use the same set of parameters to
process the representations corresponding to the device pi lots,
while we process the representation corresponding to the
received signal using another set of parameters. The speciﬁ c
architectures of different layers are presented as follows . For
ease of presentation, the scaled pilot matrix is expanded as
B= [b1,...,bN], with the column bncorresponding to the
n-th device.
1) Initial Embedding Layer
The initial embedding layer represents the Ndevice pilots
{bn}N
n=1and the received signal Yas the input features and
then transforms them into an initial representation for the
subsequent encoding layers. The input features are express ed
in real-vector forms by separating the real and imaginary pa rts.
Speciﬁcally, the input features corresponding to {bn}N
n=1are
given by
xin
n=/bracketleftBig
ℜ{bn}T,ℑ{bn}T/bracketrightBigT
∈R2Lp,∀n= 1,...,N. (5)
On the other hand, to make the proposed neural network
architecture scale adaptable to the number of antennas M, we
represent the input features corresponding to Yby vectorizing
the sample covariance matrix C/definesYYH/M:
xin
N+1=/bracketleftBig
ℜ{vec(C)}T,ℑ{vec(C)}T/bracketrightBigT
∈R2L2
p, (6)
6
whose dimension is independent of M. In section V , simula-
tion results will be provided to demonstrate the generaliza bility
with respect to different numbers of antennas M.
Given the input features/braceleftbig
xin
n/bracerightbigN+1
n=1, the initial embedding
layer applies linear projections to produce the initial emb ed-
dings. Let ddenote the dimension of the initial embeddings.
The linear projections are given by
x[0]
n=/braceleftBigg
Win
Bxin
n+bin
B,∀n= 1,...,N,
Win
Yxin
N+1+bin
Y, n=N+1,(7)
whereWin
B∈Rd×2Lpandbin
B∈Rdare the parameters for
projecting the input features/braceleftbig
xin
n/bracerightbigN
n=1, whileWin
Y∈Rd×2L2
p
andbin
Y∈Rdare the parameters for projecting the input
featurexin
N+1. In (7), the same set of parameters/braceleftbig
Win
B,bin
B/bracerightbig
is
shared among all devices’ pilots, so that the initial embedd ing
layer is scale adaptable to the number of devices in the actua l
deployment. Furthermore, the input feature corresponding to
the received signal is processed heterogeneously by anothe r
set of parameters/braceleftbig
Win
Y,bin
Y/bracerightbig
. The obtained initial embeddings/braceleftBig
x[0]
n/bracerightBigN+1
n=1from (7) are subsequently passed to Lencoding
layers as follows.
2) Encoding Layers
In each encoding layer l∈ {1,...,L}, we adopt the general
transformer encoding layer structure in Fig. 1. However, th e
architectures of MHA, FF, and normalization blocks in this
work are different from that of the standard transformer, wh ere
all the inputs in a particular layer are processed using the
same set of parameters. In contrast, since the device pilots
and the received signal have different physical meanings, w e
use one set of parameters to process the inputs/braceleftBig
x[l−1]
n/bracerightBigN
n=1
(corresponding to the device pilots), and we process x[l−1]
N+1
(corresponding to the received signal) heterogeneously us ing
another set of parameters.
Speciﬁcally, the computation in the l-th encoding layer is
described by (8) and (9), where MHAl
Band MHAl
Ydenote the
MHA computations, FFl
Band FFl
Ydenote the component-wise
FF computations, BNl
Band BNl
Yrepresent the batch normal-
ization (BN) steps [43], and the plus signs represent the ski p-
connections. The superscript lindicates that different layers
do not share parameters, while the superscripts B and Y mean
that the representations corresponding to the device pilot s
and the received signal are computed heterogeneously. In (8 ),
x[l−1]
n andx[l−1]
N+1are put outside of the set/braceleftBig
x[l−1]
j/bracerightBigN
j=1,j/ne}ationslash=n,
which implies that each x[l−1]
j is processed in the same way,
whilex[l−1]
n andx[l−1]
N+1are processed in a different way
from/braceleftBig
x[l−1]
j/bracerightBigN
j=1,j/ne}ationslash=n. Next, we explain the computations
of MHA B, MHA Y, FF B, FF Y, BN B, and BN Yin detail. For
notational simplicity, we omit the superscript with respec t to
lin the following descriptions.
a) MHA Computations: First, we present the MHA com-
putations in (8), where we use Tattention heads to extract
the relevance among the input components (see Fig. 2(a)).
To describe each attention head, we deﬁne six sets of pa-
rameters Wq
B,t∈Rd′×d,Wq
Y,t∈Rd′×d,Wk
B,t∈Rd′×d,Wk
Y,t∈Rd′×d,Wv
B,t∈Rd′×d, andWv
Y,t∈Rd′×d, whered′
is the dimension of each attention space and t∈ {1,...,T}.
For thet-th attention head, it computes a query qn,t, a key
kn,t, and a value vn,tfor eachxn(see Fig. 2(b)):
qn,t=/braceleftBigg
Wq
B,txn,∀n= 1,...,N,
Wq
Y,txN+1, n=N+1,(10)
kn,t=/braceleftBigg
Wk
B,txn,∀n= 1,...,N,
Wk
Y,txN+1, n=N+1,(11)
vn,t=/braceleftBigg
Wv
B,txn,∀n= 1,...,N,
Wv
Y,txN+1, n=N+1,(12)
where the heterogeneity is reﬂected in using parame-
ters/braceleftBig
Wq
B,t,Wk
B,t,Wv
B,t/bracerightBigT
t=1to project {xn}N
n=1(cor-
responding to the device pilots) and using parameters/braceleftBig
Wq
Y,t,Wk
Y,t,Wv
Y,t/bracerightBigT
t=1to project xN+1(corresponding to
the received signal) to different attention spaces. Then, e ach
attention head computes an attention compatibility αnjtfor
evaluating how much xnis related to xj:
αn,j,t=qT
n,tkj,t√
d′,∀n= 1,...,N+1,
∀j= 1,...,N+1,∀t= 1,...,T, (13)
and the corresponding attention weight is computed by nor-
malizing αn,j,t in[0,1]:
βn,j,t=eαn,j,t
/summationtextN+1
j′=1eαn,j′,t,∀n= 1,...,N+1,
∀j= 1,...,N+1,∀t= 1,...,T. (14)
With the attention weight βn,j,t scoring the relevance between
xnandxj, the attention value of xnat thet-th attention head
is computed as a weighted sum1:
x′
n,t=N+1/summationdisplay
j=1βn,j,tvj,t,∀n= 1,...,N+1. (15)
Finally, by combining the attention values from Tat-
tention heads with parameters/braceleftBig
Wo
B,t∈Rd×d′/bracerightBigT
t=1and
/braceleftBig
Wo
Y,t∈Rd×d′/bracerightBigT
t=1,/braceleftbig
x′
n,t/bracerightbigN+1
n=1are projected back to d-
dimensional vectors and we obtain the MHA computation
results (see Fig. 2(a)):
MHA B/parenleftBig
xn,{xj}N
j=1,j/ne}ationslash=n,xN+1/parenrightBig
=T/summationdisplay
t=1Wo
B,tx′
n,t,
∀n= 1,...,N, (16)
MHA Y/parenleftBig
xN+1,{xj}N
j=1/parenrightBig
=T/summationdisplay
t=1Wo
Y,tx′
N+1,t,(17)
where (16) and (17) correspond to the projections for the
device pilots and the received signal, respectively. Notic e that
xnandxN+1are put outside of{xj}N
j=1,j/ne}ationslash=nin (16), because
1Notice that the summation of (15) is taken over jrather than n. Therefore,
x′
n,tserves as the attention value at the t-th attention head corresponding to
xn.
7
ˆx[l]
n=

BNl
B/parenleftbigg
x[l−1]
n+MHAl
B/parenleftbigg
x[l−1]
n,/braceleftBig
x[l−1]
j/bracerightBigN
j=1,j/ne}ationslash=n,x[l−1]
N+1/parenrightbigg/parenrightbigg
,∀n= 1,...,N,
BNl
Y/parenleftbigg
x[l−1]
N+1+MHAl
Y/parenleftbigg
x[l−1]
N+1,/braceleftBig
x[l−1]
j/bracerightBigN
j=1/parenrightbigg/parenrightbigg
, n =N+1,(8)
x[l]
n=

BNl
B/parenleftBig
ˆx[l]
n+FFl
B/parenleftBig
ˆx[l]
n/parenrightBig/parenrightBig
, ∀n= 1,...,N,
BNl
Y/parenleftBig
ˆx[l]
N+1+FFl
Y/parenleftBig
ˆx[l]
N+1/parenrightBig/parenrightBig
, n=N+1.(9)
(a) The overall architecture of the MHA block.
(b) The architecture of attention head tof the MHA block.
Fig. 2. The architecture of the MHA block.
x′
n,tis computed by processing each xjusing/braceleftbig
Wk
B,t,Wv
B,t/bracerightbig
in the same way, while by processing xnandxN+1using /braceleftBig
Wq
B,t,Wk
B,t,Wv
B,t/bracerightBig
and/braceleftbig
Wk
Y,t,Wv
Y,t/bracerightbig
, respectively.
b) FF Computations: Next, we present the computations of
FFBand FF Yin (9), which adopt a two-layer MLP with a
df-dimensional hidden layer using the ReLu activation:
FFB(ˆxn) =Wf
B,2ReLu/parenleftbig
Wf
B,1ˆxn+bf
B,1/parenrightbig
+bf
B,2,
∀n= 1,...,N, (18)
FFY(ˆxN+1) =Wf
Y,2ReLu/parenleftbig
Wf
Y,1ˆxN+1+bf
Y,1/parenrightbig
+bf
Y,2,(19)
where{ˆxn}N+1
n=1is the output of (8), and Wf
B,1∈Rdf×d,
bf
B,1∈Rdf,Wf
B,2∈Rd×df,bf
B,2∈Rd,Wf
Y,1∈Rdf×d,
bf
Y,1∈Rdf,Wf
Y,2∈Rd×df, andbf
Y,2∈Rdare the parameters
to be optimized during the training procedure. In (18)-(19) ,
the heterogeneity is maintained since we use the same setof parameters/braceleftbig
Wf
B,1,bf
B,1,Wf
B,2,bf
B,2/bracerightbig
to process {ˆxn}N
n=1
(corresponding to the device pilots), while we process ˆxN+1
(corresponding to the received signal) by another set of pa-
rameters/braceleftbig
Wf
Y,1,bf
Y,1,Wf
Y,2,bf
Y,2/bracerightbig
.
c) BN Computations: For the BN computations in (8)
and (9), it computes the statistics over a batch of training
samples. Speciﬁcally, let/braceleftBig
˜x(i)
n∈Rd/bracerightBigIb
i=1denote a mini-batch
of training samples for BN computation. The BN statistics ar e
calculated as
νn=1
IbIb/summationdisplay
i=1˜x(i)
n,∀n= 1,...,N+1, (20)
Γn=/parenleftBigg
1
IbIb/summationdisplay
i=1Λ(i)
n/parenrightBigg1
2
,∀n= 1,...,N+1,(21)
8
whereΛ(i)
nis a diagonal matrix with the diagonal being/parenleftBig
˜x(i)
n−νn/parenrightBig
⊙/parenleftBig
˜x(i)
n−νn/parenrightBig
. Then, the normalization results
corresponding to the device pilots and the received signal a re
respectively given by
BN B/parenleftBig
˜x(i)
n/parenrightBig
=wbn
B⊙/parenleftBig
Γ−1
n/parenleftBig
˜x(i)
n−νn/parenrightBig/parenrightBig
+bbn
B,
∀n= 1,...,N, (22)
BN Y/parenleftBig
˜x(i)
N+1/parenrightBig
=wbn
Y⊙/parenleftBig
Γ−1
N+1/parenleftBig
˜x(i)
N+1−νN+1/parenrightBig/parenrightBig
+bbn
Y,(23)
wherewbn
B∈Rd,bbn
B∈Rd,wbn
Y∈Rd, andbbn
Y∈Rdare the
parameters to be optimized during the training procedure.
3) Decoding Layer
After the Lencoding layers, the produced hidden repre-
sentations/braceleftBig
x[L]
n/bracerightBigN+1
n=1are further passed to a decoding layer
to output the ﬁnal mapping result. The proposed decoding
layer consists of a contextual block and an output block. The
contextual block applies an MHA block to compute a context
vectorxc, which is a weighted sum of the components in/braceleftBig
x[L]
n/bracerightBigN+1
n=1:
xc=MHA C/parenleftbigg
x[L]
N+1,/braceleftBig
x[L]
n/bracerightBigN
n=1/parenrightbigg
, (24)
where MHA Cis similar to MHA Yin (17), but using dif-
ferent parameters Wq,c
t∈Rd′×d,Wk,c
B,t∈Rd′×d,Wk,c
Y,t∈
Rd′×d,Wv,c
B,t∈Rd′×d,Wv,c
Y,t∈Rd′×d,Wo,c
t∈Rd×d′,
t∈ {1,...,T}. In particular,/braceleftBig
Wk,c
B,t,Wv,c
B,t/bracerightBigT
t=1is used
to process/braceleftBig
x[L]
n/bracerightBigN
n=1(corresponding to the device pilots),
and/braceleftBig
Wq,c
t,Wk,c
Y,t,Wv,c
Y,t,Wo,c
t/bracerightBigT
t=1is used to process x[L]
N+1
(corresponding to the received signal). The speciﬁc expres sion
of MHA Cis shown in Appendix A. Each weight in xcreﬂects
the importance of each component in/braceleftBig
x[L]
n/bracerightBigN+1
n=1. Therefore,
the context vector xcintuitively decides which device pilots
to pay attention to based on the received signal.
Withxc, the output block decides the ﬁnal output, i.e.,
the active probability of each device, by scoring how well
the context vector xcand each x[L]
n,n∈ {1,...,N}match.
The relevance between the context vector xcand each x[L]
nis
evaluated by
αout
n=Ctanh/parenleftBigg
(xc)TWoutx[L]
n√
d/parenrightBigg
,∀n= 1,...,N, (25)
whereWout∈Rd×dis a parameter to be optimized during
the training procedure, and Cis a tuning hyperparameter
that controls αout
nin a reasonable range. Finally, the active
probability of each device is computed by normalizing αout
nin
[0,1]:
Pn=OUT/parenleftBig
xc,x[L]
n/parenrightBig
=1
1+e−αoutn,∀n= 1,...,N. (26)
C. Key Properties and Insights
The proposed heterogenous transformer for representing
f(·,·)has been speciﬁed as an initial embedding layer, Lencoding layer, and a decoding layer as shown in (5)-(26).
We examine some key properties of the proposed architecture
for the activity detection problem as follows.
a)Relevance Extraction Between Device Pilots and Received
Signal: Both the proposed encoding and decoding layers
are built on MHA as shown in (8) and (24), respectively.
The MHA computation is naturally a weighted sum as
shown in (15). The attention weight βn,j,t is the normal-
ization of the attention compatibility αn,j,t in (13), which
scores how well each pair of xnandxjmatch. Therefore,
with the attention weight βn,j,t reﬂecting the importance of
eachxjwith respect to xn, each encoding layer learns the
relevance among different device pilots and the received
signal. In the decoding layer, the captured relevance is
further used to compute the context vector xcin (24), which
ﬁnally extracts the relevance between each device pilot and
the received signal, and decides which device pilots to pay
attention to based on the extracted relevance.
b)Permutation Equivariant with Respect to Devices: As
shown in (7), the initial embeddings of the devices pilots/braceleftBig
x[0]
n/bracerightBigN
n=1are computed using the same parameters Win
B
andbin
B. Therefore, if any two device pilots biandbj
are exchanged, the initial embeddings x[0]
iandx[0]
jwill
be automatically exchanged as well. Similarly, since the
encoding layers produce/braceleftBig
x[L]
n/bracerightBigN
n=1through the same
computations MHAl
B, FFl
B, and BNl
B, and the decoding
layer produces {Pn}N
n=1by the same parameter Wout, we
can conclude that the ﬁnal output PiandPjwill also be
exchanged. This implies that the proposed architecture is
permutation equivariant with respect to devices.
c)Scale Adaptable and Generalizable to Different Numbers
of Devices: In all the layers of the proposed heterogeneous
transformer, the representations of different device pilo ts
are produced with the same architecture using the same set
of parameters. Therefore, the dimensions of parameters tha t
require to be optimized during the training procedure are
independent of the number of devices. This scale adaptabil-
ityempowers the whole architecture to be readily applied
to different numbers of devices, and hence generalizable to
different numbers of devices.
D. Learning Procedure
So far, we have presented the architecture and key propertie s
of the proposed heterogeneous transformer. Next, we show
thelearning procedure to optimize the parameters of heteroge-
neous transformer for device activity detection in Algorit hm 1,
which consists of a training procedure and a test procedure. As
shown in lines 8-10, we adopt a learning rate decay strategy
to accelerate the training procedure [44]. In particular, t he
learning rate ηis decreased by a factor of βafterNdtraining
epochs. During the test procedure, we adopt two metrics to
assess the performance of device activity detection, i.e., the
probability of missed detection (PM) and the probability of
false alarm (PF) [3]–[6], which are respectively given by
PM= 1−/summationtextN
n=1an˜an/summationtextN
n=1˜an, (27)
9
Algorithm 1 Learning Procedure for Activity Detection
1:Training Procedure:
2:Input: number of epochs Ne, steps per epoch Ns, batch size Nb, and
learning rate decay epoch Ndand factor β
3:Initialize: learning rate η
4:forepoch= 1,...,N e
5: forstep= 1,...,N s
6: a) Generate a batch of Nbsamples
b) Compute the mini-batch gradient of the loss function (4) o ver
the parameters of heterogeneous transformer
c) Update the parameters by a gradient descent step using the
Adam optimizer with learning rate η
7: end
8: ifepoch==Nd
9: η←βη
10: end
11: end
12: Output: heterogeneous transformer with optimized parameters
13: Test Procedure:
14: Input:Nttest samples
15: Compute the output of the trained heterogeneous transfo rmerP(i)
n,n=
1,...,N ,i= 1,...,N t
16: Determine the activity status of each device as a(i)
n=I(P(i)
n> ξ),
n= 1,...,N ,i= 1,...,N t
17: Output: (PM, PF) pairs under different ξ
PF=/summationtextN
n=1an(1−˜an)
/summationtextN
n=1(1−˜an). (28)
In (27) and (28), ˜anis the ground-truth device activity, and
the detected activity status an=I(Pn> ξ), whereξis a
threshold that continually increases in [0,1]to realize a trade-
off between PM and PF.
V. S IMULATION RESULTS
In this section, simulation results are provided for demon-
strating the beneﬁts of the proposed learning based method,
which adopts the problem formulation for learning in Sec-
tion III and the heterogeneous transformer architecture in
Section IV .
A. Simulation Setting
We consider an uplink MIMO system with IoT devices uni-
formly distributed within a cell with a 250-meter radius, and
the ratio of the active devices to the total devices is 0.1. Both
the training and test samples are generated as follows. The
pilot sequence of each device is an independently generated
complex Gaussian distributed vector with i.i.d. elements a nd
each element is with zero mean and unit variance. The path-
loss of the channel is 128.1+37.6log10Dnin dB, where Dnis
the distance in kilometers between the n-th device and the BS.
In order to reduce the channel gain variations among differe nt
devices especially for the cell-edge devices, the transmit power
of each device is controlled as pk=pmaxgmin
gk[8], where pmax
is the maximum transmit power and gminis the minimum
large-scale channel gain in the cell. The background Gaussian
noise power at the BS is −169 dBm/Hz over a 10MHz
bandwidth. The received signal is generated according to (1),
where the activity of each device is generated from Bernoull i
distribution and used as the ground-truth label for the trai ning
samples.For the proposed heterogeneous transformer architecture,
the number of encoding layers is set as L= 5, the encoding
size of each encoding layer is d= 128 , the number of attention
heads in the MHA block is T= 8, the dimension of each
attention space is d′= 32 , and the size of hidden layer of the
component-wise FF block is df= 512 . In the decoding layer,
the tuning hyperparameter that controls the result of (25) i s
set asC= 10 .
During the training procedure, we apply the Adam optimizer
in the framework of Pytorch [40] to update the parameters of
the proposed heterogeneous transformer. In Algorithm 1, th e
number of training epochs is set as Ne= 100 . The number of
gradient descent steps in each epoch is Ns= 5000 and each
step is updated using a batch of Nb= 256 training samples.
Consequently, the total number of training samples is 1.28×
106. The learning rate is initialized as η= 10−4. For the
learning rate decay strategy, we decrease ηby a factor of β=
0.1afterNd= 90 and97training epochs, respectively. After
the training procedure, the activity detection performanc e is
evaluated over Nt= 5000 test samples in terms of PM and
PF as shown in (27) and (28).
B. Performance Evaluation
First, we show the training loss of Algorithm 1 for updating
the parameters of heterogeneous transformer. During the tr ain-
ing procedure, the number of devices is set as N= 100 and
the maximum transmit power is pmax= 23 dBm. The length
of each pilot sequence is set as Lp= 7or8, and the number of
BS-antennas is set as M= 32 or64, respectively. The training
losses versus epochs under different settings are illustra ted
in Fig. 3. It can be seen that the training losses generally
decrease as the training epoch increases. In particular, du e
to the learning rate decay, the training losses have a sudden
decrease in the 90-th training epoch, which demonstrates the
effectiveness of learning rate decay in speeding up the trai ning
procedure. We also observe from Fig. 3 that the training
performance can be improved by increasing the the length of
pilot sequence or equipping with a larger number of antennas
at the BS.
Then, we test the corresponding activity detection perfor-
mance of the well-trained heterogeneous transformer in ter ms
of PM and PF. For comparison, we also provide the simulation
results of state-of-the-art covariance approach , which solves
problem (3) with the coordinate descent method [22], [23].
The PM-PF trade-offs are shown in Fig. 4, where the proposed
learning based method using heterogeneous transformer is
termed as HT, while state-of-the-art covariance approach i s
termed as Covariance. It can be seen that the proposed method
always achieves better PM-PF trade-offs than that of the
covariance approach under different settings. In particular,
whenLp= 8andM= 64 , the PM of the proposed method is
about10times lower than that of the covariance approach
under the same PF. This is because the proposed method
utilizes neural network to mimic the optimal mapping functi on
directly from the ground-truth training labels, which prov ide
the opportunity to achieve better detection performance th an
the covariance approach that only ﬁnds a stationary point of
a relaxed problem of (3).
10
0 20 40 60 80 100
Training epoch10-310-210-1Training loss
Fig. 3. The training loss of Algorithm 1 for updating the para meters of
heterogeneous transformer.
10-410-310-210-1100
PM10-410-310-210-1100PF
Fig. 4. The test performance comparison with state-of-the- art covariance
approach in terms of PM and PF.
To further demonstrate the beneﬁts of the properties in-
corporated in the proposed heterogeneous transformer, we
also perform the same tasks using MLPs for comparison. In
particular, we train different MLPs with 4-10hidden layers and
256-1024 hidden nodes using the ReLu activation. However,
without a custom design, none of the MLPs can provide a
better detection performance than even a random guess (and
hence are omitted in Fig. 4). This veriﬁes the performance
gains of the proposed heterogeneous transformer architect ure
for the activity detection problem.
The average computation times of the approaches over the
test samples are compared in Table I, where the covariance
approach is termed as Covariance, and the proposed method
is termed as either HT CPU or HT GPU, depending on whether
CPU or GPU is used. In particular, both the covariance
approach and HT CPU are run on Intel(R) Xeon(R) CPU @
2.20GHz, while HT GPU is run on Tesla T4. We can see that
the average computation time of HT CPU is about 100times
shorter than that of the covariance approach . Moreover, HTTABLE I
AVERAGE COMPUTATION TIME COMPARISON AMONG DIFFERENT
APPROACHES
Covariance HT CPU HT GPU
Lp= 7,M= 32 6 .34×10−1s6.31×10−3s1.60×10−6s
Lp= 8,M= 32 6 .24×10−1s6.32×10−3s1.63×10−6s
Lp= 8,M= 64 6 .33×10−1s6.55×10−3s2.13×10−6s
100 110 120 130 140 150
Numer of device10-310-210-1PM
(a) PM versus number of devices.
100 110 120 130 140 150
Number of device10-610-410-2100Computation time in seconds
(b) Average computation time versus number of devices.
Fig. 5. Generalization to different numbers of devices.
GPU achieves a remarkable running speed, with a running time
over105times shorter than that of the covariance approach.
This demonstrates the superiority of the proposed learning
based method for real-time implementation compared with th e
covariance approach that is based on iterative computation s.
C. Generalizability
Next, we demonstrate the generalizability of the proposed
method with respect to different numbers of devices, BS-
antennas, and different SNRs. Unless otherwise speciﬁed, t he
length of pilot sequence is set as Lp= 8 in the following
simulations. We begin by training a heterogeneous transfor mer,
11
where the device number of the training samples is ﬁxed as
N= 100 . However, we test the activity detection performance
under different device numbers from 100to150. The number
of BS-antennas is set as M= 64 and the maximum transmit
power is pmax= 23 dBm. Due to the trade-off between PM
and PF, we provide the PM when PF = PM and PF = 2PM
respectively, by appropriately setting the threshold ξ.In the
following ﬁgures, “Covariance, PF = PM” and “Covariance,
PF =2PM” denote the PM of the covariance approach when
PF = PM and PF = 2PM respectively, while “HT, PF = PM”
and “HT, PF = 2PM” represent the PM of the proposed
method when PF = PM and PF = 2PM respectively. The
activity detection performance and average computation ti me
versus number of devices are illustrated in Fig. 5(a) and
Fig. 5(b), respectively. We can see from Fig. 5(a) that while
the activity detection performances of different approach es
become worse as the number of devices Nincreases, the PM
of the proposed method is still comparable with that of the
covariance approach when Nis increased from 100 to150.
This demonstrates that the proposed method generalizes wel l
to different numbers of devices. On the other hand, Fig. 5(b)
shows that as Nincreases, the average computation times of
both the covariance approach and the proposed method on
CPU are linearly increased. However, due to the the parallel
computation of GPU, the average computation time of the
proposed method on GPU is nearly a constant, i.e., about
2×10−6second, which is much shorter than that of other
approaches.
We further demonstrate the generalizability of the propose d
method with respect to different numbers of BS-antennas. To
this end, we train a heterogeneous transformer by ﬁxing the
number of BS-antennas as M= 32 , and then test its activity
detection performance under different numbers of BS-anten nas
from32to128. The number of devices is N= 100 and the
maximum transmit power is pmax= 23 dBm. The perfor-
mance comparisons in terms of PM and average computation
time are illustrated in Fig. 6(a) and Fig. 6(b), respectivel y.
Figure 6(a) shows that as the number of BS-antennas increase s,
the proposed method always achieves much lower PM than
that of the covariance approach. Although the heterogeneous
transformer is trained under M= 32 , when we test the detec-
tion performance under M= 128 , the PM of the proposed
method isstill2times lower than that of the covariance
approach for both PF = PM and PF = 2PM. This demonstrates
that the proposed method generalizes well to larger numbers
of BS-antennas. On the other hand, Fig. 6(b) shows that the
average computation time of the proposed method on CPU is
about100times shorter than that of the covariance approach,
and the proposed method on GPU even achieves a 105times
faster running speed than that of the covariance approach.
Finally, the generalizability with respect to different SN Rs
is demonstrated in Fig. 7. The SNR of the training samples is
ﬁxed by setting the maximum transmit power as pmax= 23
dBm, while the activity detection performance is tested und er
different SNRs with pmax varying from 11to23dBm. The
numbers of devices and BS-antennas are N= 100 and
M= 32 , respectively. As shown in Fig. 7(a), when the
SNR decreases as the transmit power becomes lower, the32 48 64 80 96 112 128
Number of BS-antennas10-410-310-2PM
(a) PM versus number of BS-antennas.
32 48 64 80 96 112 128
Number of BS-antennas10-610-510-410-310-210-1100Computation time in seconds
(b) Average computation time versus number of BS-antennas.
Fig. 6. Generalization to different numbers of BS-antennas .
PM of different approaches becomes higher. However, the
proposed method still achieves much lower PM than that
of the covariance approach under different SNRs for both
PF = PM and PF = 2PM. Therefore, the proposed method
generalizes well to different SNRs. Besides the superiorit y
of activity detection performance, Fig. 7(b) shows that the
proposed method takes signiﬁcantly shorter computation ti me
than that of the covariance approach under different SNRs.
VI. C ONCLUSIONS
This paper proposed a deep learning based method with a
customized heterogeneous transformer architecture for de vice
activity detection. By adopting an attention mechanism in t he
neural network architecture design, the proposed heteroge -
neous transformer was incorporated with desired propertie s
of the activity detection task. Speciﬁcally, the proposed a rchi-
tecture is able to extract the relevance between device pilo ts
and received signal, is permutation equivariant with respe ct to
devices, and is scale adaptable to different numbers of devi ces.
Simulation results showed that the proposed learning based
12
11 13 15 17 19 21 23
Maximum transmit power in dBm0.010.0150.020.0250.030.035PM
(a) PM versus maximum transmit power.
11 13 15 17 19 21 23
Maximum transmit power in dBm10-610-510-410-310-210-1100Computation time in seconds
(b) Average computation time versus maximum transmit power .
Fig. 7. Generalization to different SNRs.
method achieves much better activity detection performanc e
and takes remarkably shorter computation time than state-o f-
the-art covariance approach. Moreover, the proposed metho d
was demonstrated to generalize well to different numbers of
devices, BS-antennas, and different SNRs.
APPENDIX A
THE EXPRESSION OF MHA C
Deﬁne ﬁve sets of parameters Wq,c
t∈Rd′×d,Wk,c
B,t∈
Rd′×d,Wk,c
Y,t∈Rd′×d,Wv,c
B,t∈Rd′×d, andWv,c
Y,t∈Rd′×d,
whered′is the dimension of each attention space and t∈
{1,...,T}. Then, we compute a query qc
tforx[L]
N+1at the
t-th attention head:
qc
t=Wq,c
tx[L]
N+1. (A.1)
The key and value corresponding to each x[L]
nare respectively
computed as
kc
n,t=/braceleftBigg
Wk,c
B,tx[L]
n,∀n= 1,...,N,
Wk,c
Y,tx[L]
N+1, n=N+1,(A.2)vc
n,t=/braceleftBigg
Wv,c
B,tx[L]
n,∀n= 1,...,N,
Wv,c
Y,tx[L]
N+1, n=N+1.(A.3)
To evaluate the relevance between x[L]
N+1and each component
of/braceleftBig
x[L]
n/bracerightBigN+1
n=1, we compute a compatibility αc
n,tusing the
queryqc
tand the key kc
n,t:
αc
n,t=(qc
t)Tkc
n,t√
d′,∀n= 1,...,N+1,∀t= 1,...,T,
(A.4)
and the corresponding attention weight is computed by nor-
malizing αc
n,tin[0,1]:
βc
n,t=eαc
n,t
/summationtextN+1
j=1eαc
j,t,∀n= 1,...,N+1,∀t= 1,...,T.
(A.5)
With the attention weight βc
n,tscoring the relevance between
x[L]
N+1and each component of/braceleftBig
x[L]
n/bracerightBigN+1
n=1, the attention value
ofx[L]
N+1at thet-th attention head is computed as
x′
t=N+1/summationdisplay
n=1βc
n,tvc
n,t. (A.6)
The expression of MHA Cis ﬁnally given by a combination of
theTattention values:
MHA C/parenleftbigg
x[L]
N+1,/braceleftBig
x[L]
n/bracerightBigN
n=1/parenrightbigg
=T/summationdisplay
t=1Wo,c
tx′
t, (A.7)
whereWo,c
t∈Rd×d′is the parameter for projecting back to
ad-dimensional vector.
REFERENCES
[1] C. Bockelmann, N. Pratas, H. Nikopour, K. Au, T. Svensson , C. Ste-
fanovic, P. Popovski, and A. Dekorsy, “Massive machine-typ e commu-
nications in 5G: Physical and MAC-layer solutions,” IEEE Commun.
Mag. , vol. 54, no. 9, pp. 59–65, Sep. 2016.
[2] Z. Dawy, W. Saad, A. Ghosh, J. G. Andrews, and E. Yaacoub, “ To-
ward massive machine type cellular communications,” IEEE Wireless
Commun. Mag. , vol. 24, no. 1, pp. 120–128, Feb. 2017.
[3] L. Liu, E. G. Larsson, W. Yu, P. Popovski, ˇC. Stefanovi´ c, and E. de Car-
valho, “Sparse signal processing for grant-free massive co nnectivity: A
future paradigm for random access protocols in the internet of things,”
IEEE Signal Process. Mag. , vol. 35, no. 5, pp. 88–99, Sep. 2018.
[4] L. Liu and W. Yu, “Massive connectivity with massive MIMO -Part I:
Device activity detection and channel estimation,” IEEE Trans. Signal
Process. , vol. 66, no. 11, pp. 2933–2946, Jun. 2018.
[5] L. Liu and W. Yu, “Massive connectivity with massive MIMO –Part II:
Achievable rate characterization,” IEEE Trans. Signal Process. , vol. 66,
no. 11, pp. 2947–2959, Jun. 2018.
[6] Z. Chen, F. Sohrabi, and W. Yu, “Sparse activity detectio n for massive
connectivity,” IEEE Trans. Signal Process. , vol. 66, no. 7, pp. 1890–
1904, Apr. 2018.
[7] Z. Sun, Z. Wei, L. Yang, J. Yuan, X. Cheng, and L. Wan, “Expl oiting
transmission control for joint user identiﬁcation and chan nel estimation
in massive connectivity,” IEEE Trans. Commun. , vol. 67, no. 9, pp.
6311–6326, Sep. 2019.
[8] K. Senel and E. G. Larsson, “Grant-free massive MTC-enab led massive
MIMO: A compressive sensing approach,” IEEE Trans. Commun. ,
vol. 66, no. 12, pp. 6164–6175, Dec. 2018.
[9] S. Jiang, X. Yuan, X. Wang, C. Xu, and W. Yu, “Joint user ide ntiﬁcation,
channel estimation, and signal detection for grant-free NO MA,” IEEE
Trans. Wireless Commun. , vol. 19, no. 10, pp. 6960–6976, Oct. 2020.
13
[10] Z. Utkovski, O. Simeone, T. Dimitrova, and P. Popovski, “Random
access in C-RAN for user activity detection with limited-ca pacity
fronthaul,” IEEE Signal Process. Lett. , vol. 24, no. 1, pp. 17–21, Jan.
2017.
[11] Z. Chen, F. Sohrabi, and W. Yu, “Multi-cell sparse activ ity detection for
massive random access: Massive MIMO versus cooperative MIM O,”
IEEE Trans. Wireless Commun. , vol. 18, no. 8, pp. 4060–4074, Aug.
2019.
[12] M. Ke, Z. Gao, Y . Wu, X. Gao, and K.-K. Wong, “Massive acce ss in
cell-free massive MIMO-based internet of things: Cloud com puting and
edge computing paradigms,” IEEE J. Sel. Areas Commun. , vol. 39, no. 3,
pp. 756–772, Mar. 2021.
[13] X. Xu, X. Rao, and V . K. Lau, “Active user detection and ch annel
estimation in uplink CRAN systems,” in IEEE ICC , 2015.
[14] J. Ahn, B. Shim, and K. B. Lee, “EP-based joint active use r detection and
channel estimation for massive machine-type communicatio ns,” IEEE
Trans. Commun. , vol. 67, no. 7, pp. 5178–5189, Jul. 2019.
[15] X. Liu, Y . Shi, J. Zhang, and K. B. Letaief, “Massive CSI a cquisition
for dense cloud-RANs with spatial-temporal dynamics,” IEEE Trans.
Wireless Commun. , vol. 17, no. 4, pp. 2557–2570, Apr. 2018.
[16] Q. He, T. Q. S. Quek, Z. Chen, Q. Zhang, and S. Li, “Compres sive chan-
nel estimation and multi-user detection in C-RAN with low-c omplexity
methods,” IEEE Trans. Wireless Commun. , vol. 17, no. 6, pp. 3931–
3944, Jun. 2018.
[17] Y . Li, M. Xia, and Y .-C. Wu, “Activity detection for mass ive connectivity
under frequency offsets via ﬁrst-order algorithms,” IEEE Trans. Wireless
Commun. , vol. 18, no. 3, pp. 1988–2002, Mar. 2019.
[18] X. Shao, X. Chen, and R. Jia, “A dimension reduction-bas ed joint
activity detection and channel estimation algorithm for ma ssive access,”
IEEE Trans. Signal Process. , vol. 68, no. 1, pp. 420–435, Jan. 2020.
[19] S. Haghighatshoar, P. Jung, and G. Caire, “Improved sca ling law for
activity detection in massive MIMO systems,” in IEEE ISIT , 2018.
[20] X. Shao, X. Chen, D. W. K. Ng, C. Zhong, and Z. Zhang, “Coop era-
tive activity detection: Sourced and unsourced massive ran dom access
paradigms,” IEEE Trans. Signal Process. , vol. 68, pp. 6578–6593, 2020.
[21] D. Jiang and Y . Cui, “ML estimation and MAP estimation fo r device
activities in grant-free random access with interference, ” inIEEE WCNC ,
2020.
[22] Z. Chen, F. Sohrabi, and W. Yu, “Sparse activity detecti on in multi-
cell massive MIMO exploiting channel large-scale fading,” IEEE Trans.
Signal Process. , vol. 69, pp. 3768–3781, 2021.
[23] U. K. Ganesan, E. Bj¨ ornson, and E. G. Larsson, “Cluster ing based
activity detection algorithms for grant-free random acces s in cell-free
massive MIMO,” IEEE Trans. Commun. , vol. 69, no. 11, pp. 7520–
7530, Nov. 2021.
[24] A. Fengler, S. Haghighatshoar, P. Jung, and G. Caire, “N on-Bayesian ac-
tivity detection, large-scale fading coefﬁcient estimati on, and unsourced
random access with a massive MIMO receiver,” IEEE Trans. Inf. Theory ,
vol. 67, no. 5, pp. 2925–2951, May 2021.
[25] Z. Chen, F. Sohrabi, Y .-F. Liu, and W. Yu, “Phase transit ion analysis for
covariance based massive random access with massive MIMO,” IEEE
Trans. Inf. Theory , to appear 2021, doi: 10.1109/TIT.2021.3132397.
[26] H. Sun, X. Chen, Q. Shi, M. Hong, X. Fu, and N. D. Sidiropou los,
“Learning to optimize: Training deep neural networks for in terference
management,” IEEE Trans. Signal Process. , vol. 66, no. 20, pp. 5438–
5453, Oct. 2018.
[27] M. Zhu, T. Chang, and M. Hong, “Learning to beamform in he t-
erogeneous massive MIMO networks,” 2020. [Online]. Availa ble:
https://arxiv.org/abs/2011.03971.
[28] Y . Shen, Y . Shi, J. Zhang, and K. B. Letaief, “Graph neura l networks for
scalable radio resource management: Architecture design a nd theoretical
analysis,” IEEE J. Sel. Areas Commun. , vol. 39, no. 1, pp. 101–115, Jan.
2021.
[29] J. Guo and C. Yang, “Learning power allocation for multi -cell-multi-
user systems with heterogeneous graph neural network,” IEEE Trans.
Wireless Commun. , to appear 2021, doi: 10.1109/TWC.2021.3100133.
[30] K. Hornik, M. Stinchcombe, and H. White, “Multilayer fe edforward
networks are universal approximators,” Neural Networks , vol. 2, no. 5,
pp. 359–366, 1989.
[31] M. Eisen and A. Ribeiro, “Optimal wireless resource all ocation with
random edge graph neural networks,” IEEE Trans. Signal Process. ,
vol. 68, pp. 2977–2991, 2020.
[32] F. Sohrabi, K. M. Attiah, and W. Yu, “Deep learning for di stributed
channel feedback and multiuser precoding in FDD massive MIM O,”
IEEE Trans. Wireless Commun. , vol. 20, no. 7, pp. 4044–4057, Jul.
2021.[33] T. Jiang, H. V . Cheng, and W. Yu, “Learning to reﬂect and t o beamform
for intelligent reﬂecting surface with implicit channel es timation,” IEEE
J. Sel. Areas Commun. , vol. 39, no. 7, pp. 1931–1945, Jul. 2021.
[34] Y . Li, Z. Chen, G. Liu, Y .-C. Wu, and K.-K. Wong, “Learnin g to
construct nested polar codes: An attention-based set-to-e lement model,”
IEEE Commun. Lett. , vol. 25, no. 12, pp. 3898–3902, Dec. 2021.
[35] Y . Cui, S. Li, and W. Zhang, “Jointly sparse signal recov ery and support
recovery via deep learning with applications in MIMO-based grant-free
random access,” IEEE J. Sel. Areas Commun. , vol. 39, no. 3, pp. 788–
803, Mar. 2021.
[36] Y . Shi, H. Choi, Y . Shi, and Y . Zhou, “Algorithm unrollin g for massive
access via deep neural network with theoretical guarantee, ”IEEE Trans.
Wireless Commun. , to appear 2021, doi: 10.1109/TWC.2021.3100500.
[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jone s, A. N. Gomez,
Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances
in Neural Information Processing Systems , 2017.
[38] P.-T. de Boer, D. P. Kroese, S. Mannor, and R. Y . Rubinste in, “A tutorial
on the cross-entropy method,” Annals of Operations Research , vol. 134,
no. 1, pp. 19–67, Jan. 2005.
[39] D. Kingma and J. Ba, “Adam: A method for stochastic optim ization,”
inInternational Conference on Learning Representations , 2014.
[40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. C hanan,
T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A . Kopf,
E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner,
L. Fang, J. Bai, and S. Chintala, “Pytorch: An imperative sty le, high-
performance deep learning library,” in Advances in Neural Information
Processing Systems , 2019.
[41] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learni ng for image
recognition,” in IEEE CVPR , 2016.
[42] J. Ba, J. Kiros, and G. E. Hinton, “Layer normalization, ” 2016. [Online].
Available: https://arxiv.org/abs/1607.06450.
[43] S. Ioffe and C. Szegedy, “Batch normalization: Acceler ating deep
network training by reducing internal covariate shift,” in International
Conference on Machine Learning , 2015.
[44] K. You, M. Long, J. Wang, and M. I. Jordan, “How does learn ing
rate decay help modern neural networks?” 2019. [Online]. Av ailable:
https://arxiv.org/abs/1908.01878.

GuyE.Blelloch
Inthepast20yearstherehasbeentreftlen­
dousprogress indeveloping andanalyzing
parallel algorithftls. Researchers havedeveloped efficient
parallelalgorithms tosolvemostproblems forwhichefficient
sequential solutions areknown.Although someofthesealgo­
rithmsareefficientonlyinatheoretical framework, manyare
quiteefficientinpracticeorhavekeyideasthathavebeenused
inefficientimplementations. Thisresearchonparallelalgo­
rithmshasnotonlyimproved ourgeneralunderstanding ofpar­
allelismbutinseveralcaseshasledtoimprovements in
sequential algorithms. Unf:ortunately therehas
beenlesssuccess indeveloping goodlan­
guages f:orprograftlftling parallel algorithftls,
particularly languages thatarewellsuitedforteachingandpro-
totypingalgorithms. Therehasbeenalargegapbetweenlan­
guagesthataretoolowlevel,requiring specification ofmany
detailsthatobscurethemeaning ofthealgorithm, andlanguages
thataretoohighlevel,makingtheperformance implications of
variousconstructs unclear.Insequential computing manystan­
dardlanguages suchasCorPascaldoareasonable J·obofbridg­
ingthisgap,butinparallellanguages building suchabridge
hasbeensignificantly moredifficult.
COMMUNICATIONS 011THEACMMarch1996/Vol. 39,No.385

Figure2. Summing 16numbers onatree.Thetotal
depth(longest chainofdependencies) is4andthe
totalwork(number ofoperations) is15.Figure 1.AdiagramofaParallelRandom Access
Machine (PRAM).Itisassumed inthismodelthatall
theprocessors canaccessmemory locations inthe
sharedmemory simultaneously inunittime.WorkandDepth
Analyzing performance isakeypartofstudying algo­
rithms.Although suchanalysis isnotusedtopredictthe
exactrunning timeofanalgorithm onaparticular ma­
chine,itisimportant indetermining howtherunning
timegrowsasafunction oftheinputsize.Toanalyzeper­
formance, aformalmodelisneeded toaccount forthe
costs.Inparallel computing, themostcommon models
arebasedonasetofprocessors connected eitherbya
sharedmemory, asintheParallel Random AccessMa­
chines(PRAM) (seeFigure1),orthrough anetwork, as
withthehypercube orgridmodels.InsuchpTocessoT-based
models,performance iscalculated intermsofthenumber
ofinstruction cyclesacomputation takes(itsrunning
time)andisusuallyexpressed asafunction ofinputsize
andnumber ofprocessors.
Animportant advance inparallel computing wasthe
introduction ofthenotionofviTtualmodels.Avirtual
modelisaperformance modelthatdoesnotattempt to
represent anymachine thatwewouldactually buildbut
ratherisahigher-level modelthatcanbemapped onto
variousrealmachines. Forexample, thePRAMisoften
viewedasavirtualmodel[25].Fromthisviewpoint, itis
agreedthataPRAMcannotbebuiltdirectly, sinceinprac­
ticeitisunreasonable toassumethateveryprocessor can
accessasharedmemory inunittime.Instead, thePRAM
istreatedasavirtualmachine thatcanbemapped onto
morerealistic machines efficiently bysimulating multiple
processors ofthePRAMonasingleprocessor ofahost
machine. Thissimulation imposes someslowdown K,but
requires afactorofKfewerprocessors, sothetotalcost
(processor-time product) remains thesame.Theadvan­
tageofvirtualmodels overphysical machine models is
thattheycanbeeasiertoprogram.
Virtualmodelscanbetakenastepfurtherandusedto
defineperformance inmoreabstract measures thanjust
running timeonaparticular machine. Apairofsuchmea­
suresareworkanddepth: WOTkisdefined asthetotal
number ofoperations executed byacomputation, and
depthisdefined asthelongestchainofsequential depen­
denciesinthecomputation. Consider, forexample, sum­
ming16numbers usingabalanced binarytree(seeFigure
2).Theworkrequired bythiscomputation is15operations
(the15additions). Thedepthofthecomputation isfour
operations, sincethelongestchainofdependencies isthe
depthofthesummation tree-the sumsneedtobecalcu­
latedstarting attheleavesandgoingdownonelevelata
time.Ingeneral, summing nnumbers onabalanced tree
requires n-1workandlog2ndepth.Workisusually
viewed asameasure ofthetotalcostofacomputation
(integral ofneededresources overtime),andalsospecifies
therunning timeifthealgorithm isexecuted onasequen­
tialprocessor. Thedepthrepresents thebestpossiblerun­
ningtimeassuming anidealmachine withanunlimited
number ofprocessors.
Workanddepthhavebeenusedinformally formany
yearstodescribe theperformance ofparallelalgorithms
[23],especially whenteaching them[16,17].Theclaimis
thatitiseasiertodescribe, thinkabout,andanalyzealgo-4
28Work
Total15 4TotalDepthOurresearch involves developing aparallellanguage
thatisusefulforteaching aswellasforimplementing par­
allelalgorithms. Toachieve this,animportant goalhas
beentodevelop alanguage thatallowshigh-level descrip­
tionsofparallelalgorithms butalsohasawell-understood
mapping ontoaperformance model(i.e.,bridges the
gap).Basedonourresearch, webelievethatthefollowing
twofeatures areimportant forachieving thisgoal:
• Alanguage-based performance modelthatuseswOTk
anddepthratherthanamachine-based modelthatuses
"running time."
•Support fornesteddata-pamllel constructs. Thisisthe
abilitytoapplyafunction inparalleltoeachelementof
acollection ofdataandtheabilitytonestsuchparallel
calls.
Inthisarticlewedescribe thesefeatures andexplainwhy
theyareimportant forprogramming parallelalgorithms.
Tomaketheideasconcrete, wedescribe theprogram­
minglanguage NESL[5],whichwedesigned basedonthe
features, andgothrough severalexamples ofhowtopro­
gramandanalyzeparallelalgorithms usingthelanguage.
WehavebeenusingNESLforthreeyearsinundergradu­
ateandgraduate coursesonparallel algorithms [7].The
algorithms wecoverinthisarticlearerelatively straight­
forward. Manymorealgorithms canbefoundthrough
theWebversionofthisarticle(available athttp://web.scan­
dal.cs.cmu.edu/www/cacm.html).
86 March1996/Vol. 39,No.3COMMU ..ICA",IDN. OF'I'R._eM
mming
Scalar
Memory VectorMemory
I
ParallelVectorProcessorIprocedure SUM(V):
n=length(V);
fori=Itolog2n
begin
Yo=odd_elts(V);
Ve=even_elts( V);
V=vector_add(V o,Ve);
end
returnV
rithmsintermsofworkanddepththanintermsofrun­
ningtimeonaprocessor-based model(amodelbasedon
Pprocessors). Furthermore, workanddepthtogether tell
usalotaboutexpected performance onvariousmachines.
Wewillreturntothesepoints,butwefirstdescribe in
moredetailhowworkanddepthcanbeincorporated into
acomputational model.Therearebasically threeclasses
ofsuchmodels-circuit models, vectormachine models,
andlanguage-based models-and webrieflydescribe
each.
CircuitModels.Incircuitmodels,analgorithm isspeci­
fiedbydesigning acircuitoflogicgatestosolvetheprob­
lem.Thecircuitsarerestricted tohavenocycles.Forex­
ample,wecouldviewFigure2asacircuitinwhichthe
inputsareatthetop,each+isanaddercircuit,andeach
ofthelinesbetween addersisabundleofwires.Thefinal
sumisreturned atthebottom.Incircuitmodels, thecir­
cuitsize(number ofgates)corresponds towork,andthe
longestpathfromaninputtoanoutputcorresponds to
depth.Although foraparticular inputsizeonecouldbuild
acircuittoimplement analgorithm, ingeneral circuit
modelsareviewedasvirtualmodelsfromwhichthesize
anddepthofthedesignstellussometime abouttheper­
formance ofalgorithms onrealmachines. Assuch,the
modelshavebeenusedformanyyearstostudyvarious
theoretical aspectsofparallelism, forexample; toprove
thatcertainproblems arehardtosolveinparallel(see[17]
foranoverview). Although themodelsarewellsuitedfor
suchtheoretical analysis, theyarenotaconvenient model
forprogramming parallelalgorithms.
VectorMachine Models.Thefirstprogrammable ma­
chinemodelbasedonworkanddepthwastheVector
Random AccessMachine (VRAM) [4].TheVRAMmodel
isasequential random-access machine (RAM)extended
withasetofinstructions thatoperateonvectors(seeFig­
ure3).Eachlocationofthememory contains awholevec­
tor,andthevectorscanvaryinsizeduringthecomputa­
tion.Thevector instructions include elementwise
operations, suchasaddingthecorresponding elements of
twovectors,andaggregate operations, suchasextracting
elements fromonevectorbasedonanother vectorofindi­
ces.Thedepthofacomputation inaVRAMissimplythe
numberofinstructions executed bythemachine, andtheFigure5.AdiagramofavectorRandom Access
Machine IVRAMIandpseudocode forsumming n
numbers onthemachine. Thevectorprocessor
actsasaslavetothescalarprocessor. Thefunc­
tionsod<LeltsandevelLelts extracttheoddand
evenelements fromavector,respectively. The
function vector_add elementwise addstwovec­
tors.Oneachiteration throughtheloopthelength
ofthevectorVhalves.Thecodeassumes nisa
powerof2,butitisnothardtogeneralize thecode
toworkWithanyn.Thetotalworkdonebythe
computation isOln+nl2+n/4+...I=Oln!,and
thedepthisaconstant timesthenumberofitera­
tions,whichisO<lognl.
workiscalculated bysumming thelengthsofthevectors
onwhichthecomputation operates. Asanexample, Fig­
ure3showsVRAMcodefortakingthesumofnvalues.
Thiscodeexecutes thesummation treeinFigure2-each
loopiteration movesdownthetreeonelevel.TheVRAM
isagainavirtualmodel,sinceitwouldbeimpractical to
buildthevectormemory because ofitsdynamic nature.
Although theVRAMisagoodmodelfordescribing many
algorithms thatusevectorsorarrays,itisnotanideal
modelfordirectlyexpressing algorithms onmorecompli­
cateddatastructures, suchastreesorgraphs.
Language-Based Models. Athirdchoicefordefining a
modelintermsofworkanddepthistodefineitdirectlyin
termsoflanguage constructs. Suchalanguage-based perfor­
mancemodelspecifies thecostsoftheprimitive instructions
andasetofrulesforcomposing costsacrossprogram ex­
pressions. Theuseoflanguage-based modelsiscertainly
notnew.AhoandUllman, intheirpopular introductory
textbook Foundations ofComputer Science[I],definesucha
modelforderiving running timesofsequential algo­
rithms.Theapproach allowsthemtodiscusstherunning
timeofthealgorithms without introducing amachine
model.Asimilarapproach canbetakentodefineamodel
basedonworkanddepth.Inthisapproach, workand
depthcostsareassigned toeachprimitive instruction ofa
language andrulesarespecified forcombining parallel
andsequential expressions. Roughly speaking, whenexe­
cutingasetoftasksinparallel, thetotalworkisthesumof
CO.MUNlCAYtON. 01='l'MEAeMMarch1996/VoJ.39, No.387
theworkofthetasksandthetotaldepthisthemaximum
ofthedepthofthetasks.Whenexecuting taskssequen­
tially,boththeworkandthedeptharesummed. These
rulesaremademoreconcrete whenwedescribe NESL'S
performance modelinthenextsection, andthealgo­
rithmsinthisarticleillustrate manyexamples ofhowthe
rulescanbeapplied.
Wenotethatlanguage-based performance models
seemtobesignificantly moreimportant forparallelalgo­
rithmsthanforsequential algorithms. UnlikeAhoand
Ullman's sequential model,whichcorresponds almostdi­
rectlytoamachine model(theRAM)andisdefined
purelyforconvenience, thereseemstobenosatisfactory
machine modelthatcaptures thenotionofworkand
depthinageneral way.
WhyWorkandDepth?
Wenowreturntothequestion ofwhymodelsbasedon
workanddeptharebetterthanprocessor-based models
forprogramming andanalyzing parallel algorithms. To
motivate thisclaimweconsider aparticular algorithm,
Quicksort, andcompare thecodeandperformance analy­
sisofaparallel versionofthealgorithm usingthetwo
typesofmodels. Wearguethatinthework-depth model
thecodeisverysimple,theperformance analysisisclosely
relatedtothecode,andthecodecaptures thenotionof
parallelism inQuicksort ataveryhighlevel.Thisisnot
truewiththeprocessor-based model.
Westartbyreviewing sequential Quicksort, forwhich
pseudocode isshowninFigure4.Astandard perfor­
manceanalysis provesthatfornkeysthealgorithm runs
inO(nlogn)timeonaverage (expected case).Asimilar
analysisprovesthatthemaximum depthofrecursive calls
isO(1ogn)expected case;wewillusethisfactlater.Quick­
sortisnothardtoparallelize. Inparticular, wecanexe­
cutethetworecursive callsinparallel, andfurthermore,
withinasingleQuicksort wecancompare alltheelements
ofStothepivotainparallelwhensubselecting theele­
mentsforSl,andsimilarly forS2andS3'Thequestions
remain: howdoweprogram thisparallel version, and
whatisitsperformance?
Wefirstconsider programming andanalyzing parallel
procedure QUICKSORT(S):
ifScontainsatmostoneelementthenreturnS
else
begin
chooseanelement arandomly fromS;
letSl'S2andS3bethesequences ofelements inSiess
than,equalto,andgreaterthana,respectively;
return(QUICKSORT(SI) followed byS2followed by
QUICKSORT(S3) )
end
Figure4.Pseudocode forQuicksort, fromAho,
Hopcroft, andUllman[21.Although originally de­
scribedasasequential algorithm, thealgorithm as
statedisnothardtoparallelize.
88 March1996/Vol. 39,No.3C•••UNICAno ...GIll'1'••AeMQuicksort withamodelbasedonworkanddepth.Figure
5illustrates theNESLcodeforthealgorithm. Thiscode
shouldbecompared withthesequential pseudocode-the
onlysignificant difference isthattheNESLcodespecifies
thatthesubselection for81,82,and83,andthetwore­
cursivecallstoQUicksort shouldbeexecuted inparallel
(inNESL,curlybrackets {}signifyparallel execution).
Sincetheparallelalgorithm doesbasically thesameopera­
tionsasthesequential version, theworkcostoftheparal­
lelversioniswithinasmallconstant factorofthetimeof
thesequential version(O(nlogn)expected case).The
depthcostofthealgorithm canbeanalyzed byexamining
therecursion treeinFigure5.Thedepthofeachofthe
blocksrepresents thesumofthedepthsofalltheopera­
tionsinasinglecalltoQuicksort (notincluding thetwo
recursive calls).Theseoperations arethetestfortermina­
tion,findingthepivota,generation 81, 82,and83,and
thetwoappends attheend.Asdiscussed inmoredetailin
thenextsection,inNESLeachoftheseoperations hascon­
stantdepth(i.e.,isfullyparallel). Thedepthofeachblock
istherefore aconstant, andthetotaldepthisthisconstant
timesthemaximum numberoflevelsofrecursion, which
wementioned earlierisO(logn)expected case.Thiscom­
pletesouranalysisofQuicksort andsaysthattheworkof
quicksort isO(nlogn)andthedepthisO(logn),both
expected case.lNotethatwehavederived performance
measures forthealgorithm basedonveryhigh-level code
andwithout talkingaboutprocessors.
Wenowconsider codeandanalysis forparallel Quick­
sortbasedonaparallelmachine modelwithPprocessors.
Weclaimthatinsuchamodelthecodewillbeverylong,
willobscure thehigh-level intuition ofthealgorithm, and
willmakeithardtoanalyzetheperformance ofthealgo­
rithm.Inparticular, thecode ~llhavetospecifyhowthe
sequence ispartitioned acrossprocessor (ingeneral, the
inputlengthdoesnotequalPandneedstobebrokenup
intoparts),howthesubselection isimplemented inparal­
lel(forgenerating Sl,S2,andS3inparallel), howthere­
cursivecallsgetpartitioned amongtheprocessors and
thenload-balanced, howthesubcalls aresynchronized,
andmanyotherdetails.Thisiscomplicated bythefact
thatinQuicksort therecursive callsaretypically notof
equalsizes,therecursion treeisnotbalanced, andtheS2
setshavetobereinserted onthewaybackuptherecur­
sion.Although codingthesedetailsmighthelpoptimize
thealgorithm foraparticular machine, theyhavelittleto
dowithcoreideas.Evenifweassumethesimplest pro­
cessor-based modelwithunit-time accesstosharedmem­
oryandbuilt-insynchronization primitives, thefullypar­
allelcodeforQuicksort injustaboutanylanguage would
requirehundreds ofJinesofcode.Thisisnotjustaques­
tionofverbosity butaquestion ofhowwethinkaboutthe
algorithm.
Relationship ofworkanddepthtorunning time.Work
anddepthcanbeviewedastherunning timeofanalgo-
'Wenotethattheparallel versionofQuicksort requires morememory
thanagoodimplementation ofthesequential version. Inparticular, the
sequential versioncanbeimplemented inplace,whiletheparallelversion
requires aboutnscratchspace.
function Quicksort(S) =
if(#S<=1JthenS
else
leta=S[rand (#S)];
51=IeinSIe<a};
52=IeinSIe==a};
53=IeinSIe>a};
R=IQuicksort(v); vin[Sl,S3]};
inR[O]++52++R[l];Work=0(nlogn)(expected)
Depth=0(logn)(expected)
QuicksortQuicksort
rithmattwolimits:oneprocessor (work)andanunlimited
numberofprocessors (depth). Infact,thecostsareoften
referred toasT1andToo.Inpractice, however, wewantto
knowtherunning timeforsomefixednumber ofproces­
SOl'S.Asimplebutimportant resultofBrent[9]showed
thatknowing thetwolimitsisgoodenough toplacerea­
sonableboundsonrunning timeforanyfixednumberof
processors. Inparticular, heshowedthatifweknowthata
computation hasworkWanddepthD,thenitwillrun
withPprocessors intimeTsuchthat
W W-:5:T<-+D.P P
Thisresultmakessomeassumptions aboutcommunica­
tionandscheduling costs,buttheequation canbemodi­
fiediftheseassumptions change. Forexample, witha
machine thathasamemory latency(thetimebetween
making aremoterequestandreceiving thereply),ofL,
theequation isW/P:5:T:5:W/P+L.D.
Let'sreturntotheexample ofsumming. Brent'sequa­
tion,alongwithourprevious analysisofworkanddepth
(W=n-1,D=log2n),tellsusthatnnumbers canbe
summed onPprocessors withinthetimebounds
(,n_-_I.:-) (n-1)- P :5:T<P+log2n.
Forexample 1,000,000 elements canbesummed on1,000
processors insomewhere between 1,000(106/103)and
1,020(106/103+log2I06) cycles,assuming wecountone
cycleperaddition. Formanyparallel machine models,
suchasthePRAMorasetofprocessors connected bya
hypercube network, thisisindeedthecase.Toimplement
theaddition, wecouldassign1,000elements toeachpro­
cessorandsumthem,whichwouldtake999cycles.WeFigure5.TheQuicksort algorithm inNESL.Theopera­
tor#returnsthelengthofasequence. Thefunc­
tionrand(n)returnsarandomnumberbetween 0
andn(theexpression 8[rand(#8)] therefore re­
turnsarandomelementof81.Thenotation {ein
81e<a}isread:"inparallelfindallelements ein8
forwhicheislessthana".Thisoperation hascon­
stantdepth,andworkproportional tothelength
of8.Thenotation {Quicksort(v): vin[81,83]}is
read:"inparallelforvin81and83,Quicksort v".
ThereSUltsarereturned asapair.Thefunction ++
appends twosequences.
couldthensumacrosstheprocessors usingatreeofdepth
log2I,000 =10,sothetotalnumber ofaddcycleswould
be1,009,whichiswithinourbounds.
Communication Costs.Aproblem withusingworkand
depthascostmeasures isthattheydonotdirectlyaccount
forcommunication costsandcanleadtobadpredictions
ofrunning timeonmachines wherecommunication isa
bottleneck. Toaddress thisquestion, let'sseparate com­
munication costsintotwoparts:latency,asdefined previ­
ously,andbandwidth, therateatwhichaprocessor can
accessmemory. Ifweassumethateachprocessor may
havemultiple outstanding requests, thenlatencyisnota
problem. Inparticular, latencycanbeaccounted forin
themapping oftheworkanddepthintotimeforama­
chine(seethepreceding), andthesimulation remains
work-efficient (i.e.,theprocessor-time product ispropor­
tionaltothetotalwork).Thisisbasedonhidingthela­
tencybyusingfewenough processors suchthatonaver­
ageeachprocessor hasmultiple paralleltasks(threads) to
executeandtherefore hasplentytodowhilewaitingfor
replies.Bandwidth isamoreseriousproblem. Forma­
chineswherethebandwidth between processors isvery
CO••U••ca'l'IO........ 11••eMMarch1996/VoI.39, No.389
muchlessthanthebandwidth tothelocalmemory, work
and.depthbythemselves willnotingeneral givegood
predictions ofrunning time.However, thenetwork band­
widthavailable onrecentparallel machines, suchasthe
CrayT3EandSCIPowerChallenge, isgreatenough to
givereasonable predictions, andweexpectthesituation
toimprove withrapidlyimproving network technology.
NestedData-Parallelism andNESL
Manyconstructs havebeensuggested forexpressing par­
allelism inprogramming languages, including fork-and­
joinconstructs, data-parallel constructs, andfutures,
amongothers.Thequestion iswhichofthesearemost
usefulforspecifying parallelalgorithms? Ifwelookatthe
parallelalgorithms thataredescribed intheliterature and
theirpseudocode, wefindthatnearlyallaredescribed as
paralleloperations overcollections ofvalues.Forexample
"inparallelforeachvertexinagraph,finditsminimum
neighbor", or"inparallelforeachrowinamatrix,sum
therow".Ofcourse,thealgorithms arenotthissimple­
theyusuallyconsistofmanysuchparallelcallsinterleaved
withoperations thatrearrange theorderofacollection,
andcanbecalledrecursively inparallel, asinQuicksort.
Thisabilitytooperateinparalleloversetsofdataisoften
referred toasdata-parallelism [IS],andlanguages basedon
itareoftenreferred toasdata-parallel languages, orcollec­
tion-oriented languages [24].Wenotethatmanyparallel
languages havedata-parallel features inconjunction with
otherformsofparallelism [3,10, 12,18].
Beforewecometotherashconclusion thatdata-paral­
lellanguages arethepanacea forprogramming parallel
algorithms, wemakeadistinction between flatandnested
data-parallel languages. Inflatdata-parallel languages, a
function canbeappliedinparalleloverasetofvalues,but
thefunction itselfmustbesequential. Innesteddata-paral­
lellanguages [4],anyfunction including parallel func­
tions,canbeappliedoverasetofvalues.Forexample, the
summation ofeachrowofthematrixmentioned previ­
ouslycoulditselfexecuteinparallelusingatreesum.We
claimthattheabilitytonestparallel callsiscriticalfor
expressing algorithms inawaythatmatches ourhigh­
levelintuition ofhowtheywork.Inparticular, nestedpar­
allelism canbeusedtoimplement nestedloopsanddi­
vide-and-conquer algorithms inparallel. (Fiveoutofthe
sevenalgorithms described inthisarticleusenestingina
crucialway.)Theimportance ofallowing nestingindata­
parallel languages hasalsobeenobserved byothers[13].
However, mostexisting data-parallel languages, suchas
HighPerformance Fortran (HPF)[14]orC*[21],donot
havedirectsupport forsuchnesting.2
NESL
ThisarticleusesNESL[5]asanexample ofanesteddata­
parallel language. Thissectiongivesanoverview ofthe
language, andthenextsectiongivesseveralexamples of
parallel algorithms described andanalyzed withNESL.
"ThecurrentHPF1.0hassomelimitedsupport fornestedcalls,andfu­
tureversions arelikelytohavesignificantly bettersupport.NESLwasdesigned toexpressnestedparallelism inasim­
plewaywithaminimum setofstructures andwasthere­
foredesigned asalanguage onitsownratherthanasan
extension ofanexisting sequential language. Theideas,
however, canclearlybeusedinotherlanguages. NESLis
looselybasedonML[19],alanguage withapowerful type
system,andonSETL[22],alanguage designed forcon­
ciselyexpressing sequential algorithms. AswithML,NESL
ismostlyfunctional (hasonlylimitedformsofsideeffects),
butthisfeature istangential tothepointsmadeinthis
article.
NESLsupports data-parallelism bymeansofoperations
onsequences-one-dimensional arrays.Allelements ofa
sequence mustbeofthesametype,andsequence indices
arezero-based (a[O]extracts thefirstelement ofthese­
quencea).Themaindata-parallel construct isapply-to­
each,whichusesaset-likenotation. Forexample, theex­
pressIOn
{a* a : ain[3,-4,-9,5]};
squares eachelement ofthesequence [3,-4,-9,5]
returning thesequence [9,16,81,25]. Thiscanberead:
"inparallel, foreachainthesequence [3,-4, -9, 5],
squarea".Theapply-to-each canbeusedovermultiple
sequences. Theexpression
{a+b : ain[3,-4,-9,5];bin[1,2,3, 4]};
addsthetwosequences elementwise returning [4,-2,
-6,9].Theapply-to-each construct alsoprovides the
abilitytosubselect elements ofasequence basedonafil­
ter.Forexample.
{a* a : ain[3,-4,-9,5]Ia>O};
canberead:"inparallel, forem::hainthesequence [3,
-4, -9, 5]suchthataisgreaterthan0,squarea".It
returnsthesequence [9,25].Theelements thatremain
maintain theirrelativeorder.Suchfiltering wasusedin
theQuicksort example.
Anyfunction, whether primitive oruserdefined, may
beappliedtoeachelementofasequence. So,forexample,
wecoulddefine
function factorial en)=
ifen==1)then1
elsen*factorial en-1);
andthenapplyitovertheelements ofasequence, asin
{factorialei) : iin[3,1,7]};
whichreturnsthesequence [6,1,5040].
Inaddition totheparallelism supplied byapply-to­
each, NESLprovides asetoffunctions onsequences, each
ofwhichcanbeimplemented inparallel. Forexample, the
function sumaddstheelements ofasequence, andthe
function reverse reverses theelements ofasequence.
Perhaps themostimportant function onsequences is
write,whichsupplies theonlymechanism tomodifymul­
tiplevaluesofasequence inparallel. Thefunction write
takestwoarguments: thefirstisthesequence tomodify,
andthesecondisasequence ofinteger-value pairsthat
90 March1996/Vol.39. No.3CO••U.lCATIONS all'1'••AeM
amming
231
Depth=I+max(Dfact(3) ,Dfaet(I), Dfact(5),Dfact(2»
=I+max(I3, 3,23,8)
=24Work=I +sum(Wfaet(3),Wfact(I), Wfact(5),Wfact(2»
=I+sum(I3, 3,23,8)
=48Wfact(n)=Dfact(n)=5n-2insertsthe-2,5,and9intothesequence atlocations 4,
2,and5,respectively, returning
[0,0,5,0,-2,9,0,0].
Ifanindexisrepeated, thenonevalueiswritten
nondeterministically. Forreaders familiar withthevari­
antsofthePRAMmodel,wenotethatthewritefunction
isanalogous toan"arbitrary" concurrent write.NESLalso
includes afunctione_write thatdoesnotallowrepeated
indicesandisanalogous toanexclusive write.Ifrepeated
indicesareusedwithe_write, thecurrent implementa­
tionreportsanerror.
Nestedparallelism issupplied inNESLbyallowing se­
quences tobenestedandallowing parallelfunctions tobe
usedinanapply-to-each. Forexample, wecouldapplythe
sumfunction inparallel'Dver anestedsequence, asinspecifywhattomodify.Foreachpair(i,v),thevaluevis
inserted intoposition iofthedestination sequence. For
example,
write([O, 0, 0,0, 0,0, 0,0],[(4,-2),(2,5),(5,9)]);
{sum(a) : ain[[2,3],[8,3,9],[7JJ},
whichwouldreturn[5,20,7].Here,thereisparallelism
bothwithineachsumandacrossthesums.TheQuicksort
algorithm showed another example ofnestedcalls-the
algorithm isitselfusedinanapply-to-each toinvoketwo
recursive callsinparallel.
ThePerformance Model
Wenowreturntotheissueofperformance models, this
timeinthecontextofNESL.Asmentioned earlier, NESL
definesworkanddepthintermsoftheworkanddepthof
theprimitive operations andrulesforcomposing the
measures acrossexpressions. WewilluseW(e)andD(e)to
refertotheworkanddepthofevaluating anexpression e.
Inmostcases,theworkanddepthofanexpression are
thesumsoftheworkanddepthofthesubexpressions. So,
forexample, ifwehaveanexpression el+e2,whereel
ande2aresubexpressions, thentheworkoftheexpres­
sIOnISFigure6.Calculating theworkanddepthof{fae­
torial(n) : nin[3,1,5,2]}
wherethe1isthecostoftheadd.Asimilarruleisusedfor
depth.Theinteresting rulesconcerning parallelism are
therulesforanapply-to-each expression:
({in(>~
D({el(a) :aine2})=1+D(e2)+maxD(el(a». (2)
tIin",'.!
Figure7.Listofsomeofthesequence functions
supplied byNESL.Theworkrequiredforeachfunction is
givenintheWorkcolumn:L(v)referstothelength
ofthesequence v.Theworkofthewrite(d, a)func­
tionactuallydepends onwhether theargument d
needstobecopiedornot,butintheexamples in
thisarticlethedifference hasnoeffect.
Operation I Description I'fork IDepth
dist(a,O
#a
a[i]
[s:e]
[s:e:d]
sum(a)
write(d,a)
a++b
drop(a,n)
interleave(a,b)
flatten(a)Createasequenceofasoflength1.
Returnlengthofsequencea.
Returnelementatpositioniofa.
Returnintegersequence fromstoe.
Returnintegersequence fromstoeUyd.
Returnsumofsequencea.
Placeelementsaind.
Appendsequences aandb.
Dropfirstnelementsofsequence a.
Interleave elementsofsequences aandb.
Flattennestedsequencea.1
11
(e-s)
(e-s)/d
L(a)
L(a)
L(a)+L(b)
L(result)
L(result)
L(result)1
1
1
1
1
logL(a)
1
1
1
1
1
COM.UIlICATIONS OPTNE.eMM;ar....h 1qqlSIV.....I~QNo ~Ot
(3)1procedure PRIMES(n):
2letAbeanarrayoflengthn
3setallbutthefirstelemeritofAtoTRUE
4forifrom2to-fYi
5begin
6ifA[i]isTRUE
7thensetallmultiples ofiuptontoFALSE
8end
Figure8.Pseudocode forthesieveofEratosthe­
nes
Thefirstrulespecifies thattheworkisthesumofthe
workofeachoftheapplications ofejtoanelementofa,
plustheworkofe2,plus1toaccount foroverheads. The
rulefordepthissimilar,buttakesthemaximum ofthe
depthofeachapplication ofej.Thissupports ourintui­
tionthattheapplications areexecuted inparallelandthat
theevaluation oftheapply-to-each completes whenthe
lastcallcompletes. Theotherinteresting rulesarethe
rulesforanifexpression, whichforworkis
W(ifeIthene2elseeg)
{w(e2)ej=TRUE
=1+W(el)+
W(eg)otherwise,
withasimilarrulefordepth.Theworkanddepthfora
function callandforscalarprimitives areeachI.Thecosts
oftheNESLfunctions onsequences aresummarized in
Figure7.Wenotethattheperformance rulescanbemore
precisely defined usinganoperational semantics [6].
Asanexample ofcomposing workanddepth,consider
evaluating theexpression
e={factorialCn) : nina},
wherea=[3,1,5,2]. Usingtherulesforworkandthe
codeforfactorial givenearlier,wecanwritethefollow­
ingequation forwork:
Wfact(n) ={I~I+W== n=I
+W.+W-+Wfact(n-l)n>1
where W~~,W"andW_aretheworkfor= = ,*,and-,
andareallI.Thetwounitconstants comefromthecostof
thefunction callandtheif-then-else rule..\ddingupthe
termsandsolvingtherecurrence givesWfact(n) =5n-2.
Sincethereisnoparallelism inthefactorial function, the
depthisthesameasthework.Tocalculate workand
depthforthefullexpression {factorialCn) : nina},we
canuseequations Iand2.Thiscalculation isshownin
Figure6.
Examples ofParallelAlgorithms inNESL
Several parallel algorithms aredescribed andanalyzed
here,providing examples ofhowtoanalyzealgorithms intermsofworkanddepthandofhowtousenesteddata­
parallelconstructs. Theyalsointroduce someimportant
ideasconcerning parallel algorithms. Again,themain
goalsaretohavethecodecloselymatchthehigh-level
intuition ofthealgorithm andtomakeiteasytoanalyze
theasymptotic performance fromthecode.
Primes
Ourfirstalgorithm findsallprimenumbers lessthann.
Thisexample demonstrates acommon technique usedin
parallelalgorithms-solving asmallercaseofthesame
problem tospeedthesolutionofthefullproblem. Wealso
usetheexample tointroduce thenotionofworkeffi­
ciency.Animportant aspectofdeveloping agoodparallel
algorithm isdesigning onewhoseworkisclosetothetime
foragoodsequential algorithm thatsolvesthesameprob­
lem.Without thiscondition wecannothopetogetgood
speedup oftheparallel algorithm overthesequential al­
gorithm. Parallel algorithms arereferred toaswork-effi­
cientrelative toasequential algorithm iftheirworkis
withinaconstant factorofthetimeofthesequential algo­
rithm.Allthealgorithms wehavediscussed sofarare
work-efficient relativetothebestsequential algorithms. In
particular, summingn numbers tookO(n)workandparal­
lelQuicksort tookO(nlogn)expected work,bothofwhich
arethesameasrequired sequentially. Forfindingprimes,
ourgoalshouldagainbetodevelop awork-efficient algo­
rithm.Wetherefore startbylookingatefficient sequential
algorithms.
Themostcommon sequential algorithm forfinding
primesisthesieveofEratosthenes, whichisspecified in
Figure8.Thealgorithm returnsanarrayinwhichtheith
position issettoTRUEifiisaprimeandtoFALSEotherwise.
Thealgorithm worksbyinitializing thearrayAtoTRUE
andthensettingtoFALSEallmultiples ofeachprimeit
finds.Itstartswiththefirstprime,2,andworksuptovn.
Thealgorithm onlyneedstogouptovn,sinceallcom­
positenumbers (nonyrimes) lessthannmusthaveafac­
torlessorequaltoYn.Ifline7isimplemented bylooping
overthemultiples, thenthealgorithm canbeshownto
takeO(nlog logn)time,andtheconstant issmall.The
sieveofEratosthenes isnotthetheoretically bestalgo­
rithmforfindingprimes,butitisclose,andwewouldbe
happytoderiveaparallelalgorithm thatiswork-efficient
relative toit(i.e.,doesO(nlog logn)work).
Itturnsoutthatthealgorithm asdescribed hassome
easyparallelism. Inparticular, line7canbeimplemented
inparallel. InNESL,themultiples ofavalueicanbegen­
eratedinparallel withtheexpression
[2*i:n:i]
andcanbewrittenintothearrayAinparallel withthe
writefunction. Usingtherulesforcosts(seeFigure7),
thedepthoftheseoperations isconstant andtheworkis
thenumberofmultiples, whichisthesameasthetimeof
thesequential version. Giventheparallelimplementation
ofline7,thetotalworkofthealgorithm isthesameasthe
sequential algorithm, sinceitdoesthesamenumber of
operations, andthedepthofthealgorithm isO(vn),since
92 March1996/VoI.39, No.3CD••U..ICAy.....OFTHEacM
..mining
eachiteration oftheloopinlines5-8hasconstant depth
andthenumberofiterations is\hi,.Notethatthinking of
thealgorithm intermsofworkanddepthallowsasimple
analysis (assuming weknowtherunning timeofthese­
quential algorithm) withoutourhavingtoworryabout
howtheparallelism mapsontoamachine. Inparticular,
theamountofparallelism variesgreatlyfromthefirstiter­
ation,inwhichwehaven/2multiples of2toknockoutin
parallel, tothelastiteration, wherewehaveonly\hi,mul­
tiples.Thisvarying parallelism wouldmakeitmessyto
program andanalyzeonaprocessor-based model.
Wenowconsider improving thedepthofthealgorithm
withoutgivingupanywork.Wenotethatifweweregiven
alltheprimesfrom2upto\hi"wecouldthengenerate all
themultiples oftheseprimesatonce.TheNESLcodefor
generating allthemultiples is
{[2*p:n:p]: pinsqr_primes};
wheresqr_primes isasequence containing alltheprimes
upto\hi,.Thiscomputation hasnestedparallelism, since
thereisparallelism acrosS'the sqr_primes (outerparallel­
ism)andalsoingenerating themultiples ofeachprime
(innerparallelism). Thedepthofthecomputation iscon­
stant,sinceeachsubcallhasconstant depth,andthework
isO(nloglogn),sincethetotalnumberofmultiples when
summed acrossthesubcalls isthesameasthenumber of
multiples usedbythesequential version.
Wehaveassumed thatsqr_primes wasgiven,butto
generate theseprimeswecansimplycallthealgorithm
function primes(n) =
ifn==2then([]int)
elserecursively on\hi,.Figure9showsthefullalgorithm for
findingprimesbasedonthisidea.Insteadofreturning a
sequence offlags,thealgorithm returnsasequence with
thevaluesoftheprimes. Forexample, primesClO)
wouldreturnthesequence [2,3,4,7]. Thealgorithm re­
cursively callsitselfonaproblem ofsize\hi,andtermi­
nateswhenaproblem ofsize2isreached. Theworkand
depthcanbeanalyzed bylooking atthepictureatthe
bottomofFigure9.Clearlymostoftheworkisdoneatthe
toplevelofrecursion, whichdoesO(nloglogn)work.The
totalworkistherefore alsoO(nlog logn).Nowlet'scon­
siderthedepth.Sinceeachrecursion levelhasconstant
depth,thetotaldepthisproportional tothenumber of
levels.Tocalculate thisnumber, wenotethatthesizeof
theproblem atleveliisn1/2'andthatwhenthesizeis2,the
algorithm terminates. Thisgivesustheequation n1/2"=2,
Figure9.Thecodefortheprimesalgorithm, an
exampleofoneleveloftherecursion, andadia­
gramoftheworkanddepth.Inthecode[ Iintindi­
catesanemptysequence ofintegers. Thefunction
isqrttakesthesquarerootofaninteger.Thefunc­
tionflattentakesanestedsequence andflattensit.
Thefunction dist(a,n)distributes thevalueatoa
sequence oflengthn.Theexpression {Iin[O:nlifl
inflagsIfl}canbereadas"foreachifrom0ton
andeachflinflagsreturntheiifthecorresponding
flistrue".Thefunction drop(a,n) dropsthefirstn
elements ofthesequence a.
letsqr_primes =primes(isqrt(n»;
composites ={[2*p:n:p]: pinsqr_primes};
flat_camps =flatten(composites);
flags =write(dist(true,n),{(i,false): iinflat_comps});
indices =Iiin[O:n];flinflagsIfl}
indrop(indices, 2);
Example forprimes(20):
sqr_primes
composites
flat_camps
flags
indices
result=[2,3]
=[[4,6,8,10,12,14,16,18] ,[6,9,12,15,18]]
=[4,6,8,10,12,14,16,18,6,9,12,15,18]
=[t,t,t,t,f,t,f,t,f,f,f,t,f,t,f,f,f,t,f,t]
=[0,1,2,3,5,7,11,13,17,19]
=[2,3,5,7,11,13,17,19]
Depthprimes(n)1::=:::==::;:;:;:-_n_IO_g __IO_g_n ~
primes(nl/2)Inl/2loglognl/2~
primes(nI/4)0
primes(2)0
COM.UN.CAno ....... 11••eMMarch1996/Vol. 39.No.3g:!
wheredisthedepthweseek.Solvingford,thismethod
givesd=loglogn.Thecostsaretherefore:
W=O(nlog logn)
DO(loglogn)
Thisalgorithm remains work-efficient relativetothese­
quential sieveofEratosthenes andgreatlyimproves the
depth.
SparseMatrixMultiplication
Sparsematrices, whicharecommon inscientific applica­
tions,arematrices inwhichmostelements arezero.To
savespaceandrunning tillIeitiscriticaltostoreonlythe
nonzero elements. Astandard representation ofsparse
matrices insequential languages isanarraywithoneele­
mentperrow,eachofwhichcontains alinked-list ofthe
nonzero valuesinthatrowalongwiththeircolumnnum­
ber.Asimilarrepresentation canbeusedinparallel. In
NESLasparsematrixcanberepresented asasequence of
rows,eachofwhichisasequence of(column-number,
value)pairsofthenonzero valuesintherow.Thematrix
A[i:~ ~:~l.~~]o1.02.0-1.0
o01.02.0
isrepresented inthiswayas
A=[[(0,2.0),(1,1.0)],
[(0,-1.0),(1,2.0),(2,-1.0)],
[(1,1.0),(2,2.0),(3,-1.0)],
[(2,1.0),(3,2.0)]],
whereAisanestedsequence. Thisrepresentation canbe
usedformatrices witharbitrary patterns ofnonzero ele­
ments,sinceeachsubsequence canbeofadifferent size.
Acommon operation onsparsematrices istomultiply
thembyadensevector.Insuchanoperation, theresultis
thedot-product ofeachsparserowofthematrixwiththe
densevector.TheNESLcodefortakingthedot-product of
asparserowwithadensevectorxis:
sum({v*x[i] :(i,v)inrow})
Thiscodetakeseachindex-value pair(i,v)inthesparse
row,multiplies vbytheithvalueofx,andsumsthere­
sults.Theworkanddepthiseasilycalculated usingthe
performance rules.Ifnisthenumber ofnonzero ele­
mentsintherow,thenthedepthofthecomputation isthe
depthofthesum,whichisO(logn),andtheworkisthe
SUllloftheworkacrosstheelelllents, whichisO(n).
Thefullcodeformultiplying asparsematrixArepre­
sentedbyadensevectorxrequires thatweapplythecode
toeachrowinparallel, whichgives
{sum({v*x[i] :(i,v)inrow})
:rowinA}.
Thisexalllple hasnestedparallelism, sincethereisparal­
lelislllbothacrosstherowsandwithineachrowforthe
dotproducts. Thetotaldepthofthecodeisthemaximum
94 March1996/VoI.39. No.3COMMUNlCA'I'IOIlIi OP'I'HEACMofthedepthofthedotproducts, whichisthelogarithm of
thesizeofthelargestrow.Thetotalworkisproportional
tothetotalnumber ofnonzero elements.
PlanarConvex-Hull
Ournextexample solvestheplanarconvexhullproblem:
Givennpointsinaplane,findwhichofthemlieonthe
perimeter ofthesmallest convexregionthatcontains all
points.Thisexample showsanother useofnestedparal­
lelismfordivide-and-conquer algorithms. Thealgorithm
weuseisaparallelQuickhull [20],sonamedbecauseofits
similarity totheQuicksort algorithm. AswithQuicksort,
thestrategy istopicka"pivot"element, splitthedata
basedonthepivot,andrecurseoneachofthesplitsets.
AlsoaswithQuicksort, thepivotelement isnotguaran­
teedtosplitthedataintoequallysizedsets,andinthe
worstcasethealgorithm requires O(n2)work;however, in
practice thealgorithm isoftenveryefficient.
Figure10showsthecodeandanexample oftheQuick­
hullalgorithm. Thealgorithm isbasedontherecursive
routinehsplit.Thisfunction takesasetofpointsinthe
plane«x,y)coordinates) andtwopointspIandp2known
tolieontheconvexhullandreturnsallthepointsthatlie
onthehullclockwise frompItop2,inclusive ofpI,but
notofp2.InFigure10,givenallthepoints[A,B,0,. . . ,
P],pIA,andp2=P,hsplitwouldreturnthese­
quence[A,B,J,0].Inhsplit,theorderofpIandp2
matters, sinceifweswitchAandP,hsplitwouldreturn
thehullalongtheotherdirection [P,N,0].
Thehsplitfunction firstremoves alltheelements that
cannotbeonthehullbecause theyliebelowthelinebe­
tweenpIandp2(whichwedenotebypl-p2). Thisis
donebyremoving elements whosecrossproduct withthe
linebetweenpIandp2isnegative. InthecasepI=A
andp2P,thepoints[B,D,F,G,H,J,K,M,0]would
remainandbeplacedinthesequence packed. Thealgo­
rithmnowfindsthepointpmfarthest fromthelinepl­
p2.Thepointpmmustbeonthehull,sinceasalineat
infinityparallel topl-p2movestowardpl-p2,itmust
firsthitpm.Thepointpm(Jintherunning example) is
foundbytakingthepointwiththemaximulll crossprod­
uct.Oncepmisfound,hsplitcallsitselftwicerecursively
usingthepoints(p1,pm)and(pm,p2)(intheexample,
(A,J)and(J,P)).Whentherecursive callsreturn,
hsplitflattens theresult,thereby appending thetwo
subhulls.
Theoverallconvex-hull algorithm worksbyfinding
thepointswithminilllum andmaximum xcoordinates
(thesepointsmustbeonthehull)andthenusinghsplitto
findtheupperandlowerhull.Eachrecursive callhascon­
stantdepthandO(n)work.However, sincemanypoints
mightbedeletedoneachstep,theworkcouldbesignifi­
cantlyless.AswithQuicksort, theworst-case costsareW=
O(n2)andD=O(n).Formhullpointsthebestcasetimes
areO(logm)depthandO(n)work.'Itishardtostatethe
average-case time,sinceitdepends onthedistribution of
theinputs.Otherparallelalgorithms fortheconvex-hull
problem runinD=O(logn),andW=O(n)intheworst
case[16],buthavelargerconstants.
mmlng
W(n)=W(n/2)+kn=O(n)
D(n)=D(n/2)+k=O(logn)
Figure11.CodeforthefastFouriertransforms,
thescanoperation, andforfindingtheJ<!hsmallest
elementofasetTheparticular codeshownworksonlyonsequences that
havealengthequaltoapoweroftwo,butitisnothardto
generalize ittoworkonsequences ofanylength.
Work=O(n)
Depth=O(Jogn)
Work=O(n)
(expected)
Depth=0(Jogn)
(expected)Work=O(nlogn)
Depth=O(Jogn)
function scan(a)=
if#a==1then[0]
else
lete=even_elts(a);
0=odd_elts(a);
s=scan({e+0:eine;0inoj)
ininterleave(s,ls +e:sins;eine));function ffHa,w) =
if#a==1thena
else
letr={fft(b,even_elts(w»:
bin[even_elts(a) ,odd_elts(a)]}
infcadd(a, cmult(b, w»:
ainr[O]++do];
binr[I)++r[I);
winwl;
function kth_smallest(s, k)=
letpivot=s[#s/2];
lesser=Ieinsle<pivot);
greater =Ieinsle>pivot!;
inif(k<#Iesser)then
kth_smallestOesser, k)
elseif(k>=#s-#greater) then
kth_smallest(greater, k-'(#S-#greater»
elsepivot;asacontaining allthecomplex nthrootsofunity.TheFFT
iscalledrecursively ontheoddandevenelements ofa.
Theresultsarethencombined usingcaddandCIDuit
(complex addition andmultiplication). Assuming that
caddandcIDulttakeconstant workanddepth,thenthe
recursion givesusthecosts:
W(n)=2W(n/2)+kn=O(nlogn)
D(n)=D(n/2)+k=O(logn).
Theplus-scan operation (calledall-prefix-sums) takesa
sequence ofvaluesandreturnsasequence ofequallength
forwhicheachelement isthesumofallprevious elements
intheoriginal sequence. Forexample, executing aplus­
scanonthesequence [3,5,3,1,6]returns[0,3,8,II,
12].Thiscanbeimplemented asshowninFigureII.The
algorithm worksbyelementwise addingtheoddandeven
elements andrecursively solvingtheproblem onthese
sums.Theresultoftherecursive callisthenusedtogen­
eratealltheprefixsums.Thecostsare:
Pfunction cross_product(o,line) =
let(xo,yo)=0;
«xl,yl),(x2,y2» =line
in(xl-xoh(y2-yo) -(yl-yoh(x2-xo);
function convex_hull (points)=
letx=Ix:(x,y)inpoints};
minx=points(min_index(x)];
maxx=points(max_index (x)]
inhsplit(points,minx,maxx) ++hsplit(points,
maxx,minx) ;
A
(ABCDEFGRI]KLMN0p]
A(BDFGRJKMO] P[CEILN]
A(BFJ](0]PN(CE]
ABJOPNCfunction hsplit(points,pl,p2) =
letcross=lcross_produet(p,(pI,p2»: pinpoints};
packed={p:pinpoints;cincrossIplusp(c)}
inif(#packed <2)then[pI]++packed
else
letpm=points[max_index (cross)]
inflatten({hsplit(packed,pl,p2):
piin[pI,pm]; p2in[pm,p2]});ThreeOtherAlgorithms
Weconclude ourexamples withbriefdiscussions ofthree
otheralgorithms: thefastFourier transform (FFT),the
scanoperation (allprefixsums),andanalgorithm for
findingthekthsmallest element ofaset.Allthecodeis
showninFigureII.Thesealgorithms furtherdemon­
stratetheconciseness ofnesteddata-parallel constructs.
Weusethestandard recursive versionfortheFFT[II].
Thesecondargument wisasequence ofthesamelength
Figure10.CodeandexampleoftheQuickhull al­
gorithm. Eachsequence intheexample showsone
stepofthealgorithm. SinceAandParethetwox
extrema, thelineAPistheoriginalsplitline.JandN
arethefarthestpointsineachsubspace fromAP
andare,therefore, usedforthenextlevelofsplits.
Thevaluesoutsidethebrackets arehullpointsthat
havealreadybeenfound.
CO.MUN.CA'I'IOil. 011TIiE.CMMarchI996/Vol. 39,No.395
Avariation ofQuicksort canbeusedtofindthekth
smallest element ofasequence [11].Thisalgorithm calls
itselfrecursively onlyonthesetofelements containing the
result.Hereweconsider aparallel versionofthisalgo­
rithm.Mterselecting thelesserelements, if#lesser is
greaterthank,thenthekthsmallestelement mustbelong
tothatset.Inthiscase,thealgorithm callskthsmallest
recursively onlesserusingthesamek.Otherwise, the
algorithm selectstheelements thataregreaterthanthe
pivot,andcansimilarly findifthekthelement belongsin
greater. Ifitdoesbelongingreater, thealgorithm calls
itselfrecursively butmustnowreadjustkbysubtracting
thenumber ofelements lessthanorequaltothepivot.If
thekthelement belongs inneitherlessernorgreater,
thenitmustbethepivot,andthealgorithm returnsthis
value.Forsequences oflengthn,theexpected workofthis
algorithm isO(n),whichisthesameasthetimeofthe
serialversion. Theexpected depthisO(logn),sincethe
expected depthofrecursion isO(logn).
summary
TheNESLlanguage wasdesigned tobeusefulforpro­
gramming andteaching parallel algorithms. Forthese
purposes, itwasimportant thatitallowsimpledescrip­
tionsofalgorithms thatcioselymatchourhigh-level intui­
tion,andalsothatitsupplyawell-defined modelforana­
lyzingperformance. Webelieve thelanguage has
successfully achieved thesegoals.Therearemanyaspects
ofNESL,andthepurpose ofthisarticlewastoextractthe
twofeatures thataremostimportant forprogramming
parallel algorithms. Theyare:
• Aperformance modelbasedonworkanddepth.An
important aspectisthatthemodelisdefined directlyin
termsoflanguage constructs ratherthantryingtoap­
pealtoanyintuition ofamachine. Asdiscussed, the
modelisavirtualoneforwhichwegivemappings onto
running timesforvariousphysical machine models.
•Theuseofdata-parallel constructs forexpressing paral­
lelismandtheabilitytonestsuchconstructs. Wecer­
tainlydonotmeantoexclude anyotherparallel con­
structs,buthavingsomewayofmapping afunction
overasetofvaluesinparallelseemscriticalforexpress­
ingmanyparallel algorithms.
Thisarticleissuggesting achangeintheunderlying mod­
elsweuseforanalyzing parallelalgorithms. Inparticular,
itsuggests thatwemoveawayfromusingtheoretical per­
formance models basedonmachines tousingmodels
basedonlanguages. Asmentioned inthearticle,someref­
erenceworksalready informally analyze parallel algo­
rithmsintermsofworkanddepthbeforemapping them
ontoaPRAM[16,17].Wesuggestthattheextrastepbe
takenofformalizing amodelbasedonworkanddepth.
Withthisformalmodel,thePRAMcanbecutoutofthe
loop,directlymapping themodelontomorerealistic ma­
chines.Wefurthermore arguethatlanguage-based mod­
elsseemtobethemostreasonable waytodefineapro­
grammingmodel basedonworkanddepth.
Afullimplelnentation ofNESLiscurrently available on
96 March1996/VoI.39. No.3COMMU.lCATIONS 011TN8AeMtheWorld-Wide Web.Thecompiler isbasedonatech­
niquecalledflattening nestedparallelism [4]andcompiles
toanintermediate language calledVCODE.Benchmark
resultsforthisimplementation fortheConnection Ma­
chinesCM-2andCM-5andtheCrayC90aredescribed in
[8].TheseresultsshowthatNESL'Sperformance iscom­
petitive withthatofmachine-specific codesforthose
benchmarks.
Acknowledgments
IwouldliketothankMarcoZagha,UziVishkin, Jay
Sipelstein, Margaret Reid-Miller, TakisMetaxas, Bob
Harper, Jonathan Hardwick, JohnGreiner, Jacques
Cohen,andSiddhartha Chatterjee formanyhelpfulcom­
mentsonthisarticle.Siddhartha Chatterjee, Jonathan
Hardwick, JaySipelstein, andMarcoZaghahelpedinthe
designofNESLanddidalltheworkimplementing the
intermediate languages VCODE andCVL.Thisresearch
wassponsored inpartbytheAdvanced Research Projects
Agency (ARPA)undergrantnumber F33615-93-1-1330,
andinpartbyanNSFYoungInvestigator Award.
References
1.Aho,A.V.,andUllman, J.D.Foundations of Science.
Computer SciencePress,NewYork,1992.
2.Aho,A.V.,Hopcroft, J.E.,andUllman, J.D.The and
ofComputer Addison-Wesley, Reading,
Mass.,1974.
3.Arvind, R.,Nikhil,S.,andPingali, K.K.I-structures: Data
structures forparallelcomputing. ACMTrans.Program.
11,4(Oct.1989),598-632.
4.Blelloch, G.E.VectoT Data-Parallel MIT
Press,Cambridge, Mass.,1990.
5.Blelloch, G.E.NEsL:Anesteddata-parallel language (version
2.6).Tech.Rep.CMU-CS-93':129, SchoolofComputer Sci­
ence,Carnegie MellonUniv.,1993.
6.Blelloch, G.E.,andGreiner,J. Parallelism insequential func-
tionallanguages. In onFunctional
PTogTamming andComputer Architecture (June1995).
7.Blelloch, G.E.,andHardwick,J.C. Classnotes:Programming
parallelalgorithms. Tech.Rep.CMU-CS-93-115, Schoolof
Computer Science, Carnegie MellonUniv.,1993.
8.Blelloch, G.E.,Chatterjee, S.,Hardwick, J.C.,Sipelstein, J.,
andZagha,M.Implementation ofaportable nesteddata­
parallel language. ].Parallel DistTib. 21,1(Apr.
1994),4-14.
9.Brent,R.P.Theparallel evaluation of arithmetic
expressions.]. ACM21,2(1974),201-206.
10.Chandy, K.M.,andMisra,J.PaTallel AFoun-
dation.Addison-Wesley, Reading, Mass.,1988.
11.Cormen, T.H.,Leiserson, C.E.,andRivest,R.L.IntToduction
toAlgorithrns. Cambridge, Mass.,1990.
12.Feo,J.T.,Cann,D.C.,andOldehoeft, R.R.Areportonthe
Sisallanguage project.]. ParallelDistrib.Comput. 10,4(Dec.
1990),349-366.
13.Hatcher, P.,Tichy,W.F.,andPhilippsen, M.Acritiqueofthe
programming language C*.Commun. ACM35,6(June1992),
21-24.
14.HighPerformance FortranForum.High Fortran
Language May 1~~3.
15.Hillis,W.D.,andSteele,G.L.'Jr.Dataparallel algorithms.
Commun. ACM29,12(Dec.1986),12.
IIIlng
16.J<ija,JAnIntroduction toParallelAlgorithms. Addison-Wesley,
Reading, Mass.,1992.
17.Karp,R.M.,andRamachandran, V.Parallelalgorithms for
sharedmemory machines. InHandbook ofTheoretical Computer
Science- VolumeA:Algorithms andComplexity, J.VanLeeuwen,
Ed.MITPress,Cambridge, Mass.,1990.
18.Mills,P.H.,Nyland, L.S.,Prins,JF.,Reif,JH.,andWagner,
R.A.Prototyping parallelanddistributed programs inPro­
teus.Tech.Rep.UNC-CH TR90-041, Computer Science
Dept.,Univ.ofNorthCarolina, 1990.
19.Milner,R.,Tofte,M.,andHarper, R.TheDefinition ofStan­
dardML.MITPress,Cambridge, Mass.,1990.
20.Preparata,F.P.,andShamos, M.l.Computational Geometry­
AnIntroduction. Springer-Verlag, NewYork,1985.
21.Rose,JR.,andSteele,G.L.,Jr.C*:Anextended Clanguage
fordataparallelprogramming. InProceedings ofthe2dInter­
national Conference onSupercomputing, Vol.2(May)1987,pp.
2-16.
22.Schwartz,JT., Dewar,R.B.K.,Dubinsky, E.,andSchonberg,
E.Programming withSets:AnIntroduction toSETL.Springer­
Verlag,NewYork,1986.
23.Shiloach, Y,andVishkin, U.AnO(n2logn)parallel Max-Flowalgorithm. J.Algorithms 3(1982),128-146.
24.Sipelstein, JandBlelloch, G.E.Collection-oriented lan­
guages.InProceedings oftheIEEE79,4(Apr.1991),pp.504­
523.
25.Vishkin, U.Parallel-design distributed-implementation
(PDDI)general purpose computer. Theor.Comput. Sci.32
(1984),pp.157-172.
AbouttheAuthor:
GUYE.BLELLOCH isanassociate professor ofComputer Sci­
enceatCarnegie MellonUniversity. Author's PresentAddress:
Department ofComputer Science, Carnegie MellonUniversity,
Pittsburgh, PA15213-3891; email:blelloch@cs.cmu.edu
Permission tomakeadigital/hard copy ofpartorallofthisworkforper­
sonalorclassroom useisgranted withoutfeeprovided thatcopiesarenot
madeordistributed forprofitorcommercial advantage, thecopyright
notice,thetitleofthepublication anditsdateappear,andnoticeisgiven
thatcopying isbypermission ofACM,Inc.Tocopyotherwise, torepub­
lish,topostonservers,ortoredistribute tolistsrequires priorspecific
permission and/orafee.
©ACM0002-0782/96/0300 $3.50
CALLFOR1997ACMFELLOWS NOMINATIONS
Thedesignation "ACMFellow"maybeconferred uponthoseACMMemberswhohavedistinguished themselves by
outstanding technicalandprofessional achievements ininformation technology, whoarecurrentvotingmembers of
ACMandhavebeenvotingmembersforthepreceding fiveyears.AnyvotingmemberofACMmaynominateanoth­
ermemberforthisdistinction. Nominations mustbereceivedbytheACMFellowsCommittee nolaterthanAugustI
ofeachyearandmustbedeliveredtotheCommittee onformsprovidedforthispurpose(seebelow).
Nomination information organized byaprincipal nominator includes:
l)excerptsfromthecandidate's currentcurriculum vitae.listingselectedpublications, patents,technical
achievements, honors,andotherawards.
21adescription oftheworkofthenominee,drawingattentiontothecontributions whichmeritdesignation asFellow.
3)supporting endorsements fromfiveACMMembers.
ACMFellowsnomination formsandendorsement formsmaybeobtained fromACMbywritingto:
ACMFellowsNomination Committee
ACMHeadquarters
1515Broadway
NewYork,NewYork10036-5701
nominate-fellows@]acm.org
Theformscanalsobeaccessed onthefollowing:
http://www.acm.org/awards/fellows/nominatioD1_packet.htmI
Completed formsshouldbesentbyAugustI.1996tooneofthefollowing:
ACMFellowsCommittee
ACMHeadquarters
1515Broadway
NewYork,NewYork10036-5701
or
nominate-fellows@acm.org
or
+1-212-869-0824 -fax
C:OMMUNICA'I'IONS OPTHE.CMMarch1996/Vol. 39,No.397

TOWARDS MODULAR MACHINE LEARNING SOLUTION DEVELOPMENT :
BENEFITS AND TRADE -OFFS
Samiyuru Menik1Lakshmish Ramaswamy1
ABSTRACT
Machine learning technologies have demonstrated immense capabilities in various domains. They play a key
role in the success of modern businesses. However, adoption of machine learning technologies has a lot of
untouched potential. Cost of developing custom machine learning solutions that solve unique business problems
is a major inhibitor to far-reaching adoption of machine learning technologies. We recognize that the monolithic
nature prevalent in today’s machine learning applications stands in the way of efﬁcient and cost effective
customized machine learning solution development. In this work we explore the beneﬁts of modular machine
learning solutions and discuss how modular machine learning solutions can overcome some of the major solution
engineering limitations of monolithic machine learning solutions. We analyze the trade-offs between modular and
monolithic machine learning solutions through three deep learning problems; one text based and the two image
based. Our experimental results show that modular machine learning solutions have a promising potential to reap
the solution engineering advantages of modularity while gaining performance and data advantages in a way the
monolithic machine learning solutions do not permit.
1 I NTRODUCTION
Machine learning (ML) has gained a lot of attention over the
past years. Machine learning technologies have become a
part of many organizational workﬂows and day to day tasks
of individuals knowingly or unknowingly. Big tech compa-
nies and academic entities are taking the lead in developing
cutting edge ML technologies that push the boundaries of
what ML can accomplish. Cutting edge computer vision and
language modeling technologies provide a good example
for this. Beyond the heightened attention, existing appli-
cations and large scale organization and academic driven
developments, there is a lot of untouched potential to ML.
We believe that this potential lies in ground level applica-
tion domains that are usually away from the mainstream
attention. Think of organizations that operate in non-tech fo-
cused business domains. These can include educational and
research institutes, healthcare facilities, transportation units
and government organizations. These organizations have a
number of workﬂows that can be improved using ML tech-
nologies. Depending on the scale, these organizations are
more likely to have a traditional information technology and
software engineering staff to fulﬁll the tech requirements.
However, these organizations may not have the budgets,
resources and expertise to focus on producing custom ML
solutions for their workﬂows. Making cutting edge tech-
1School of Computing, University of Georgia, Athens, Geor-
gia, United States. Correspondence to: Samiyuru Menik
<sm19812@uga.edu >nologies accessible to a wider range of organizations and
different organizational levels and individuals that operate
in a wide range of domains is one of the big challenges
that ML as a ﬁeld face today. Enabling wide scale adoption
of ML technologies has the potential of achieving the next
level of business process optimizations, service quality im-
provements and user experiences. As an example, today’s
high level decision makers of large organizations already
use machine learning technologies intensively in their deci-
sion making processes. But how much of these technologies
are practical and cost efﬁcient to be implemented for the
use of lower levels of the organizational hierarchy? As
another more concrete example, a large scale organization
may implement a cutting edge machine learning solution to
ﬁlter resumes in their hiring process. How practical is it to
access such technologies to implement a similar system for
a smaller scale organization that matches their customized
business requirements? Recurring theme here is that, when
making machine learning technologies more accessible, 1/
cutting edge technologies should be accessible to a wider
audience through means that are easy to grasp 2/ it should
be possible to implement customized machine learning solu-
tions that match business requirements at a lower cost. We
believe that further improving software engineering prac-
tices in machine learning solution development, especially
focusing on deep learning approaches, can provide an ef-
fective solution to the aforementioned challenge. Today’s
prevalent deep learning solutions are monolithic. These
solutions are mostly large black boxes that produce the rightarXiv:2301.09753v1  [cs.LG]  23 Jan 2023
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
answer to a given problem when fed with a large amount of
relevant data. Major inﬂuence for this trend is coming from
end to end deep learning technologies. Unlike traditional
ML methods that involve time consuming and labor inten-
sive feature engineering steps, end to end machine learning
attempts use a larger single model that aims to learn all inter-
mediate steps required to solve the problem at hand without
needing much hand engineering during the learning process.
This is usually achieved by employing large parameterized
models that can take advantage of large amounts of training
data. This way of developing ML solutions signiﬁcantly cut
down the human effort needed to implement ML solutions
that produce impressive results for a variety of ML problems.
However, this approach of developing ML solutions has a
set of key disadvantages as well. They became especially
apparent when developing ML solutions outside of cutting
edge tech focused organizations.
One of the key limitations of prevalent end to end machine
learning solutions is the lack of modularity. The lack of
modularity results in a number of other critical engineering
challenges. Usually one monolithic solution is only applica-
ble to a single problem. Since the solution can not be broken
down into semantically meaningful and more general sub-
modules, the effort that went into the system can not be
easily reused in other contexts. Also, it is not possible to re-
place parts of the system as new technologies are introduced
with improved performance characteristics. Further, the
solutions have to rely on speciﬁc datasets that are focused
on the speciﬁc problem. Traditionally engineering practices
encourage modular solutions in general to overcome the
aforementioned challenges and develop more maintainable
and cost efﬁcient solutions. We believe that encouraging
modularity in deep learning solutions can bring a number of
advantages to deep learning solution development that other
more traditional engineering domains already experience.
Fig. 1 shows a visual representation of monolithic ML so-
lution development in comparison to modular ML solution
development. As a ﬁrst step towards this end, we explore the
cost beneﬁt trade-offs of solving machine learning problems
in a multi stage modular way in comparison to solving them
with a monolithic deep learning solution. In this work, we
perform this analysis through three example problems. For
each example problem, we will implement deep learning
solutions in two ways. One of the solutions will be devel-
oped taking a monolithic approach. The other solution will
be developed taking a modular approach. Then, we will be
evaluating the two approaches quantitatively and qualita-
tively to analyze the trade-offs of each approach. One of the
objectives of our experiments is to study if ML modules can
demonstrate modularity characteristics similar to traditional
non-ML software modules. During the evaluation we will
focus on several aspects of monolithic and modular solution
development. 1/ Exploring how the two approaches will beaffected by dataset availability and cost of data acquisition.
2/ Study the differences of solution development effort be-
tween monolithic approach and the modular approach. 3/
Trade offs between model performance in terms of accuracy
and efﬁciency. 4/ Maintainability of the solutions developed
with the two approaches.
2 M ODULARITY IN MACHINE LEARNING
Modularity is a familiar approach when solving complex
problems in many domains. It simpliﬁes the problem solv-
ing process by breaking down a complex problem into more
manageable subparts. After breaking the problem into sub-
problems the subproblems can be solved independently.
This process brings a number of advantages to the solu-
tion engineering process. The goal of making ML solutions
modular is to enable these advantages to ML solution engi-
neering.
Combinatorial generalization is one of the main advan-
tages of modular ML models. In general an ML model
that is trained for a speciﬁc problem is only useful for that
speciﬁc problem. Therefore the monolithic models that can
not be broken down into submodules are not usable outside
of the problem that it intends to solve. On the other hand
modular ML models are composed of more than one ML
model that each model solves a sub problem of the original
problem. This allows parts of the modular ML models to
be reused in different contexts beyond the original problem.
This enables mixing and matching modules to create new
unique models to solve different problems. This helps to
minimize the chances of having to start from the scratch
when developing unique solutions. On the other hand, when
using end to end learning methods that learn a monolithic
model, very often, each new problem is a unique problem
that has to be solved starting from fundamental technolo-
gies.
The same is true for datasets that are used to train ML mod-
els. Training monolithic models require a dataset speciﬁc
for the intended problem that each datapoint maps a speciﬁc
input type to a speciﬁc target type with other required char-
acteristics. Such a dataset is only useful to train a model
similar to the original problem. In comparison, modular ML
models are a composition of submodules where each model
is trained with a dataset that matches the subproblem. At the
same time, the process of breaking down a larger problem
into subproblems usually results in subproblems that are
simpler and more general. Therefore, within a development
ecosystem, modular ML solutions increase the chances of
reusing datasets to solve different problems.
Domain expert intervention to simplify the learning pro-
cess by incorporating domain knowledge when developing
solutions. One of the ways to do this is by decomposing the
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
Monolithic ML  
ModelML
Module
ML
Module
ML
ModuleML
Module
ML
Module
ML
ModuleML
Module
ML
Module
ML
ModuleML
Module
ML
Module
ML
ModuleML
Module
ML
Module
ML
Module
Developing solutions using ML Modules
from an eco-system≈
Developing end to end
monolithic models
Figure 1. Visualization of monolithic ML solution development in comparison to modular ML solution development in an echo-system. In
the modular solution development paradigm, ML modules are reused to develop different solutions by different users enabling enhanced
accessibility to technologies.
problem in a way that the resulting subproblems are more
data efﬁcient and simpler to learn. As a simple example,
think about a multi-digit classiﬁcation problem. In this case,
a domain expert may break down the problem to detect each
digit individually using a simpler model and later aggre-
gate the classiﬁcation results to ﬁnd the classiﬁcation for
the original multi digit input. This simple decomposition
made the subproblem simpler as well as more data efﬁcient
while increasing the amount of data points per classiﬁca-
tion class. In traditional software engineering modularity
allows engineering teams to divide work across individuals
or specialized teams. This minimizes the development over-
head by assigning units of work with cross-cutting concerns
to one individual or one team. In this process a module
boundary can deﬁne a unit of work that can be assigned
to an individual or a team. In addition to that this helps to
reduce dependencies among units of work and parallelize
the development process.
Performance tuning ability is higher with modular ML
models. Modularity enables breaking down a problem into
subproblems and addressing them separately. Since sub-
problems tend to be more general than the overall high level
problem, ﬁnding technologies and existing solutions for
the subproblems is likely to be easier. This enables more
options for solution developers to make performance trade-
offs at the subproblem level. Further, modularity makes it
easier to do incremental improvements to solutions. Indi-
vidual modules in modular systems can be replaced with
different modules with the same functionality but with dif-
ferent characteristics. As an example, one may trade offaccuracy for faster performance at one subproblem of the
modular solution to meet a business requirement at hand.
With monolithic solutions, making performance trade-offs
usually require replacing the whole model with a one that
has the required characteristics.
Maintainability of modular ML models is higher compared
to monolithic ML models. Since the submodules of modular
ML models can be replaced with different models with
the same functionality, newer or improved technologies
emerge, submodules can be upgraded without requiring
major changes to the entire solution. In addition to that,
since modular models are a composition of semantically
meaningful models, they are more human understandable.
This opens up more opportunities to verify and monitor
modular models. This simpliﬁes the process of isolating
issues and troubleshooting.
Economies of scale effect can be harnessed better with mod-
ular ML models from a development ecosystem perspective.
Since modular ML models can take better advantage of
existing ML modules by reusing them to create different
higher level solutions, the modules that are more commonly
reused in many problems get a higher demand from the
development community. As a result of this, more demand-
ing modules are likely to be further improved within the
ecosystem due to the economies of scale effect and these
improvements can be exploited by the downstream solutions.
On the other hand, in end to end ML, this effect is relatively
less prominent since the learned solutions are monolithic
and specialized to individual problems making them less
usable in other contexts.
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
3 C ASE STUDIES
As discussed before, we will be using three example prob-
lems in order to empirically highlight the beneﬁts and trade-
offs of modular and monolithic machine learning solutions.
First example problem is a text based sentiment analysis.
The other two problems are a satellite image classiﬁcation
problem and a near infrared (NIR) ﬁeld prediction problem.
3.1 Text based Sentiment Analysis
Text based sentiment analysis is useful in a number of busi-
ness contexts. For instance, businesses use sentiment analy-
sis to understand consumer sentiment towards their brand
and the products. In today’s internet based global market
setting, analyzing the sentiment of a text in a given language
is important for many business organizations. In this section
we are using this problem as a proxy to study the trade-offs
of monolithic and modular machine learning solutions.
Deep learning technologies have demonstrated state of the
art performance in sentiment analysis tasks. Current preva-
lent deep learning based models are monolithic in nature
(Tan & Le, 2019; Devlin et al., 2019). Monolithic deep learn-
ing solutions for sentiment analysis train machine learning
models end to end to predict the sentiment of a text from
a given source language. More modular solution for senti-
ment analysis is to develop the solution using two modules
that solve the problem in two intuitive stages. The ﬁrst stage
is to translate the source language text to a suitable target
language. The second stage is to analyze the sentiment of
the translated text. This approach enables the opportunity
to train a sentiment analysis model for a language with a
larger and more representative sentiment analysis dataset
or to ﬁnd a sentiment analysis model already trained for
a speciﬁc language that demonstrates higher performance
characteristics. In this section we will be implementing
two sentiment analysis solutions one monolithic and one
modular as described before and compare the advantages
and disadvantages of the two approaches.
This experiment is performed with a Spanish sentiment
analysis problem. Off the shelf pretrained language mod-
els are used to implement the monolithic solution and the
two stage modular solution for this problem. The mono-
lithic version of the experiment is conducted using an exist-
ing BERT based pretrained multilingual sentiment analysis
model (Wolf et al., 2019). The model is already trained
with 150k English, 80k Dutch, 137k German, 140k French,
72k Italian and 50k Spanish sentiment analysis data points.
The model predicts the sentiment of the input text in a 1
to 5 scale where 1 is the least positive and 5 is the most
positive sentiment. The ﬁrst stage of the modular version is
implemented with an existing pretrained spanish to english
translation model (Wolf et al., 2019; Tiedemann, 2020). The
second stage uses the same sentiment analysis model thatis used in the monolithic version. Further, the monolithic
solution and the modular solution were distilled (Hinton
et al., 2015) into a smaller model to see the performance
characteristics of the distilled solutions in comparison. At a
high level, the distillation process is analogous to program
compilation in software development. In software develop-
ment, the result of compilation is an object that runs much
faster during deployment. However, the compiled object is
less meaningful to humans compared to the program written
using a high level programming language. Similarly, in the
case of a modular ML solution, the distillation results in a
faster solution but compromises the explainability that is
present in the modular solution. Distillation is performed
with a smaller convolutional architecture as the student net-
work in both monolithic and modular cases. Input to this
model is an integer sequence that was created using a word
dictionary that contains a unique index for each word in
the corpus. Inputs to the student models are truncated to a
maximum length of 500 words and padded appropriately if a
sequence is short. The architecture of the distillation student
network is shown in ﬁg. 2. The distillation is done by con-
Figure 2. Architecture of the sentiment analysis student model.
sidering the teacher model as a blackbox to keep the process
more generally applicable. In the distillation process the
student is trained to imitate the teacher by minimizing the
cross entropy loss between the teacher model’s output and
the student model’s output. This distillation process does
not require any labeled data points. It only needs unlabeled
text from the input language.
Fig. 3 shows a diagram of the monolithic and the mod-
ular solutions used in the experiment with their distilled
counterparts.
Performance characteristics of these models are compared
using the test set of the spanish portion of the amazon mul-
tilingual product review sentiment dataset (Keung et al.,
2020). This contains 30000 sentiment analysis data points.
Each datapoint has spanish product review text and a 1 to 5
star rating that correspond to the product review text.
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
Figure 3. Monolithic (left) vs modular (right) sentiment analysis
models and the models distilled from them.
3.2 Satellite Image Classiﬁcation and NIR prediction
Satellite image based remote sensing is useful in a number
of real world applications such as land survey, surveillance,
trafﬁc monitoring etc. In this section we are studying the
trade-offs between modular vs monolithic ML solutions us-
ing a satellite image classiﬁcation problem. In this problem
we are classifying cloudy satellite images based on the Eu-
roSAT dataset (Helber et al., 2019) into 10 classes of land
use and land cover. We will be implementing one mono-
lithic model and two modular models for this classiﬁcation
problem. Performance of these three models will then be
evaluated based on accuracy and latency. The accuracy of
the solutions are also compared with a large percentage
of weight pruning. Further, we evaluate the performance
of models distilled from the monolithic and two modular
models. Next, we reuse a module from the modular classiﬁ-
cation solutions to predict the near infrared (NIR) band of
the EuroSAT based cloudy satellite images. This modular
NIR band prediction model is compared with a monolithic
model for the same task. Unlike in the previous example,
in this example we are training custom ML models for the
problem.
EuroSAT dataset has a red green blue (RGB) version and
a 13 band version. For our experiments, we use the RGB
and NIR bands in the EuroSAT dataset. EuroSAT dataset
only contains clear satellite images without obstructions.
We alter this dataset by adding a cloud overlay to the RGB
bands using the approach proposed by Kenji et al. (Enomoto
et al., 2017). Fig. 4 shows a sample of the altered EuroSAT
dataset. Added clouds makes the dataset more challeng-
Figure 4. EuroSAT data points before (top row) and after (bottom
row) adding cloud layers.ing for prediction tasks compared to the original EuroSAT
dataset. Since we want to represent the low labeled data
regime that is commonly seen in real world industrial appli-
cation domains, we use only a 20% of the EuroSAT labeled
data to train, validate and test the models for the classiﬁ-
cation and NIR band prediction tasks. In the case of the
classiﬁcation task we use the cloudy RGB image as the in-
put and the corresponding classiﬁcation label as the target.
In the case of NIR prediction, the cloudy RGB image is
used as the input and the corresponding NIR band is used as
the target. The rest of the data that is not used for the clas-
siﬁcation and NIR prediction is used to train, validate and
test the cloud removal module that is used in the modular
models. It should be noted that this training step does not
utilize the labels from the original EuroSAT dataset. This
cloud removal dataset has the RGB images with the cloud
overlay as the input and the corresponding cloud free RGB
images as the target. Such an unlabeled dataset is relatively
easy to acquire in larger quantities in practice in the real
world as well since it does not involve manual data labeling.
Our monolithic classiﬁcation model is trained end to end,
validated and tested using the classiﬁcation split of the
cloudy satellite images. The network architecture used in
the monolithic model is shown in ﬁg. 5. This architecture
downsamples the feature maps while increasing the number
of channels as the layers progress from input to the output.
In the ﬁnal layers the output of the convolutional layers are
ﬂattened and sent through dense layers to do the classiﬁca-
tion. Dropouts are used before the features are fed to the
dense units. The modular versions perform the classiﬁcation
Figure 5. Architecture of the satellite image classiﬁcation model.
in two stages. The ﬁrst stage performs cloud removal. Cloud
removal is performed using the encoder decoder network
architecture that is shown in ﬁg. 6. First part of this archi-
tecture, the encoder part, downsamples the feature maps
using 6 downsampling blocks. Each downsampling block
has a convolutional layer, batch normalization layer and a
leaky relu activation. The ﬁrst downsampling block does
not have batch normalization. Each downsampling block
from input to output reduces the feature map size while
increasing the number of channels. The second part of the
architecture, the decoder part, upsamples the feature maps
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
that are downsampled by the decoder using four upsam-
pling blocks. Each upsampling block has a convolutional
transpose layer, dropout layer and a relu activation layer.
The ﬁrst upsampling block does not have a dropout layer.
Each upsampling layer from input to output increases the
width and the height of feature maps while reducing the
number of channels. As shown in the architecture ﬁgure,
skip connections are used from the downsampling blocks
to upsampling blocks with a mirror-like correspondence
to incorporate low level features to the latter upsampling
layers. Finally the output of the upsampling layers are sent
through another convolutional transpose layer with three
channels and a sigmoid activation function. This architec-
ture is adapted from the U-net (Ronneberger et al., 2015)
and pix2pix (Isola et al., 2017) architectures. The second
Figure 6. Encoder decoder architecture of the cloud removal
model.
stage performs classiﬁcation on the cloud removed images.
In the ﬁrst modular solution the classiﬁer is trained using the
output of the cloud removal module. This modular version
represents the case where the classiﬁcation data available for
training is cloudy. We will be calling this solution modular
(I). In the second modular version the classiﬁer is trained
using cloud free satellite images. This modular version rep-
resents the case where the classiﬁcation data available for
training are cloud free. We will be calling this solution mod-
ular (II). This corresponds to making a barebone pretrained
satellite image classiﬁcation module to be published in a
model repository to be used by others. All classiﬁers have
the same network architecture shown in ﬁg. 5 and they are
trained using the classiﬁcation data split that was discussed
before. The cloud removal module is trained with the re-
maining data split that was described before. The output of
the cloud removal module is shown in ﬁg. 7. Further, three
distilled models are trained from each monolithic, modular
(I) and modular (II) solutions. Distillation is done by using
the same approach that was used in the sentiment analysis
case. However, before the distillation, student models that
correspond to the monolithic solution and the modular (I)
version are pre-trained with the available labeled cloudy
classiﬁcation data. Student model of the modular (II) ver-
Figure 7. Output (bottom row) of the cloud removal module when
it is fed with cloudy data (top row).
sion is not pretrained this way because it represents the case
where cloudy labeled classiﬁcation data is not available.
4 T RADE -OFFS OF MODULARITY
In this section we will use the solutions we developed for
the three example problems to compare and contrast the
trade-offs between monolithic and modular solutions.
4.1 Accuracy
This section compares the accuracy of the monolithic and
modular solutions using solutions developed for the senti-
ment analysis and satellite image classiﬁcation problems.
4.1.1 Sentiment Analysis
For the sentiment analysis case, the spanish product review
test set from the amazon multilingual product review sen-
timent dataset is used to compare the accuracy of the two
original monolithic and modular solutions and the two dis-
tilled versions of the solution. In this experiment we use
a one-off accuracy measure to quantify the performance.
Here we consider a prediction of the model as correct if the
predicted star rating is exact or off by only one. If not, we
consider the prediction as incorrect. We believe that this
measure is more realistic because there is no universally
agreeable star rating for a given review text. The results of
this experiment are shown in ﬁg. 8.
Figure 8. Sentiment analysis accuracy of monolithic and modular
solutions including their distilled versions.
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
The results in ﬁg. 8 shows that the original monolithic
solution has less than 1 percentage point higher accuracy
over the original modular solution. The solution distilled
from the monolithic solution is less than 2 percentage points
lower in accuracy compared to the original monolithic so-
lution. The model distilled from the modular solution is
less than 1 percentage point lower compared to the original
modular solution. The model distilled from the modular
version has 0.3% higher accuracy compared to the model
distilled from the monolithic solution. The results show that
modular solutions can be comparable in terms of accuracy
to monolithic solutions.
4.1.2 Satellite Image Classiﬁcation
The test split of the EuroSAT dataset is used to measure the
accuracy of the models and the results are shown in ﬁg. 9.
Figure 9. Cloudy satellite image classiﬁcation accuracy of mono-
lithic and modular solutions including their distilled versions.
The results in ﬁg. 9 shows that the modular (I) solution that
was trained with cloudy labeled data with the added cloud
removal module performs the best out of the three solutions.
It has been able to achieve 11.34% accuracy improvement
over the monolithic solution when comparing the original
non-distilled solutions. When comparing the distilled so-
lutions, the modular (I) solution was able to achieve 10%
accuracy improvement over the corresponding monolithic
solution. This accuracy improvement can be attributed to
the cloud removal module in the modular version that was
capable of utilizing low cost unlabeled data. Modular (II)
solution in which the classiﬁcation module is trained with
clean data has the lowest accuracy. However, this solution at-
tains its accuracy level without using any labeled data points
from its target problem, cloudy satellite image classiﬁcation.
4.2 Latency
This section compares the latency of the monolithic and
modular solutions using solutions developed for the senti-
ment analysis and satellite image classiﬁcation problems.4.2.1 Sentiment Analysis
The spanish product review test set of the amazon review
dataset is used to compare the latency of the two origi-
nal monolithic and modular solutions and the two distilled
versions of the solution. In this experiment we measure
the time it takes for each model to process the 30000 test
data points. The results are shown in ﬁg. 10. The experi-
ments are conducted on a machine with Intel(R) Xeon(R)
CPU @ 2.20GHz, 13298580 kB of RAM and a Tesla P100
16280MiB GPU.
Figure 10. Sentiment analysis latency of monolithic and modular
solutions including their distilled versions.
The results in ﬁg. 10 shows that the original modular so-
lution is much slower compared to the original monolithic
solution. This performance drop is mainly due to the trans-
lation model used in the ﬁrst stage of the modular solution.
However, the solutions distilled from each of the original
solutions are much faster as we can expect. These results
suggest that developing modular solutions and distilling
them into smaller models can result in efﬁcient solutions
with minor accuracy trade-offs. The additional beneﬁt of
this approach is that the modular solution development pro-
vides a number of engineering advantages as discussed in
the introduction section.
4.2.2 Satellite Image Classiﬁcation
To compare the latency of the monolithic and modular so-
lutions, the time that each solution takes to process 56700
data points is measured. The same is done with the distilled
solutions. The latency measurements for each solution is
taken on a machine with Intel(R) Xeon(R) CPU @ 2.20GHz,
13298580 kB of RAM and a Tesla P100 16280MiB GPU.
The results are shown in ﬁg. 11.
The latency results in ﬁg. 11 shows that the non-distilled
monolithic solution took 63.83% less time compared to the
fastest modular solution to process the data load. However,
distilled versions have been able to address this issue by
achieving low latencies comparable to the monolithic ver-
sion, still having better accuracy than the monolithic version
in the case of modular (II) solution.
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
Figure 11. Cloudy satellite image classiﬁcation latency (lower the
better) of monolithic and modular solutions with the respective
distilled versions.
4.3 Reusability
In this section we are testing whether an ML module that
was used in one problem can be used in another problem.
This is different from general transfer learning where base
layers of a larger model are transferred to a similar task.
Here we are reusing a semantically meaningful ML module
in a different problem.
After the classiﬁcation comparison, the cloud removal mod-
ule is reused in a different task to evaluate the reusability
of the module. In this task the RGB bands of the cloudy
EuroSAT dataset are used to predict the NIR band for the
image. Two monolithic and modular solutions are imple-
mented for this task. The monolithic solution is trained end
to end on the cloudy RGB images. Monolithic model uses
an encoder decoder network architecture similar to the one
used in the cloud removal module but with a single output
channel. The modular version predicts the NIR band in two
stages. The ﬁrst stage removes the clouds from training data
by reusing the cloud removal module that was trained dur-
ing the previous classiﬁcation task. The second stage uses
the output of the cloud removal module to predict the NIR
band. Model used in the second stage uses the same network
architecture used in the monolithic version. Two distilled
models are created in this case as well using the monolithic
and modular versions of the solution. Both modular and
monolithic versions of the solutions are trained using the
NIR prediction training data split that was explained before.
The distillation was performed following the same process
used for distilling the monolithic and modular (I) classiﬁca-
tion models. However, in this case the distilled models are
pre trained with the NIR prediction dataset before the distil-
lation. After the training and distillation steps, test split of
the NIR prediction dataset is used to measure the accuracy
of each solution. The results of this experiment are shown
in ﬁg. 12. The latency of each solution is measured for
processing 56700 data points using each version of the solu-
tion on a machine with Intel(R) Xeon(R) CPU @ 2.20GHz,13298580 kB of RAM and a Tesla P100 16280MiB GPU.
The latency results are shown in ﬁg. 13.
Figure 12. Mean squared error (lower the better) of monolithic and
modular NIR prediction solutions including their distilled versions.
Figure 13. Latency (lower the better) of monolithic and modular
NIR prediction solutions with the respective distilled versions.
The results in ﬁg. 12 and 13 resembles the pattern we ob-
served in the classiﬁcation case with monolithic and modular
(I) solutions. The modular solution has higher performance
in terms of accuracy/error. However, it has higher latency
compared to the monolithic version as one would expect.
The solution distilled from the modular model has lower
error compared to the both original monolithic solution and
the solution distilled from the monolithic solution while
having comparable latency values.
4.4 Maintainability
In this section, we empirically highlight the trade-offs of
the monolithic and the modular solutions with respect to
maintainability as the requirements change. We will be
considering a case where the requirements of the model
change to handle noisy images in the cloudy satellite image
classiﬁcation problem.
In this experiment, we modify the classiﬁcation dataset that
we used before by adding gaussian noise to the RGB chan-
nels to represent noisy satellite images with cloud cover.
Fig. 14 shows the images after adding noise. Then the noisy
data is fed to the previously trained monolithic classiﬁca-
tion model and the modular (I) classiﬁcation model and the
classiﬁcation accuracy is measured using a held out test set.
Additionally, we create an improved modular solution by up-
dating the cloud removal module of the modular (I) solution.
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
Figure 14. Cloudy EuroSAT data points after adding gaussian
noise.
The cloud removal module is updated by training it with
unlabeled noisy cloud images. This improvement makes
the cloud removal module robust to noise. In the improved
modular solution, the classiﬁcation module remains to be
the same module used in the original modular (I) solution.
The accuracy of the improved modular solution is measured
with the same noisy held out test data. Accuracy of each
model is shown in ﬁg. 15. As shown in the results in ﬁg.
Figure 15. Classiﬁcation accuracy for noisy satellite images with
cloud cover.
15, both monolithic and modular (I) solutions signiﬁcantly
drop in accuracy when fed with noisy images. However, the
modular (I) solution has the capacity to replace modules
with improved ones. By replacing the cloud removal mod-
ule with an updated noise robust cloud removal module, the
improved modular solution could achieve around 2 times
higher accuracy compared to the original modular (I) solu-
tion. It should be noted that this accuracy gain was achieved
without making any changes to the classiﬁcation module
and by only using noisy unlabeled data. The monolithic
solution can not be improved this way using unlabeled data.
Improving the monolithic solution needs labeled data that
are usually labor intensive to produce.
5 O PEN CHALLENGES
Modularity in ML has a number of advantages. We dis-
cussed them in detail qualitatively and quantitatively. How-
ever, modular ML has several main open challenges when it
comes to developing effective modular ML models.
Breaking down the original problem into a set of subprob-
lems is not always feasible. Some problems do not have
semantically meaningful subproblems. In these cases we
have to consider the problem as an atomic unit and utilize
end to end learning methods. After that the trained model
has the potential to be used in a future Modular ML model.
In situations where new models have to be trained to solvesubproblems, ﬁnding datasets for them can sometimes be
challenging. This issue can be mitigated by performing the
problem decomposition to match data that is available. In
this case, the trade-offs of different problem decompositions
should be further studied. Further, synthetic data can be
helpful to ﬁll some of the data gaps.
In some cases, module compositions may not perform in
synergy as expected. Sometimes even if the individual
models perform well, when they are composed together to
solve a larger problem, the performance can be unexpectedly
low. This can happen due to various incompatibilities among
submodules. Studies on ML adversarial attacks may be able
to shed some light in this regard. Further understanding the
reasons for such failures and methods is important to make
modular ML applicable in a wider array of problems.
Lack of feature rich repositories to publish and search ML
models and datasets is another challenge to using modu-
lar ML in practice. Existing systems for publishing and
searching ML models do not provide sufﬁcient metadata
and semantically meaningful search capabilities to look for
models and datasets that can satisfy speciﬁc requirements.
More advanced ML models and data repositories with se-
mantic search capabilities and metadata support (Menik &
Ramaswamy, 2021) should be developed to make modular
ML practical.
Addressing these challenges through future work is key to
reaping the beneﬁts of modular machine learning.
6 R ELATED WORK
There are several existing lines of work that are related to
modularity in machine learning. There have been attempts
to break pre-trained neural networks into a set of modules
based on the learned weights. Ultimate goal of this is to ﬁnd
semantic modules within a learned neural network. Csordas
et. al. (Csord ´as et al., 2021) have attempted to do this by
learning weight masks to identify subnetworks for target
tasks. They were able to ﬁnd some specialized subnetworks
in trained neural networks with some other interesting in-
sights. However, to the best of our knowledge, these works
have so far not been able to ﬁnd strong semantically mean-
ingful modules within learned monolithic neural networks
that can be reused in other contexts. Ensemble learning
methods (Sagi & Rokach, 2018) has a sense of modularity.
An ensemble model is not a single monolithic unit. Ensem-
ble methods like bagging, stacking and boosting combine
several machine learning models to create a more robust
single solution. In these methods each model in the ensem-
ble will be specialized in one aspect of the whole problem.
These specialized parts together create a better ﬁnal solu-
tion to the overall machine learning problem. However, the
modules in ensemble models are usually not reusable in
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
other systems since they are specialized to a given speciﬁc
ensemble that solves one problem. Another line of work
that attempts to incorporate modularity in deep learning is
routing networks (Cases et al., 2019). Modular question
answering approach proposed by Jacobs (Andreas et al.,
2016) is another example for similar work. This line of
work mainly tries to learn a set of modules and a controller
to compose these modules to solve problems. This learn-
ing process is usually done in an end to end fashion. Even
though these approaches have shown good performance in
the problems that the model was trained to, the modules
have not shown to be much useful outside of the problem
in concern. Therefore, routing networks, in their current
state, are not yet capable of addressing the problems that
we discussed earlier. Transfer learning techniques (Zhuang
et al., 2020) allow machine learning model developers to
train customized solutions with less amount of training data
by ﬁne tuning an already existing machine learning model
that is usually pre-trained with a large dataset. Transfer
learning has shown to be very effective when an already
existing model is similar to the target task. There are few
downsides to transfer learning approaches. First, the model
developers usually have to be knowledgeable about the pre-
existing model internals in order to be able to modify it to
match the target machine learning problem. The modiﬁed
model has to be retrained with new training data that better
represent the new machine learning problem. This is dif-
ferent from traditional software engineering modules that
attempt to expose a clean interface to the users by hiding
the module internals. When it comes to making machine
learning technologies more accessible to a wider audience,
transfer learning techniques should be further improved to
enable a more ready to use modularity that minimizes the
chances of developers having to work with the internals of
complex network architecture.
7 S UMMARY
In this work, while acknowledging the immense potential
and positive impact of current deep learning technologies,
we discussed the challenges and limitations of widespread
monolithic deep learning technologies with respect to sys-
tems engineering concerns especially when it comes to
wider adoption of these technologies in diverse organiza-
tions. We pointed out semantic modularity in machine learn-
ing as an interesting avenue to address a number of problems
in this regard. Next, as a ﬁrst step, we used three example
problems to explore the beneﬁts and trade-offs between de-
veloping machine learning solutions in a monolithic way
and developing them in a multi-stage modular way. The
experiments showed how modular solutions can reuse ex-
isting pretrained models and exploit more data to achieve
higher accuracy and overcome data limitations in ways that
monolithic solutions do not permit. Further, we used black-box knowledge distillation to overcome the performance
challenges that modular solutions can have and showed the
impact of accuracy and latency in comparison to original
monolithic and modular solutions. The experimental results
in this work suggest that it is very interesting to further inves-
tigate the potential of multi stage modular machine learning
solution development in contrast to widespread monolithic
end to end machine learning solution development.
REFERENCES
Andreas, J., Rohrbach, M., Darrell, T., and Klein, D. Learn-
ing to compose neural networks for question answering.
arXiv preprint arXiv:1601.01705 , 2016.
Cases, I., Rosenbaum, C., Riemer, M., Geiger, A., Klinger,
T., Tamkin, A., Li, O., Agarwal, S., Greene, J. D., Juraf-
sky, D., et al. Recursive routing networks: Learning to
compose modules for language understanding. In Pro-
ceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguis-
tics: Human Language Technologies, Volume 1 (Long
and Short Papers) , pp. 3631–3648, 2019.
Csord ´as, R., van Steenkiste, S., and Schmidhuber, J. Are
neural nets modular? inspecting functional modularity
through differentiable weight masks. In 9th International
Conference on Learning Representations, ICLR 2021,
Virtual Event, Austria, May 3-7, 2021 . OpenReview.net,
2021. URL https://openreview.net/forum?
id=7uVcpu-gMD .
Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT:
pre-training of deep bidirectional transformers for lan-
guage understanding. In Burstein, J., Doran, C., and
Solorio, T. (eds.), Proceedings of the 2019 Conference of
the North American Chapter of the Association for Com-
putational Linguistics: Human Language Technologies,
NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7,
2019, Volume 1 (Long and Short Papers) , pp. 4171–4186.
Association for Computational Linguistics, 2019. doi:
10.18653/v1/n19-1423. URL https://doi.org/
10.18653/v1/n19-1423 .
Enomoto, K., Sakurada, K., Wang, W., Fukui, H., Mat-
suoka, M., Nakamura, R., and Kawaguchi, N. Filmy
cloud removal on satellite imagery with multispectral
conditional generative adversarial nets. In 2017 IEEE
Conference on Computer Vision and Pattern Recognition
Workshops, CVPR Workshops 2017, Honolulu, HI, USA,
July 21-26, 2017 , pp. 1533–1541. IEEE Computer Soci-
ety, 2017. doi: 10.1109/CVPRW.2017.197. URL https:
//doi.org/10.1109/CVPRW.2017.197 .
Helber, P., Bischke, B., Dengel, A., and Borth, D. Eurosat:
A novel dataset and deep learning benchmark for land
Towards Modular Machine Learning Solution Development: Beneﬁts and Trade-offs
use and land cover classiﬁcation. IEEE J. Sel. Top. Appl.
Earth Obs. Remote. Sens. , 12(7):2217–2226, 2019. doi:
10.1109/JSTARS.2019.2918242. URL https://doi.
org/10.1109/JSTARS.2019.2918242 .
Hinton, G. E., Vinyals, O., and Dean, J. Distilling the knowl-
edge in a neural network. CoRR , abs/1503.02531, 2015.
URL http://arxiv.org/abs/1503.02531 .
Isola, P., Zhu, J., Zhou, T., and Efros, A. A. Image-
to-image translation with conditional adversarial net-
works. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, Honolulu, HI,
USA, July 21-26, 2017 , pp. 5967–5976. IEEE Computer
Society, 2017. doi: 10.1109/CVPR.2017.632. URL
https://doi.org/10.1109/CVPR.2017.632 .
Keung, P., Lu, Y ., Szarvas, G., and Smith, N. A. The
multilingual amazon reviews corpus. In Webber, B.,
Cohn, T., He, Y ., and Liu, Y . (eds.), Proceedings of the
2020 Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP 2020, Online, November 16-
20, 2020 , pp. 4563–4568. Association for Computational
Linguistics, 2020. doi: 10.18653/v1/2020.emnlp-main.
369. URL https://doi.org/10.18653/v1/
2020.emnlp-main.369 .
Menik, S. and Ramaswamy, L. Towards a robust knowl-
edge graph-enabled machine learning service description
framework. In 15th IEEE International Conference on Se-
mantic Computing, ICSC 2021, Laguna Hills, CA, USA,
January 27-29, 2021 , pp. 104–107. IEEE, 2021. doi:
10.1109/ICSC50631.2021.00026. URL https://doi.
org/10.1109/ICSC50631.2021.00026 .
Ronneberger, O., Fischer, P., and Brox, T. U-net: Con-
volutional networks for biomedical image segmentation.
In Navab, N., Hornegger, J., III, W. M. W., and Frangi,
A. F. (eds.), Medical Image Computing and Computer-
Assisted Intervention - MICCAI 2015 - 18th Interna-
tional Conference Munich, Germany, October 5 - 9,
2015, Proceedings, Part III , volume 9351 of Lecture
Notes in Computer Science , pp. 234–241. Springer, 2015.
doi: 10.1007/978-3-319-24574-4 n28. URL https://
doi.org/10.1007/978-3-319-24574-4_28 .
Sagi, O. and Rokach, L. Ensemble learning: A survey. Wiley
Interdisciplinary Reviews: Data Mining and Knowledge
Discovery , 8(4):e1249, 2018.
Tan, M. and Le, Q. V . Efﬁcientnet: Rethinking model
scaling for convolutional neural networks. In Chaud-
huri, K. and Salakhutdinov, R. (eds.), Proceedings of
the 36th International Conference on Machine Learning,
ICML 2019, 9-15 June 2019, Long Beach, California,
USA, volume 97 of Proceedings of Machine LearningResearch , pp. 6105–6114. PMLR, 2019. URL http://
proceedings.mlr.press/v97/tan19a.html .
Tiedemann, J. The Tatoeba Translation Challenge – Realis-
tic data sets for low resource and multilingual MT. In Pro-
ceedings of the Fifth Conference on Machine Translation ,
pp. 1174–1182, Online, November 2020. Association
for Computational Linguistics. URL https://www.
aclweb.org/anthology/2020.wmt-1.139 .
Wolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C.,
Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M.,
and Brew, J. Huggingface’s transformers: State-of-the-
art natural language processing. ArXiv , abs/1910.03771,
2019.
Zhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y ., Zhu, H., Xiong,
H., and He, Q. A comprehensive survey on transfer
learning. Proceedings of the IEEE , 109(1):43–76, 2020.

iDML: Incentivized Decentralized Machine
Learning
Haoxiang Yu, Hsiao-Yuan Chen, Sangsu Lee, Sriram Vishwanath, Xi Zhengy, Christine Julien
Department of Electrical and Computer Engineering, University of Texas at Austin
fhxyu, littlecircle0730, sethlee, sriram, c.julien g@utexas.edu
yDepartment of Computing, Macquarie University, james.zheng@mq.edu.au
Abstract —With the rising emergence of decentralized and
opportunistic approaches to machine learning, end devices are
increasingly tasked with training deep learning models on-devices
using crowd-sourced data that they collect themselves. These
approaches are desirable from a resource consumption perspec-
tive and also from a privacy preservation perspective. When the
devices beneﬁt directly from the trained models, the incentives
are implicit – contributing devices’ resources are incentivized by
the availability of the higher-accuracy model that results from
collaboration. However, explicit incentive mechanisms must be
provided when end-user devices are asked to contribute their
resources (e.g., computation, communication, and data) to a task
performed primarily for the beneﬁt of others, e.g., training a
model for a task that a neighbor device needs but the device
owner is uninterested in. In this project, we propose a novel
blockchain-based incentive mechanism for completely decentral-
ized and opportunistic learning architectures. We leverage a
smart contract not only for providing explicit incentives to end
devices to participate in decentralized learning but also to create
a fully decentralized mechanism to inspect and reﬂect on the
behavior of the learning architecture.
Index Terms —pervasive computing, decentralized machine
learning, blockchain, incentive mechanisms, smart contracts
I. I NTRODUCTION
With the continued development of machine learning tech-
niques and the increasing computing power of personal de-
vices, machine learning has been heavily applied to various
user-facing tasks, such as photo classiﬁcation, input prediction,
smart home systems, etc [1]. These tasks commonly rely on
deep neural networks (DNNs) that learn patterns over large
data sets. Achieving high accuracy for those tasks requires a
massive amount of training data. However, largely because
of privacy concerns and limited communication bandwidth
between end-user devices and the cloud, gathering sufﬁcient
data to train robust models is challenging [2].
To support increased user privacy and enable applications to
operate in communication-constrained environments, machine
learning has expanded to include a variety of distributed learn-
ing approaches, where end devices assume some of the burden
associated with training. Most well-known are approaches to
federated learning , where a central server coordinates a vast
array of end-user devices to iteratively train a shared global
model. End devices receive a snapshot of a model, perform
some rounds of training of the model using locally available
data, then send model updates to the central server. The server
aggregates the received updates, generates a new global model,then repeats the process. In a pure federated learning design,
the central server does not have any training data of its own,
which makes it challenging for it to detect whether end nodes’
contributions to the shared model are valid.
In some contexts, relying on a central server to coordinate
the learning process of a distributed set of end devices is
either not possible (e.g., the devices are not well connected
to the Internet) or not optimal (e.g., the devices desire to learn
more personalized models rather than a single global model).
Thus, approaches have emerged that completely decentralize
the learning process and instead rely only on opportunistic
device-to-device collaboration among end-user devices. Exist-
ing research has shown that such decentralized approaches can
achieve high model accuracy under a variety of conditions [3],
[4]. Throughout this paper, we refer to such approaches that
rely only on device-to-device encounters to train models on-
device as decentralized opportunistic learning . Decentralized
opportunistic learning uses ephemeral encounters to support
on-device training in collaboration with encountered neighbors
and their local (private) data. As a straightforward example,
gossip learning [4] simply decentralizes the entire learning
process, using only opportunistic pair-wise exchanges between
nodes to incrementally learn the same shared global model
learned in a centralized approach. As a second example,
inOpportunistic Collaborative Learning (OppCL) [3], rather
than exchanging gradients learned while seeking convergence
to a global model, devices learn individual models inﬂuenced
by the unique combination of a device’s own data and the
data it encounters. In OppCL, when a device encounters
another device, it shares its personal model parameters with
the neighbor who then uses their own local data to compute
an update speciﬁc to the learning device’s goal.
Decentralized opportunistic learning requires many activ-
ities of end devices, and each of these activities requires
devices to contribute resources. By its nature, any approach
to decentralized learning relies on end devices to donate their
data, computational, and communication resources. As such,
incentivizing participation in the learning process remains a
signiﬁcant open challenge to realizing decentralized oppor-
tunistic learning in practice. To support on-device training,
devices must collect training data, which may need to be
labeled with valid labels. For instance, if the application is
to train an object detection system [5], the data collected
are photographs, and each photo must be labeled with thearXiv:2304.05354v1  [cs.LG]  10 Apr 2023
primary object in the frame. If the application is next word
prediction [6], the data collected is the potentially sensitive text
that the user types, with the labels coming “for free” based on
the sequences of words input. Data collection and labeling
consume resources either explicitly (e.g., the user actively
labels data) or implicitly based on the computation required
to label. The data must be stored, and it must be used to train,
which requires computational resources from each end node.
And as the participating devices collaborate, their exchanges
also consume valuable communication resources.
In addition, while the design of decentralized opportunistic
learning protects a user’s privacy by preventing the direct
sharing of raw data, it is difﬁcult for the recipient of a model
update to ascertain whether the other parties participating are
faithfully holding up their obligations to the algorithm. Further,
because the entire process is decentralized and opportunistic,
it is difﬁcult to inspect and reﬂect on the provenance of the
model —- to do so requires storing and sharing what happens
during training, how the model’s accuracy evolves, what data
is used to train the model, etc. More generally, due to the
decentralized architecture, it can be difﬁcult to avoid or detect
attackers of all kinds, including those seeking to interrupt
training and those seeking to subvert it.
The completely decentralized and distributed nature of this
problem naturally lends itself to blockchain [7], a type of
distributed ledger technology, and associated smart contracts,
a term originally referring to the automation of legal con-
tracts [8] that now popularly refers to the code scripts that run
synchronously on multiple nodes of a distributed ledger [9].
The natural mechanisms for blockchain to store and compute
in a decentralized way makes it ripe for integration with
decentralized opportunistic learning. In particular, we pro-
pose to combine the advantages of decentralized opportunistic
learning with the smart contract to (1) robustly incentivize
participation in opportunistic learning; (2) provide a means
to record and reﬂect on model provenance in decentralized
learning; and (3) examine a new opportunistic decentralized
learning pipeline that is robust to a well-deﬁned attack model.
This paper makes the following concrete contributions:
We developed a blockchain-based smart contract that
can be applied to decentralized opportunistic learning
structures that incentivize participants to contribute to the
learning process.
We design mechanisms into the smart contract to support
using it to record and examine essential information
related to the progress of a completely decentralized
learning execution.
We demonstrate how the use of this smart contract can
prevent attacks on both the new incentive mechanism and
the decentralized opportunistic learning algorithm.
We evaluate the incentive system in simulation with
different opportunistic yet fully decentralized learning
algorithms in place. The developed techniques will be
released as open-source artifacts.
The paper is organized as follows: in Section II, we discuss
the related work in decentralized learning, smart contracts, andincentive mechanisms. In Section III, we present our system
design and smart contract workﬂow. In Section IV, we use a
simulation system to benchmark the behavior of our proposed
system with two different learning algorithms, two different
datasets, and two typical data distributions in decentralized
learning (IID/Non-IID). In the ﬁnal section of the paper, we
conclude and explore possible future work.
II. R ELATED WORK
In recent years, the research community has introduced
several new models of distributed learning, which provides
new opportunities to beneﬁt from users’ data and comp-
tuational resources with a minimum risk of privacy leaks.
The related work of this research has four major compo-
nents: (1) distributed and decentralized learning approaches;
(2) blockchain and smart contracts; (3) incentive mechanisms
for distributed and decentralized learning; and (4) attacks on
incentive mechanisms and decentralized learning algorithms.
A. Distributed Learning
The growth in mobile device capabilities and the desire for
data privacy have motivated federated and decentralized learn-
ing. In federated learning, devices collaborate to construct a
global model in a way that maintains the data privacy for each
individual device [10], [11]. Applications of federated learning
in pervasive computing include wake word detection, also
known as keyword spotting in smart home voice assistants,
where federated learning protects the potentially private audio
data collected at users’ end devices [12]. Another classic fed-
erated learning application is next word prediction on a mobile
device keyboard [13]. Other applications have also explored
on-device image classiﬁcation and image processing [14].
These traditional approaches to federated learning assume
the existence of a single central coordinator, and the goal
of the devices in model aggregation is to collaboratively
learn a single global model that is shared among all of the
participants. This differs from decentralized and opportunistic
approaches, in which the learning process progresses only
through opportunistic encounters between devices. In this
work, we consider two different styles of decentralized oppor-
tunistic learning: gossip-based learning [4] and opportunistic
collaborative learning [3]. In gossip-based learning, each time
a device encounters another, it performs a merge-update-
send cycle. In this process, the encountered neighbor shares
its locally trained model, the receiving device merges the
received model into its local model, then the receiving device
performs training on the merged model using only the local
data. Of course, this is commonly a two-way process; in
a bi-directional encounter, both devices perform the merge-
update-send process independently. In gossip-based learning,
the goal is for an intermittently interconnected set of nodes to
eventually learn a consistent shared global model. In contrast,
in the opportunistic collaborative architecture, each device
seeks to learn its own personalized model but to oppor-
tunistically integrate learning based on neighbors’ local data
on-demand. In opportunistic collaborative learning, a device
that encounters another device engages in a send-train-return-
merge cycle. In this process, when a device encounters a
neighbor, it sends the neighbor a copy of the locally stored
model. The neighbor performs a round of training on that
model using the neighbor’s local data then returns the updated
model. The device merges the updated model with the stored
model and continues training with the local data.
As can be seen from these descriptions, both models ask
devices to contribute local resources to support the learning
process for themselves and other devices. A signiﬁcant chal-
lenge in designing such a decentralized and opportunistic sys-
tem is incentivizing devices to contribute their local resources
for the greater good and in a secure manner. This is the main
problem we tackle in this paper.
B. Blockchain and Smart Contract
In this paper, we use a smart contract as the program to
execute our approaches with fairness, security, and auditability.
In this context, a smart contract is a computer program that can
automatically execute on a blockchain platform. In particular,
we leverage blockchain support for executing smart contracts
in order to implement the incentive mechanism described in
this paper. Once a smart contract has been deployed, all of
the participants can review whether the contract is fair and
whether executing it is in their best interest. In addition, all of
the actions made by participants also can be viewed by other
participants within the smart contract; existing research also
shows that executing a smart contract on the blockchain can
guarantee the correctness of the voting result [15].
While blockchain is commonly associated with digi-
tal currency, several platforms have emerged that apply
blockchain beyond digital currency. In particular, the Ethereum
ecosystem allows one to run user-deﬁned programs on the
blockchain [16]. Using Ethereum, for instance, a user can
translate their contract into a programming language, compile
the program to bytecode, and execute it on the Ethereum
Virtual Machine (EVM). Because the contract in the EVM
can execute by itself, we call the program a smart contract.
C. Incentive Mechanisms
Our goal in this work is to provide digital incentives to
encourage users to contribute their resources to decentralized
learning algorithms. Existing work has applied game theory to
design incentive mechanisms for centralized federated learn-
ing [17]–[20]. In game theory, participants make individual
decisions that affect other participants’ actions. Zhang et
al. used a Stackelberg game to incentivize participation in
federated learning by using the server as the leader and the
edge nodes as the followers [21]. The system works in two
stages: ﬁrst, the server announces the total reward then the
edge nodes use that information to determine their training
strategies to maximize their individual rewards. Lim et al. used
coalitional game theory to hierarchically incentivize federated
learning by rewarding model owners based on their marginal
contributions to the model aggregation process [22].Researchers have also considered auctions to support in-
centives in federated learning setting. Prior work has explored
several different auction strategies, including sealed-bid, for-
ward, reverse, double, and combinatorial auctions [17], [23],
[24]. Further, Zhu et al. explored different aspects of the
auction process, including power, sensing, data resources, and
training resources as they relate to federated learning [20].
However, game theory and auction approaches are limited
because they assume that all of the participants act in the
same way. Also, these approaches are not safe and lack of
auditability. For instance, they cannot detect attacks nor record
essential information for the learning progress.
Beyond game theoretic and auction-based mechanisms, re-
cent work has also explored the application of blockchain
to design incentive mechanisms, in particular for federated
learning approaches that involve a central coordinator. Wang et
al. designed a federated learning framework that allows clients
to share local gradients as transactions in the blockchain.
Designated workers validate these transactions and pack them
into blocks [18]. Similarly, FLChain, proposed by Bao et al.,
is a blockchain-based ecosystem that ensures provenance and
maintains auditable aspects of the federated learning model
[25]. These works show us how to effectively ﬁnd and remove
dishonest nodes. They have also shown us the possibility
to maintain a user’s privacy while also making the learning
process auditable. More generally, they have characterized the
goals of conﬁdentiality, auditability, and fairness [25], which
we will also use to frame our research.
However, these approaches to applying blockchain to in-
centives in distributed learning focus exclusively on federated
learning settings that involve a central coordinator, and these
approaches are not directly applicable to a completely decen-
tralized learning structure where we do not assume that all
of the devices request the global model to conduct the next
round of training. Further, their approaches to information
sharing to support the learning process itself rely heavily
on the blockchain, which in turn incurs very high costs and
limits the potential applicability of the approach to applications
that can justify this cost. In contrast, we propose to interact
with the blockchain via smart contract for only essential
interactions related to model provenance and incentives. In
doing so, we keep the distributed learning protocol itself off of
the blockchain, allowing a more agile and resource-conscious
implementation.
D. Attackers in Incentivized Decentralized Learning Settings
In general, the decentralized learning and the incentive
mechanism proposed by this paper faced two major types
of attack, which we refer to as incentive mechanism attacks
andlearning attacks . Although the focus of this paper is on
providing incentives for the participants, the design can also
protect the system from both types of attacks.
Incentive Mechanism Attacks. Anincentive mechanism
attack is an attack that is enabled by the design of the incentive
mechanism itself. In such an attack, due to the motivation of
the rewards provided by the incentive mechanism, an attacker
will try to get a reward without performing the real work (e.g.,
by providing fake or useless training information). To thwart
incentive mechanism attacks, we carefully design our incentive
mechanism to both detect and prevent these, as described in
Section III below.
Learning attacks. The distributed nature of decentralized
learning opens itself to a variety of attacks that can disrupt
the process. In particular, these attacks can be categorized
as: (1) evasion attacks, in which an attacker aims to cause
the model to make incorrect predictions through the use of
adversarial testing data [26]; (2) poisoning attacks, in which
the attacker contaminates the training data to cause the model
to misclassify the input [27]; and (3) exploratory attacks in
which an attacker reverses the model in an attempt to recover
information about the training data [28]. In the context of the
decentralized opportunistic learning approaches addressed in
this paper, the poisoning attack is the most relevant; in the
remainder of this paper, we refer to such attacks simply as
learning attacks .
There are some existing solutions to detect these learning
attacks, but to date, they have only been designed for use in
centralized federated learning settings. For instance, Sun et al.,
identiﬁes the backdoor attack on federated learning and pro-
pose a norm thresholding method, in which the central server
rejects any model updating having a norm larger than some
thresholdM[29]. They show that the performance depends
on the proportion of nodes in the system that are adversaries
Krum, proposed by Blanchard et al., is an aggregation rule that
can be used to make a distributed stochastic gradient descent
conmputation resilient to Byzantine failures [30].
Since in the decentralized setting, clients have full control
of the data as they are the only owners of the data, a simple
learning attack can also be launched by simply ﬂipping the
labels of training samples from the target class [27] before
performing the training.
Existing work on addressing these attacks has primarily
viewed these attacks more at the node or model level rather
than at the level of the decentralized system. Moreover, exist-
ing solutions rely on a central server’s global view i.e., they
support centralized federated learning and not decentralized
approaches. In contrast, by using smart contracts and voting
mechanisms, our approach allows any participant to identify
and drop models that do not improve the accuracy of the
model provided by the other participants. At the system level,
users who produce harmful or unimproved contributions over
time will be punished and automatically eliminated from the
environment over time.
III. S YSTEM DESIGN
This work provides a generic platform to incentivize partici-
pation in decentralized opportunistic learning. In decentralized
opportunistic learning, this participation, which includes con-
tributing resources like computation, communication, and data,
is essential to the success of the learning process, in particular
with respect to the achievable accuracy of a learned model.
Fig. 1 shows, abstractly, how this incentive mechanism, called
Fig. 1: System Architecture
iDML for i ncentivied D ecentralized M achine L earning, relates
to existing decentralized and opportunistic learning algorithm
implementations.
In iDML, there are three roles: (1) the Learner is the
participating device that seeks to beneﬁt from a particular
encounter in a decentralized opportunistic learning algorithm;
(2) the Neighbor is the participating device that contributes
their resources so the learner can learn; and (3) the Validators
are the participants who validate the neighbor’s contribution.
A device can take on more than one role at different times;
for a given encounter, the validators must be distinct from
the learner and neighbor, however, in a symmetric protocol
(e.g., gossip learning), two nodes in an encounter may simul-
taneously beneﬁt from each other. In this case, iDML treats
the encounter as two encounters, one with each device in the
learner role and the encountered device in the neighbor role.
As Fig. 1 shows, we design iDML as a plug-in; the
smart contract implementation is kept separate from the
core decentralized learning functionality, making it portable
and generalizable. In order for iDML to interoperate with
an existing decentralized learning algorithm, we implement
an algorithm-speciﬁc adaptor , whose role is effectively to
choreograph the interactions between iDML above and the
implementation of decentralized learning, below. In addition,
we also implement algorithm-speciﬁc validators. As described
in more detail below, the speciﬁc steps of validation vary based
on the learning algorithm in use. Above, iDML is responsible
for maintaining the smart contract and coordinating across the
participants in the three roles.
A. Motivating Participation in Decentralized Learning
The primary goal of our system is to motivate the partici-
pants to actively participate in the decentralized opportunistic
learning process and to provide compensation for their con-
tributed resources. The overall ﬂow of the iDML approach
is shown in Fig. 2. While the ﬁgure depicts many steps, the
process is, overall, intuitive, and entails nine steps:
Fig. 2: A system view of iDML. Except for step 1 and 3, all
other steps belong to iDML.
1) The learner and neighbor encounter one another, and the
underlying decentralized learning protocol is triggered.
2) Before learning starts, the iDML Adapter is invoked,
and the learner is requested to pay a predeﬁned and
agreed-upon amount of tokens into the smart contract.
The neighbor device checks this prepayment before re-
commencing the learning activity.
3) Once the prepayment is done and veriﬁed, the learner
and neighbor complete whatever collaborative task is
demanded by the decentralized learning algorithm.
4) After the collaborative task completes, both the learner
and neighbor inform the smart contract.
5) Once the validators receive a validation request, they
perform the validation by using the algorithm-speciﬁc
validation implementation.
6) After the validators validate the model, they vote by
sending the result to the smart contract.
7) After a predeﬁned number of votes has been received on
the smart contract or a max time has passed, any one
party requests the result within the smart contract.
8) Then, the smart contract distributes the awards or applies
the penalty to the neighbor and validators based on the
predeﬁned and consensus methods.
We next present the workﬂow of our approach as centered
around the smart contract. Throughout, we connect each step
back to the numbered steps in Fig. 2, though we omit step 3,
which focus on the decentralized learning process and not on
the role of the smart contract.
Step 0 (Initialization) : To initialize the use of the smart
contract, prior to any of the steps shown in Fig. 2, the
individuals who own the devices participating in decentralized
opportunistic learning must join the process by exchanging
their digital currencies with tokens, a form of digital currency,
supported by our smart contract with the ERC-20 token stan-
dard1. The smart contract has a predeﬁned rate with one digital
currency but it can use that digital currency as the bridge to
1https://ethereum.org/en/developers/docs/standards/tokens/erc-20/
Fig. 3: Initializing the smart contract ( Step 0 ).
Fig. 4: Pre-payment and checking pre-payment before initiat-
ing encounter-driven training process ( Step 1 and 2 ).
exchange with other digital currencies. For a participant to
serve as a Neighbor orValidator in iDML, they need to stake
a predeﬁned amount of tokens to smart contract as pledges .
The award is paid from the Learner to the Neighbor and
Validator once a speciﬁc opportunistic learning process is
successfully ﬁnalized. The complete initialization process is
shown in Fig. 3.
Step 1 (Initialize Encounter) and Step 2 (Pre-Training
Check): After an encounter is initiated and two participants
tentatively agree to collaborate, the two participants (i.e., the
Learner who will beneﬁt from the encounter and the Neigh-
bor who will provide resources to support the decentralized
learning algorithm) both check in with the smart contract
before continuing. In particular, the Learner informs the smart
contract of the identity of the neighbor it will collaborate with
and authorizes the prepayment (second arrow in Fig. 2). The
method of computing the right amount of payment is based
on a consensus between the neighbor and the learner. The
prepayment is based on the communication and computational
resource requirements of the decentralized learning process
and can be deﬁned globally with all of the participants or
privately negotiated during the encounter step. The Neighbor
will check the validity of the prepayment with the smart
contract. If the prepayment is insufﬁcient, the Neighbor can
reject the collaboration, and the prepayment will be returned
to the Learner ’s account in the smart contract. Otherwise,
theLearner andNeighbor will collaborate as deﬁned by the
decentralized learning algorithm. This pre-training check-in
with the smart contract is depicted in Fig. 4.
Step 4 (Learning Complete): Step 3 (initialize learning) in
Fig. 5: Completing the Learning Task ( Step 4 ).
Fig. 2 completes by the decentralized opportunistic learning
algorithm, without any interaction with the smart contract.
However, when the collaboration process completes and the
Learner is given the result (e.g., model parameters), the iDML
Adapter layer on the Neighbor intercepts and makes a copy
of the result and sends the “learning complete” signal and the
result’s md5 to the smart contract in iDML. In addition, the
Learner also reports to the smart contract the result’s md5
through the iDML Adapter layer on the Learner’s side. At
this step, there are two possible outcomes. In the ﬁrst case
(Case 1 in Fig. 5), assuming the results from the Learner
and Neighbor match, the validation process can begin. On
the other hand (Case 2 in Fig. 5), several different possibilities
may prevent the process from continuing. Due to the nature
of decentralized learning, either the Learner or the Neighbor
may not reachable or may be unable to send or receive the
results of the learning process. In this case, as the Neighbor
has likely contributed its resources to the training process, a
portion of the prepayment will be given to the Neighbor as
a show of good faith. On the other hand, as the Learner did
not beneﬁt from the collaboration, the rest of the prepayment
will return to the Learner .
In determining the magnitude of the small payment to a
Learner , we seek a balance that prevents the participation of
a malicious learner and/or neighbor. Over time, if a particular
Neighbor orLearner builds up a history of incomplete
interactions within the smart contract, this information can
be used in Step 2 before continuing with future encounters.
For instance, the smart contract can check the history of a
Neighbor ’s block by block in the underlying blockchain for
deeper veriﬁcation.
Step 5 (Model Validation): To kick off the validation step,
theLearner shares the learning inputs and results with the
validators (ﬁrst arrow for step 5) and notiﬁes the smart contract
that the validation has been initialized (second arrow for step
5). Then, the validators begin to verify whether the Neighbor ’s
contribution is solid. The validators are selected based on
Fig. 6: Finalizing the encounter ( Steps 6-8) .
the same criteria as the neighbor. On the smart contract, the
learning result is indexed by using an md5 hash of the model
weights and the validator’s id, which prevents the learner from
sending the wrong model to the validators. As shown in Fig. 1,
the veriﬁcation method is dependent on the particular decen-
tralized learning algorithm used. This step invokes a number of
validators suitable for the learning algorithm (e.g., empirically
tuned). Each validator votes on whether they believe the result
is valid, based on an assessment of an updated model relative
to the Learner ’s previous model. That is, validator checks
whether the Neighbor ’s contribution to learning was positive.
Because we allow for some uncertainty in the result, we
introduce the notion of tolerance (). Also, as the accuracy
of the model trained by decentralized learning increases, the
space for model improvement is reduced. However, in such
cases, incorporating learning contributions from encountered
neighbors can still help the learners’ model become more
generalizable. Based on these ﬁndings, an updated learned
model is considered valid if its resulting accuracy is either
better than the input model’s original accuracy or not worse
than the input model by . While we transmit vote results
as clear texts in our prototype, existing research shows that
voting can be secured to avoid bias in the result [15].
Step 6 through 8 (Finalization): At the last steps, the
voting result can be computed if one of the following two
Explanation
r amount of reward the learner requested to pay.
rn amount of reward the neighbor receives.
rt
v amount of reward for validators who voted yes.
rf
v amount of reward for validators who voted no.
pn
r percentage of reward the neighbor should receive.
pv
r percentage of reward validators should receive ( pv
r= (1 pn
r)).
nt
v number of the validators who are voted yes.
nf
v number of the validators that are voted no.
V voting threshold; number of votes required to ﬁnalize
 Model accuracy tolerance.
conditions is satisﬁed: 1) there are more than V(the vot-
ing threshold) votes or 2) the max voting time, which can
be predetermined on smart contract, has passed. pn
ris the
percentage of the award that will be given to the neighbor
andpv
ris the percentage of the award that will be distributed
among all of the validators. Both pn
randpv
rare conﬁgurable
per decentralized learning algorithm in the smart contract, and
pv
r+pn
r= 1. We usent
vto represent the number of validators
that are voted the model has a reasonable contribution, and nf
v
is the number voted oppositely. Finalization can be requested
by any participant in the system and has the following possible
outcomes, which is also summarized in Fig. 6. Please note, in
the ﬁgure, the brown dollar sign means the reward distributed
from the Learner , the blue dollar sign means the penalty from
the neighbor, and the pink dollar sign means the penalty from
the validator(s). For simplicity, in the ﬁgure we refer validators
C-1 and C-2 as good ones while C-3 as the bad one in Cases
4 and 5.
Case 1: If there is no voting in the system ( nt
v=nf
v= 0),
we assume the model provided a solid contribution.
rn=r (1)
Case 2: If all validators believe the Neighbor has provided
a reasonable contribution (i.e., nt
v6= 0;nf
v= 0), the reward
will be given as:
rn=pn
rrandrt
v=pv
rr
ntv(2)
Case 3: If all validators believe the Neighbor has not
provided a reasonable contribution (i.e., nt
v= 0;nf
v6= 0),
the prepayment will be returned to the Learner and the
reward/penalty will be given as:
rn= randrf
v=r
ntv(3)
Case 4: If a majority of validators believe the Neighbor has
provided a reasonable contribution but some others do not
(i.e.,nt
vnf
v>0). The reward/penalty will be given as:
rn=pn
rrandrt
v=pv
rr nf
vrf
v
ntv
rf
v= pv
rr
ntv+nf
v(4)
Case 5: If a majority of validators believe the Neighbor ’s
contribution is not a reasonable improvement but someothers believe it is (i.e., 0< nt
v< nf
v). The prepayment
will be returned to the Learner and the reward/penalty is:
rn= randrt
v= r
ntv+nf
v
rf
v=r nt
vrt
v
nf
v(5)
B. Malicious Model Prevention
As discussed previously, there are two signiﬁcant possible
classes of attacks on a system like iDML: incentive mechanism
attacks and Learning attacks. Our approach is designed to
prevent both classes.
Incentive mechanism attacks. The incentive mechanism
attacks are those exposed by the incentive mechanism itself.
In these situations, attackers aim to gain the reward associated
with the learning process without actually engaging in mean-
ingful collaboration. A setting where all participants of the
system train and share models with other participants without
ill intention is not realistic given participants may sabotage
training by returning random model weights; such behavior is
driven by the fact that model training at a single participant
level is neither optimal nor efﬁcient. Unfortunately, existing
systems are not equipped with ways to detect such malicious
behavior, which results in diminished accuracy and usefulnesss
of the entire learning system. Such an attack can be addressed
by leveraging iDML’s validation steps (Steps 5 and 6 in Fig. 2).
Furthermore, the validation result may lead to penalties
for the attacker. Step 0 in Fig. 3 asks the neighbor to stake
their token before collaboration, which provides a mechanism
for the smart contract to penalize a dishonest Neighbor
participant. In Eq. 3 and Eq. 5, we guarantee that the penalty
the attacker may receive is equal to or slightly larger than what
they may earn (e.g., when the Neighbor is also a “positive”
validator), which makes the mathematical expectation for the
incentive mechanism attack equal to or less than 0.
Learning attacks. Learning attacks seek to cause the output
of the learning process to misclassify an input, which is also
commonly referred to as a poisoning attack. In iDML, the
staking and the validation steps are also effective at preventing
such learning attacks. In the case that Neighbor participants
are dishonest in any of their actions, a penalty will be applied
to the token they staked to the smart contract.
IV. E VALUATIONS
In this section, we evaluate iDML with two state-of-the-
art decentralized learning algorithms: gossip learning [4] and
opportunistic collaborative learning (OppCL) [3].
A. Datasets and Models
Our evaluation relies on two widely used classiﬁcation
datasets: CIFAR-10 [31], which consists of 60,000 32*32
colorized images in 10 classes, and Fashion-MNIST [32],
which includes 70,000 28x28 grayscale images in 10 classes.
We use 10,000 images in each dataset as the testing dataset.
For the training data, we split the remaining samples among
the potential training devices (i.e., the Neighbor participants)
in iDML using two methods: (1) Independent and Identical
Distribution (IID): the training data is evenly distributed,
without overlap, to all of the simulated participants; (2)
Non-Independent and Identical Distribution (Non-IID): each
Neighbor participant has the same amount of data, without
overlap, but each neighbor has only 2 classes of the data.
For both learning algorithms, we choose those learning
models that are small enough to be compatible with training
on resource-constrained devices and adaptle to different model
sizes. For the gossip learning approach that we employ [4],
all participants rely on the same model structure; upon an en-
counter, the devices simply exchange snapshots of their current
model parameters, which are aggregated locally before training
continues. We use 10 convolutional layers, max pooling, and
dropout layers. In contrast, for our opportunistic learning
model, we use three different sizes for the convolutional neural
network (CNN) models, which have 6, 10, ad 14 layers, max
pooling, and dropout layers.
Decentralized and opportunistic learning relies on encoun-
ters between participating nodes. We rely on commonly used
datasets of contact patterns [33]–[35]. However, these datasets
are biased towards some popular nodes which are not suitable
for our experiments. Instead of using these encounter patterns
directly, we use insights from these datasets to randomly gen-
erate larger encounter patterns that provide evenly distributed
contact patterns for 50 simulated participants.
B. Evaluation Platform and Setting
We implement our smart contract in Solidity2and deploy
it on Ganache3, a smart contract tool. The decentralized
opportunistic learning simulation system is implemented using
Python with TensorFlow. The simulation is running on a
Workstation with Ubuntu 18.04 LTS System, i7-9800X CPU,
128G Memory, and NVIDIA GeForce RTX 2080 Ti GPU.
iDML’s validation process depends on the decentralized
learning algorithm. Thus, we separately implemented the val-
idation process for each algorithm. For gossip learning, we
let the Learner keep a copy of the model before performing
the receive-merge-train cycle, and share both the old and new
model with the Validator . For OppCL, the Learner will not
merge the model after it has been received but share both the
model with the Validator to validate. For both algorithms,
theValidator will evaluate the accuracy of the model and
vote based on the result. The other parameters used in our
experiments are listed in Table I.
C. Experimental Results
In our ﬁrst and primary experiment, our goal is to evaluate
the effectiveness of iDML in providing incentives to partici-
pate in decentralized opportunistic learning. In particular, we
evaluated the voting accuracy under the settings of different
number of validators and training tolerance ( ) for both gossip
learning and OppCL. Note, the tolerance value gives a lower-
bound of the trained model’s accuracy ( deﬁnition is on Step
2https://soliditylang.org/
3https://trufﬂesuite.com/ganache/TABLE I: Simulation Parameters
Parameter Value
Stake threshold for a user to become Neighbor orValidator 100
V oting threshold V 3
Max V oting waiting time (block) 50000
Number of tokens each participant exchanged 1000000
Number of tokens each Neighbor /Validator staked initially 200
Tolerancefor F-MNISTkIIDkOppCL (For Fig. 8 and 9) 0.030
Fig. 7: V oting accuracy with different and voters settings
5). The voting accuracy reﬂects how many training instances
are validated by our validation algorithm. The instance is
regarded as validated if the model’s accuracy tested on the
validation set satisﬁes the tolerance setting. If the model is
more generalizable, it is more likely the model can pass our
validation.
In particular, Fig. 7 shows iDML’s effectiveness in verifying
whether a Neighbor created a valid contribution for the
Learner , given a speciﬁc threshold and number of Validator s
(i.e., voters). Fig. 7 shows in IID settings, both learning
algorithms show stable voting accuracy pattern across different
number of voters. Through another in-depth study using very
few number of voters (skipped due to brevity), we observe
three voters is a good choice in the settings. Though in non-IID
settings, the voting accuracy ﬂuctuates a bit among different
number of voters, considering the costs of having additional
voters, smaller number of voters are desirable (e.g., 3).
We also observe the different behavior of Gossip and
OppCL in IID and Non-IID settings. It shows our adapters
for both learning algorithms work as the trend shows the
native characteristics of both algorithms. For instance, Gossip
learning uses a merge-update-send cycle and OppCL adopts
ashare-train-return cycle. The difference (Gossip is more
generalizable than OppCL per training instance) caused more
variance in voting accuracy in the models with OppCL,
compare with the Gossip learning.
One practical value of iDML is that, under the assumption
that participants’ behaviors will be driven by clearly-deﬁned
incentives, the explicit incentive mechanism we deﬁned will
result in the effective engagement of system participants
and a higher resulting model accuracy from collaboration.
Multiple rounds of simulations show that the current system
design awards laborious Neighbor participants, while those
who hinder training by any means are marginalized from
participation in the system. In real-world applications, indeed,
human behaviors may be more complex and may not be
as directly driven as machines are. Yet, many applied cases
show that a clearly deﬁned reward (e.g., monetary value and
promotions) encourages desired behaviors.
In our ﬁnal experiment, we assess the degree to which iDML
also prevents learning attacks. We assign different numbers of
learning attackers. Each of these attackers ﬂip the label of
images in their dataset (e.g. they, mark trucks as airplanes
and/or deer as dogs). We inject these attackers into the system
randomly and check the model accuracy after every encounter
round for all of the participants with the testing dataset.
As a baseline, we ﬁrst experiment without iDML. We plant
0,3, and 10attackers in the system respectively. Fig. 8 shows
that under OppCL, an increased number of attackers causes
the system to incur a delay in accuracy increase. Further,
the system ceases to increase learning accuracy after several
rounds of encounters. The more attackers in the system, the
lower the ending model’s average accuracy. Then, we redo
the experiment with 10attackers with iDML; the result is
shown as the red dotted line in Fig. 8. It shows that iDML
ﬁrst follows a similar trend of the 10attackers experiment
Fig. 8: Model accuracy for different numbers of attackers
Fig. 9: Progression of staked tokens for participants during the
training process
and then recovers. To understand the cause of this behavior,
we examine the amount of staking in the smart contract for
each encounter around with all of the participants (Fig. 9).
The results using different algorithm and dataset combinations
are similar to Fig. 8 and Fig. 9 and are therefore omitted for
brevity.
We believe that at the ﬁrst stage (before 40000 blocks),
even though iDML rejected some of the attacker’s models,
these attackers are still active in the learning system. In
the meantime, iDML continues to penalize those attackers
as they provide malicious models and/or act as malicious
“positive” validator. After the turning point ( 40000 blocks),
the staking for attackers falls below the threshold, and they
can no longer collaborate in the system. After that, iDML
recovers the learning system from the attacks and the model
accuracy begins improving once again.
D. Discussion
While these results show the promise of an approach like
iDML, there remain some additional constraints and limita-
tions associated both with its reliance on blockchain and the
design of the system itself.
1) Blockchain: The proposed system is a decentralized plat-
form that utilizes the blockchain network to provide secure and
efﬁcient data storage and sharing. However, there are certain
limitations that come with using blockchain technology.
Operation cost : Most operations in a block chaian
network (e.g., (EVM opcodes), including stack (POP,
PUSHX, DUPX, SWAPX), memory (CALLDATACOPY ,
CODECOPY , etc.), and storage (SLOAD and SSTORE)
[36],) incur a cost known as “gas”; the used gas is
then converted to a monetary fee that is dependent on
TABLE II: Example cost of one learning encounter in iDML(cost baselines from Jan 17, 2023 04:27 UTC)
Step Gas Used Cost (ETH Network) Cost (Polygon Network)
2. Pre-Training Check (Learner) 43919 $0.9623 $0.0025
4. Learning Complete (Neighbor) 96172 $2.1072 $0.0055
4. Learning Complete (Learner) 46921 $1.0281 $0.0027
6. V otes (Validator) 78869 $1.7281 $0.0045
7. Result Check and 8. Distribute Award (Any one party) 191344 $4.1925 $0.0109
Total 457225 $10.0182 $0.0261
the cost baseline of the network being used. Table II
shows the gas used on two example networks for a single
encounter in iDML. In employing iDML, the application
designer must navigate the tradeoff between the beneﬁt
to deploying incentives against their cost—iDML will not
be appropriate for all applications, but Table II shows
that the costs (1) can be made reasonable and (2) can be
estimated in advance. In the near future, the cost can be
lower with new technologies, such as Polygon Miden4.
Security risk: As the proposed system relies on the safety
and reliability of the blockchain, any security issues that
occur on the blockchain may also affect the proposed
system. For example, if a hacker successfully performs
a51% attack and gains control of the blockchain, they
could potentially tamper with the data stored on the
proposed system [37].
Limited privacy : If the smart contract is deployed to a
public chain, such as ETH or Polygon, the information
can be accessed by the public, which may be a concern
for leaking sensitive data such as personal encounter
information. Therefore in the design of iDML, we in-
tentionally keep model weights and data distributions off
the blockchain to reduce the privacy risk.
2) System: The system relies on validators to determine
whether a neighbor’s contribution is valid, but due to the
nature of the machine learning task and the randomness of
datasets and the learning process, ﬂuctuations in accuracy
are natural and not necessarily indicative of an invalid con-
tribution. Depending on the particular validator(s) a learner
encounters, the validation result may vary. We have applied
a validation tolerance value ( ) to reduce this issue, but it
cannot be completely eliminated. Additionally, the accuracy
may not always fully reﬂect the improvement of a model.
Other evaluation metrics, such as recall, precision, F1, and
loss, may better reﬂect the improvements. Future work could
explore choosing performance metrics that better ﬁt a speciﬁc
learning task, and iDML could be updated to allow different
learners to mandate the use of different metrics. In this study,
however, we rely on accuracy to show the potential for iDML
to incentivize neighbor participation.
Finally, iDML has been speciﬁcally designed as an incen-
tive mechanism for decentralized machine learning. We have
included components of the design to prevent or mitigate po-
tential attacks on the system, but these are not comprehensive.
For instance, iDML does, by design, mitigate (though it does
4https://polygon.technology/solutions/polygon-midennot completely prevent) model poisoning attacks that may be
launched by malicious neighbors by relying on the validators
to sign on each neighbor’s contribution. On the other hand, if
a group of attackers conspires to pose as malicious neighbors
and validators, the system cannot mitigate this attack. In the
future, by evalauting iDML in the context of real encounter
patterns in mobile networks, we can test the difﬁculty, cost,
and potential damage associated with launching such an attack
As a more nuanced example, the tolerance is useful because
it allows for some expected ﬂuctuations in model accuracy,
but a persistent attacker could attempt to manipulate a model
by working within the tolerance range to shift the model’s
direction. However, to successfully launch such an attack, a
neighbor would have to be able to inﬂuence a learner’s model
repeatedly over time (assuming is reasonably small), so an
easy way to mitigate this attack is to ensure diversity in the
selected neighbors.
V. C ONCLUSION AND FUTURE WORK
We have presented a framework called iDML for incen-
tivizing decentralized and opportunistic machine learning. The
evaluation results show that the system can effectively com-
pensate neighboring devices that provides communication and
computational resources to assist other participants’ in their
model learning tasks. In addition, the results show the system
can detect attackers in the system and avoid malicious models
being merged into the user’s model. Also, the evaluation of
different decentralized learning algorithms with different data
and data distributions shows the wide range of the use cases of
our proposed iDML system. In future work, one can evaluate
the system with large-scale real-world contract patterns. On
the other side, to better incentivize participants, a data item’s
rarity could also be considered and provided corresponding
rewards.
REFERENCES
[1] T. S. Ajani, A. L. Imoize, and A. A. Atayero, “An overview of
machine learning within embedded and mobile devices–optimizations
and applications,” Sensors , vol. 21, no. 13, p. 4412, Jun 2021. [Online].
Available: http://dx.doi.org/10.3390/s21134412
[2] Y . Zhan, J. Zhang, Z. Hong, L. Wu, P. Li, and S. Guo, “A survey of
incentive mechanism design for federated learning,” IEEE Transactions
on Emerging Topics in Computing , vol. 10, no. 2, pp. 1035–1044, 2022.
[3] S. Lee, X. Zheng, J. Hua, H. Vikalo, and C. Julien, “Opportunistic
federated learning: An exploration of egocentric collaboration for per-
vasive computing applications,” in 2021 IEEE International Conference
on Pervasive Computing and Communications (PerCom) , 2021, pp. 1–8.
[4] I. Colin, A. Bellet, J. Salmon, and S. Cl ´emenc ¸on, “Gossip dual averaging
for decentralized optimization of pairwise functions,” in International
Conference on Machine Learning . PMLR, 2016, pp. 1388–1396.
[5] Z.-Q. Zhao, P. Zheng, S.-t. Xu, and X. Wu, “Object detection with deep
learning: A review,” IEEE transactions on neural networks and learning
systems , vol. 30, no. 11, pp. 3212–3232, 2019.
[6] T. Yang, G. Andrew, H. Eichner, H. Sun, W. Li, N. Kong,
D. Ramage, and F. Beaufays, “Applied federated learning: Improving
google keyboard query suggestions,” CoRR , vol. abs/1812.02903, 2018.
[Online]. Available: http://arxiv.org/abs/1812.02903
[7] Z. Zheng, S. Xie, H.-N. Dai, X. Chen, and H. Wang, “Blockchain
challenges and opportunities: A survey,” International journal of web
and grid services , vol. 14, no. 4, pp. 352–375, 2018.
[8] N. Szabo, “Formalizing and securing relationships on public networks,”
First monday , 1997.
[9] W. Zou, D. Lo, P. S. Kochhar, X.-B. D. Le, X. Xia, Y . Feng, Z. Chen,
and B. Xu, “Smart contract development: Challenges and opportunities,”
IEEE Transactions on Software Engineering , vol. 47, no. 10, pp. 2084–
2106, 2021.
[10] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N.
Bhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings et al. ,
“Advances and open problems in federated learning,” Foundations and
Trends® in Machine Learning , vol. 14, no. 1–2, pp. 1–210, 2021.
[11] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y.
Arcas, “Communication-Efﬁcient Learning of Deep Networks from
Decentralized Data,” in Proceedings of the 20th International
Conference on Artiﬁcial Intelligence and Statistics , ser. Proceedings
of Machine Learning Research, A. Singh and J. Zhu, Eds., vol. 54.
PMLR, 20–22 Apr 2017, pp. 1273–1282. [Online]. Available:
https://proceedings.mlr.press/v54/mcmahan17a.html
[12] D. Leroy, A. Coucke, T. Lavril, T. Gisselbrecht, and J. Dureau, “Fed-
erated learning for keyword spotting,” in ICASSP 2019-2019 IEEE
International Conference on Acoustics, Speech and Signal Processing
(ICASSP) . IEEE, 2019, pp. 6341–6345.
[13] A. Hard, K. Rao, R. Mathews, S. Ramaswamy, F. Beaufays, S. Augen-
stein, H. Eichner, C. Kiddon, and D. Ramage, “Federated learning for
mobile keyboard prediction,” arXiv preprint arXiv:1811.03604 , 2018.
[14] Y . Xiong, H. J. Kim, and V . Hedau, “Antnets: Mobile convolutional neu-
ral networks for resource efﬁcient image classiﬁcation,” arXiv preprint
arXiv:1904.03775 , 2019.
[15] J. Lyu, Z. L. Jiang, X. Wang, Z. Nong, M. H. Au, and J. Fang, “A secure
decentralized trustless e-voting system based on smart contract,” in 2019
18th IEEE International Conference On Trust, Security And Privacy In
Computing And Communications/13th IEEE International Conference
On Big Data Science And Engineering (TrustCom/BigDataSE) , 2019,
pp. 570–577.
[16] G. Wood et al. , “Ethereum: A secure decentralised generalised trans-
action ledger,” Ethereum project yellow paper , vol. 151, no. 2014, pp.
1–32, 2014.
[17] Y . Jiao, P. Wang, D. Niyato, B. Lin, and D. I. Kim, “Toward an
automated auction framework for wireless federated learning services
market,” IEEE Transactions on Mobile Computing , vol. 20, no. 10, pp.
3034–3048, 2021.
[18] G. Wang, C. X. Dang, and Z. Zhou, “Measure contribution of partici-
pants in federated learning,” in 2019 IEEE International Conference on
Big Data (Big Data) , 2019, pp. 2597–2604.
[19] T. Song, Y . Tong, and S. Wei, “Proﬁt allocation for federated learning,”
in2019 IEEE International Conference on Big Data (Big Data) , 2019,
pp. 2577–2586.
[20] X. Tu, K. Zhu, N. C. Luong, D. Niyato, Y . Zhang, and J. Li, “Incentive
mechanisms for federated learning: From economic and game theoretic
perspective,” IEEE Transactions on Cognitive Communications and
Networking , vol. 8, no. 3, pp. 1566–1593, 2022.
[21] T. Zhang, Z. Shen, J. Jin, X. Zheng, A. Tagami, and X. Cao, “Achieving
democracy in edge intelligence: A fog-based collaborative learning
scheme,” IEEE Internet of Things Journal , vol. 8, no. 4, pp. 2751–2761,
2021.
[22] W. Y . B. Lim, Z. Xiong, C. Miao, D. Niyato, Q. Yang, C. Leung,
and H. V . Poor, “Hierarchical incentive mechanism design for federated
machine learning in mobile networks,” IEEE Internet of Things Journal ,
vol. 7, no. 10, pp. 9575–9588, 2020.
[23] S. Fan, H. Zhang, Y . Zeng, and W. Cai, “Hybrid blockchain-based
resource trading system for federated learning in edge computing,” IEEE
Internet of Things Journal , vol. 8, no. 4, pp. 2252–2264, 2021.
[24] Y . Deng, F. Lyu, J. Ren, Y .-C. Chen, P. Yang, Y . Zhou, and Y . Zhang,
“Fair: Quality-aware federated learning with precise user incentive andmodel aggregation,” in IEEE INFOCOM 2021 - IEEE Conference on
Computer Communications , 2021, pp. 1–10.
[25] X. Bao, C. Su, Y . Xiong, W. Huang, and Y . Hu, “Flchain: A blockchain
for auditable federated learning with trust and incentive,” in 2019 5th
International Conference on Big Data Computing and Communications
(BIGCOM) , 2019, pp. 151–159.
[26] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi ´c, P. Laskov,
G. Giacinto, and F. Roli, “Evasion attacks against machine learning at
test time,” in Machine Learning and Knowledge Discovery in Databases ,
H. Blockeel, K. Kersting, S. Nijssen, and F. ˇZelezn ´y, Eds. Berlin,
Heidelberg: Springer Berlin Heidelberg, 2013, pp. 387–402.
[27] A. Shafahi, W. R. Huang, M. Najibi, O. Suciu, C. Studer, T. Dumitras,
and T. Goldstein, “Poison frogs! targeted clean-label poisoning attacks
on neural networks,” 2018.
[28] X. Zhou, M. Xu, Y . Wu, and N. Zheng, “Deep model poisoning attack
on federated learning,” Future Internet , vol. 13, no. 3, 2021. [Online].
Available: https://www.mdpi.com/1999-5903/13/3/73
[29] Z. Sun, P. Kairouz, A. T. Suresh, and H. B. McMahan, “Can
you really backdoor federated learning?” 2019. [Online]. Available:
https://arxiv.org/abs/1911.07963
[30] P. Blanchard, E. M. El Mhamdi, R. Guerraoui, and J. Stainer, “Ma-
chine learning with adversaries: Byzantine tolerant gradient descent,”
Advances in Neural Information Processing Systems , vol. 30, 2017.
[31] A. Krizhevsky, G. Hinton et al. , “Learning multiple layers of features
from tiny images,” 2009.
[32] H. Xiao, K. Rasul, and R. V ollgraf. (2017) Fashion-mnist: a novel image
dataset for benchmarking machine learning algorithms.
[33] L. Ozella, D. Paolotti, G. Lichand, J. P. Rodr ´ıguez, S. Haenni, J. Phuka,
O. B. Leal-Neto, and C. Cattuto, “Using wearable proximity sensors to
characterize social contact patterns in a village of rural malawi,” EPJ
Data Science , vol. 10, no. 1, p. 46, 2021.
[34] R. I. Ciobanu and C. Dobre, “CRAWDAD dataset upb/mobility2011 (v.
2012-06-18),” Downloaded from https://crawdad.org/upb/mobility2011/
20120618, Jun. 2012.
[35] ——, “CRAWDAD dataset upb/hyccups (v. 2016-10-17),” Downloaded
from https://crawdad.org/upb/hyccups/20161017, Oct. 2016.
[36] “Ethereum virtual machine (evm).” [Online]. Available: https://
ethereum.org/en/developers/docs/evm/
[37] I. Eyal and E. G. Sirer, “Majority is not enough: Bitcoin mining is
vulnerable,” Commun. ACM , vol. 61, no. 7, p. 95–102, jun 2018.
[Online]. Available: https://doi.org/10.1145/3212998

Search-time Efficient Device Constraints-Aware
Neural Architecture Search
Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal
Indian Institute of Technology
{oshin.dutta,sumeet }@ee.iitd.ac.in, kanvar.tanu@gmail.com
Abstract. Edge computing aims to enable edge devices, such as IoT
devices, to process data locally instead of relying on the cloud. However,
deep learning techniques like computer vision and natural language pro-
cessing can be computationally expensive and memory-intensive. Creat-
ing manual architectures specialized for each device is infeasible due to
their varying memory and computational constraints. To address these
concerns, we automate the construction of task-specific deep learning ar-
chitectures optimized for device constraints through Neural Architecture
Search (NAS). We present DCA-NAS, a principled method of fast neu-
ral network architecture search that incorporates edge-device constraints
such as model size and floating-point operations. It incorporates weight
sharing and channel bottleneck techniques to speed up the search time.
Based on our experiments, we see that DCA-NAS outperforms manual
architectures for similar sized models and is comparable to popular mo-
bile architectures on various image classification datasets like CIFAR-10,
CIFAR-100, and Imagenet-1k. Experiments with search spaces—DARTS
and NAS-Bench-201 show the generalization capabilities of DCA-NAS.
On further evaluating our approach on Hardware-NAS-Bench, device-
specific architectures with low inference latency and state-of-the-art per-
formance were discovered.
Keywords: Neural Architecture Search ·DARTS ·Meta-Learning ·
Edge Inference ·Constrained Optimization
1 Introduction
In recent years, there has been significant progress in developing Deep Neural
Network (DNN) architectures [33,47,34] for edge and mobile devices.However,
designing DNN architectures for specific hardware constraints and tasks is a
time-consuming and computationally expensive process [3]. To address this, Neu-
ral Architecture Search (NAS) [2,32,49] has become popular as it discovers op-
timal architectures given a task and network operations. Despite its success,
traditional NAS techniques cannot guarantee optimal architecture for specific
devices with hardware constraints such as storage memory and maximum sup-
ported FLOPs. To address this concern, researchers have developed hardware-
aware algorithms [36,4] that find optimal device architectures with low resourcearXiv:2307.04443v1  [cs.CV]  10 Jul 2023
2 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal
Fig. 1: DCA-NAS framework:Weight sharing in the search space and Derived cells
lowers the search time from other DNAS. Target device constraint is used to query
search constraint from look-up graph for constrained optimization.
training overhead and search time. These methods often use inference latency [4],
FLOPs [36] or a combination of hardware metrics [36] as constraints scaled by
a tunable factor. However, the time to tune the scaling factor is often not con-
sidered within the NAS search time and can be ten times the reported search
time. To address these issues, we propose the Device Constraints-Aware NAS
(DCA-NAS), a principled differentiable NAS method that introduces total al-
lowable model size or floating-point operations (FLOPs) as constraints within
the optimization problem, with minimal hyper-parameter tuning. Unlike infer-
ence latency which is task dependent, FLOPs and memory are specified with a
given hardware and thus are appropriate for our generic method. The approach
is adaptable to other hardware metrics such as energy consumption or infer-
ence latency using additional metric-measuring functions. The paper make the
following significant contributions:
–It introduces a fast method that uses weight sharing among operations in
the search space and channel bottleneck, along with a differentiable resource
constraint, for continuous exploration of the search space.
–A training pipeline that allows a user to input device memory or FLOPs and
search for optimal architecture with minimal hyper-parameter tuning.
–Our extensive experimentation on vision datasets- CIFAR-10, CIFAR-100,
TinyImagenet, Imagenet-1k and inference-latency comparisons of trained
models on Hardware-NAS-bench demonstrate the efficiency of our method.
The generalization of our method to different search spaces is shown with
experiments on DARTS and NAS-Bench.
2 Related Work
Neural Architecture Search Popular approaches [12,22,1] designed architec-
tures for high performance on specific tasks or datasets with the traditional
deep learning perspective that bigger is better, resulting in computationally
and memory-intensive inference on edge devices. Network pruning [13], chan-
nels removal [26,34] and weights/activations quantization [8,50] can compress
Search-time Efficient Device Constraints-Aware Neural Architecture Search 3
architectures, but require pre-training, hyperparameter tuning, and often lack
transferability.Neural Architecture Search (NAS) methods such as Reinforce-
ment Learning [30,4], Evolutionary Learning [11,21] and Differentiable Neural
Architecture Search (DNAS) [25,43] can automatically search for architectures
without user intervention, and can transfer across similar tasks. DNAS with
surrogate metrics [42,48] have also been used to explore the architecture search
space. However, architectures found by DNAS methods are not optimized for
deployment on edge devices and smaller models obtained by reducing layers or
channels are often sub-optimal.
Hardware-aware Neural Architecture search Certain NAS methods opti-
mize [4,40,3,19] for constraints such as latency, inference speed [41], FLOPS [36,37],
memory usage [24]. Some use a separate DNN to predict constraint metrics and
evolutionary search to obtain hardware-aware optimal models [36,3], while oth-
ers consider real-time latencies of edge devices or provide specific architectures
for specific devices [27,7]. However, these methods require significant search time
and tuning of scaling factors controlling the trade-off between the performance
and the constraint, and do not always account for optimal architectures. In con-
trast, we use a differentiable hardware-aware objective function with generic
hardware metrics, and do not require a tunable scaling factor. Certain meth-
ods [3,29,9] train a supernet first and then search for a smaller architecture,
but this is only efficient when there are more than fifteen different edge devices
with different limitations or deployment scenarios [3] as training the supernet
takes huge resources-32 V100s taking about 1,200 GPU hours. Search stage fol-
lowed by evaluation, as done in our approach is more efficient when the different
number of possible edge devices is less than fifteen.
3 DCA-NAS: Device Constraints Aware Fast Neural
Architecture Search
We present the preliminary gradient-based NAS objective function in section 3.1
and then formulate the problem of incorporating the hardware-awareness in NAS
as a constrained optimization problem in section 3.2 followed by techniques
to reduce the search time in section 3.3. The framework of our approach is
illustrated in Figure 1.
3.1 Gradient-based NAS Objective Function
Popular DNAS techniques [25,46] have two stages, the search phase and the eval-
uation phase. During the search phase, given a task or a dataset the techniques
search for a network of cells, which are directed acyclic graphs with Nnodes. The
edges of the graph are network layers, whose operations are to be selected from
a pre-defined set Ocontaining operations such as 3x3 separable convolution and
identity operations with trainable weights wo. The search is made differentiable
by making the choice of a particular operation to be a softmax of architecture
weights αof all operations. Thus, the intermediate output zjat node jis given
by,
zj=X
o∈Oexp
αi,j
o	
P
o′∈Oexp
αi,j
o′	·o
wi,j
o,zi
(1)
4 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal
3.2 DCA-NAS formulation
Previous DNAS approaches [25,45,46] did not focus on searching architectures
specifically for inference on resource-constrained devices. In contrast, we for-
mulate the DNAS objective function as a constrained optimization problem by
incorporating device resource constraints (memory or FLOPs) in the search ob-
jective function. The constrained bi-level optimization problem is written as,
minαLval(w∗(α), α)
s.t. w∗(α) = argminwLtrain(w, α)
s.t. ks(α)≤Kd(2)
where training dataset is split into train andvalto optimize wandαsimultane-
ously in each iteration subject to the constraint that the architecture’s number
of parameters or FLOPs ksmust be less than or equal to the device resource
constraint Kd. The following equation calculates the architecture’s number of
parameters or FLOPs during search given the number of cells cn. Our method
can also be adapted to use other metrics such as latency and energy consumption
with additional metric measuring functions.
ks(α) =cnX
(i,j)∈NX
o∈Oexp{αi,j
o} ∗b(o)P
o′∈Oexp{αi,j
o′}(3)
Tackling the difference in search and evaluation networks The size of
the architecture in the search phase ksis different from the architecture size in
evaluation phase due to the softmax weighting factor in equation 3 (demonstra-
tion can be found in the appendix). To address this, we introduce a tighter bound
on the search constraint Kd′, which is less than the device resource constraint
Kd. A lookup graph (LUG) needs to be made for each dataset by varying Kd′
within appropriate bounds and running the algorithm until convergence each
time to obtain the corresponding device resource constraint Kd. The computa-
tion time of the LUG can be reduced by running the searches in parallel. Thus,
on incorporating the tighter constraint by looking-up the graph for the given
device resource constraint Kdalong with the trainable Lagrange multiplier λin
Equation 2, the objective function is re-written as,
eL=Lval(w∗(α), α) +λ(ks(α)−LUG (Kd))
s.t.w∗(α) = argminwLtrain(w, α)(4)
3.3 Techniques to reduce search time
Channel Bottleneck We use convolutional layers of 1x1 kernel to reduce the
depth of output channels of operations in the search space to save computation
time and memory overhead.
Derived Cell and Weight sharing . During architecture search, only one cell
with trainable αis used to optimize architecture parameters. The target network
for inference is built by stacking cells with architectures derived from highly
weighted operations. This can be done during search by deriving the other cell
Search-time Efficient Device Constraints-Aware Neural Architecture Search 5
Table 1: Performance comparison of architectures evaluated on visual datasets-
CIFAR-10 and TinyImagenet. ’(CIFAR-10)’ indicates search with CIFAR-10. ’X M’ in
’DCA-NAS-X M’ denotes the input memory constraint. RCAS- Resource Constrained
Architecture Search
Dataset Search Method Accuracy Parameters GPU
Strategy (%) (Million) Hours
CIFAR-10 manual PyramidNet-110 (2017) [12] 95.74 3.8 -
manual VGG-16 pruned (2017) [16] 93.4 5.4 -
evolution Evolution + Cutout (2019) [39] 96.43 5.8 12
random NAO Random-WS (2019) [31] 96.08 3.9 7.2
gradient ENAS + micro + Cutout (2018) [30] 96.46 4.6 12
gradient DARTS + Cutout (2nd) (2018) [25] 97.24 ±0.09 3.3 24
gradient SNAS + Cutout (2018) [43] 97.15 2.8 36
gradient PC-DARTS (2019) [45] 97.43 ±0.07 3.6 2.4
gradient SGAS (2020) [23] 97.34 3.7 6
gradient DrNAS (2020) [6] 97.46 ±0.03 4.0 9.6
gradient DARTS+PT (2021) [38] 97.39 ±0.08 3.0 19.2
gradient Shapley-NAS (2022) [42] 97.53 ±0.04 3.4 7.2
RCAS DCA-NAS- 3.5 M (CIFAR-10) 97.2±0.09 3.4 1.37
Tiny ImageNet manual SqueezeNet (2016) [18] 54.40 - -
manual PreActResNet18 (2020) [22] 63.48 - -
manual ResNet18 (2016) [15] 58.4 6.4 -
manual DenseNet (2020) [1] 62.73 11.8 -
gradient DARTS+ Cutout (2018) [25] 62.15 ±0.15 7.3 219
RCAS DCA-NAS- 3.5 M 61.34±0.09 3.5 12.5
RCAS DCA-NAS- 3.5 M (CIFAR-10) 61.4±0.15 3.4 1.37
Fig. 2: Plots show that DCA-NAS method discovers models with fewer parameters
than other NAS methods and manual architectures without sacrificing prediction per-
formance to a large extent.
architectures from the first at each iteration [46]. The arrangement of the cells
for search is given in the appendix. This derived cell saves computation and
memory overhead. A weight sharing strategy [46] among same operations with
the same originating node ito all nodes i < j < N has been applied within a cell.
This is motivated by the observation that non-parametric operations operating
on the representation of a node produce the same feature map irrespective of the
output node and thereby extended to parametric operations. Thus, Equation 1
may be re-written to the following,
zj=X
o∈Oexp
αi,j
o	
P
o′∈Oexp
αi,j
o′	·o
wi
o,zi
(5)
4 Experimental Results
Our approach is evaluated on two search spaces- DARTS and NAS-Bench with
vision datasets- CIFAR10, TinyImagenet, Imagenet-16-20 and Imagenet-1k. The
details of the search space and implementation is given in the appendix
6 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal
Table 2: Performance and comparison of architectures evaluated on Imagenet-1k. The
label ”(Imagenet)” indicates that the architecture has been searched and evaluated on
Imagenet-1k.; else it is searched on CIFAR-10. ’X M’ in ’DCA-NAS-X M’ denotes the
input memory constraint
Method Test Error (%) Parameters FLOPS Search Cost Search
top-1 top-5 (Mil) (Mil) (GPU days) Strategy
Inception-v1 (2015) [35] 30.2 10.1 6.6 1448 - manual
MobileNet V1 (2017) [17] 29.4 10.5 4.2 569 - manual
MobileNet V2 (2018) [33] 72.0 91.0 3.4 300 - manual
ShuffleNet 2 ×(v2) (2018) [28] 25.1 - 5 591 - manual
MnasNet-92 (2020) [14] 25.2 8.0 4.4 388 - RL
AmoebaNet-C (2019) [31] 24.3 7.6 6.4 570 3150 evolution
DARTS+Cutout (2018) [25] 26.7 8.7 4.7 574 1.0 gradient
SNAS (2018) [43] 27.3 9.2 4.3 522 1.5 gradient
GDAS (2019) [10] 26.0 8.5 5.3 545 0.3 gradient
BayesNAS (2019) [49] 26.5 8.9 3.9 - 0.2 gradient
P-DARTS (2018) [30] 24.4 7.4 4.9 557 0.3 gradient
SGAS (Cri 1. best) (2020) [23] 24.2 7.2 5.3 585 0.25 gradient
SDARTS-ADV (2020) [5] 25.2 7.8 6.1 - 0.4 gradient
Shapley-NAS (2022) [42] 24.3 - 5.1 566 0.3 gradient
RC-DARTS (2019) [20] 25.1 7.8 4.9 590 1 RCAS
DCA-NAS 25.1 8.1 5.1 578 0.06 RCAS
ProxylessNAS (GPU) (2019) [4](Imagenet) 24.9 7.5 7.1 465 8.3 gradient
PC-DARTS (2019) [45] (Imagenet) 24.2 7.3 5.3 597 3.8 gradient
DrNAS (2020) [6] (Imagenet) 24.2 7.3 5.2 644 3.9 gradient
DARTS+PT (2021) [38] (Imagenet) 25.5 - 4.7 538 3.4 gradient
Shapley-NAS (2022) [42] (Imagenet) 23.9 - 5.4 582 4.2 gradient
RCNet-B (2019) [44] (ImageNet) 25.3 8.0 4.7 471 9 RCAS
DCA-NAS- 5.5 M(Imagenet) 24.4 7.2 5.3 597 1.9 RCAS
4.1 Results on DARTS search space
Transferability- learning of coarse features during search. We transfer
the architecture searched on CIFAR-10 to train and evaluate the model weights
on TinyImagenet in Table 1 and ImageNet-1k in Table 2. This transferred model
yields higher performance than manually designed architectures [33,28] for the
target dataset. It is observed that performance of the transferred model is com-
parable to the architecture searched on the target dataset itself which can be
attributed to the architecture learning coarse features than objects during search.
Performance versus Device-Constraints trade-off DCA-NAS discovers
2 to 4% better-performing architectures than manual designs with a memory
constraint of 3.5 million parameters on CIFAR-10 and similar performance on
TinyImagenet as in Table 1. On Imagenet-1k, DCA-NAS yields models with sim-
ilar performance to other NAS methods [42,6,45] with a constraint of 5.5 million
parameters (taken to yield similar sized models as other NAS methods) as in Ta-
ble 2. We vary the input device resource constraint and plot the performance of
the searched models against the number of parameters in Figure 2. As observed,
DCA-NAS searched models can yield 15x lower sized models than manual ar-
chitectures like PyramidNet-272 [12] with at most 1% reduction in accuracy on
CIFAR-10. On TinyImagenet, DCA-NAS yields models similar in performance
but 6x smaller in size than the manual Resnet variant. In comparison to Prox-
ylessNAS [4] for Imagenet-1k, DCA-NAS yields 32% smaller model in terms of
model parameters for similar accuracy. In comparison to DNAS methods [25,45]
for each of the three datasets, we observe that the performance of the DCA-NAS
searched models is retained to a certain extent as resources are further limited
Search-time Efficient Device Constraints-Aware Neural Architecture Search 7
Fig. 3: Plots show DCA-NAS searched models with similar performance but lower
inference latency (on two devices- Pixel 3 and Raspberry Pi 4) to previous SOTA NAS
method- PC-DARTS when evaluated on NAS-Bench dataset.
after which the model performance degrades. DCA-NAS model of similar size has
the advantage of better performance (by 1%) and being automatically searched
over MobileNet-v2 [33], a manually designed network on Imagenet-1k.
Search time comparison For evaluation on TinyImagenet in Table 1, the
architecture searched on CIFAR-10 with DCA-NAS yields model in the low-
est search time which indicates the search-time efficiency of the transferability
property. Our method requires about 4x lower search cost than SGAS [23] which
performs the best among the other transferred architectures and 16x lower search
time than the other resource-constrained approach [20] for similar performance
as seen in Table 2. Moreover, ProxylessNAS [4] takes about 4x more search time
than DCA-NAS whereas PC-DARTS takes about 2x more search time with no
capability to constraint model size.
4.2 Results on NAS-Bench-201 search space
Performance and Latency comparisons on different devices Our method
reports the mean by averaging over five runs with different random seed. Figure
3 compares the performance of models searched with DCA-NAS and PC-DARTS
by varying the latency constraints. It shows that unlike PC-DARTS, DCA-NAS
can search for more efficient models which have lower inference latency for similar
test accuracy. Moreover, we observe that models with similar performance have
lower latency when tested on Pixel 3 than on Raspberry Pi 4 due to a faster
RAM in Pixel 3. DCA-NAS takes the lowest search time among all the NAS
methods due to the addition of search-time-efficient techniques while being at-
par in terms of performance across all datasets.
5 Ablation Study
Effectiveness of various algorithmic augmentations for faster search:
We analyze the effectiveness of algorithmic augmentations mentioned preciously 3.3
to reduce search cost in our study. We sequentially add weight sharing, channel
bottleneck, and derived cells to the baseline DARTS [25] method and measure
search time and accuracy. Weight sharing, channel bottleneck, and derived cells
was observed to significantly reduce search memory overhead, enabling us to use
larger batch sizes and reducing overall search cost as seen in Figure 4a. Adding
the resource-constraint in the final DCA-NAS method negligibly increases search
8 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal
(a)
 (b)
Fig. 4: (a) Ablation study with CIFAR-10 dataset- Each component added to DARTS
leads to the reduction in the search cost of DCA-NAS while performance is retained.
WS- Weight Sharing, CB- Channel Bottleneck, DC- Derived Cell, RC- Resource Con-
straint, BS- Batch Size (b) Shows stability of performance of DCA-NAS searched mod-
els for runs with varying seeds on CIFAR-10 dataset.
cost while maintaining performance.
Stability of the approach: We test stability by running the search algorithm
independently five times with different initial seeds and the same constraints and
hyperparameters. The architectures found during each run have similar perfor-
mance when re-trained and evaluated as shown in Fig. 4b. Smaller models have
lower performance due to restrictions in model complexity compared to larger
models.
6 Conclusion
We present DCA-NAS, a device constraints-aware neural architecture search
framework which discovers architectures optimized to the memory and compu-
tational constraints of an edge device in a time-efficient manner. It does so by
incorporating a constraint in terms of the number of parameters or floating point
operations (FLOPs) in the objective function with the help of a Lagrange multi-
plier. DCA-NAS in essence searches for a Pareto optimal solution given the edge
device memory or FLOPs constraint. Moreover, it enables architecture search
with search cost 4 to 17 times lower than the previous state-of-the-art Hardware-
aware NAS approaches. DCA-NAS can discover models with size about 10 to
15 times lower than manually designed architectures for similar performance.
In comparison to DARTS and its other NAS variants, DCA-NAS can discover
models upto 3x smaller in size with similar performance. This hardware-aware
approach can be generalized to any future updates to differential neural architec-
ture search and possibly to training-free methods of NAS with some adaptation.
Acknowledgement
We thank the anonymous reviewers; Profs. Surendra Prasad and Brejesh Lall of
IIT Delhi; and colleagues at Cadence India for their valuable feedback and inputs.
This research is supported by funding from Cadence India; the first author is
also supported by a fellowship from the Ministry of Education, India.
Search-time Efficient Device Constraints-Aware Neural Architecture Search 9
References
1. Abai, Z., Rajmalwar, N.: Densenet models for tiny imagenet classification (2020)
2. Baker, B., Gupta, O., Raskar, R., Naik, N.: Accelerating neural architecture search
using performance prediction (2017)
3. Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-All: Train One Network
and Specialize it for Efficient Deployment (Apr 2020), http://arxiv.org/abs/1908.
09791, arXiv:1908.09791 [cs, stat]
4. Cai, H., Zhu, L., Han, S.: Proxylessnas: Direct neural architecture search on target
task and hardware (2019)
5. Chen, X., Hsieh, C.J.: Stabilizing differentiable architecture search via
perturbation-based regularization. In: International conference on machine learn-
ing. pp. 1554–1565. PMLR (2020)
6. Chen, X., Wang, R., Cheng, M., Tang, X., Hsieh, C.J.: Drnas: Dirichlet neural
architecture search. arXiv preprint arXiv:2006.10355 (2020)
7. Chu, G., Arikan, O., Bender, G., Wang, W., Brighton, A., Kindermans, P.J., Liu,
H., Akin, B., Gupta, S., Howard, A.: Discovering multi-hardware mobile models via
architecture search. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 3022–3031 (2021)
8. Courbariaux, M., Bengio, Y., David, J.P.: Binaryconnect: Training deep neural
networks with binary weights during propagations (2016)
9. Ding, Y., Wu, Y., Huang, C., Tang, S., Wu, F., Yang, Y., Zhu, W., Zhuang, Y.:
Nap: Neural architecture search with pruning. Neurocomputing 477, 85–95 (2022)
10. Dong, X., Yang, Y.: Searching for a robust neural architecture in four gpu hours.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 1761–1770 (2019)
11. Elsken, T., Metzen, J.H., Hutter, F.: Efficient multi-objective neural architecture
search via lamarckian evolution. arXiv: Machine Learning (2019)
12. Han, D., Kim, J., Kim, J.: Deep pyramidal residual networks. In: Proceedings of
the IEEE conference on computer vision and pattern recognition. pp. 5927–5935
(2017)
13. Han, S., Mao, H., Dally, W.J.: Deep compression: Compressing deep neural net-
works with pruning, trained quantization and huffman coding (2016)
14. He, C., Ye, H., Shen, L., Zhang, T.: Milenas: Efficient neural architecture search
via mixed-level reformulation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 11993–12002 (2020)
15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 770–778 (2016)
16. He, Y., Zhang, X., Sun, J.: Channel pruning for accelerating very deep neural net-
works. In: Proceedings of the IEEE International Conference on Computer Vision.
pp. 1389–1397 (2017)
17. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., An-
dreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for
mobile vision applications (2017)
18. Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K.:
Squeezenet: Alexnet-level accuracy with 50x fewer parameters and ¡0.5mb model
size (2016)
19. Jiang, Q., Zhang, X., Chen, D., Do, M.N., Yeh, R.A.: EH-DNAS: End-to-End
Hardware-aware Differentiable Neural Architecture Search. arXiv:2111.12299 [cs]
(Nov 2021), http://arxiv.org/abs/2111.12299, arXiv: 2111.12299
10 Oshin Dutta, Tanu Kanvar, and Sumeet Agarwal
20. Jin, X., Wang, J., Slocum, J., Yang, M.H., Dai, S., Yan, S., Feng, J.: Rc-
darts: Resource constrained differentiable architecture search. arXiv preprint
arXiv:1912.12814 (2019)
21. Jozefowicz, R., Zaremba, W., Sutskever, I.: An empirical exploration of recurrent
network architectures. In: Proceedings of the 32nd International Conference on In-
ternational Conference on Machine Learning - Volume 37. p. 2342–2350. ICML’15,
JMLR.org (2015)
22. Kim, J.H., Choo, W., Song, H.O.: Puzzle mix: Exploiting saliency and local statis-
tics for optimal mixup (2020)
23. Li, G., Qian, G., Delgadillo, I.C., M¨ uller, M., Thabet, A., Ghanem, B.: Sgas: Se-
quential greedy architecture search (2020)
24. Lin, J., Chen, W.M., Lin, Y., Gan, C., Han, S., et al.: Mcunet: Tiny deep learning
on iot devices. Advances in Neural Information Processing Systems 33, 11711–
11722 (2020)
25. Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search. arXiv
preprint arXiv:1806.09055 (2018)
26. Liu, Z., Li, J., Shen, Z., Huang, G., Yan, S., Zhang, C.: Learning efficient convolu-
tional networks through network slimming (2017)
27. Lyu, B., Yuan, H., Lu, L., Zhang, Y.: Resource-Constrained Neural Architecture
Search on Edge Devices. IEEE Transactions on Network Science and Engineer-
ing9(1), 134–142 (Jan 2022). https://doi.org/10.1109/TNSE.2021.3054583, con-
ference Name: IEEE Transactions on Network Science and Engineering
28. Ma, N., Zhang, X., Zheng, H.T., Sun, J.: Shufflenet v2: Practical guidelines for
efficient cnn architecture design. In: Proceedings of the European conference on
computer vision (ECCV). pp. 116–131 (2018)
29. Mu˜ noz, J.P., Lyalyushkin, N., Akhauri, Y., Senina, A., Kozlov, A., Jain, N.:
Enabling NAS with Automated Super-Network Generation (Dec 2021), http:
//arxiv.org/abs/2112.10878, arXiv:2112.10878 [cs]
30. Pham, H., Guan, M.Y., Zoph, B., Le, Q.V., Dean, J.: Efficient neural architecture
search via parameter sharing. In: ICML (2018)
31. Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image clas-
sifier architecture search (2019)
32. Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Ku-
rakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th In-
ternational Conference on Machine Learning - Volume 70. p. 2902–2911. ICML’17,
JMLR.org (2017)
33. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-
verted residuals and linear bottlenecks. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 4510–4520 (2018)
34. Srivastava, A., Dutta, O., Gupta, J., Agarwal, S., AP, P.: A variational informa-
tion bottleneck based method to compress sequential networks for human action
recognition. In: Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision. pp. 2745–2754 (2021)
35. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: Proceedings
of the IEEE conference on computer vision and pattern recognition. pp. 1–9 (2015)
36. Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., Le QV, M.:
platform-aware neural architecture search for mobile. 2019 ieee. In: CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR). pp. 2815–2823 (2019)
Search-time Efficient Device Constraints-Aware Neural Architecture Search 11
37. Tan, M., Le, Q.: Efficientnet: Rethinking model scaling for convolutional neural
networks. In: International conference on machine learning. pp. 6105–6114. PMLR
(2019)
38. Wang, R., Cheng, M., Chen, X., Tang, X., Hsieh, C.J.: Rethinking architecture
selection in differentiable nas. arXiv preprint arXiv:2108.04392 (2021)
39. Wistuba, M.: Deep learning architecture search by neuro-cell-based evolution with
function-preserving mutations. In: Berlingerio, M., Bonchi, F., G¨ artner, T., Hurley,
N., Ifrim, G. (eds.) Machine Learning and Knowledge Discovery in Databases. pp.
243–258. Springer International Publishing, Cham (2019)
40. Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., Tian, Y., Vajda, P., Jia, Y.,
Keutzer, K.: Fbnet: Hardware-aware efficient convnet design via differentiable neu-
ral architecture search. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 10734–10742 (2019)
41. Wu, Y., Gong, Y., Zhao, P., Li, Y., Zhan, Z., Niu, W., Tang, H., Qin, M., Ren, B.,
Wang, Y.: Compiler-Aware Neural Architecture Search for On-Mobile Real-time
Super-Resolution (Jul 2022), http://arxiv.org/abs/2207.12577, arXiv:2207.12577
[cs, eess]
42. Xiao, H., Wang, Z., Zhu, Z., Zhou, J., Lu, J.: Shapley-NAS: Discovering Operation
Contribution for Neural Architecture Search (Jun 2022), http://arxiv.org/abs/
2206.09811, arXiv:2206.09811 [cs]
43. Xie, S., Zheng, H., Liu, C., Lin, L.: Snas: stochastic neural architecture search. In:
International Conference on Learning Representations (2018)
44. Xiong, Y., Mehta, R., Singh, V.: Resource constrained neural network architecture
search: Will a submodularity assumption help? In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 1901–1910 (2019)
45. Xu, Y., Xie, L., Zhang, X., Chen, X., Qi, G.J., Tian, Q., Xiong, H.: Pc-darts: Par-
tial channel connections for memory-efficient architecture search. arXiv preprint
arXiv:1907.05737 (2019)
46. Yang, Y., You, S., Li, H., Wang, F., Qian, C., Lin, Z.: Towards improving the
consistency, efficiency, and flexibility of differentiable neural architecture search.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 6667–6676 (2021)
47. Zhang, X., Zhou, X., Lin, M., Sun, J.: Shufflenet: An extremely efficient convolu-
tional neural network for mobile devices. In: Proceedings of the IEEE conference
on computer vision and pattern recognition. pp. 6848–6856 (2018)
48. Zheng, X., Fei, X., Zhang, L., Wu, C., Chao, F., Liu, J., Zeng, W., Tian,
Y., Ji, R.: Neural Architecture Search with Representation Mutual Infor-
mation. In: 2022 IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 11902–11911. IEEE, New Orleans, LA, USA (Jun
2022). https://doi.org/10.1109/CVPR52688.2022.01161, https://ieeexplore.ieee.
org/document/9878903/
49. Zhou, H., Yang, M., Wang, J., Pan, W.: Bayesnas: A bayesian approach for neural
architecture search (2019)
50. Zhu, C., Han, S., Mao, H., Dally, W.J.: Trained ternary quantization (2017)
Appendix
A Deriving cell architectures
The searched cells are stacked to form the network whose weights are trained and
evaluated. The layers of this network during the evaluation phase is varied from
4 to 20. It can be seen that the models searched with DARTS with only 2-cells
perform equally well as those of 8-cell search for target model with layers more
than 10. Hence, in our experiments, instead of training architecture parameters
for all 8 cells, we train only 2 cells- one normal and the other reduction cell. The
architecture of the other 6 cells stacked to form the network during search are
derived from either the normal or the reduction cell as shown in Figure 1.
B Calculation of search-stage architecture size
The size of the architecture in the search phase ksis different from the architec-
ture size in evaluation phase due to the softmax weighting factor in equation 3
(demonstrated in Figure 2). To address this, we introduce a tighter bound on
the search constraint Kd′, which is less than the device resource constraint Kd.
A lookup graph (LUG) needs to be made for each dataset by varying Kd′within
appropriate bounds and running the algorithm until convergence each time to
obtain the corresponding device resource constraint Kd. The computation time
of the LUG can be reduced by running the searches in parallel.
C Algorithm
The practical implementation of our resource-constrained gradient descent-based
approach is illustrated in Algrorithm 1.
D Implementation Details
The experiments with the smaller vision datasets-MNIST, FashionMNIST, CIFAR-
10, Imagenet-16-120 and TinyImagenet were run on a single Tesla V100 GPU.
Training and evaluation on Imagenet-1k was performed on a cluster containing
eight V100 GPUs.
The super-net used for search with smaller vision datasets except Imagenet-1k
consists of 8 cells, with 6 normal cells and 2 reduction cells, and an initial num-
ber of channels set to 16. Each cell has 6 nodes, with the first 2 nodes in cell k
serving as input nodes. The super-net is trained for 50 epochs with a batchsize
of 512, and optimized using SGD with a momentum of 0.9 and weight decay of
3e−4. The learning rate is initially set to 0.2 and gradually reduced to zero
Appendix 13
Fig. 1: Top: shows the regular DARTS cell with nodes connected by weighted opera-
tions and the derived cell made of top-weighted operations. Bottom: Shows the network
comprising the normal cell (bold border) and reduction cells (dotted border) with train-
able architecture parameters (red border) and the derived cells (green border) without
any architecture parameters.
Fig. 2: Demonstrates the calculation of memory size of a single cell in the architecture
during - Left: search phase. Right: evaluation phase
using a cosine scheduler. Architecture parameters αare optimized using Adam
optimizer, with a learning rate of 6 e−4, a momentum of (0 .5,0.999), and a
weight decay of 1 e−3. The search is run 5 times, and the architecture with the
highest validation accuracy is chosen. For evaluation, the target-net has 20 cells,
with 18 normal cells and 2 reduction cells, and an initial number of channels set
to 36. The target-net is trained for 600 epochs with a batchsize of 96, optimized
using SGD with a momentum of 0.9, weight decay of 3e-4, and gradient clipping
of 5. The initial learning rate is set to 0.025 and gradually reduced to zero using
a cosine scheduler. Additional settings include a cutout length of 16, dropout
rate of 0.2, and use of an auxiliary head. For Imagenet-1k, We reduce the input
size from 224 ×224 to 28 ×28 using three convolution layers with a stride of 2.
The super-net for search has 8 cells starting with 16 channels, and the target-net
for evaluation has 14 cells starting with 48 channels. Both search and evaluation
use a batch size of 1,024. In search, we train for 50 epochs with a learning rate
of 0.5 (annealed down to zero using a cosine scheduler), and a learning rate of
6e−3 for architecture parameters. In evaluation, we train for 250 epochs using
14
Algorithm 1 DCA-NAS - gradient descent based search method
Assign random weights to αi,jon edges i, jdenoting weights of operations in the
mixed set
Input look-up graph Gand device memory constraint Kd
Look-up corresponding search memory constraint Kd′from G
Calculate total search time memory size ks(α)
while not converged do
Calculate eL(w, α, λ ) =Lval(w(α), α) +λ(ks(α)−Kd′)
Update weights wby descending ▽weLtrain(w, α, λ )
Update αby descending ▽αeLval(w∗, α, λ )
Calculate total search time memory size ks(α)
Calculate loss as in equation 4
Update λ
end while
Derive the final architecture based on the learned αby connecting the top weighted
operations among the mixed set
the SGD optimizer with a momentum of 0.9 and a weight decay of 3 e−5, and
adopt an auxiliary head and the label smoothing technique.
E Model performance by varying FLOPs constraint on
CIFAR10, TinyImagenet and Imagenet-1k
Instead of model parameters, we also experiment with FLOPs as the constraint
in our objective function. As shown in Figure 3, our method DCA-NAS retains
performance till a certain FLOPs constraint, after which it degrades. In com-
parison to manual architectures, our NAS approach yields models which require
much smaller FLOPs and hence would have lower latency.
Fig. 3: Plots show that DCA-NAS method discovers models with fewer FLOPs
than other NAS methods and manual architectures without sacrificing prediction
performance.

Generating Sequential Code From Parallel Code 
Jeanne Ferrante 
IBM T. J. Watson Research Center 
P.O. Box 704 
Yorktown Heights, New York 10598 
Mary Mace 
IBM 
Research Triangle Park, NC 27709 
Barbara Simons 
IBM Almaden Research Center 
Department of Computer Science 
San Jose, CA 95120-6099 
We consider the problem of generating sequential code for parallel programs written in a la.nguage which 
contains a FORALL operator, predicates and statements. This problem can arise when compiling for a 
multiprocessor where each processor is sequential, and in the vectorization of sequential programs. We 
present a necessary and sufficient condition for determining if extra guard variables or duplicate code must 
be inserted when generating a correct sequential program from a given, well-structured parallel program. 
We also have an efficient method for checking whether this condition occurs in the parallel program. This 
method gives rise to an algorithm for generating sequential code from parallel code which inserts guard 
variables (or duplicate code) only when determined by our necessary and sufficient condition. Given the 
choice to insert duplicate code, an exponential blow-up in size may result. We suggest some simple heuristics 
to limit such blow-up. We also show that in a restricted case, the minimal size sequential code can be 
efficiently generated. However, we show that a problem closely related to finding the minimal size 
sequential code (or equivalently, the minimum number of guards to be inserted) is NP-Complete. We 
conjecture that this problem in the general case is indeed NP-Complete. 
1 .O Introduction 
The problem of generating good sequential code for parallel 
programs arises in several different contexts. One context 
involves code generation from parallel intermediate languages for 
sequential processors, which arises for both multiprocessor and 
sequential machines. The PTRAN group uses an intermediate 
representation for sequential programs called the Program 
Dependence Graph (PDG), in which data and control dependences 
[6, 71 expose potential parallelism. In generating code for a 
sequential processor from the PDG, we do not wish to increase 
the code size (by duplicating code) or increase the number of 
Boolean tests (by inserting guards) unless necessary. Using the 
method presented here, we can generate a control flow graph 
from the control dependence subgraph without duplication of 
code or insertion of guards, unless necessary. 
Permission to copy without fee all or part of this material is granted provided 
that the copies are not made or distributed for direct commercial advantage, 
the ACM copyright notice and the title of the publication and its date appear, 
and notice is given that copying is by permission of the Association for 
Computing Machinery. To copy otherwise, or to republish, requires a fee and/ 
or specific permission. 
0 1988 ACM O-89791-272-1/88/0007/0582 $1 SO Another such context is vectorization of sequential programs. In 
this context, source-to-source vectorizers such as [3] use a 
transformation called “IF-conversion”. This transformation adds 
Boolean guards for each statement in a loop and then performs 
Boolean simplification on these guards, a costly procedure. If 
some of the statements in the loop cannot be vectorized, it is 
desirable to emit better sequential code than that of the guarded 
version. The method presented here (and a graphical represen- 
tation of Boolean guards which requires no I3oolean simplification 
[7]) allows us to generate sequential code from guarded 
statements in the loop, without duplicate code or guards unless 
necessary. 
An additional context is the development of parallel algorithms. 
For portability and generality, parallel algorithms might be 
developed under the idealized assumption of unlimited 
parallelism, e.g., an infinite number of processors. In the context 
of a real and necessarily finite machine, this unlimited parallelism 
would have to be bounded and thus sequentialized. 
Yet another context for this problem is register transfer level 
design languages [4]. Many such languages have a “Multiple Do” 
operator included, where all the targets of the Multiple Do are 
executed in parallel. These operators arise naturally since the 
hardware described by the language executes in parallel. To 
simulate a design written in such a language on a sequential 
582 

machine, we must generate first a sequential program. Since logic 
designs can be very large and simulation is a very important part 
of the design process, we would like to produce the smallest and 
fastest code possible, that is, without any duplication of the 
original code or extra Boolean tests, unless necessary. The 
algorithm presented here produces such code. 
A final context in which this problem arises is the integration of 
non-interfering versions of programs, as considered in [9]. There, 
an intermediate representation similar to the PDG of [7] is used 
to represent different versions of an initial program, a merge is 
formed from these versions, and if possible a sequential program 
is generated. Although the merge as defined may not correspond 
to a valid parallel program, if it does, our method will generate a 
correct sequential version of it without duplication or guards, 
unless it is necessary. 
2.0 Problem Statement 
We consider programs in any parallel programming language with 
the following characteristics. The language consists of statements 
and predicates as in any sequential procedural language; the 
language also includes a FORALL operator, whose operands can 
be any number of targets. Such parallel programs are considered 
to be rooted, directed graphs whose nodes are predicates, 
statements, or FORALL’s, and whose outgoing edges indicate 
possible transfer of control (for a predicate) or a target (for a 
FORALL node), We refer to such graphs, with the restrictions 
imposed which follow, as Parallel Program Graphs or PPG’s. The 
semantics of a FORALL node is such that all of its targets must 
be executed whenever the FORALL is reached. No order of 
execution is specified for targets of FORALL nodes. Note that 
in PPG’s, nodes can be merge nodes, that is, the target of several 
predicates or FORALL’s. If any incoming edge to a merge node 
is executed, then the merge node must be executed. 
We impose several further restrictions on a graph to be a PPG. 
First, for simplicity we assume onb FORALL’s can be merge 
nodes. Second, to avoid semantic ambiguity in the language, we 
impose the following condition: Given any two non-intersecting 
paths from a node M to a merge node, M must always be a 
predicate node (i.e., it cannot be a FORALL node.) This 
condition rules out graphs like that of Figure 1, where it is 
unclear whether merge node M should be executed once or twice. 
Third, we assume all nodes in the PPG are reachable from the 
root. These conditions are not overly restrictive; for example, 
all PDG’s are PPG’s. Before stating our problem, we introduce some additional 
terminology. A control flow graph or cfg is a directed, rooted 
graph whose nodes are predicates or statements, and whose 
outgoing edges indicate possible transfer of control (for a 
predicate) or a target (for a statement). This definition differs 
only trivially from the usual notion of control flow graph [l]. 
Note that cfg’s differ from PPG’s in that they lack FORALL 
nodes, and statements can have successors. Since FORALL 
nodes do not appear in cfg’s, the only choice of execution order 
occurs at predicate nodes. 
Given a PPG the problem is to generate a correct control flow 
graph (i.e., without FORALL nodes) for the program, without 
duplication of statements and predicates in the PPG, and without 
insertion of additional predicates, if possible. Only control flow 
graphs which preserve execution order of statements and 
predicates are considered correct. For example, a correct control 
flow graph for the PPG in Figure 2 must have predicate Pl occur 
before predicate P2, since Pl is always executed before P2 in the 
parallel program. However, in that PPG, nodes M2 and M3, as 
children of the FORALL node Ml, may be executed in any order. 
Thus in the cfg, either choice of order for the descendants of M2 
and M3 might suffice. 
Definition: Given a PPG, a correct control flow graph for the 
PPG which has the same statement and predicate nodes as the 
PPG and has no duplication of nodes or extra guards is called a 
concise corresponding cfg. 
A concise corresponding cfg always implies a linear ordering of 
its nodes consistent with execution order. 
It is shown in [5] that it is not always possible to generate a 
concise corresponding cfg for a PPG. An example of such a PPG 
is shown in Figure 2. However, it is easy to show that it is always 
possible to generate a control flow graph from a PPG aNowing 
either duplication of code or insertion of extra guards. Such a 
control flow graph for a PPG is referred to here as a corresponding 
cfg. Corresponding cfg’s showing the two alternatives of extra 
guard variables and duplicate code are shown for the PPG of 
Figure 2 in Figure 3 . A procedure for the construction of cfg’s 
with duplicate code is contained in the Appendix. Exponential 
blow-up in size can result from applying this procedure. 
Figure 1. Graph which is not a PPG 
583 Figure 2. Example needing guards or duplication 
(a) (b) 
Figure 3. Inserting guards and duplicate code 
3.0 Comparison to Previous Work 
The algorithm of [5] could not be applied to a PPG unless a 
concise corresponding cfg was known to exist. We have 
completely characterized the structural condition under which a 
concise corresponding cfg does exist. Therefore, our algorithm 
can be applied in much more general situations, such as register 
transfer design languages, when a concise corresponding cfg is 
not known in advance to exist. Our algorithm is equally as 
efficient as that algorithm, and produces an equivalent result, 
whenever a concise corresponding cfg does exist. 
The work of [ 1 l] solves the linearization problem in an efficient 
manner, but again under the assumption that a concise 
corresponding cfg is known to exist. His concern is code 
generation for PDG’s, and if restrictions are placed on allowable 
transformations, this assumption will hold for PDG’s. His 
algorithm could not be used in the more general context of PPG’s, 
or where such restrictions don’t hold. 
In [9], a parallel program representation is used to represent 
different versions of an initial program, a merge is formed from 
these versions, and if possible a sequential program is generated. 
However, only limited control constructs are considered. 
4.0 Problem Solution 
We now present our main result, which is a necessary and 
sufficient condition for determining if extra guards or duplicate 
code must be inserted in constructing a control flow graph from 
a PPG. As shown in [5], this condition is easily checked by a 
simple algorithm that colors edges of the PPG (and is linear in the 
number of such edges). We expand this result in Section 4.2 so 
that a given PPG is always transformed into one which satisfies 
CC. This result, coupled with the code generation algorithm of 
[S] (included in the Appendix), yields an algorithm for generating 
a control flow graph for a PPG, inserting extra guards or duplicate 
code only when determined by our necessary and sufficient 
condition. Our first step towards the solution is to consider only 
re&cible[2] PPG’s. This restriction ensures that back edges are 
uniquely determined, all strongly connected regions are single- 
entry [2], and nested strongly connected regions called restricted 
regions can be identified [8]. We use the term loop to have the 
precise meaning of restricted regions in [8]. 
Our next step towards a solution is to consider the problem of 
determining a correct sequencing for descendants of a FORALL 
node. In particular, the problem arises when more than one of 
these descendants are merge nodes. Consider the PPG in 
Figure 4. (Here, nodes Mi are FORALL nodes, and nodes l-7 
are predicate and statement nodes.) We want to determine if we 
can order merge nodes M4 and M5 so that nodes 5 and.6 can be 
generated in the correct order in the control flow graph (without 
duplicating either node or adding extra guards). In this case the 
order <M4, MS> is correct, and we obtain the control flow graph 
in Figure 4(b). If the reverse order <MS,M4> is chosen, the 
resulting control flow graph is a semantically incorrect represen- 
tation of the original program. However, FORALL nodes do not 
imply any ordering of their children. In the context of this type 
of node, some criterion needs to be applied to determine if there 
is a correct sequencing. 
The criterion that can be used in this case is the presence of 
predicate 2. The reason node 5 mu,st go first is that, because of 
the branch due to predicate node 2., a situation exists where 5 is 
sometimes executed and sometimes not, whereas 6 is always 
executed whenever 5 is. In order for a path through 5 to go 
through 6, and a path around 5 to also go through 6, logically, 5 
must be sequenced before 6. 
However, it is possible that such an order of merge nodes might 
not exist. It is shown in [5] that such a correct sequencing does 
not exist for the PPG of Figure 2, i.e., duplication of code or 
insertion of guards is necessary. We prove here that a condition 
defined in [S] is both necessary and sufficient for the existence 
of a correct sequencing without duplication or extra guards. 
584 
MO 
R 
Figure 4. (a) 
PPG and corresponding control flow graph 1 n -2 u -1 n 
lu-1n3 
(b) 
To define this condition, as in [5] we associate with each merge 
node A in the PPG a Boolean expression B(A) which consists of 
some of the predicates in the PPG. B(A) describes all non-cyclic 
paths through predicates from the entry node to A. (Here, 
separate instances of the same predicate in the PPG are treated 
as distinct predicates.) For example, B(M4) and B(M5) are 
shown for the PPG in Figure 4. B(A) summarizes the conditions 
under which the node A is executed, ignoring the effects of loops. 
It is shown in [5] that the effects of loops can be ignored in 
forming these Boolean expressions for merge nodes, since they 
can have no effect upon sequencing in reducible PPG’s. We 
henceforth assume that the given PPG is a directed acyclic graph, 
or DAG. 
Let C(A) denote the set of truth values under which B(A) is true. 
We can now define the condition in question, henceforth referred 
to as CC: 
Given the merge node successors of a FORALL node A, 
There is an order of these merge nodes, say 
Ml . ..Mk. such that 
(CC) C(M1) c C(M2) c . . E C(Mk) 
For the merge nodes M4 and MS in Figure 4, C(M4) E C(M5), 
and so if B(M4) is true, then B(M5) is also true. This implies that 
if M4 is executed, then MS is also executed. Logically, if a node 
M is sometimes executed and sometimes not, and U is always 
executed, M should be sequenced before U. Condition CC is an 
exact representation of this situation. 
It is shown in [S] that CC is a sufficient condition for sequencing 
two merge nodes which are children of the same FORALL node, 
and an efficient algorithm is given for determining if CC holds. 
This algorithm operates on the PPG by coloring edges which 
represent the Boolean expressions for merge nodes and checking 
local conditions on the colored edges. We show here that CC is 
also a necessary condition. 
4.1 CC Is Necessary 
To accomplish this, we introduce the following terminology. Let 
G be a rooted DAG, and define the level number of a node to be 
the length of the longest path to the root. (The level of the root is 0). Define leveli to be all nodes of G with level number i. We 
say that level i is smaller (larger) than level j if i<j (i>j). 
Let PG be a PPG DAG, and assume that there is no concise 
corresponding cfg for PG. Given any corresponding cfg FG for 
PG with duplicate nodes, duplication can occur at many nodes u, 
each of which has a level number I(u) in PG. We let f(FG) be the 
minimum of the I(u), for all nodes u duplicated in FG. We denote 
by f the maximal value of any of the f(FG). We then pick FG to 
be a corresponding cfg for PG that contains no guard variables 
with the constraint that f(FG)= I. In addition, we require that if 
FFG is another cfg for PG with /(FFG)= I, then the number of 
nodes with level number 1 in FFG that must be duplicated is at 
least as large as the number of such nodes in FG. 
Let A be a node that is duplicated in FG with level number I 
in PG. A must be a predicate or statement node, since cfg’s only 
contain predicate and statement nodes. Since A appears in FG, 
it cannot be a merge node in PG, and so it must have a unique 
predecessor in PG. 
Lemma 1. A’s immediate predecessor in PG must be a FORALL. 
Proof: Suppose to the contrary that A’s immediate predecessor 
is a predicate variable. (Note that statement nodes in PPG’s have 
no successors, and thus this is the only alternative). Call this 
predecessor P. We know by the definition of I that P is not 
duplicated in FG, since P is of level less than I. We construct a 
new cfg FG’ for PG with no guard variables, no replicated nodes 
with level less than 1 in PG, and one fewer replicated node with 
level number I in PG, contradicting the minimality assumption of 
FG. Let FG’ be identical to FG from level 0 through level I - 1 
(in FG). We first argue that there is no node between P and A in 
FG. Since A must follow P in execution order of PG, and since 
execution order must be preserved between PG and FG, it follows 
that A must immediately follow P‘ in execution order in FG. 
Therefore, there can be no such node between P and A in FG. 
Consequently, we can insert A into FG’ without replicating A. 
Given the labelled edge which connects P to A in PG, we add that 
edge from P to A in FG’. We then insert into FG’ all remaining 
nodes from the level of node A in FG so that they are identical to 
their counterparts in FG. 
585 
At this point we have constructed a cfg FG’ which contains at 
least one less node from level I of PG that requires duplication 
than does FG and which is a correct corresponding cfg for the 
equivalent nodes of PG. All that remains is to demonstrate that 
FG’ can be extended to be a corresponding cfg for PG without 
replicating any of the nodes already in FG’ (so far constructed). 
FG’ can indeed be so extended by applying the procedure gendup 
from the Appendix from each of the nodes at the level of A in FG, 
which only generates nodes at the level of A or higher (with 
replication allowed). I 
Lemma 2. If A and its FORALL node siblings satisfy condition 
CC, then we can “unduplicate” A in the construction of FG’. 
Proof This is given by the procedure for sequencing merge nodes 
in [5]. n 
Theorem 1 states that CC is both a necessary and sufficient 
condition for sequencing merge nodes. 
Theorem 1. Let PG be a PPG. Then 
Condition CC holds for every FORALL node 
if and only if 
there exists a concise corresponding cfg for PG. 
Proof:: 
(If): Is proved in [5]. 
(Only If): Follows from Lemmas 1 and 2. 
Further, if I is the smallest level containing merge nodes succes- 
sors (of a FORALL node) which do not satisfy CC, then there 
exists a corresponding cfg for PG which does not replicate any 
of the nodes from the first l-l levels of PG. W 
4.2 Generation of Sequential Code 
In this section, we show how to generate sequential code for a 
given PPG PG even if CC is not satisfied. To do this, we first 
show how to color the edges of the PPG to determine the order 
relations that must be followed by merge node children of the 
same FORALL node. We represent these order relations in an 
order graph. Any cycle in the order graph corresponds to a set of 
merge nodes that cannot be linearly ordered, and hence fail to 
satisfy CC. 
We next show how to transform the PPG to finally obtain a PPG 
PG’ that satisfies CC. We accomplish this by transformations to 
the PPG which remove cycles in its order graph. Two alternative 
transformations are considered: inserting a guard variable, and 
making a duplicate copy of a subgraph. 
We briefly outline a well-known technique to insert guards to 
remove multiple incoming edges from a merge node. We also 
show how duplicate code can be straightforwardly used. 
Unfortunately, in this case, the size of the resulting graph could 
double. We sketch some heuristics to limit such blow-up. We 
also show that in a restricted case, the minimal size such cfg can 
be obtained. 
Repeated applications of either of these transformations may be 
necessary to finally obtain a non-cyclic order relation. The 
resulting PPG PG’ satisfies CC. To generate a control flow graph 
we then apply the code generation algorithm of [S] to PG’. We 
include this code generation algorithm gen in the Appendix. 4.2.1.1 Determining Order of Merge Nodes 
In this section we show how to determine the order that must be 
followed by any merge nodes that are children of the same 
FORALL node. As such relations are determined, we summarize 
them in a directed graph called an order gruph. 
To simplify our procedure for determining order, we assume that 
in the given PPG there are no superfluous FORALL nodes, i.e., 
given any FORALL node that has a FORALL node successor, 
that successor must be a merge: node. Any PPG can be 
transformed to one satisfying this restriction by applying the 
transformation in Figure 5. 
\ 
Figure 5. Transformation to remove superfluous 
FORALL nodes 
We first show how to color the edges of the PPG to determine the 
order relations that must be followed by merge node children of 
the same FORALL node. Given two such merge nodes, we 
associate a different color with each merge node. For each merge 
node, we traverse the graph backwards from the merge node, 
coloring edges with the merge node’s color. For merge node M, 
this effectively colors the edges of the graph representing B(M), 
the Boolean expression for M. Since the same predicate can 
appear in B(M) for many merge nodes M. edges can be colored 
with many colors. For example, in Figure 7, we show the 
coloring for merge nodes Ml and M3. The process of coloring 
the graph for all merge nodes takes time at most O(m E), where 
E is the number of edges and m is the number of merge nodes in 
the PPG. 
We next show how to construct the order graph for the PPG. We 
start our construction of the order graph by making equivalent all 
merge nodes which have exactly the same colored edges that 
touch predicate nodes. The nodes of the order graph are the 
equivalence classes of this equivalence relation. The edges of the 
order graph are determined by the procedure given in this section. 
Initially, there are no edges. 
We now give a procedure for determining the order between two 
inequivalent merge node children of the same FORALL node, 
assuming the PPG has been colored for the merge nodes. As such 
order is determined, directed edges will be added to the order 
graph. Let Ml and M2 be any two merge node children of the 
same FORALL node, as in Figure 6. Here, we ignore all other 
colors on the edges except those of Ml and M2. In Figure 6, 
Ml ‘s color is represented by a dot.ted line, and 442 ‘s color by a 
dashed line. 
The procedure compares the Boolean expressions B(M1) and B( 
M2) delineated by the coloring of the PPG by checking local 
conditions on colored edges touching predicate nodes. For each 
case illustrated for a predicate node P in Figure 6(a), the edges 
indicated in Figure 6(b) are added in the order graph between 
Ml and M2 (if they do not already exist). 
586 
Ml ..,, 1: 
M2 
m @---+@I @j----Q 
Case 1 Case 2 Case 3 
Figure 6. Determining order relations 
Case 1 determines that the merge nodes cannot be sequenced; 
neither of the Boolean expressions represented logically implies 
the other. Here, Ml and M2 are incomparable, and we add a 
cycle of length two in the order graph to indicate this. Case 2 
determines that the merge node whose color corresponds to the 
singly-colored edge, Ml, should be first in sequence. In this case, 
the Boolean expression for Ml has a clause which is more 
restrictive than some clause in the M2’s expression, and so it is 
only possible for the first Boolean expression to logically imply 
the second. In this case, we add an edge in the order graph from 
Ml to M2. Case 3 determines that the merge node corresponding 
to the singly-colored edge, Ml, should be Iast in sequence. In this 
case, the Boolean expression for Ml has an additional clause, and 
so it is only possible that it is logically implied by the second. 
Since we assume that Ml and M2 are inequivalent, one of these 
cases must occur. 
Following this procedure for the colored merge nodes in 
Figure 7, we find an instance of case 3 for each of the predicates 
Pl and P2. The first instance determines that M3 > Ml, and the 
second Ml > M3. This creates a cycle in the order graph. 
,._..’ ./ ‘\ 
\ \ \ \ \ \ \ \ ‘. \ ‘\ \ 
‘. \ 
‘. \ 
‘. 4; 
Ml > M3 ---- M3 > Ml _--_ 
Figure 7. PPG with coloring for Ml and M3 To fully determine the order graph, we carry out the above 
ordering procedure for all pairs of inequivalent merge node 
children of a FORALL node. Given a fully colored graph, this 
can be done in a second pass over the PPG, examining local 
patterns at a predicate node and checking whether any of the 
Cases 1, 2 or 3 apply. The full construction of the order graph 
can thus be done in time at most O(m2 E), where E is the number 
of edges and 171 is the number of merge nodes in the PPG. 
Carrying out this procedure fully for the PPG in Figure 7 would 
generate a second cycle in the order graph between M2 and M3. 
4.2.2 Inserting Guards 
As previously stated, cycles in the order graph can be broken by 
inserting extra guard variables. Given a cycle, a merge node 
involved in the cycle is chosen. Then at each incoming edge to 
the merge node in the PPG, a guard variable for the merge node 
is set to ‘T’, as in Figure 3, and the merge node and its subgraph 
are made control dependent only on that guard. (Of course, the 
guard must be initialized to ‘F’). This makes the node no longer 
a merge, and breaks the cycle of which it was part. 
4.2.3 Duplicating Code 
When a cycle is found in the order graph of a PPG, we could 
break the cycle by choosing a merge node involved in the cycle, 
and for each incoming edge to the merge node, duplicating the 
merge node and its subgraph. This procedure gendup is illustrated 
in Figure 3. However, it may not be necessary to perform such 
duplication for each incoming edge. 
Instead, one of the merge nodes M involved in the cycle, and an 
incoming edge in the PPG to that merge node, are selected. We 
then perform the following transformation in the PPG: we make 
a duplicate copy of M’s subgraph for the incoming edge to M, and 
redirect the incoming edge to M to the copy of M. We refer to 
this transformation as splitting, and define it more formally below. 
We then recolor the graph for the selected merge node and its 
duplicate, and update the order graph accordingly. This 
transformation is repeated until the resulting order graph is 
cycle-free. Since each application of this transformation could 
cause the size of the resulting graph to double, exponential 
blow-up may result, 
We introduce here some notation, to be used here and in the 
NP-Completeness proof of Section 5.0. Let G = (NJ) be a 
directed acyclic graph, and let i E N. We call 
G(i) = (N(i), E(i)) the s&graph reachable from i. N(i) = i 
together with all the nodes reachable from i, and E(i) =a11 the 
587 
edges induced by nodes in N(i). That is, an edge is in E(i) if and 
only if it is in E and both endpoints are in N(i). 
Let i and j be sibling nodes at least one of which must be 
duplicated. If we choose to duplicate i, then we say that i has 
been split. When we split i, we construct a new graph H as 
follows. Initially H = G. First, construct a second copy of G(i), 
called G’(i), and include its nodes and edges in H. Then choose 
some edge (w, i) that causes condition CC to be not satisfied, 
delete (w, i), and insert (w, i’) in H. Note that we have not given 
a rule on how to choose (w, i). In general a “wrong” choice for 
(w, i) can result in a graph with more than the minimum number 
of duphcations. A splitting S is a set of nodes to be split and edges 
to be “moved”. 
In the PPG of Figure 7, selecting either incoming edge to Ml will 
result in neither copy of Ml being a merge node, and thus the 
cycle between Ml and M3 will be broken. A similar choice will 
break the other remaining cycle in the order graph. 
4.2.3.1 Heuristics 
To cut down on the amount of duplication in the general case, a 
simple cost function can be used to determine the choice of merge 
node M, i.e., among possible nodes, M is chosen to be the node 
with minimum cost. One such cost function is the size of the 
subgraph below M. An alternative is to minimize the number of 
FORALL nodes in the subgraph below M whose merge succes- 
sors do not satisfy CC. We expect these simple heuristics to work 
well in practice since we do not anticipate that many levels of 
nodes will fail to satisfy CC. Much as in the manner described ablove, CC’ can be shown to be 
both a necessary and sufficient condition on the exist.ence of a 
concise corresponding cfg for a given PPG, with additional order 
specified. 
5.0 NP-Completeness of a Closely Related 
Problem 
The problem that we would like to show is NP-Complete is the 
following. Given a PPG PG containing nodes that do not satisfy 
CC, determine a set of nodes to duplicate so that CC is satisfied 
when they are duplicated and the resulting cfg has minimal size. 
This is equivalent to the problem ‘of determining for the same 
PPG, a set of nodes for which the minimal number of guards can 
be inserted to satisfy CC. While wme believe that this problem is 
NP-Complete, we have been able to prove the following case, 
which is a constrained version of the general problem, although 
stronger in limiting the degree of the graph. 
Recall that a splitting S is a set of nodes to be split and edges to 
be “moved”, as defined in Section 4.2.3. Let G = (N, E) be a 
PPG that does not satisfy CC, and let S be a splitting such that 
when S is applied to G, the resulting graph satisfies CC. We use 
the notation S:G = (S:N, S:E) and S:G(i) = (S:N(i),S:E(i)) for 
the resulting “split” graph and for the “split” subgraph reachable 
from i. We let 1 S 1 denote the size of set S. 
Problem Split 
4.2.3.2 Restriction to Bounded Degree Forests Input. Let G(r) = (N(r),E(r)) be a. rooted PPG with root r and 
Consider the subgraph of a PPG PG generated by all FORALL 
nodes and their merge node successors which fail to satisfy CC. 
Suppose that this subgraph is a forest F of trees, each of bounded 
degree. For this case we can construct a minimal size PPG PG’. with all nodes having in-degree and out-degree bounded above 
by 2. In addition, there are some “marked” nodes and bounds B 
and K. If node j E N is marked, then for any splitting S we have 
1 S:No’) 1 < B. 
Problem: Find a splitting S so that S:G(r) satisfies CC, 
Lemma 3. Given PPG PG. Suppose the subgraph of PG 
generated by all FORALL nodes and their merge node successors 
which do not satisfy CC is a bounded degree forest F of trees. 
Then PG can be transformed to a minimal size PPG satisfying 
CC by the splitting transformation. 1 S:N(j) ) < I3 for j marked, and ) S:N(r) 1 is less than K. 
Theorem 2. Problem Split is NP-complete. 
Proof. Clearly, Problem Split is in NP, since we can simply guess 
a splitting S and then verify that condition CC is satisfied, 
1 S:N(j) ) < B for j marked, and I S:N(r) I < K. To show that 
the problem is NP-complete, we reduce 3-SAT to Problem Split. 
Proof Sketch: The appropriate choice in applying the transfor- 
mation at a given FORALL node is made by trying all possible 
orderings of the duplications and choosing the ordering which 
results in the smallest number of duplications overall for that 
node. (Note that the order in which the duplications are 
performed might make a difference in the final result). n 
4.3 Incorporating Data Dependence 
We finally consider the problem of generating a corresponding 
cfg for a PPG when some partial order on successors of a 
FORALL node, such as data dependence [lo], has been 
specified. In this case, it can be shown that CC can be extended 
to a condition CC’ as follows: 
Given a FORALL node A, and a partial order on the successors 
of A, 
There is an order of merge node successors of A, say 
Ml . . . Mk, which is consistent with the given partial 
order, such that 
(CC’) C(M1) 5 C(M2) E . . . C C(Mk) PROBLEM ~-SAT. Given a Boolean expression E in conjunctive 
normal form with no more than three variables in a clause, is there 
a truth assignment to the variables such that E evaluates to 
“true”? 
Given an instance of Problem 3-SAT we construct an instance 
of Problem Split, and show that there is a solution to the instance 
of Problem Split if and only if there is an assignment of values to 
the variables of S-SAT such that the expression evaluates to true. 
Let E be an instance of 3-SAT with variables x,, h, . . , x, and 
clauses C,, C,, . . . , C, . For ease of exposition we assume that 
n = 2.~ and m = 2:. This is not a necessary assumption; however, 
one can easily pad the expression with dummy clauses and 
variables at the cost of increasing the construction by at most a 
factor of 2. 
The intuition behind the proof is that we encode each variable as 
a subgraph such at least one of each pair of leaves that share a 
common FORALL parent has to be split. We encode “true” by 
splitting all the even-numbered leaves and “false” by splitting all 
the odd-numbered leaves. We encode each clause by a rooted 
subgraph that has leaves from the variable subgraphs that 
correspond to the variables in th.e clause. The roots of the 
subgraphs corresponding to the clause nodes are the marked 
588 
nodes, and the value of B is the number of nodes in the subgraph 
after splitting so that all the variables evaluate to “false”. 
In this proof we use the inverse notion of level from that used in 
lemma 1, nameIy the level of a node is the length of the longest 
path from it to a leaf. (All leaves are at level 0, and the root is 
at the highest level). For each x, we construct a subgraph 
G(P,,) = (N(P,,), E(P,,)) having z + 3 levels. The nodes at level 
0 are all merge nodes, the nodes at level 1 are all FORALL nodes, 
the nodes at levels 2 are all predicate nodes, the nodes at levels 
3,4, I.. , t + 2 are all FORALL nodes, and the root node, 
denoted P, is a predicate node. 
Level j contains ~J+~-J nodes for 1 5 j 5 z + 3 and level 0 
contains 2+2 nodes. Hence, 
1 N(P XI ) ( = 2+* + 2’+* + p+’ + * * * + 2’ + 20 = 
p+3 + p+* - 1 = 12m - 1 , where m = 2. Let Q, be the number 
of nodes at level j, and number the level j nodes 
0, 1, . . . , Q, - 1. If k is a level 1 node, then k is made adjacent 
to nodes k and k + 1 mod 2z+2 at level 0. If k is a level j node, for 
j 2 2, then k is made adjacent to nodes k and k + Q, at level 
j - 1. Note that all non-leaf nodes have out-degree two and all 
nodes except for the the root and the leaves have in-degree one. 
To encode the clauses, we first partition the level 1 nodes of each 
G(P.J into adjacent groups of 4, with nodes 0, 1, 2, 3 in group 1, 
nodes 4, 5, 6, 7 in group 2, and so on. Within each group the 
nodes are numbered base 4, i.e. they are numbered 0, 1,2, 3. 
Suppose clause C, = (x,, X,, xk). Then we create nodes as, &yq 
with (Ye @,,y, respectively) adjacent to group 4 of the level 1 nodes 
in G(P,) (G(P,), G(P+) respectively). If the literal occurs in the 
true form, then the level 1 nodes that (~~(&y~ respectively) is 
adjacent to are nodes 0 and 1. If the literal occurs in the false 
form (i.e. X,), then the corresponding level 1 nodes are 1 and 2. 
The nodes (Ye and & are made descendents of node S,, which, 
together with node yq, is made a descendent of node lYr, Note that 
(Y?, /3, and y, must all be predicate nodes (by the definition of a 
PPG); 6, and Tu are FORALL nodes. This process is repeated for 
each clause C,. 
The final portion of the construction involves linking the various 
components together into a (in- and out-) degree two rooted 
graph. This is done using a binary tree construction, with the 
leaves of the binary tree being the rr nodes and the interior nodes 
being FORALL nodes. A separate but similar binary tree 
construction is performed on the variable nodes with the leaves 
being the Pxi nodes and the interior nodes being FORALL nodes. 
The root of the final graph is a predicate P(?oot), which has the 
roots of the two binary trees as children. Call this graph 
G ran, = (N,,, E,“,) This gives 
IN,,,,1 =n(12m-l)+n/2+n/4+...+2+1 
+5m+m/2+m/4+...+2+1+1 
= n(12m - 1) + n - 1 + Sm + m = 12mn + 6m - 1 . 
After splitting the minimum cardinality is 
1 N,,,, 1 + n2’+’ = 14mn + 6m - 1 . To complete the 
construction, we set B = 23 and z = 14mn + 6m. 
In the proof we must show that G,,, is a PPG and that a splitting 
that satisfies the conditions of Problem Split exists if and only if 
the corresponding instance of 3-SAT is satisfiable. The proof 
follows from the following lemmas. 
Lemma 4. G,,, is a PPG. 
Proof. We first argue that G(P,) is a PPG. The subgraph 
consisting of levels 1 through z + 2 forms a binary tree. (Only 
level 0 nodes have more than one parent, i.e. are merge nodes). 
Consequently, there is a unique path from P, to each level 1 node, 
with all even (odd, respectively) numbered level 1 nodes being descendents of node 0 (1 respectively) at level z + 2. In addition, 
there is a unique distinct path from every level 0 node to each of 
the two level z + 2 nodes. Therefore, only level 0 nodes have 
multiple paths to any node in G(PJ, and that node is P,! , a 
predicate. 
It is straightforward to verify that Tr is a PPG, since ay, /3,, and yv 
are all predicate nodes and all have distinct descendents. Finally, 
the binary tree construction for linking the components guaran- 
tees that only P(ruot) has more than one distinct path to any other 
node. n 
Lemma 5. Condition CC is satisfied (ignoring the constraints of 
the marked nodes) if for each G(p,) either all the even numbered 
level 0 nodes or all the odd numbered level 0 nodes are split. 
Proof. We shall prove lemma 2 for G(P,,); the proof generalizes 
to each G(PJ. Recall that the level 2 nodes in p,, are all predicate 
nodes. Call these nodes P(O), P(l), . , P(2’+’ - 1). To simplify 
the notation we write j + 1 or j - 1, with the understanding that 
we really mean j + 1 or j - 1 mod 2;+*. P(j) is adjacent to level 
1 nodes j andj + 2”‘; level 1 node j is adjacent to level 0 nodes 
j and j + 1. Assume that the edge from P(j) to level 1 node j is 
the “true” edge and that the edge from P(j) to level 1 node 
j + CP+’ is the “false” edge. Let B(j) be the Boolean expression 
associated with leaf node j. Then, considering only the predicates 
in CV’J, we have B(j) = PO’- 1)VPo’) for 
0 <j < 21+‘, B(j) = P(j - 2+’ -1)VPfJ - 2-+I) for 
2z+’ < j < 2+2, B(0) = P(2:+’ - l)VP(O) , and 
B(2”‘) = P(2z+1 -l)VP(O). Clearly, any two sibling leaf nodes 
do not satisfy condition CC. (We have omitted P,, from the 
Boolean expressions above because each leaf node has P,,Vp,,). 
Now suppose that we split the odd numbered leaf nodes of 
G(P,,) , and call the nodes after splitting 0, 1, l’, 2, 3, 3’ . . The 
splitting can be done such that for k odd B(k) = P(k - 1) and 
B(k’) = P(k). If instead we split the even numbered nodes of 
G(PJ, then we get nodes 0, 0’, 1, 2, 2’ . . As above the splitting 
can be done such that for k even B(k) = P(k - 1) and 
B(k’) = P(k). In both cases if k is a node that is split and we pair 
node k with node k - 1 and node k’ with node k + 1, then the 
new sibling pairs satisfy condition CC. 
Now we must show that the introduction of the predicates from 
the clause subgraphs does not invalidate the splitting. Suppose 
that (Y, is adjacent to predicate nodes 0 and 1 in G(P,,). Using the 
same conventions as above, this implies that 
B(0) = P(o)vP(2’+‘)vcll B( 1) = P(O)VP( l)V&,VZ,. 
B(2) = P(l)VP(2)VZ, .‘I If B(1) is the node that is split, then 
setting B(1) = P(O)Va, and B(1)’ = P(l)VZ, allows condition 
CC to be satisfied. If instead B(0) and B(2) are the nodes that 
are split, then B(0)’ = P(O)Va,,B(2) = P(l)&?, allows condition 
CC to be satisfied (since in this case both B(0) and B(2) imply 
B(1). n 
Lemma 6. 1 S:N(r,) I < 23 if an only if at least one of the 
variables in C, evaluates to “true”. 
Proof. Each variable that evaluates to “false” results in an 
additional node being included in S:N(T,) because the “split” 
node is in the middle of the three level 0 nodes that are reachable 
from r4. n 
Lemma 7. I S:N,,, I < 14mn Y 6m if an only if each S:G(eJ has 
a consistent splitting. 
Proof. The proof follows from a simple counting argument. n 
The proof of the theorem now follows from the observation that 
a consistent splitting is equivalent to a consistent assignment of 
truth values to variables. 
589 
6.0 Conclusion 
In this paper we presented a necessary and sufficient condition 
for determining if extra guard variables or duplicate code must 
be inserted when generating a correct sequential program from a 
given, well-structured parallel program. We have also presented 
an algorithm for generating sequential code from parallel code 
which inserts guard variables (or duplicate code) only when 
determined by our necessary and sufficient condition. Given the 
choice to insert duplicate code, where an exponential blow-up in 
size may result, some simple heuristics to limit such blow-up were 
suggested. It was also shown that in a restricted case, the minimal 
size sequential code can be efficiently generated. A problem 
closely related to finding the minimal size sequential code (or 
equivalently, the minimum number of guards to be inserted) was 
shown to be NP-Complete. 
REFERENCES 
1. 
2. 
3. 
4. 
5. A.V. Aho, R. Sethi, and J.D. Ullman. Compilers: Princi- 
ples, Techniques, and Tools. Addison-Wesley, 1986. 
A. V. Aho and J. D. Ullman. Principles of Compiler 
Design. Addison-Wesley, 1977. 
J. R. Allen, Ken Kennedy, Carrie Porterfield, and Joe 
Warren. Conversion of Control Dependence to Data 
Dependence. Conf. Rec. Tenth ACM Symposium on 
Principles of Programming Languages, 1983. 
Mario R. Barbacci. A Comparison of Register Transfer 
Languages for Describing Computers and Digital 
Systems. IEEE Transactions on Computers, 
C-24:137-150, February 1975. 
J. Ferrante and M. E. Mace. On Linearizing Parallel 
Code. Conf. Rec. Twelfth ACM Symp. on Principles of 
Programming Languages, pages 179-189, New Orleans, 
LA, January 1985. Many open problems remain in this work. We conjecture that the 
problem of finding the minimal size sequential code (or 
equivalently, the minimum number of guards to be inserted) is in 
general NP-Complete. We do not know whether this also holds 
for the case of bounded degree PPG’s. There may be other 
restricted cases of PPG’s where the :minimal size sequential code 
can easily be computed. In addition, we do not know how well 
our code generation algorithm will work in practice. 
Acknowledgement: We would like to thank Dave Alpern for 
several helpful and lively technical discussions, and David Bowen 
for an optimal algorithm for a special case not included here. 
We would also like to thank Cathy MC Carthy for doing the 
figures for this paper. 
6. 
7. 
8. 
9. 
10. 
11. J. Ferrante and K. Ottenstein. A Program Form Based 
On Data Dependency In Predicate Regions. Conf. Rec. 
Tenth ACM Symposium on Principles of Programming 
Languages, pages 217-23 1, Austin, TX, January 1983. 
J. Ferrante, K. Ottenstein, and J. Warren. The Program 
Dependence Graph and its Use in Optimization. ACM 
Transactions on Programming Languages and Systems, 
pages 319-349, July 1987. 
Susan L. Graham and Mark Wegman. A Fast and Usually 
Linear Algorithm for Global Flow Analysis. Journal of 
the Association for Computing Machinery, 23( 1): 172-202, 
January 1976. 
Susan Horwitz, Jan Prins, and Thomas Reps. Integrating 
Non-Interfering Versions of Programs. Conf Rec. 
Fifteenth ACM Symposium on Principles of Programming 
Languages, pages 133-145, January 1987. 
D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and 
M. Wolfe. Dependence Gra.phs and Compiler Optimiza- 
tions. Conference Record (of 8th ACM Symposium on 
Principles of Programming Languages, 198 1. 
Karl J. Ottenstein. Private Communication, 
1987. Sequential Code Generation for the PDG. 
590 
7.0 Appendix 
Generating Control Flow Graphs With Replication 
procedure gendup (A) returns (header node of control flow DAG built, with replication allowed) 
if A is a statement 
then generate node A; return(A); 
if A is a predicate 
then 
generate node A; 
for each successor S of A with label L 
make 
return(A); - edge(A, gendup(S), L) 
if A is a FORALL node 
then 
Let A, . . . A, = successors of A; 
target(gendup(A,), A,’ = gendup(A,)); 
target((A,‘, A,’ = gendup(A,)); 
target(A,-,‘, A,,,’ = gendup(A,)); 
return(A,‘) 
Auxiliary Procedures 
procedure make - edge (A&L) creates an edge from node A to node B labelled L. 
procedure target(X,Y) targets all open branches of the subgraph starting at X with Y. 
Notes on the procedure gendup 
1. Note that the procedure will be called multiple times for a merge node. 
Replication will occur for every non-tree edge in the original DAG. 
The procedure will not always produce a tree, however, 
because of the effect of the procedure target. 
2. Note that in generating the graph for the successors of a 
FORALL node, any order can be used for the successors. 
(If data dependence is taken into account, this will not be true.) 
3. To prove this procedure is correct, we have to show that the conditions 
under which any node in the original control flow DAG is executed are identical 
to the conditions under which it would be executed in the result. 
4. To turn this into a generation routine for general graphs, we have to identify 
strongly connected regions first, and in case A is the header of such a region, 
add appropriate node to the end of the successor list of A (in this case, A will 
alwavs be a FORALL bv construction). 
591 
Generating Control Flow Graphs Without Replication 
procedure gen (A) returns (header node of subgraph built) 
if A is a statement 
then generate node A ; return(A); 
if A is a predicate 
then 
generate node A; 
for each successor S of A with label L 
if S is not a merge node 
then make edge(A, gen(S), L) 
else make <dge(A, gen({ S 1, L) 
return(A); - 
if A is a FORALL node 
then 
Al,...,Am = the non-merge successors of A; 
Ml ,...,Mj = the merge successors of A; 
target(gen(Al), A2’ = gen(A2)) 1; 
target(A2’, A3’ = gen(A3)); 
target(A’m - 1, Am’ = gen(Am)); 
target(Am’, gen( [M, M,]); 
retum(A1’) if it exists, (Ml’) otherwise. 
if A is a set of merge nodes 
then 
if code for A has already been generated, 
then return( where M is the first element of A in order. 
else 
Let S be the set of nodes related in the order graph to A; 
Let Tl ,...,Tk = order(S); 
target(gen(Tl), T2’ = gen(T2)); 
target(T2’, T3’ = gen(T3)); 
target(Tk - l’, Tk’ = gen(Tk)); 
return(Ti’), where Ti is the first element of A in S. 
end gen. 
Auxiliary Procedures 
procedure make - edge (A,B,L) creates an edge from node A to node B labelled L. 
procedure target(X,Y) targets all open branches of the subgraph starting at X with Y. 
procedure order(X1, X2,...,Xj) returns the input merge nodes in order determined as in section 4.2.1.1. 
procedure order by forward 
forward data de&d&es. - dd(S) returns an ordered list of the elements in S, ordered by 
Note: In order to handle recursive calls to gen, a global table is created to store 
for each merge node M, the head M’ of the subgraph built for M. 
This is entered in the table as soon as the first node in the subgraph is 
created by any call to gen. 
592 

0
Algorithms Sequential and Parallel—A Unified Approach, Second Edition
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Algorithms Sequential and Parallel—A Unified
Approach, Second Edition
Russ Miller
Laurence Boxer
© 2005 Career & Professional Group, a division of Thomson Learning Inc.
Published by Charles River Media, Inc., an imprint of Thomson Learning, Inc.
All rights reserved.
The first edition of this book was previously published by: Pearson Education, Inc.
No part of this publication may be reproduced in any way, stored in a retrieval system of
any type, or transmitted by any means or media, electronic or mechanical, including, but
not limited to, photocopy, recording, or scanning, without prior permission in writing from
the publisher.
Editor:  David Pallai
Cover Design:  Tyler Creative
CHARLES RIVER MEDIA, INC.
10 Downer Avenue
Hingham, Massachusetts 02043
781-740-0400
781-740-8816 (FAX)
info@charlesriver.com
www.charlesriver.com
Russ Miller and Laurence Boxer. Algorithms Sequential and Parallel: A Unified Approach,
Second Edition.
ISBN: 1-58450-412-9
All brand names and product names mentioned in this book are trademarks or service
0
Algorithms Sequential and Parallel—A Unified Approach, Second Edition
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Algorithms Sequential and Parallel—A Unified
Approach, Second Edition
Russ Miller
Laurence Boxer
© 2005 Career & Professional Group, a division of Thomson Learning Inc.
Published by Charles River Media, Inc., an imprint of Thomson Learning, Inc.
All rights reserved.
The first edition of this book was previously published by: Pearson Education, Inc.
No part of this publication may be reproduced in any way, stored in a retrieval system of
any type, or transmitted by any means or media, electronic or mechanical, including, but
not limited to, photocopy, recording, or scanning, without prior permission in writing from
the publisher.
Editor:  David Pallai
Cover Design:  Tyler Creative
CHARLES RIVER MEDIA, INC.
10 Downer Avenue
Hingham, Massachusetts 02043
781-740-0400
781-740-8816 (FAX)
info@charlesriver.com
www.charlesriver.com
Russ Miller and Laurence Boxer. Algorithms Sequential and Parallel: A Unified Approach,
Second Edition.
ISBN: 1-58450-412-9
All brand names and product names mentioned in this book are trademarks or service

marks of their respective companies. Any omission or misuse (of any kind) of service
marks or trademarks should not be regarded as intent to infringe on the property of
others. The publisher recognizes and respects all marks used by companies,
manufacturers, and developers as a means to distinguish their products.
Library of Congress Cataloging-in-Publication Data
Miller, Russ.
Algorithms sequential and parallel: a unified approach / Russ Miller and Laurence
Boxer.— 2nd ed.
p. cm.
Includes bibliographical references and index.
978-1-58450-412-2
ISBN 1-58450-412-9 (hardcover: alk. paper)
1. Computer algorithms. 2. Computer programming. I. Boxer, Laurence. II. Title.
QA76.9.A43M55 2005
005.1—dc22
2005010052
06 7 6 5 4 3 2
CHARLES RIVER MEDIA titles are available for site license or bulk purchase by
institutions, user groups, corporations, etc. For additional information, please contact the
Special Sales Department at 781-740-0400.
To my wife, Celeste, and my children, Brian, Amanda, and Melissa.
—Russ Miller
To my wife, Linda; my daughter and son-in-law, Robin and Mark Waldman; and my
son, Matthew.
—Laurence Boxer
TeamUnknown Release
0
Preface
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Preface
Overview
A major thrust of computer science is the design, analysis, implementation, and scientific
evaluation of algorithms to solve critical problems. In addition, new challenges are being
offered in the field of computational science and engineering, an emerging discipline that
unites computer science and mathematics with disciplinary expertise in biology,
chemistry, physics, and other applied scientific and engineering fields. Computational
science and engineering is often referred to as the "third science," complementing both
theoretical and laboratory science. These multidisciplinary efforts typically require
efficient algorithms that run on high-performance (typically parallel) computers in order to
generate the necessary computer models and simulations.
With advances in computational science and engineering, parallel computing continues
to merge into the mainstream of computing. It is therefore critical that students and
scientists understand the application and analysis of algorithmic paradigms to both the
(traditional) sequential model of computing and to a variety of parallel models.
Many computer science departments offer courses in "Analysis of Algorithms,"
"Algorithms," "An Introduction to Algorithms," or "Data Structures and Their Algorithms" at
the junior or senior level. In addition, a course in "Analysis of Algorithms" is required of
most graduate students pursuing a degree in computer science. Throughout the 1980s,
the vast majority of these course offerings focused on algorithms for sequential (von
Neumann) computers. In fact, not until the late-1980s did courses covering an
introduction to parallel algorithms begin to appear in research-oriented departments.
Furthermore, these courses in parallel algorithms were typically presented to advanced
graduate students. However, by the early 1990s, courses in parallel computing began to
emerge at the undergraduate level, especially at progressive four-year colleges.
It is interesting to note that throughout much of the 1990s, traditional algorithms-based
courses changed very little. Gradually, such courses began to incorporate a component

of parallel algorithms, typically one to three weeks near the end of the semester. During
the later part of the 1990s, however, it was not uncommon to find algorithms courses that
contained as much as 1/3 of the material devoted to parallel algorithms.
In this book, we take a very different approach to an algorithms-based course. Parallel
computing has moved into the mainstream, with clusters of commodity-off-the-shelf
(COTS) machines dominating the list of top supercomputers in the world
(www.top500.org ), and smaller versions of such machines being exploited in many
research laboratories. Therefore, the time is right to teach a fundamental course in
algorithms that covers paradigms for both sequential and parallel models.
The approach we take in this book is to integrate the presentation of sequential and
parallel algorithms. Specifically, we employ a philosophy of presenting a paradigm, such
as divide-and-conquer, and then discussing implementation issues for both sequential
and parallel models. Due to the fact that we present design and analysis of paradigms for
sequential and parallel models, the reader might notice that the number of paradigms we
can treat within a semester is limited when compared to a traditional sequential
algorithms text.
This book has been used successfully at a wide variety of colleges and universities.
Prerequisites: We assume a basic knowledge of data structures and mathematical
maturity. The reader should be comfortable with notions of a stack, queue, list, and
binary tree, at a level that is typically taught in a CS2 course. The reader should also be
familiar with fundamentals of discrete mathematics and Calculus. Specifically, the reader
should be comfortable with limits, summations, and integrals.
TeamUnknown Release
0
Preface
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Overview of Chapters
Background material for the course is presented in Chapters 1 , 2, and 3. Chapter 1
introduces the concept of asymptotic analysis. While the reader might have seen some of
this material in a course on data structures, we present this material in a fair amount of
detail. The reader who is uncomfortable with some of the fundamental material from a
Freshman-level Calculus sequence might want to brush up on notions such as limits,
summations and integrals, and derivatives, as they naturally arise in the presentation and
application of asymptotic analysis. Chapter 2  focuses on fundamentals of induction and
recursion. While many students have seen this material in previous courses in computer
science and/or mathematics, we have found it important to review this material briefly
and to provide the students with a reference for performing the necessary review. In
Chapter 3 , we present the Master Method, a very useful cookbook-type of system for
evaluating recurrence equations that are common in an algorithms-based setting.
Chapter 4  presents an overview of combinational circuits and sorting networks. This work
is used to motivate the natural use of parallel models and to demonstrate the blending of
architectural and algorithmic approaches. In Chapter 5 , we introduce fundamental
models of computation, including the RAM (a formal sequential architecture) and a
variety of parallel models of computation. The parallel models introduced include the
PRAM, mesh, hypercube, and the Coarse-Grained Multicomputer, to name a few. In
addition, Chapter 5  introduces terminology such as shared-memory and distributed-
memory.
The focus of Chapter 6  is the important problem of matrix multiplication, which is
considered for a variety of models of computation. In Chapter 7 , we introduce the parallel
prefix operation. This is a very powerful operation with a wide variety of applications. We
discuss implementations and analysis for a number of the models presented in Chapter 5
and give sample applications. In Chapter 8 , we introduce pointer jumping techniques and
show how some list-based algorithms can be efficiently implemented in parallel.
In Chapter 9 , we introduce the powerful divide-and-conquer paradigm. We discuss
applications of divide-and-conquer to problems involving data movement, including
sorting, concurrent reads/writes, and so forth. Algorithms and their analysis are presented

for a variety of models.
Chapters 10  and 11 focus on two important application areas, namely, Computational
Geometry and Image Processing. In these chapters, we focus on interesting problems
chosen from these important domains as a way of solidifying the approach of this book in
terms of developing machine independent solution strategies, which can then be tailored
for specific models, as required.
Chapter 12  focuses on fundamental graph theoretic problems. Initially, we present
standard traversal techniques, including breadth-first search, depth-first search, and
pointer jumping. We then discuss fundamental problems, including tree contraction and
transitive closure. Finally, we couple these techniques with greedy algorithms to solve
problems, such as labeling the connected components of a graph, determining a minimal
spanning forest of a graph, and problems involving shortest or minimal-weight paths in a
graph.
Chapter 13  is an optional chapter concerned with some fundamental numerical
problems. The focus of the chapter is on sequential algorithms for polynomial evaluation
and approximations of definite integrals.
TeamUnknown Release
0
Preface
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Recommended Use
This book has been successfully deployed in both elective and required courses, with
students typically ranging from juniors (3rd-year undergraduates) to 2nd-year graduates.
A student in a course using this book need not be advanced in a mathematical sense,
but should have a basic, fundamental, background.
TeamUnknown Release

0
Preface
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Correspondence
Please feel free to contact the authors directly with any comments or criticisms
(constructive or otherwise) of this book. Russ Miller may be reached at
miller@buffalo.edu  and Laurence Boxer may be reached at boxer@niagara.edu . In
addition, a Web site for the book can be found from
http://www.cse.buffalo.edu/pub/WWW/faculty/miller/research.htm . This Web site contains
information related to the book, including pointers to education-based pages, relevant
parallel computing links, and errata.
TeamUnknown Release

0
Preface
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Acknowledgments
The authors would like to thank several anonymous reviewers for providing insightful
comments, which have been used to improve the presentation of this book. We would
like to thank the students at SUNY-Buffalo who used early drafts of this book in their
classes and provided valuable feedback. We would like to thank Ken Smith, a member of
the technical support staff at SUNY-Buffalo, for providing assistance with Wintel support.
We would also like to thank our families for providing us the support necessary to
complete this time-consuming project.
Russ Miller & Laurence Boxer, 2005
TeamUnknown Release

0
Chapter 1 - Asymptotic Analysis
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 1: Asymptotic Analysis
Overview
We live in a digital-data-driven society that relies increasingly on simulation and
modeling for discovery. Data is increasing at an astonishing rate, typically two to three
times the rate of increase of processing power and network bandwidth. Thus, to
compete in a knowledge-based economy, students must learn to collect, organize,
maintain, analyze, and visualize data efficiently and effectively.
A comprehensive study of algorithms includes the design, analysis, implementation,
and experimental evaluation of algorithms that solve important problems. These
include enabling problems, such as sorting, searching, and transferring data; as well
as applications-oriented problems, such as retrieving a reservation record, forecasting
the weather, or determining the positions of atoms in a molecule to improve rational
drug design.
In this chapter, we introduce some basic tools and techniques that are required to
evaluate effectively both a theoretical and an experimental analysis of algorithms. It is
important to realize that without analysis, it is often difficult to justify the choice of one
algorithm over another or to justify the need for developing a new algorithm.
Therefore, a critical aspect of most advanced data structures or algorithms courses is
the development of techniques for estimating the resources (running time, disk space,
memory, and so forth) required for a given algorithm. As an aside, we should point out
that a course covering proofs of correctness for algorithms is also critical, because
having fast algorithms that produce incorrect results is not desirable. However, for
pragmatic reasons, nontrivial proofs of correctness are not covered in this text.
Throughout this book, we will focus on resources associated with a given algorithm.
Specifically, we will be concerned with quantities that include the number of
processors, the size of the memory, and the running time required of an algorithm. A

comparison of such quantities will allow for a reasonable comparison between
algorithms, typically resulting in an informed choice of an algorithm for a target
application. For example, such analyses will allow us to make a more informed
decision on which sorting algorithm to use on a sequential machine, given data with
certain properties that are maintained in certain data structures. We should point out
that when computing solutions to numerical problems, one must often consider the
quality of the solution. Although this topic is critical, we believe it is covered in a more
comprehensive fashion in "Numerical Methods" or "Computational Science" courses
than is possible in a course on algorithms. In fact, most of the algorithms we consider
in this book can be viewed as "nonnumerical" in nature.
In practice, it often turns out that we are more concerned with time than with memory.
This statement may surprise students thinking of relatively small homework projects that,
once freed of infinite loops, begin printing results almost immediately. However, many
important applications require massive processing of large data sets, requiring hours or
even days of CPU time. Examples of these applications are found in areas such as
molecular modeling, weather forecasting, image analysis, neural network training, and
simulation. Aside from the dollar cost of computer time, human impatience or serious
deadlines can limit the use of such applications. For example, it helps to have a weather
forecast only if it is made available in advance of the forecast period. By contrast, it is not
uncommon to be able to devise algorithms and their associated data structures such that
the memory requirements are quite reasonable, often no more than a small multiple of
the size of the data set being processed.
In this chapter, we develop mathematical tools for the analysis of resources required by a
computer algorithm. Because time is more often the subject of our analysis than memory,
we will use time-related terminology; however, the same tools can naturally be applied to
the analysis of memory requirements or error tolerance.
TeamUnknown Release
0
Chapter 1 - Asymptotic Analysis
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Notation and Terminology
In this section, we introduce some notation and terminology that will be used throughout
the text. We make every effort to adhere to traditional notation and standard terminology.
In general, we use the positive integer n to denote the size of the data set processed by
an algorithm. We can process an array of n entries, for example, or a linked list, tree, or
graph of n nodes. We will use T(n) to represent the running time of an algorithm
operating on a data set of size n.
An algorithm can be implemented on various hardware/software platforms. We expect
that the same algorithm operating on the same data values will execute faster if
implemented in the assembly language of a supercomputer rather than in an interpreted
language on a personal computer (PC) from, say, the 1980s. Thus, it rarely makes sense
to analyze an algorithm in terms of actual CPU time. Rather, we want our analysis to
reflect the intrinsic efficiency of the algorithm without regard to such factors as the speed
of the hardware/software environment in which the algorithm is to be implemented; we
seek to measure the efficiency of our programming methods, not their actual
implementations.
Thus, the analysis of algorithms generally adheres to the following principles:
Ignore machine-dependent constants:  We will not be concerned with how fast an
individual processor executes a machine instruction.
Look at growth  of T(n) as n 8: Even an inefficient algorithm will often finish its
work in an acceptable time when operating on a small data set. Thus, we are usually
interested in T(n),  the running time of an algorithm, for large n (recall that n is
typically the size of the data input to the algorithm).
Growth Rate:  Because asymptotic analysis  implies that we are interested in the
general behavior of the function as the input parameter gets large (we are interested
in the behavior of T(n) as n 8:), this implies that low-order terms can (and should)
be dropped from the expression. In fact, because we are interested in the growth

rate of the function as n gets large, we should also ignore constant factors when
expressing asymptotic analysis. This is not to say that these terms are irrelevant in
practice, just that they are irrelevant in terms of considering the growth rate of a
function. So, for example, we say that the function 3n3 + 10 n2 + n + 17 grows as n3.
Consider another example: as n gets large, would you prefer to use an algorithm
with running time 95 n2 + 405n + 1997 or one with a running time of 2 n3 + 12? We
hope you chose the former, which has a growth rate of n2, as opposed to the latter,
which has a growth rate of n3. Naturally, though, if n were small, one would  prefer 2
n3 + 12 to 95 n2 + 405 n + 1997. In fact, you should be able to determine the value
of n that is the breakeven point. Figure 1.1  presents an illustration of this situation.
Figure 1.1: An illustration of the growth rate of two functions, f(n) and g(n).
Notice that for large values of n, an algorithm with an asymptotic running time of
f(n) is typically more desirable than an algorithm with an asymptotic running time
of g(n). In this illustration, large is defined as n = n0
Asymptotic Notation
At this point, we introduce some standard notation that is useful in expressing the
asymptotic behavior of a function. Because we often have a function that we wish to
express (more simply) in terms of another function, it is easiest to introduce this
terminology in terms of two functions. Suppose f and g are positive functions of n. Then
ƒ(n) = T(g(n))  (read " f of n is theta  of g of n" if and only if  there exist positive
constants c1, c2, and n0 such that c1g(n)f(n)=c2 g(n) whenever n=n0. See Figure
1.2.
Figure 1.2: An illustration of T notation. f(n) = T(g(n))  because functions f(n) and
g(n) grow at the same rate for all n = n0
f(n) = O(g(n )) (read " f of n is oh of g of n" if and only if  there exist positive constants
c and n0 such that f(n) = cg(n)  whenever n = n0. See Figure 1.3 .
Figure 1.3: An illustration of O notation. f(n) = 0(g(n))  because function f(n) is
bounded from above by g(n) for all n = n0
f(n) = O(g(n)) (read " f of n is omega  of g of n" if and only if  there exist positive
constants c and n0 such that cg(n) = ƒ(n) whenever n = n0. See Figure 1.4 .
Figure 1.4: An illustration of O notation. f(n) = O(g(n))  because function f(n) is
bounded from above by g(n) for all n = n0
f(n) = o(g(n )) (read " f of n is little oh  of g of n" if and only if  for every positive
constant C there is a positive integer n0 such that f(n) < Cg(n)  whenever n = n0. See
Figure 1.5 .
Figure 1.5: An illustration of o notation: f(n) = o(g(n))
ƒ(n)= O(g(n)) (read " f of n is little omega  of g of n" if and only if  for every positive
constant C there is a positive integer n0 such that f(n) > Cg(n)  whenever n = n0- See
Figure 1.6 .
Figure 1.6: An illustration of   notation: f(n) = O(g(n))
Strictly speaking, T, O, O, o, and   are set-valued functions. Therefore, it would be
appropriate to write (3 n2 + 2) e T(n2). In fact, some authors have tried to use this
membership notation "correctly," but it has not caught on. In the literature, it is more
common to see this idea expressed as 3n2 + 2 =  0(n2). This notation is certainly not
correct in the mathematical sense; however, it is the standard. The expression 3n2 + 2 =
T(n2) is read as "3 n squared plus 2 is theta  of n squared." Note that one does not write
T(n2) = 3 n2 + 2.
The set-valued functions T, O, O, o, and  (O are referred to as asymptotic notation.
Recall that we use asymptotic notation to simplify analysis and capture growth rate.
Therefore, we want the simplest  and best function as a representative of each T, O, O,
o, and O O expression. Some examples follow.
Example
Given f(t) = 5 + sin t and g(t) =  1, then 5 + sin t = 0(1) because 4 = 5 + sin t = 6. (See
Figure 1.7 .) Note also that f(t) = O(1) and f(t) = O(1), but the best choice for notation is to
write f(t) = T(1) because T conveys more information than either O or O.
Figure 1.7: Graph of f(t) = 5  + sin t
More Notation
We will often find the floor and ceiling  functions useful. Given a real number x, there is a
unique integer n such that
We say that n is the "floor of x," denoted
In other words, 
 is the largest integer that is less than or equal to x. Similarly,
given a real number x, there is a unique integer n such that
Then n + I is the "ceiling of x," denoted
In other words, 
 is the smallest integer that is greater than or equal to x. For
example, 
Notice for all real numbers x we have
It follows that 
.
Also, in describing the assignment of a value to a variable, we will use either the equal
sign or the left arrow (both are widely used in computer science). That is, either of the
notations
or
will mean "assign the value of right as the new value of left."
Example
Show that
for p > 1, a fixed constant. First, we consider an upper bound on the summation. We
know that
because the summation contains n terms, the largest of which is np. Therefore, we know
that
Next, we consider a lower bound on the sum. Notice that it is easy to derive a trivial lower
bound of O(n), because there are n terms in the summation, the least of which is equal to
1. However, we can derive a more useful, larger, lower bound. Notice that
Notice that in

there are n—para  [n/2] terms, where ([ n/2+1 p) is the smallest term. Therefore, we know
that[
Because 2P+1 is a constant, we have
Therefore, we know that
TeamUnknown Release
0
Chapter 1 - Asymptotic Analysis
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Asymptotic Relationships
Useful relationships exist among T, O, O, o, and  , some of which are given in the
following proposition. The reader might want to try to prove some of these. (An instructor
might want to assign some of these as homework problems.)
Proposition:  Let f and g be positive functions of n. Then
ƒ(n) =O(g(n))  Û (n) = O(ƒ(n)) 1.
ƒ(n) T (g(n))  Û g(n) T (ƒ(n)) 2.
ƒ(n) = (g(n))  Û [ƒ(n) = O(g(n))  and ƒ(n) = O(g(n))] 3.
ƒ(n) = O(g(n))  Û g(n) =  (ƒ(n)) 4.
ƒ(n) = O(g(n)) Û
5.
ƒ(n) =  (g(n)) Û
6.
ƒ(n) = o(g(n) ) Û (ƒ(n)) = O(g(n)) , but the converse is false. 7.
ƒ(n) =  (g(n)) Þ (ƒ(n)) = O(g(n)) , but the converse is false. 8.
ƒ(n) is bounded above and below by positive constants if and only if ƒ(n) = T(1). 9.

9.
Asymptotic Analysis and Limits
To determine the relationship between functions/and g, it is often useful to examine.
The possible outcomes of this relationship, and their implications, follow:
L = 0: This means that g(n) grows at a faster rate than f= O and hence (indeed,
f=o(g) and f  T(g)).
L = 8This means that ƒ(n) grows at a faster rate than g(n), and hence thatf= O(g)
(indeed, f=  (g) and f  T(g)).
L   0 is finite: This means that f(n) and g(n) grow at the same rate, to within a
constant factor, and hence that f=  (g), or equivalently, g = T(f). Notice that this
also means that f= 0(g), g = 0(f), f= Q(g),  and g = O(f).
There is no limit:  In the case where 
 does not exist, this technique
cannot be used to determine the asymptotic relationship between ƒ(n) and g(n).
We now give some examples of how to determine asymptotic relationships based on
taking limits of a quotient.
Example
Let
Then we can show that ƒ(n)=T(g(n))  because
(dividing both numerator and denominator by n2)
Example
If P(n) is a polynomial of degree d, then P(n) = T(nd). The proof is left to the reader as an
exercise.
Example
Compare n100 and 2n. We remind the reader of a useful result.
We have
We can apply L'Hopital's Rule to the numerator and denominator of this limit 100 times.
After this, we have
Therefore, we know that n100 = O(2n and 2n=O(n100). In addition, using some of the
properties previously presented, we know that n100 = o(2n) and 2n = (n100). Further, we
know that n100 T(2n).
At this point, it is reasonable to discuss logarithmic notation and to note that logarithms
play an important role in asymptotic analysis and will be used frequently throughout this
text. As appropriate, we will use fairly standard terminology in referring to logarithms. We
write log ex as ln x, log 2x as lg x, and log 10x as log x.
We now continue with an example that uses logarithms.
Example
Let f(n) = ln n and g(n) = n. Then, by applying L'Hopital's Rule, we have
which evaluates as
Therefore, ln n = O(n).
We remind the reader that log bx = (log 6a)(log ax) for positive a, b, and x with a 1 b.
Therefore, because log b a is a constant, log bx =T(log ax). That is, the base of a logarithm
is irrelevant inside asymptotic notation, except that we assume a,b>1 (so that the
logarithms are positive, because we generally have x > 1 in such contexts).
Summations and Integrals
Because many algorithms involve looping and/or recursion, it is common for the analysis
of an algorithm to include a dependence on some function ƒ(n) that is best expressed as
the sum of simpler functions. For example, it may be that the dominant term in an
analysis of an algorithm can be expressed as ƒ(n) = h(1) + h(2) + … + h(n). When we
consider the worst-case number of comparisons in the InsertionSort routine later in this
chapter, we will find that the total number of comparisons can be computed as ƒ(n) = 1 +
2 + 3 + … + n = n(n + 1)/2 = T(n2).
We first consider the case where the function h(i) is nondecreasing. (Notice that the
worst-case number of comparisons used in InsertionSort, as mentioned previously, uses
the nondecreasing function h(i) = i.) Specifically, let
where h is nondecreasing. (An illustration of this situation is presented in Figure 1.8 ).
Figure 1.8: An illustration of bounding the summation 
 h(i)
by the integral of the nondecreasing function h(t). On the left, we demonstrate how to
use the integral 
 to derive an upper bound on the
summation by aligning the rectangles to the right. Notice that
. On the right, we show how to use the
integral 
 to derive a lower bound on the summation by
aligning the rectangles to the left. Notice that 
.
Therefore, we have 
To evaluate ƒ(n), we can consider summing n unit-width rectangles, where the i th
rectangle has height h(i). In Figure 1.8 , we present these rectangles in two ways to obtain
tight bounds on the asymptotic behavior of the total area of the rectangles (in other
words, the value of f(n)). On the left, we draw the rectangles so that the i th rectangle is
anchored on the left. That is, the left edge of the rectangle with height h(i) is at value i on
the x-axis. In this way, you will notice that each rectangle is below the curve of h(t), where
t takes on values between 1 and n + 1 (assuming 1 is the value of the lower bound and n
is the value of the upper bound in the sum).
Conversely, on the right of Figure 1.8 , we draw the rectangles so that the ith rectangle is
anchored on the right. That is, the right edge of the rectangle with height h(i) is at value i
on the x-axis. This allows us to use the rectangles to bound the area of the curve,
between 0 and n (assuming that 1 is the value of the lower bound and n is the value of
the upper bound) from above. Notice that in Figure 1.8 , we give the relationships of the
area under the curve bounding the rectangles (left)
and the rectangles bounding the area under the curve (right side). In addition, we show
how to combine these relationships to obtain a bound on the summation by related
integrals.
The method of determining asymptotic analysis of a summation by integration is quite
powerful. Next, we give several examples and, in doing so, illustrate a variety of
techniques and review some basic principles of integration.
Example
Find the asymptotic complexity of
First, we consider the integral bounding principles that were given previously. Because
the function h(i) = i is nondecreasing, we can apply the conclusion directly and arrive at
the bound
Evaluating both the left and right sides simultaneously yields
which can be evaluated in a fairly routine fashion, resulting in
Working with the right side of this inequality, we can obtain
Further simplification of the right side can be used to give

for n=1. Therefore,
Because the function
is bounded by a multiple of n2 on both the left and right sides, we can conclude that
Example
Find the asymptotic complexity of
First, it is important to realize that the function 
 is a nonincreasing  function. This
requires an update in the analysis presented for nondecreasing functions. In Figure 1.9 ,
we present a figure that illustrates the behavior of a nonincreasing function over the
interval [a, b].  Notice that with the proper analysis, you should be able to show that
Based on this analysis, we can now attempt to produce an asymptotically tight bound on
the function g(n). First, we consider a lower bound on g(n). Our analysis shows that
Because

we know that g(n) is bounded below by ln n.
Next, we consider an upper bound on g(n). Notice that if we apply the result of our
analysis for a nonincreasing function blindly, we obtain
Unfortunately, this result does not yield a useful upper bound. However, notice that the
cause of this infinite upper bound is evaluation of the integral at the specific point of 0.
This problem can be alleviated by carefully rewriting the equation to avoid the
problematic point. Let's consider the more restricted inequality
Notice that the integral evaluates to ln n. Therefore, if we now add back in the
problematic term, we arrive at
Combining the results of both the upper and lower bounds on g(n), we arrive at
large enough (verify). Therefore,

Figure 1.9: An illustration of bounding the summation 
 for a
nonincreasing function f. For f nonincreasing, we can derive the relationship  b+1a
f(t)dt= 
 f(i)= ba-1 f(t)dt
Example
As our final example of evaluating the asymptotic behavior of a summation by integrals,
we consider the function
for p<0. (We showed earlier that
However, we now show how to obtain this result by another method.) Consider the
derivative of kp. For k > 0, we have
Therefore, the function kp is an increasing function. A quick sketch of an increasing
function ( f is increasing  if u < v Ü f(u) < f(v)), in a setting more general than illustrated
earlier, appears in Figure 1.10 .
Using the analysis associated with Figure 1.10 , we have both
Because n+1<2 n for n = 1,
which, based on asymptotic properties given earlier in this chapter, yields the expected
solution of
Figure 1.10: An increasing function in the [a,b] range. We have
TeamUnknown Release
0
Chapter 1 - Asymptotic Analysis
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
Rules for Analysis of Algorithms
The application of asymptotic analysis is critical to provide an effective means of evaluating both the
running time and space of an algorithm as a function of the size of the input. In this section, we present
fundamental information about the analysis of algorithms and give several algorithms to illustrate the major
points of emphasis.
Fundamental operations execute in T (1) time:  Traditionally it is assumed that "fundamental"
operations require a constant amount of time (that is, a fixed number of computer "clock cycles" to
execute. We assume that the running time of a fundamental operation is bounded by a constant,
irrespective of the data being processed. Such operations include the following:
Arithmetic operations (+, — , , /) as applied to a constant number (typically two) of fixed-size
operands.
Comparison operators (<, = , >, = , =,   ) as applied to two fixed-size operands.
Logical operators (AND, OR, NOT, XOR) as applied to a constant number of fixed-size operands.
Bitwise operations, as applied to a constant number of fixed-size operands.
I/O operations that are used to read or write a constant number of fixed-size data items. Note this
does not include input from a keyboard, mouse, or other human-operated device, because the
user's response time is unpredictable.
Conditional/branch operations.
The evaluation of certain elementary functions. Notice that such functions need to be considered
carefully. For example, when the function sin   is to be evaluated for "moderate-sized" values of

  , it is reasonable to assume that T (1) time is required for each application of the function.
However, for very large values of   , a loop dominating the calculation of sin   might require a
significant number of operations before stabilizing at an accurate approximation. In this case, it
might not be reasonable to assume T (1) time for this operation.
We mention additional fundamental properties.
Suppose the running times of operations A and B are, respectively, 0(ƒ (n)) and 0(g(n))  . In this
case, the performance of A followed by B takes 0(ƒ (n) + g(n))  time. Note that this analysis holds
for T , O , o , and   , as well.
Next, suppose that each application of the body of a loop requires 0(ƒ (n)) time, and the loop
executes its body 0(g(n))  times. The time required to execute the loop (that is, all performances of
its body) is 0(f(ri)g(n))  . A similar property holds for T , O , o , and   .
EXAMPLE (INSERTIONSORT)
As an example, we consider the analysis of InsertionSort  , a simple sorting technique that is introduced in
many first-semester computer science courses. Suppose we are given a set of data arbitrarily distributed in
an array and we want to rearrange the data so that it appears in increasing order. We give pseudocode for
the algorithm and then present an analysis of both its time and space requirements. Note that later in this
book, we compare more advanced algorithms to InsertionSort, and also show how InsertionSort can be
effectively exploited in restricted situations, for example, where the set of data presented to InsertionSort is
such that no item is very far from where it belongs.
Subprogram InsertionSort( X )
Input:  an array X of n entries
Output:  the array X with its entries in ascending order
Local Variables:  indices current, insertPlace
Action:
For current = 2 to n do
   {The first (current-1) entries of X are ordered.
   This is why current is initially set to 2.}
   Search X[1... current-1] to determine the index, denoted as  insertPlace
  e {1,..., current -1}, where X[current] should be inserted.
   Make a copy of X[current].
   Shift the elements X[insertPlace,...,  current-1] down by one position
   into elements X[insertPlace+1,..., current].
   Place the copy of X[current] into its proper position at
     X[insertPlace].
End For
The previous description presents a top-level view of InsertionSort. An example is given in Figure 1.11  . We
observe that the search called for in the first step of the loop can be performed by a straightforward
sequential search that requires O(k) time, where k is the value of current  . The reader should verify that this
requires T (k) time on average. Alternately, an O(logk)  time binary search can be performed, as will be
discussed in the chapter on Induction and Recursion. Thus, the total search time is
Figure 1.11: An example of InsertionSort. It is initially assumed that the first item (4) is in the correct
position. Then the second item (3) is placed into position with respect to all of the items in front of it,
result-ing in (3,4) being properly ordered. The algorithm continues until the last item (2) is placed in its
proper position with respect to the items (1,3,4,5) that are in front of it
time if sequential searches are used, and
time if binary searches are used. Notice that O -notation is used, because both results represent upper
bounds on the search time.
Regardless of which search is used to locate the position that X[current]  should be moved to, notice that on
average, it will require current/  2 movements of data items to make room for X[current]  . In fact, in the worst
case, the insert step always requires X[current]  to be moved to position number 1, requiring current  data
items to be moved. Therefore, the running time of the algorithm is dominated by the data movement, which
is given by
where movement k is 0 in the best case, k - 1 in the worst case, and (k - 1)/2 in the average case. Hence,
the running time of InsertionSort is T (n ) in the best case (when data is already sorted and a sequential
search from ( current  - 1) down to 1 is used), T (n 2 ) in the average (or expected) case, and T (n 2 ) in the
worst case. The reader should verify these results by substituting the appropriate values into the summation
and simplifying the equation. Notice that the average- and worst-case running times are dominated by the
data movement operations.
Finally, notice that T (n ) space is required for the algorithm to store the n data items. More important, the
amount of extra space required for this algorithm is constant, that is, T (1). An insertion routine follows.
Subprogram Insert( X, current, insertPlace)
Insert X[current] into the ordered
subarrary X[1 current-1  ] at position
insertPlace  .
We assume 1 = insertPlace  = current  = n
Local variables: index j , entry-type hold
Action:
If current   insertPlace, then {there's work to do}
    hold = X[current]
   For j = current -1 downto insertPlace, do
     X[j + 1] =  X[j]
   End For
     X[insertPlace] = hold
End If
For completeness, we present an efficient implementation of InsertionSort based on the analysis we have
presented.
Subprogram InsertionSort  (x, n )
{This is a simple version of InsertionSort with sequential search.}
   For i = 2 to n, do
       hold = x[i]
       position = 1
     While hold > x[position], do
      position = position + 1
     End While
     If position < i, then
      For j = i downto position, do
       x[j] = x[j - 1]
      End For
      x[position] = hold
     End If
   End For
End InsertionSort
It is often possible to modify an algorithm designed for one data structure to accommodate a different data
structure. The reader should consider how Insertion-Sort could be adapted to linked lists (see Exercises).
EXAMPLE: BINSORT
Sorting is a fundamental problem in computer science because a major use of computers is to maintain
order in large collections of data. Perhaps for this reason, researchers have developed many algorithms for
sorting. Some of these are considerably faster than others. Yet, sometimes the asymptotically slower
algorithms are useful because, for example, they may be very fast on relatively small data sets or they may
be very fast on sets of data that exhibit certain characteristics. We will present several sorting algorithms in
this book and examine such issues.
In the previous section  , we presented an analysis of InsertionSort. In one of the exercises at the end of this
chapter, we present SelectionSort, a fairly straightforward, useful sorting routine that exhibits the same
worst case T (n 2 ) running time as InsertionSort. Later in the book, we present alternative comparison-
based sorting algorithms that exhibit optimal T (n log n ) worst case running times. In fact, many of you may
already be familiar with the result that states that comparison-based sorting requires O (n log n ) time.
Although O (n log n ) is a lower bound on general comparison-based sorting, one might ask whether or not
it is possible to sort a set of data in o(n log n ) time. In fact, although this is not possible in general, it is
possible given a set of data that is not "arbitrary." An important theme that runs through this book is that
one should attempt to design an o(n log n ) time-sorting algorithm if one knows something about the data
a priori  .
For example, suppose you know that you are required to sort data that is chosen from a restricted set.
Maybe you know that the keys can take on only O(n) distinct values. In this case, one can employ a BinSort
algorithm. BinSort is modeled on the process of placing each member of a collection of numbered items
(such as machine parts) into a correspondingly numbered bin. Alternatively, one might think about sorting a
deck of cards by going through the deck once, tossing all the aces in one pile, all the 2s in another, and so
on. Once you have gone through all the cards and created your 13 bins, you simply need to concatenate
the bins to create the final sorted set. Notice that if you sort more than one deck of cards, you still need only
13 bins. Given one complete deck of cards, each bin will wind up with exactly four cards in it. An example of
Bin-Sort is presented in Figure 1.12  .
Figure 1.12: BinSort applied to an array of 10 items chosen from [1 5]. In (a), the initial array of data is
given. In (b), the set of empty bins are created. In (c), the bins are shown after a complete pass through
the array. In (d), the array is recreated by concatenating the bins
Next, we give a description of BinSort under the assumption that the range of data is the integer values
from 1 to n . It is important to note (in terms of the proof that O (n log n ) comparisons are required to sort
an arbitrary set of data by a comparison-based sort) that BinSort is not a "comparison-based" sorting
algorithm. That is, BinSort does not rely on comparing data items to each other. In fact, the algorithm never
compares two data items.
Subprogram BinSort( X )
Sort the array X via the BinSort algorithm.
We assume entries of X have integer key values 1 n .
Local variables: indices j, s ;
temp  , an array of pointers, each representing a stack
Action:
For   j = 1 to n, do
{make temp[j] an empty stack}
      temp[j] = null
For   j = 1 to n, do
     push(X[j], temp[X[j].key])
s = 1
For   j = 1 to n, do
     while emptyStack(temp[s])
      s   s + 1
     end while
     pop(temp[s], X[j])
End For
An analysis of the algorithm follows. It is easy to see that the first two For loops each require T (n ) time,
after which each element is in one of the n bins.
The initializations of s requires T (1) time. The final For loop requires that every item be examined once,
hence, requires T (n ) time. Hence, the entire algorithm requires T (n ) time. Further, notice that the
algorithm requires T (n ) space to store the items and only T (n ) additional space (for indices and stack
pointers). We observe that the linear amount of additional space requires only a small constant of
proportionality, because the items themselves are placed on the stacks (no copies of the items are ever
made). Therefore, the algorithm is optimal in terms of running time that is, executing faster (asymptotically)
means not examining all of the items, in which case you might miss an item that is out of order and is
efficient in terms of space.
TeamUnknown Release
0
Chapter 1 - Asymptotic Analysis
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Limitations of Asymptotic Analysis
Suppose a given problem has two algorithmic solutions. Further, suppose these
algorithms have the same asymptotic running times and the same asymptotic space
requirements. This situation might make it difficult to choose between the two algorithms,
because the asymptotic analysis provides some guidelines for behavior, but it also hides
high-order constants and low-order terms. In fact, suppose that algorithm A is five times
faster than algorithm B for problems of a given size. Because five is just a constant, this
will be hidden in the O-notation. Similarly, because low-order terms are masked with O-
notation, it may be that one algorithm is superior for small data sets (where the low-order
terms are important) but not for large data sets (where these low-order terms are,
appropriately, masked).
Consider the problem of sorting a set of data, and assume that based on knowledge of
the input, you decide that a general, comparison-based sorting algorithm is required.
Among your choices are algorithms that copy data and algorithms that do not copy data
(for example, sorting can be done via pointer manipulation rather than by copying data).
Suppose, for example, we consider three algorithms whose running times are dominated
by the following steps:
Algorithm A: T(n2) comparisons, T(n2) copying operations
Algorithm B: T(n2) comparisons, T(n) copying operations
Algorithm C: T(n2) comparisons, T(n) pointer manipulation operations
All three algorithms run in T(n2) time, yet we should expect A to be slower than B, and B
to be slower than C. For example, suppose the data being sorted consists of 100-byte
data records. Then, at the machine level, every copying operation (an assignment
statement of the form x   y) can be thought of as a loop of the following form:
For byteNumber  = 1 to 100, do

Therefore, a data-copying operation takes time proportional to the size of the data entity
being copied. Thus, given data entries of significant size (where significant  is machine-
dependent—on some machines this may mean data items larger than 100 bytes,
whereas on other machines this may mean data items larger than 1,000 bytes), we
expect Algorithm A to be slower than Algorithm B, even though the two algorithms have
the same asymptotic running time.
Pointers of 4 bytes (32 bits) can theoretically be used to address 232 bytes (4 Gigabytes)
of memory. A sorting algorithm that uses T(n) pointer manipulations might involve three
to four pointer assignments, hence perhaps 12 to 16 bytes of assignments, per data
movement. Therefore, such an algorithm would typically be more efficient than an
algorithm that copies data, so long as the data items are sufficiently long. Of course, on
real machines, some of these conjectures must be tested experimentally, because
instruction sets and compilers can play a major role in the efficiency of an algorithm.
TeamUnknown Release
0
Chapter 1 - Asymptotic Analysis
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Common Terminology
We conclude this chapter by giving some common terminology that will be used
throughout the text. These terms are fairly standard, appearing in many texts and the
scientific literature.
An algorithm with running time
is said to run in
T(1)
constant time
T(log n)
logarithmic time
O(logk n), k a positive integer
polylogarithmic time
o(log n)
sublogarithmic time
T(n)
linear time
o(n)
sublinear time
T(n2)
quadratic time
O(f(n )), where f(n) is a polynomial
polynomial time
An algorithm is said to run in optimal time  if its running time T(n) = 0(f(n)) is such that any
algorithm that solves the same problem requires O(f(n)) time. It is important to note that
in terms of notions such as optimality  or efficiency , one compares the running time of a
given algorithm  with the lower bound on the running time to solve the problem  being
considered. For example, any algorithm to compute the minimum entry of an unsorted
array of n entries must examine every item in the array (because any item skipped could
be the minimal item). Therefore, any sequential algorithm to solve this problem requires
O(n) time. So, an algorithm that runs in T(n) time is optimal.
Notice that we use the term optimal  to mean asymptotically optimal . An optimal algorithm
need not be the fastest possible algorithm to give a correct solution to its problem, but it
must be within a constant factor of being the fastest possible algorithm to solve the
problem. Proving optimality is often difficult and for many problems optimal running times
are not known. There are, however, problems for which proof of optimality is fairly easy,

some of which will appear in this book.
TeamUnknown Release
0
Chapter 1 - Asymptotic Analysis
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we have introduced fundamental notions and terminology of analysis of
algorithms. We have discussed and given examples of various techniques from algebra
and calculus, including limits, L'Hopital's Rule, summations, and integrals, by which
algorithms are analyzed. We have also discussed the limitations of asymptotic analysis.
TeamUnknown Release

0
Chapter 1 - Asymptotic Analysis
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
The notion of applying asymptotic analysis to algorithms is often credited to Donald E.
Knuth ( www-cs-faculty.Stanford.EDU/~knuth/ . Although it served as the foundation for
part of his seminal series The Art of Computer Programming , Knuth, in fact, traces O-
notation back to a number theory textbook by Bachmann in 1892. The o-notation was
apparently first introduced by Landau in 1909, but the modern use of this notation in
algorithms is attributed to a paper by D.E. Knuth that appeared in 1976 ("Big omicron and
big omega and big theta," ACM SIGACT News , 8(2): 18-23.) Historical developments of
the asymptotic notation in computer science can be found in reviews by D.E. Knuth and
in Algorithmics: Theory and Practice  by Brassard and Bratley (Prentice Hall, 1988). One
of the early books that earned "classic" status was The Design and Analysis of Computer
Algorithms , by A.V. Aho, J.E. Hopcroft, and J.D. Ullman, which was released by Addison-
Wesley in 1974. More recent books that focus on algorithms and their analysis include
Introduction to Algorithms , by T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein
(2nd ed.: MIT Press, Cambridge, MA, 2001), and Computer Algorithms/C++  by E.
Horowitz, S. Sahni, and S. Rajasekaran (Computer Science Press, New York, 1996).
TeamUnknown Release

0
Chapter 1 - Asymptotic Analysis
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
Exercises
Rank the following by growth rate: n, n ½ , log n , log(log n ), log2 n , (1/3)n , 4, (3/2)n , n ! 1.
Prove or disprove each of the following.
f(n ) = O(g(n  )) Þ g(n ) = O(f(n  )) a.
f(n ) + g(n ) = T (max{ ƒ (n), g(n  )}) b.
f(n ) = O([f(n  )]2 ) c.
f(n ) = O{g(n  ) Þ g(n ) = O (f(n )) d.
f(n ) + o(f(n )) = T (f(n )) e.2.
Use O, o , O ,   , and T to describe the relationship between the following pairs of functions:
logk n, n e , where k and e are positive constants a.
n k , c n , where k and c are constants, k > 0, c > 1 b.
2n , 2n/2c.3.
Prove that 17n1/6 = O(n1/5 ) 4.
Prove that 
5.
6.

5.
Given a set of n integer  values in the range of [1, … ,100], give an efficient sequential algorithm to sort these
items. Discuss the time, space, and optimality of your solution.6.
(Total function):  Determine the asymptotic running time of the following algorithm to sum a set of values. Show
that it is optimal.
Function Total( list )
Input:  an array, list , of numeric entries indexed from 1 to n .
Output:  the total of the entries in the array
Local variables:  integer index  , numeric subtotal
Action:
subtotal = 0
For index = 1 to n, do
     subtotal = subtotal + list[index]
Return subtotal
7.
(Selection sort):  Determine the asymptotic running time of the following algorithm, which is used to sort a set
of data. See Figure 1.13  . Determine the total asymptotic space and the additional asymptotic space required.
Subprogram SelectionSort( List )
Input:  array List [1… n ], to be sorted in ascending order according to the key field of the records
Output:  the ordered List
Algorithm:  SelectionSort, as follows
For each position in the List , we
Determine the index corresponding to the entry from the unsorted portion of the List that is a
minimum.a.
Swap the item at the position just determined with the current item.b.
Local variables:  indices ListPosition, SwapPlace
Action:8.
{ListPosition  is only considered for values up to n - 1, because once
the first n - 1 entries have been swapped into their correct positions,
the last item must also be correct.}
For ListPosition = 1 to n - 1:
                                {Determine the index of correct entry
                                for ListPosition and swap the entries.}
     SwapPlace =  MinimumIndex(List,ListPosition);
     Swap(List[SwapPlace], List[ListPosition])
End For
End Sort
Subprogram Swap( A, B )
Input  : Data entities A, B
Output  : The input variables with their values interchanged, for example, if on entry we have A = 3 and B =
5, then at exit we have A = 5 and B = 3.
Local variable:  temp  , of the same type as A and B
Action:
    temp = A; {Backup the entry value of A}
    A = B;      {A gets entry value of B }
    B = temp   {B gets entry value of A }
End Swap
Function MinimumIndex( List, startIndex  )
Input:  List [1… n ], an array of records to be ordered by a key field; startlndex  , the first index considered.
Output:  index of the smallest key entry among those indexed startIndex  … n (the range of indices of the
portion of the List presumed unordered).
Local variables:  indices bestIndexSoFar, at
Action:
    bestIndexSoFar = startIndex;
               {at is used to traverse the rest of the index subrange}
   For at = startIndex + 1 to n, do
     If List[at].key < List[bestIndexSoFar].key
     then bestIndexSoFar = at
   End For
   return bestIndexSoFar
End MinimumIndex
Figure 1.13: An example of SelectionSort. A complete pass is made through the initial set of data to
determine the item that belongs in the front of the list (1). A swap is performed between this minimum
element and the element currently in the front of the list. Next, a pass is made through the remaining four
items to determine the minimum (2) of these elements. This minimum element is swapped with the current
second item (3). The procedure continues until n —1  items have been properly ordered because this forces
all n items to be properly ordered
Earlier in this chapter, we gave an array-based implementation of Insertion-Sort. In this problem, we consider a
linked list-based version of the algorithm.
Subprogram InsertionSort( X )
For every current entry of the list after the first entry:
   Search the sublist of all entries from the first entry to the current entry
   for the proper placement (indexed insertPlace) of the current entry in the
   sublist;
   Insert the current entry into the same sublist at the position insertPlace.
End For
Suppose we implement the InsertionSort algorithm as just described for a linked list data structure.
What is the worst-case running time for a generic iteration of the Search step?a.
What is the worst-case running time for a generic instance of the Insert step?b.
Show that the algorithm has a worst-case running time of T (n 2 ). c.
Although both the array-based and linked-list-based implementations of InsertionSort have worst case
running times of T (n 2 ), in practice, we usually find that the linked-list-based implementation
(assuming the same data, in the same input order) is faster. Why should this be? (Think in terms ofd.9.
10.
entries consisting of large data records.)d.
Array implementations of both InsertionSort and SelectionSort have T (n 2 ) worst case running times. Which is
likely to be faster if we time both in the same hardware/software environment for the same input data? Why?10.
 
TeamUnknown Release
0
Chapter 2 - Induction and Recursion
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 2: Induction and Recursion
In this chapter, we present some fundamental mathematical techniques that are used
throughout the book. Many of these techniques, including recursion and mathematical
induction, are taught in courses such as calculus and discrete mathematics. For some
readers, much of this chapter will serve as a review and will require very little time,
whereas for others, a more careful reading might be in order.
Mathematical induction and the related notion of recursion are useful tools in the
analysis of algorithms. Mathematical induction,  which we will often refer to simply as
induction, \is a technique for proving statements about consecutive integers, roughly,
by inducing  our knowledge of the next case from that of its predecessor. Recursion  is
a technique of designing algorithms in which we divide  a large problem into smaller
subproblems, solve  the subproblems recursively,  and then combine (or stitch
together) the solutions to our subproblems to obtain a solution to the original problem.
One of the critical steps in this process is that of (recursively) dividing a problem into
sub-problems. For example, to solve a given problem P1 by recursion, we might first
divide P1 into two subproblems, P2 and P3, recursively solve these subproblems, and
then stitch together their results to obtain the required result for P1. To solve P2 and
P3, we might divide problem P2 into subproblems P4 and P5, and similarly divide
problem P3 into subproblems P6 and P7. Before stitching together P4 and P5, and
similarly P6 and P7, these problems must first be solved. Therefore, we might
recursively divide problems P4, P5, P6, and P7 into subproblems, recursively solve
them, and so on. This recursive subdivision of problems typically continues until
subproblems have simple/trivial solutions. Thus, recursion resembles induction in that
a recursive algorithm solves a problem by making use of its capability to solve simpler
problems, inducing a solution from the solutions of these simpler problems.
Mathematical Induction

Suppose we have a statement about positive integers, and we want to show that the
statement is always true. Formally, let P(ri) be a predicate,  a statement that is true or
false, depending on its argument n, which we assume to be a positive integer. Suppose
we wish to show P(ri) is always true.
Principle of Mathematical Induction:  Let P(n) be a predicate, where n is an
arbitrary positive integer. Suppose we can accomplish the following two steps:
Show that, P(1) is true. 1.
Show that whenever P(k) is true, we can derive that P(k+ 1) is also true. 2.
If we can achieve these two goals, it follows that P(ri) is true for all positive integers
n.
Why does this work? Suppose we have accomplished the two steps given above.
Roughly speaking (we'll give a mathematically stronger argument next), we know from
step 1 that P(1) is true, and thus by step 2 that P(1 +  1) = P(2)  is true, P(2+ 1) = P(3) is
true, P(3 +  1) = P(4)  is true, and so forth. That is, step 2 allows us to induce the truth of
P(ri) for every positive integer n from the truth of P(1).
The assumption in step 2 that P(k)= true is called the inductive hypothesis,  because it is
typically used to induce the conclusion that the successor statement P(k +  1) is true.
The Principle of Mathematical Induction is stated above as an assertion. Further, we
have also given an informal argument as to its validity. For the sake of mathematical
completeness, we will prove the assertion next. The proof we give of mathematical
induction depends on the following axiom:
Greatest Lower Bound Axiom:  Let X be a nonempty subset of the integers such
that the members of X have a lower bound (in other words, there is a constant C
such that for every x G X, x C ). Then a greatest lower bound for X exists, that is, a
constant C0 such that C0 is a lower bound for the members of X and such that C0 is
greater than any other lower bound for X.
Proof of the Principle of Mathematical Induction:  We argue by contradiction.
Suppose the Principle of Mathematical Induction is false. Then there is a predicate
P(n) on positive integers that yields a counterexample, that is, for which steps 1 and
2 are true and yet, for some positive integer k, P(k)  is false. Let
Then k eS, so S  f It follows from the Greatest Lower Bound Axiom that S has a
greatest lower bound k0e S, a positive integer. That is, k0 is the first value of n such that
P(n) is false. By step 1, P(1) = true,  so k0 > 1. Therefore, k0— 1 is a positive integer.
Notice that by choice of k0, we must have P(k0— 1) = true.  It follows from step 2 of the
Principle of Mathematical Induction that P(k0)= P((k 0- 1) + 1) = true, contrary to the fact
that k0 eS. Because the contradiction results from the assumption that the principle is
false, the proof is established.
TeamUnknown Release
0
Chapter 2 - Induction and Recursion
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Induction Examples
Example
Prove that for all positive integers 
Before we give a proof, we show how you might guess the formula to be proven if it
weren't already given to you. If we let 
 we have
and, writing the terms on the right side of the previous line in reverse order,
Again, note that the current exposition is not a proof, due to the imprecision of the " …"
notation. We add the imprecise "equations" (a) and (b), noting that when we add the fth
term of the right side of (a) to the fth term of the right side of (b), the sum is n + 1 for all
i(the pairs are 1 and n; 2 and n- 1; and so on); because there are n such pairs, this gives
Proof that 
: The equation claims that the sum of the first n positive
integers is the formula on the right side of the equal sign. For n = 1,  the left side of the
asserted equation is

and the right side of the asserted equation is
Thus, for n = 1,  the asserted equation is true, and we have achievedthe first step of an
induction proof.
Suppose the asserted equation is valid for n = k,  for some positive integer k(notice such
an assumption is justified by the previous step). Thus, our inductive hypothesis is
(substituting k for n in the equation to be proved) the equation
Now we want to prove the asserted equation is true for the next term, n = k +  1. That is,
we want to prove that substituting n = k +  1 into the equation to be proved, which gives
yields a true statement.
Consider the left side of the previous equation. We can rewrite the left side as
Substituting from the inductive hypothesis, we conclude
as desired. Thus, our proof is complete.
Example
Prove that n! > 2  for all integers n=4. Notice that you may view this as a statement
about all positive integers, not just those greater than or equal to 4, by observing that the
assertion is equivalent to the statement that for all positive integers j, (j+ 3)! > 27+3. This
observation generalizes easily so that mathematical induction can be viewed as a
technique for proving the truth of predicates
defined for all integers greater than or equal to some fixed integer m. In this generalized
view of induction, the first step of an inductive proof requires showing that^w) = true.  The
proof of our assertion follows.
We must show the assertion to be true for the first case considered, which is the case n =
4. Because 4! = 24>16 = 2, the assertion is true for this case.
Suppose k! > 2k for some integer k > 4. Based on this, we want to show that (k + 1)! > 2 +.
Now, (k+ 1)! = (k + 1)(k!),  which (by the inductive hypothesis and the assumption that k >
4) is an expression at least as large as 5(2*) > 2(2*) = 2k+1, as desired. This completes the
proof.
Example
(Calculus example) Prove that 
, for all integers n.
Proof:  Even though this statement is about all integers, we can use mathematical
induction to give the proof for n, an arbitrary positive integer, and then use fundamental
rules of calculus to handle other values of n.
First, assume that n is a positive integer. For n = 1,  the assertion simplifies to
which is true. Next, consider the inductive step. Suppose the assertion is true for some
positive integer k. That is, the inductive hypothesis is the statement
Now, consider the case of n = k +  1. By exploiting the product rule of calculus and the
inductive hypothesis, we have
as desired. Thus, the proof is complete for positive integers n. For n= 0, the assertion
simplifies to
which is true.
Finally, if n < 0, we can apply the quotient rule to the result of applying our assertion to
the positive integer— n. That is,
as desired. Therefore, we have shown that for all integers n,
TeamUnknown Release
0
Chapter 2 - Induction and Recursion
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Recursion
A subprogram that calls upon itself (either directly or indirectly) is called recursive.  To the
beginner unfamiliar with this notion, it may sound like a recipe for an infinite loop, as
indeed it can be if not used with care. In fact, recursion is often used as a form of looping.
However, recursion should be used so that a recursive subprogram's self-reference is
made only with "simpler" data. That is, each time a program calls itself, it does so with a
smaller/simpler instance of the problem. To avoid infinite recursion, it is crucial that when
the program is invoked with a small enough (that is, simple enough) set of data, the
subprogram will compute the required answer and return without issuing another call to
itself. This action of returning without issuing another recursive call is critical in allowing
the outstanding calls to resolve their problems and return to the routine that called them.
In fact, it is critical to proving that recursive calls eventually resolve, which is often shown
by proving that successive calls are always to smaller instances of the problem and that
one or more base cases exist for all suitably small instances of the problem that may
occur.
Notice, then, the similarity of mathematical induction and recursion. Just as mathematical
induction is a technique for inducing conclusions for "large n" from our knowledge of
"small n," recursion allows us to process large or complex data sets based on our ability
to process smaller or less complex data sets.
A classical example of recursion is computing the factorial function,  which has a recursive
definition. Although it can be proven that for n > 0, n! ("n factorial " is the product of the
integers from 1 to n (and thus a common way of computing n! is based on a For loop),
the definition of n ! is recursive and lends itself to a recursive calculation.
Definition:  Let n be a nonnegative integer. Then n! is defined by

For example, we use the definition to compute 3! as follows. From the recursive
definition, we know that 3! = 3x2!. Thus, we need the value of 2!. Again (and again, as
necessary) we use the second line of the recursive definition. Therefore, we know that 3!
= 3x2! = 3x2x1! = 3x2x1x0!. At this point, however, we proceed differently, because the
first line of the definition tells us that 0! = 1. This is the simplest case of n considered by
the definition of n!, a case that does not require further use of recursion. Such a case is
referred to as a base case (a recursive definition or algorithm may have more than one
base case). It is the existence of one or more base cases, and logic that drives the
computation toward base cases, that prevents recursion from producing an infinite loop.
In our example, we substitute 1 for 0! to resolve our calculations. If we proceed in the
typical fashion of a person calculating with pencil and paper, we would make this
substitution in the above and complete the multiplication:
Typical computer implementation of this example's recursion follows. Substitute 0! = 1 to
resolve the calculation of 1!, obtaining 1! = 1×0! = 1×1 = 1; next, substitute the result of 1!
in the calculation of 2!, obtaining 2! = 2×1! = 2×1 = 2; finally, substitute the result for 2!
into the calculation of 3!, which yields 3! = 3×2! = 3×2 = 6
Next, we give a recursive algorithm for computing the factorial function. It is important to
note that this algorithm is given for illustrative purposes only. If one really wants to write
an efficient program to compute factorial, a simple tight loop would be much more
efficient (depending on compilers).
Integer function factorial (integer n)
Input:  n is assumed to be a nonnegative integer.
Algorithm:  Produce the value of n! via recursion.
Action:
If 
n =
 0, then return 1
Else return 
n
 
x
 
factorial(n
 
-
 1)
How do we analyze the running time of such an algorithm? Notice that although the size
of the data set does not decrease with each invocation of the procedure, the value of n
decreases monotonically with each successive call. Therefore, let T(n) denote the
running time of the procedure with input value n. We see from the base case of the
recursion that T(0) =  0(1), because the time to compute 0! is constant. From the
recurrence given previously, we can define the time to compute n!, for n> 0, as T(n) = T(n
— 1) + ©(1).The conditions
form a recursive (recursion) relation.  We wish to evaluate T(n) in such a way as to
express T(n) without recursion. A naïve approach uses repeated substitution of the
recursive relation. This results in
It is important to note the pattern that is emerging: T(n) = T(n — k) + k  0(1). Such a
pattern will lead us to conclude that T(n) = T (0) + n 0(1), which by the base case of the
recursive definition, yields T(n) =  0(1) + n 0(1) = ( n+1)0(1) = 0( n).
Indeed, the conclusion that we have arrived at is correct. However, the "proof" given is
not correct. Although naïve arguments are often useful for recognizing patterns, they do
not serve as proofs. In fact, whenever one detects a pattern and uses such a conclusion
in a proof, you can rest assured that there is a logical hole in the proof. After all, this
argument fails to rule out the possibility that the pattern is incorrect for some case that
wasn't considered. Such an approach reminds us of the well-known Sidney Harris
cartoon in which a difficult step in the derivation of a formula is explained with the phrase
"THEN A MIRACLE OCCURS" (see www.sciencecartoonsplus.com/gallery.htm ). Thus,
once we think that we have recognized a solution to a recursion relation, it is still
necessary to give a solid mathematical proof.
In the case of the current example, the following proof can be given. We observe that the
0-notation in condition (1) is a generalization of proportionality. Suppose we consider the
simplified recursion relation:
Our previous observations lead us to suspect that this turns out to be T(n) = n + 1,  which
we can prove by mathematical induction, as follows.
For n= 0, the assertion is T(0) =  1, which is true.
Suppose the assertion T(n) = n +  1 is true for some nonnegative integer k(thus, our
inductive hypothesis is the equation T(k) = k+  1). We need to show T(k+ 1) = k + 2.  Now,
using the recursion relation (2) and the inductive hypothesis, we have T(k +  1) = T(k) + 1 =
(k+ 1) + 1 = k + 2,  as desired.
Thus, we have completed an inductive proof that our recursion relation (2) simplifies as
T(n)= n+ 1. Because condition (1) is a generalization of (2), in which the 0-interpretation
is not affected by the differences between (1) and (2), it follows that condition (1) satisfies
T(n) = T(n). Thus, our recursive algorithm for computing n! requires Q(n) time.
TeamUnknown Release
0
Chapter 2 - Induction and Recursion
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
Binary Search
Recursion is perhaps more commonly used when the recursive call involves a large reduction in the size of
the problem. An example of such a recursive algorithm is binary search.  Finding data is a fundamental
computer operation, in which efficiency is crucial. For example, although we might not mind spending 30
seconds searching a phone book or dictionary for an entry, we probably would mind spending 30 minutes to
perform such a task. Phone books and dictionaries are examples of sorted databases, in which we can take
advantage of the fact that the data is ordered when we attempt to find an element. For example, when
searching a phone book for "Miller," you would not start at the very beginning and search entry by entry,
page by page, in hopes of finding "Miller". Instead, we would open the phone book to the middle and decide
whether "Miller" appears on the pages before, after, or on the current page being examined.
We now consider the impact of performing a search on a sorted versus an unsorted set of data. First,
consider the problem of searching a set of data in which there is no guarantee of order. In this case, we
consider a traditional sequential search  in which each item is examined in sequence. Notice that in the worst
case, every item must be examined, because the item we are looking for might not exist or might happen to
be the last item listed. So, without loss of generality, let's assume that our sequential search starts at the
beginning of the unordered database and examines the items in sequence until either
the item that is sought is found (the search succeeds), or
every item has been examined without finding the item sought (the search fails).
The data is not known to be ordered, so the sequential examination of data items is necessary, because
were we to skip over any item, the skipped item could be the one that we wanted (see Figure 2.1  ).

Figure 2.1: An example of sequential search. Given the array of data, a search for the value 4 requires
five key comparisons. A search for the value 9 requires three key comparisons. A search for the value 1
requires seven key comparisons to determine that the requested value is not present
Thus, we give the following algorithm for a sequential search.
Subprogram SequentialSearch(A, searchValue, success, foundAt)  Algorithm:  Perform a
sequential search on the array X[1… n] for searchValue.  If an element with a key value of searchValue
is found, then return success  = true and foundAt,  where searchValue=X[foundAt];  Otherwise, return
success = false.  Local variable: index position
Action:
position= 1;
Do
  success = (searchValue = X[position].key)
  If success,then foundAt= position
  Else position= position+ 1
 While (Not success)and (position =n)
 Return success, foundAt
End Search
Analysis:
It is easily seen that the set of instructions inside the loop requires 0(1) time, that is, constant time per
instruction. In the worst case, where either the search is unsuccessful (requiring that we examine every item
to verify this) or that the item we are searching for is the last item in your search, the loop body will be
executed n times. Thus, one can say that the worst-case sequential search requires 0 (n ) time. Assuming
that the data is ordered in a truly random fashion, a successful search will, on average, succeed after
examining half (n /2)of the entries. That is, a successful search of an unordered database in which the items
are randomly distributed, requires 0 (n ) time on average. Of course, we might get lucky and find the item we
are searching for immediately, which tells us that the time required for the "best-case search" is 0(1).
Now, consider the case of searching an ordered database, such as a phone book. Think about designing an
algorithm that mimics what you would do with a real phone book, that is, grab a bunch of pages and flip back
and forth, each time grabbing fewer and fewer pages, until the desired item is located. Notice that this
method considers relatively few data values compared to the sequential search. A question we need to
consider is whether or not this algorithm is asymptotically faster than the sequential algorithm, because it
may be faster by just a high-order constant or low-order term. Before we consider a proper analysis of this
binary search, we present a detailed description of the algorithm.
Subprogram BinarySearch(A, searchValue, success, foundAt, minlndex,maxlndex)
Algorithm  : Binary search algorithm to search subarray X[minlndex maxlndex]for a key field equal to
searchValue.
The algorithm is recursive. To search the entire array, the initial call is Search(X, searchValue, success,
foundAt, 1, n).
If searchValue  is found, return success = true  and foundAt  as the index at which searchValue  is found;
otherwise, return success = false.
Local variable: index midlndex
Action:
If minIndex > maxIndex ,  then
                                                     {The subarray is empty}
    success = false, foundAt = 0
Else                                              {The subarray is nonempty}
    If searchValue = X[midIndex].key], then
       success = true, foundAt =  midIndex
    Else {searchValue 
 
 X[midIndex].key}
      If searchValue > X[midIndex].key], then
       BinarySearch(X, searchValue, success, foundAt,
              minIndex, midIndex - 1)
      Else {searchValue > X[midIndex].key }
       BinarySearch(X, searchValue, success, foundAt,
              midIndex + 1, maxIndex);
    End {searchValue 
 
 X [midIndex].key}
  End {Subarray is nonempty}
  Return success, foundAt
End Search
See Figure 2.2  .Notice that the running time, T(n),  of our binary search algorithm satisfies the recursion
relation:
Figure 2.2: An example of binary search. Given the array of data, a search for the value 4 requires two
key comparisons (6,4). A search for the value 9 requires three key comparisons (6,8,9). A search for the
value 1 requires three key comparisons (6,4,3) to determine that the value is not present
To analyze the worst-case running time implied by this recursion relation, we can again use the nave
approach of repeated substitution into this recursive relation to try to find a pattern, interpret the pattern for a
base case (which enables us to express the pattern without recursion), and then try to prove the resulting
assertion by mathematical induction. This results in an expansion that looks like
Notice that the pattern beginning to emerge is that T(n) = T(n/2k ) + k  X 0(1), where the argument of T
reaches the base value 1 = n/2k when k = log 2 n. Such a pattern would lead us to the conclusion that
Based on this "analysis," we conjecture that a binary search exhibits a worst-case running time of T (log
n)and, therefore, in general, binary search has a running time of O (log n )
Notice that in our earlier "analysis," we made the simplifying assumption that n is a (positive integer) power of
2. It turns out that this assumption only simplifies the analysis of the running time without changing the result
of the analysis (see the Exercises).
As before, it is important to realize that once we have recognized what appears  to be the pattern of the
expanded recursion relation, we must prove our conjecture. To do this, we can use mathematical induction.
We leave the proof of the running time of binary search as an exercise for the reader.
The term binary,  when applied to this search procedure, is used to suggest that during each iteration of the
algorithm, the search is being performed on roughly half the number of items that were used during the
preceding iteration. Although such an assumption makes the analysis more straightforward, it is important for
the reader to note that the asymptotic running time holds so long as at the conclusion of each recursion
some fixed fraction of the data is removed from consideration.

 
TeamUnknown Release
0
Chapter 2 - Induction and Recursion
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
Merging and MergeSort
Many efficient sorting algorithms are based on a recursive paradigm in which the list of data to be sorted is split
into sublists of approximately equal size. Each of the resulting sublists is sorted (recursively), and finally the
sorted sublists are combined into a completely sorted list (see Figure 2.3  ).
The recursion relation that describes the running time of such an algorithm takes the form:
where S(n) is the time required by the algorithm to split a list of n entries into two sublists of (approximately) n /2
entries apiece, and C(n) is the time required by the algorithm to combine two sorted lists of (approximately) n /2
entries apiece into a single sorted list. An example of such an algorithm is MergeSort,  discussed next.

Figure 2.3: Recursively sorting a set of data. Take the initial list and divide it into two lists, each roughly half
the size of the original. Recursively sort each of the sublists, and then merge these sorted sublists to create
the final sorted list
To merge  a pair of ordered  lists X and Y is to form one ordered list from the members of X u Y . This operation is
most natural to describe when the lists are maintained as linked  (that is, pointer-based) lists. In the following
discussion, we consider our data to be arranged as a singly linked list in which each data record has
a field called sortkey,  used as the basis for sorting,
a field or group of fields that we call otherinfo,  used to store information pertinent to the record that is not
used by the sort routine, and
a field called next,  which is a pointer to the next element of the list.
Remember that a programming language typically has a special pointer constant ("NULL" in C and C++; "nil" in
Pascal and LISP; "Nothing" in Visual Basic) used as the value of a pointer that does not point to anything. Figure
2.4 presents a representation of such a data structure. Notice that in Figure 2.4  , we assume the sortkey  data is
of type integer.
Figure 2.4: An illustration of a linked list in a language that supports dynamic allocation. Notice that the head
of the list is simply a pointer and not a complete record, and that the last item in the list has its next pointer
set to NULL
In the diagram, "head" represents a pointer variable that is necessary to give access to the data structure. An
algorithm to merge two ordered linked lists containing a total of n elements in O(n) time is given next (see Figure
2.5 .)
Subprogram Merge(headl, head2, headMerge)
Input:  headl  and head!  point to two ordered lists that are to be merged. These lists are ordered with respect
to field sortkey.
Output:  This routine produces a merged list addressed by headMerge.  Local variable:  atMerge,  a pointer
to a link of the merged list
Action:
  If head1 = null, then return headMerge =  head2
  Else                                {The first input list is nonempty}
    If head2 = null, then return headMerge = head1
    Else                                {Both input lists are nonempty}
     If head1.sortkey = head 2.sortkey,  then
                           {Start merged list with 1st element of 1st list}
        headMerge = head1; head1= head1.  next
     Else                  {Start merged list with 1st element of 2nd list}
       headMerge = head2; head 2 = head 2.next
     End                                  {Decide first merge element}
     atMerge = headMerge;
     While head1   null  and head 2   null,  do
       If head1.sortkey = head 2.sortkey  then
                                            {Merge element of 1st list}
         atMerge.next = head1;
         atMerge = head1;
         head1= head1.next
       Else                                 {merge element of 2nd list}
         atMerge.next = head2;
         atMerge = head2;
         head 2 = head 2.next
       End If
     End While
                   {Now, one of the lists is exhausted, but the other
                             So concatenate the unmerged portion of the
                                    unexhausted list to the merged list.}
     If head1= null,  then atMerge.next = head2
     Else atMerge.next = head 1
    End Else                            {Both input lists are nonempty}
  End Else                                {First input list is nonempty}
  Return headMerge
End Merge

Figure 2.5: An example of merging two ordered lists, head1  and head2  , to create an ordered list headMerge
. Snapshots are presented at various stages of the algorithm
It is useful to examine the merge algorithm for both the best-case (minimal) running time and the worst-case
(maximal) running time. In the best case, one of the input lists is empty, and the algorithm finishes its work in 0(1)
time. Now consider the worst-case scenario, in which when one of the input lists is exhausted, only one item
remains in the other list. In this case, because each iteration of the While loop requires a constant amount of
work to merge one element into the merged list that is being constructed, the running time for the entire
procedure is 0
We note also that the algorithm processes every element of one of its input lists. Therefore, the running time of
this simple merge algorithm is 0(&), where k is the number of nodes (from both input lists) that have been
merged when the first input list is exhausted. So if both input lists have length 0 (n ) (if you are merging two lists
of length n /2, for example), then the running time of this merge algorithm is 0
In addition to being able to merge two ordered lists, the MergeSort algorithm requires a routine that will split a list
into two sublists of roughly equal size. Suppose you were given a deck of cards and didn't know how many cards
were in the deck. A reasonable way to divide the deck into two piles so that each pile had roughly the same
number of cards in it would be to deal the cards alternately between the two piles. An algorithm for splitting a list
follows.
Subprogram  Split(headln, headOut)
Algorithm:  Split an input list indexed by headln  (a pointer to the first element) into two output lists by
alternating the output list to which an input element is assigned.
The output lists are indexed by headOut[0  1].
Local variables:  current_list,  an index alternating between output lists temp,  a temporary pointer to current
link of input list
Action:
                                             {Initialize output lists as empty}
    headOut[0] = headOut[1] = null;
    current_list   = 0;
 While headIn   null, do
      temp = headIn  ;
      headIn = headInnext.   ;
      temp.next = headOut[current _list];
      headOut[current_list] = temp;
      current_list = 1 -current_list              {Switch value between 0, 1}
  End While
  Return headOut
 End Split
In the Split algorithm, each iteration of the loop takes one element from the input list and places it at the head of
one of the output lists. This requires 0(1) time. Thus, if the list has n elements, the algorithm uses T (n) time.
We have introduced and analyzed the tools necessary for MergeSort, so we now present algorithm MergeSort.
Subprogram  MergeSort(head)
Algorithm:  Sort a linked list via the Mergesort algorithm
Input:  a linked list indexed by head,  a pointer to the first element
Output:  an ordered list
Local variables:  temp[0 … 1], an array of two pointers
Action:
  If head > null, then                     {Input list is nonempty}
    If head.next > null, then
                 {There's work to do, as the list has at least 2 element}
     Split(head, temp);
     MergeSort(temp [0]);
     MergeSort(temp [1]);
     Merge(temp[0], temp[1], head)
    End If
  End If
  Return head
End Sort
Before we analyze the MergeSort algorithm, we make the following observations. The algorithm is recursive, so a
question that should be raised is, "what condition represents the base case?" Actually, two base cases are
present, but they are both so simple that they are easily missed.
Consider the statement "If head  ? null, then" in Subprogram MergeSort. The consequent action does not seem
like the simple case we expect in a base case of recursion. It does, however, suggest that we consider the
opposite case, head = null.  The latter case is not mentioned at all in the algorithm, yet clearly it can happen.
This, in fact, is a base case of recursion. Notice that if head = null,  there is no work to be done because the list is
empty. It is tempting to say that when this happens, no time is used, but we should attribute to this case the 0(1)
time necessary to recognize that head = null.
Consider the inner "If" clause, "If head.next"  "null."  Notice that this condition is tested only when the outer If
condition is true and, therefore, represents the condition of having a list with at least one element beyond the
head element; hence, this condition represents the condition that the list has at least two elements. Thus,
negation of the inner If condition represents the condition of having a list with exactly one node (because the
outer If's condition being true means there is at least one node). As previously, the condition head.next = null
results in no listed action, corresponding to the fact that a list of one element must be ordered. As earlier, we
analyze the case head.next = null  as using 0(1) time. It is important to observe that a piece of code of the form
If A, then
 actionsForA
End If A
is logically equivalent to
If not A, then   {no action}
Else           {A is true}
    actionsForA
End Else A
Analysis:  Let T(n) be the running time of the MergeSort algorithm, which sorts a linked list of n items. Based on
the previous analysis, we know that S(n) = &(n)  , and that C(n) = 0(n).  Given the time for splitting and combining,
we can construct a recurrence equation for the running time of the entire algorithm, as follows.
Before we proceed further, notice that the latter equation, in the worst case, could be written as
However, we leave the demonstration that these equations are equivalent as an exercise to the reader. To
proceed with the analysis, we again consider using repeated substitution as a means of obtaining a conjecture
about the running time. Therefore, we have
The emerging pattern appears to be T(n) = 2k T(n/2) + k T (n),reaching the base case 1 = n/2 h for k = log2 n. This
pattern would result in a conjecture that
Our conjecture can be proved using mathematical induction on k for n = 2  (see Exercises). Therefore, the
running time of our MergeSort algorithm is Q(nlogn).
 
TeamUnknown Release
0
Chapter 2 - Induction and Recursion
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we have introduced the related notions of mathematical induction and
recursion. Mathematical induction is a technique for proving statements about sets of
successive integers (often, all integers greater than or equal to some first integer) by
proving a base case and then proving that the truth of a successor case follows from the
truth of its predecessor. Recursion is a technique of solving problems by dividing the
original problem into multiple smaller problems, solving the latter (by repeating the
division step discussed earlier if a simple base case  has not yet been reached), and
combining the solutions to the smaller problems to obtain the desired solution to the
original problem. Examples of both of these powerful tools are presented, including
applications to fundamental data processing operations such as searching and sorting.
TeamUnknown Release

0
Chapter 2 - Induction and Recursion
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
A classic reference for the material presented in this chapter is Fundamental Algorithms,
volume 1 of The Art of Computer Programming,  by Donald Knuth. The book, published
by Addison-Wesley, originally appeared in 1968 and, along with the companion volumes,
is a classic that should be on every computer scientist's desk. An excellent book on
discrete mathematics is the book Discrete Algorithmic Mathematics  by S.B. Maurer and
A. Ralston (Addison-Wesley Publishing Company, Reading, MA, 1991). An interesting
book, combining discrete and continuous mathematics, is Concrete Mathematics  by R.L.
Graham, D.E. Knuth, and O. Patashnik (Addison-Wesley Publishing Company, Reading,
MA, 1989). Finally, we should mention an excellent book, Introduction to Algorithms,  by
T.H. Cor-men, C.E. Leiserson, R.L. Rivest, and C. Stein (2nd ed.: MIT Press, Cambridge,
MA, 2001). This book covers fundamental mathematics for algorithmic analysis in a
thorough fashion.
TeamUnknown Release

0
Chapter 2 - Induction and Recursion
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
Note 
The first two exercises may be completed with non-recursive algorithms. These
algorithms may be used in subsequent exercises.
Devise a 0(n) time algorithm that takes as input an array X and produces as output
a singly linked list Y such that the 2th element of Y has the same data as the 2th
entry of X Prove that the algorithm runs in 0(n) time.1.
Devise a 0(n) time algorithm that takes as input a singly linked list X and produces
as output an array 7 such that the 2th entry of 7 has the same data as the 2th
element of X Prove that the algorithm runs in 0(n) time.2.
(Arithmetic progression) Show that a recursive algorithm with running time satisfying
3.
(Geometric progression) Show that a recursive algorithm with running timesatisfying
4.
(Binary search) Show that the recursion relation used with the binary
searchalgorithm,
satisfies T(n) = O (log n) when n = 2k for some nonnegative integer k. Hint:  Your
proof should use mathematical induction on k to show that5.

satisfies T(n) < 1 + log 2 n.
Even if n is not an integer power of 2, the previous recursion relation satisfies T(n) =
O(log n). Prove this assertion, using the result for the case of n being a power of 2.
Hint:  Start with the assumption that 2 < n < 2+ for some positive integer k. One
approach is to show that only one more item need be examined, in the worst case,
than in the worst case for n = 2k. Another approach is to prove that we could work
instead with the recursion relation
then show how this, in turn, yields the desired conclusion.6.
Prove that Subprogram MergeSort has a running time of T(nlogn) by showing that
the recursion relation used in its earlier partial analysis,
satisfies T(n) = &(nlogn) . As earlier, this can be done by an argument based on the
assumption that n = 2k, for some nonnegative integer k, using mathematical
induction on k.7.
Show that an array of n entries can be sorted in T(nlogn) time by an algorithm that
makes use of the MergeSort algorithm given previously. Hint:  See Exercises 1 and
2.8.
The sequence of Fibonacci numbers f f f , is defined recursively as follows:
Develop a nonrecursive &(n) time algorithm to return the nth Fibonacci number.9.
Show that the running time of the following recursive algorithm (based on the
previous definition) to produce the nth Fibonacci number is (o(n). (The moral is that10.
the naïve use of recursion isn't always a good idea.)10.
integer function fibonacci(n)
Outputs the nth Fibonacci number
Input:  n, a nonnegative integer
Action:
If n = 2, then return 1
Else return fibonacci(n -2)+ fibonacci(n -1)
Hint:  The analysis can be achieved by the following steps:
Show that the running time T(ri) can be analyzed via a recursion relation T(n) = T(n -
1) + T(n- 2) + T(1).
Show the recursion relation obtained previously implies T(n) > 2T(n -  2)
Use the previous steps to show that T(n) = G)(n).  Note it is not necessary to find an
explicit formula for either fn or T(n) to achieve this step.
TeamUnknown Release
0
Chapter 3 - The Master Method
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 3: The Master Method
Overview
The Master Method  is an extremely important tool that can be used to provide a
solution to a large class of recursion relations. This is important for developing a
cadre of techniques that can be used effectively and efficiently to evaluate the time,
space, and other resources required by an algorithm.
Consider a recurrence of the form
where a = 1 and b > 1 are constants f(n) is a positive function. If T(n) is the running
time of an algorithm for a problem of size n, we can interpret this recurrence as
defining T(n) to be the time to solve a subproblems of size n/b, plus f(n), which is the
sum of
the time to divide the original problem into the a subproblems, and
the time to combine the subproblems’ solutions to obtain the solution to the
original problem
Consider the problem of sorting a linked list of data using the MergeSort algorithm
developed in the previous chapter  (see Figure 3.1 ). Assume that we split a list of
length n into two lists, each of length n/2, recursively sort these new lists, and then
merge them together. In terms of our general recurrence equation, this gives a = 2
subproblems to solve, each of size n/2 (that is, b = 2).

Further, the interpretation is that f(n) is the time to split the list of length n into two lists
of length n/2 each, plus the time to merge two ordered lists of length n/2 each into an
ordered list of length n. See Figure 3.2
Figure 3.1: A recursion tree representing the recurrence equation T(n) = aT(n/b) +
f(n). The number of problems to be solved at each (horizontal) level of recursion is
listed, along with the size of each problem at that level. Time is used to represent
the time per problem, not counting recursion, at each level
Figure 3.2: A recursion tree for MergeSort, as represented by T(n) = 2T(n / 2) +
T(n). Notice that level i of the recursion tree ( i   {1,2,…,log2n}) requires a total of
TxT(n/T) = T(n)time
TeamUnknown Release
0
Chapter 3 - The Master Method
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Master Theorem
The Master Method  is summarized in the following Master Theorem .
Master Theorem:  Let a = 1 and b > 1 be constants. Let f(n) be a positive function
defined on the positive integers. Let T(n) be defined on the positive integers by
where we can interpret n / b as meaning either  n/b  or   n/b . Then the following
hold:
Supposete ƒ(n)O(nlog ba-e) for some constant e> 0. Then T(n)= T (nlog ba). 1.
Suppose ƒ(n) = T(nlog ba). Then T(n) = T(nlog ba log n). 2.
Suppose ƒ(n) = O.(nlog ba+e) for some constant e> 0, and there are constants c
and N, 0 <c< 1and N > 0, such that n / > N   af(n/b) = cƒ(n). Then T(n)=T(ƒ(n)).3.
The reader should observe that the Master Theorem does not cover all instances of the
equation (3.1) .
Below, we sketch a proof for the Master Theorem. The proof is provided as a
convenience to those who have the mathematical skills, interest, and background to
appreciate it, but should be skipped by other readers.
Proof of the Master Theorem (optional)
We start under the simplifying assumption that the values of n considered are non-

negative integral powers of b The advantage of this assumption lies in the fact that at
every level of recursion, n/b is an integer. Later, we show how to handle the general
case.
Lemma 1:  Let a=1 and b >1 be constants, and let ƒ(n) be a nonnegative function defined
on integral powers of b. Let T(n) be defined on integral powers of b by the recurrence
Then
Remarks:  The asserted pattern can be guessed by simplifying an iterated expansion of
the recurrence
Since alogbn = nlogba the last term in the expanded recurrence is T(nlog ba), and the other
terms yield
as asserted earlier. Once we have guessed the pattern, we prove it by mathematical
induction.
Proof of Lemma 1:  We establish our claim by showing that
where we consider n = bi for nonnegative integers i. Therefore, the base case is i = 0
(that is, n = 1). In this case, the
term of the assertion is an empty sum, which by convention has value 0. Therefore, the
assertion is true since the right side of the asserted equation is
Thus, the base case of the induction is established.
Suppose the assertion is true for integer powers i of b, where 0 =i = p. In particular, the
assertion is true for n = bp. Then, we have
Now consider n = bp+1. By the hypothesized recurrence, we have
(using the inductive hypothesis)
(because blogba = a
which, since p = log Aa= 1, is the desired result. This completes the induction proof. Next,
we give asymptotic bounds for the summation term that appears in the conclusion of the
statement of Lemma 1.
Lemma 2:  Let a= 1 and b > 1 be constants, and let g(n) be a nonnegative function
defined on nonnegative integral powers of b. Let g(n) be a function defined on integral
powers of b by
1.
2.
If ƒ(n) = O(nlogba- ) for some constant e>0 then g(n)= O(nlog ba). 1.
If ƒ(n) = T(nlogba) then g(n) = T(nlogbn log n). 2.
If there are positive constants c < 1 and N > 0 such that n/b > N  af(n/b) =cƒ(n),
then g(n)= T(ƒ(n)).3.
Proof:  For case 1, substituting the hypothesis of the case into the definition of the
function g(n) yields
(use the formula for the sum of a geometric series)
(because b and eare constants) O(nlog ba), as claimed.
For case 2, it follows from the hypothesis of the case that f(n/bk)=
 When
we substitute the latter into (3.2), we have
as claimed.
For case 3, observe that all terms of the sum in (3.2) are nonnegative, and the term
corresponding to k = 0 is ƒ(n). Therefore, g(n) = O.(ƒ(n)). The hypothesis of the case, that
there are constants 0 < c < 1 and N> 0  such that n/ b> N    af(n/b) = cƒ(n), implies (by an
easy induction argument that is left to the reader) that n / bk> N  ak f(n/bk)= c k ƒ(n).
When we substitute the latter into (3.2), we get
The first summation in the latter expression has a fixed number of bounded terms, so this
summation satisfies
 Therefore, our asymptotic
evaluation of g(n) depends on the second summation:
But
Because the latter summation is a geometric series with decreasing terms, it follows that
We previously showed that g(n) = O.(ƒ(n)), so it follows that gin) = Tifin)),  as claimed.
Now we prove a version of the Master Method for the case in which n is a nonnegative
integral power of b.
Lemma 3:  Let a = 1 and b > 1 be constants, and let ƒ(n) be a nonnegative function
defined on integral powers of b. Let T(n) be defined on integral powers of b by the
recurrence
Then we have
1.
If ƒ(n) = O(nlog ba-e) for some constant e > 0 then T(n) = T(nlog ba). 1.
If ƒ(n)=T(nlog ba) then T(n)= T(nlog ba log n). 2.
If ƒ(n) = O(nlog ba+e) for some constant e > 0, and if n/b> N   af(n/b) = cƒ(n) for
some positive constants c < 1 and N, then T(n)= T(ƒ(n)).3.
Proof:  First, we observe by Lemma 1 that T(n) = T(nlog ba) + g(n), where
In case 1, it follows from case 1 of Lemma 2 that
In case 2, it follows from case 2 of Lemma 2 that
In case 3, it follows from case 3 of Lemma 2 that g(n) = T(ƒ(n)), and (by Lemma 1)
Because ƒ(n)=O(nlog b a+e), it follows that T(n) = T(ƒ(n)).
The General Case
Lemma 3 states the Master Method for the case that n is a nonnegative integral power of
b. Recall that the importance of this case is to guarantee that at every level of recursion
the expression n/b is an integer. For general n, however, the expression n/b need not be
an integer. We can therefore substitute  n/b or  n/b  for n/b in the recurrence (3.1) and
attempt to obtain similar results.
Because
this will enable us to demonstrate that a small discrepancy in the value of the
independent variable often makes no difference in asymptotic evaluation. In the following
discussion, we develop a version of the Master Method using the expression  a/b  for n/b
in the recurrence (3.1); a similar argument can be given if, instead, we use  n/b  for n/b
in (3.1).
Consider the sequences defined by the recursive formulas
and
Because b > 1, these are nonincreasing sequences of integers. We have
and more generally (the reader should be able to prove the following lower bound for mi,
and the following upper bound for ni, via simple induction arguments),
Thus,
Because n i is integer-valued, we have
Suppose, then, that we use the recurrence
and expand this recurrence iteratively to obtain
The reader should be able to prove by induction that for 0 = i =   log bn  = 1,
In particular, for i =   log bn  = 1,
Now,
Because 
 we have 
.Substituting
these last two results into the previous equation for T(n),  we have
This equation is much like that of the conclusion of Lemma 1. Similarly, if we modify (3.3)
to obtain the recurrence

then we similarly obtain
Let
We wish to evaluate g(n) and g'(n) asymptotically.
In case 1, we have the hypothesis that ƒ(n) = O(nlogba- ) for some constant e >0. Without
loss of generality, we have log b = e =0. There is a constant c> 0 such that for sufficiently
large n k > N,
where
is a constant.
For such k, 
 It follows that
The former summation, a geometric series, is O(alog bn) = O(nlog ba). In the latter
summation, there are T(1) terms, because n k > N corresponds to small values of k. It
follows that
Hence, 
 as desired. A similar argument shows
.
In case 2, the hypothesis that ƒ(n) = T(nlog ba) implies there are positive constants c and
C such that for sufficiently large m k and n k, say, m k, n k > N,
where
 is a constant, and similarly, there is a constant D > 0 such
that
Therefore, for such 
. So,
In the first summation, the values of f(n k) are bounded, because n k <N. Thus, the
summation is bounded asymptotically by the geometric series
The second summation in the expansion of g(n) is simplified as
Substituting these into the previous equation for g(n), we obtain
Hence, T(n) = O(nlogba log n). Similarly,
Notice that
Therefore,
It follows that g(n)= T(nlog b a log n) and g'(n) = T(nlog b a log n). Therefore,
In case 3, an analysis similar to that given for case 3 of Lemma 2 shows g(n)= T(ƒ(n)), as
follows. Recall the hypotheses of this case: ƒ(n)=O.(nloe ba+e)for some constant e > 0, and
there are constants 0 < c < 1 and N > 0 such that n/b > N   af(n/b) =cƒ(n). As earlier, it
follows by a simple induction argument that for
we have
Therefore,
Because ƒ(n) = O(nlogba+ ) and alogbn = nlogba we have g(n) = O(ƒ(n)), and therefore T(n) =
T(nlogba) = O( ƒ(n)).
Equation (3.3)  implies T(n) = T(ƒ(n)), so it follows that T(n) = T(ƒ(n)), as desired. A similar
argument shows T'(n) = T(ƒ(n)).
Thus, in all cases, whether we use  n/b  or  n/b  as our interpretation of n/b in (3.1), we
have obtained the results asserted in the statement of the Master Theorem. Therefore,
the proof of the Master Theorem is complete.
Example
Consider the recurrence
that occurs in the analysis of some image processing algorithms. We have
By case 1 of the Master Theorem , T(n) = T(n).
Example
Consider the recurrence
that occurs in the analysis of Binary Search. Then f(n) = 1 = nlog21, so by case of the
Master Theorem, T(n) = T(log n).
Example
Consider the recurrence
that occurs in the analysis of MergeSort. We have a = 2, b = 2,  and f(n) = n = nlogba. So,
by case 2 of the Master Theorem, T(n) = T(n log n).
Example
Consider the recurrence
that occurs in the analysis of many mesh computer algorithms that will be presented later
in the text. We have
and
So, by case 3 of the Master Theorem, T(n) = n½.

TeamUnknown Release
0
Chapter 3 - The Master Method
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we present and prove the Master Theorem, which provides simple
methods for solving many types of recursive relationships. We show how to use this
theorem with several examples.
TeamUnknown Release

0
Chapter 3 - The Master Method
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
In this chapter, we focus on the Master Method, a cookbook approach to solving
recurrences of the form T(n) = aT(n/b) + ƒ(n). This approach has been well utilized in
texts by E. Horowitz and S. Sahni, including Computer Algorithms/C++,  by E. Horowitz, S.
Sahni, and S. Rajasekaran (Computer Science Press, New York, 1996). Our proof is
based on the one given in Introduction to Algorithms  by T.H. Cormen, C.E. Leiserson,
R.L. Rivest, and C. Stein (2nd ed.: MIT Press, Cambridge, MA, 2001). The paper, "A
general method for solving divide-and-conquer recurrences," by J.L. Bentley, D. Haken,
and J.B. Saxe, SIGACT News,  12(3): 36-44, 1980, appears to serve as one of the
earliest references to this technique.
TeamUnknown Release

0
Chapter 3 - The Master Method
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
For each of the following recurrences, either solve via the Master Theorem, or show it is
not applicable, as appropriate. If the Master Theorem is not applicable, try to solve the
recurrence by another means.
1.
T(n) = T(n - 2) + 1 2.
3.
4.
5.
6.
7.
8.

7.
8.
TeamUnknown Release
0
Chapter 4 - Combinational Circuits
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 4: Combinational Circuits
Asignificant portion of the computing cycles in the 1960s and '70s was devoted to
sorting (i.e., organizing) data. As a result, a substantial effort was put into developing
efficient sorting techniques. In this section, we consider an early hardwarebased
implementation of sorting, proposed by Ken Batcher in 1968. In his seminal 1968
paper, Batcher proposed two algorithms, namely, BitonicSort and Odd-Even
Mergesort. Both of these algorithms are based on a MergeSort framework, both are
given for hardware, and in the case of the former, Batcher makes the insightful
observation that such an algorithm would be very efficient on a parallel computer with
certain interconnection properties. The focus of this chapter is on BitonicSort.
Combinational Circuits and Sorting Networks
We begin this chapter with a presentation of combinational circuits, a simple hardware
model involving a unidirectional (one-way) flow of data from input to output through a
series of basic functional units. When we present diagrams of combinational circuits, the
flow of information is represented by lines and the functional units are represented by
boxes. It is understood that, in these diagrams, the information flows from left to right.
After this introduction, we discuss Batcher's Bitonic Merge Unit, as applied to
combinational circuits. We then present an in-depth analysis of the running time of the
Bitonic Merge routine on this model. Finally, we conclude with a combinational circuit
implementation and analysis of Bitonic MergeSort, which exploits this very interesting
Bitonic Merge unit.@
Combinational circuits  were among the earliest models developed in terms of providing a
systematic study of parallel algorithms. They have the advantage of being simple, and
many algorithms that are developed for this model serve as the basis for algorithms
presented later in this book for other models of parallel computing. A combinational
circuit can be thought of as taking input from the left, allowing data to flow through a

series of functional units in a systematic fashion, and producing output at the right. The
functional units in the circuit are quite simple. Each such unit performs a single operation
in 0(1) (constant) time. These operations include logical operations such as AND, OR,
and NOT, comparisons such as <, >, and =, and fundamental arithmetic operations such
as addition, subtraction, minimum, and maximum. These functional units are connected
to each other by unidirectional  links, which serve to transport the data. These units are
assumed to have constant fan-in  (the number of links entering a processor is bounded by
a constant) and constant fan-out  (the number of links exiting a unit is bounded by a
constant).
In this chapter, we restrict our attention to comparison-based networks in which each
functional unit simply takes two values as input and presents these values ordered on its
output lines. Finally, it should be noted that there is no feedback (that is, no cycles) in
these circuits.
Sorting Networks
We consider a comparison-based combinational circuit that can be used as a general-
purpose sorting network. Such sorting networks  are said to be oblivious  to their inputs
because this model fixes the sequence of comparisons in advance; that is, the sequence
of comparisons is not a function of the input values. Notice that some traditional sorting
routines, such as Quicksort or Heapsort, are not oblivious in that they perform
comparisons that are dependent on the input data.
BitonicSort was originally defined in terms of sorting networks. It was intended to be used
not only as a sorting network, but as a simple switching network for routing multiple
inputs to multiple outputs. The basic element of a sorting network is the comparison
element,  which receives two inputs, say, A and B,and produces both the minimum of A
and B and the maximum of A and B as output, as shown in Figure 4.1 .
Figure 4.1: An illustration of a comparison element, the fundamental element of a
sorting network. The comparison element receives inputs A and B and produces
min(A,B) and max( A,B)
Definition:  A sequence a = <a, a, …, a> of p numbers is said to be bitonic  if and only
if
1.
2.
a1< a2<…> ak<…> a, for some k, 1 < k > p,  or 1.
a1> a2<…> ak<…> a, for some k, 1<k<p, or 2.
a can be split into two parts that can be interchanged to give either of the first
two cases.3.
The reader should notice that by including the third case in the definition, the first two
cases become equivalent. The third case can be interpreted as stating that a circular
rotation of the members of the sequence yields an example of one of the first two cases.
For example, the sequence <3,2,1,6,8,24,15,10 > is bitonic, because there is a circular
rotation of the sequence that yields <6,8,24,15,10,3,2,1 >, which satisfies case 1.
A bitonic sequence can therefore be thought of as a circular list that obeys the following
condition. Start a traversal at the entry in the list of minimal value, which we will refer to
as x. Then, as you traverse the list in either direction, you will encounter elements in
nondecreasing order until you reach the maximum element in the list, after which you will
encounter elements in nonincreasing order until you return to x. Notice that if we have
duplicate elements in the sequence (list), there will be plateaus in the list, where multiple
items of the same value appear contiguously, as we perform this traversal.
Before introducing a critical theorem about bitonic sequences, we make an important
observation about two monotonic  sequences. Given one ascending sequence and one
descending sequence, they can be concatenated to form a bitonic sequence. Therefore,
a network that sorts a bitonic sequence into monotonic order can be used as a merging
network to merge (sort) a pair of monotonic sequences (which are preprocessed by such
a concatenation step).
Theorem:  Given a bitonic sequence a = (a, a, …, a), the following hold:
a) d =<min {ai, an+i} >ni=l =<min {a1, an+1}, min {a2, an+2},…, min {an, a2n}> is
bitonic.
b)e = <max{ ai, an+i}> ni=1= <max{a 1, an+1},max{a 2, an+2 },…,max{;a n, a2n}> is
bitonic.
c)max( d) < min( e).
Proof:  Let di = mini{ ai, an+i} and ei = max{ ai, an+i}, 1 = i =n. We must prove that i) d is
bitonic, ii) e is bitonic, and iii) max(d) = min( e). Without loss of generality, we can assume
that a=1 a2,=…= aj-1=aj=aj+1=…= a2n, for some j such that n= j =2n.
Suppose an = a2n. For 1 = i= n, if n + i= j then the choice of j implies ai=an+i, but if n +
i= j, then a Now consider the case where a n < a 2n Because a is nondecreasing for i = j
and nonincreasing for i= j, and because a j-n =aj, there is an index k, j = k = 2n, for
which ak-n = ak and ak-n+1 = ak+1. This is illustrated in Figure 4.3 .
Figure 4.2: An illustration of a bitonic sequence <a> in which and aj is a maximal
element of <a>, where n = j = 2n
Figure 4.3: An illustration of a bitonic sequence < a> in which an > a 2n, aj is a maximal
element of < a>, where n = j = 2n, and there exists a pivot element k such that ak-n =
ak and ak-n+1 = ak+1
First, consider the sequence d. For 1= i= k — n,  we have either
i + n=j, which implies ai =ai+n’ or
i + n > j , in which case ai = ak-n =ak =ai+n’
the last inequality in the chain following from
Thus, for 1=i= k — n,  we have di = a i. Further, this subsequence of d is non-decreasing.
Next, notice that di = a n+i for k — n =i=n, because for such i,
ai = akn+1 (because k-n + 1 = i= n= j) = ak+1 (by choice of k)=ai+n (because j< k+1 =
i+n)..
Further, this subsequence of d is nonincreasing. Therefore, d comprises a non-
decreasing subsequence followed by a nonincreasing subsequence. By the first part
of the bitonic sequence definition, we know that d is bitonic.
Now consider the sequence e. Notice that e i= a n+i for 1 = i = j— n. Further, this
subsequence of e is nondecreasing. Next, notice that ei = a n+i for j — n= i= k-n.
Further, this subsequence is easily seen to be nonincreasing. Finally, notice that ei =
ai for k — n< i= n. This final subsequence of e is nondecreasing. Therefore, e is
bitonic by case three from the definition because we also have that e n =an= an+1 =
e1 See Figure 4.3 .
Now, consider the relationship between bitonic sequences d and e. Notice that
max(d) = max{a k-n, ak+1} and min( e) = min{a k, ak-n+1 }. It follows easily that max( d) =
min(e), completing the proof for the case of an > a 2n.
TeamUnknown Release
0
Chapter 4 - Combinational Circuits
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
Bitonic Merge
The previous theorem gives the iterative rule for constructing a bitonic merge  unit, that is, a
unit that will take a bitonic sequence as input (recall that a bitonic sequence is created
easily from two monotonic sequences) and produce a monoto-nic sequence as output.
(See Figure 4.4  .) It is important to note that this is only the merge step, and that this merge
step works on bitonic sequences. After we finish our discussion and analysis of the merge
unit, we will show how to utilize this merge unit to sort data via BitonicSort.
We now present the bitonic merge algorithm. The input to the routine is the bitonic
sequence A and the direction that A is to be sorted into  (ascending or descending). The
routine will produce a monotonic sequence Z,  ordered as requested.
Subprogram BitonicMerge(A, Z, direction)
Procedure:  Merge bitonic list A , assumed at top level of recursion to be of size 2N, to
produce list Z , where Z is ordered according to the function direction  , which can be
viewed as a function with values < or >.
Local variables:  i: list index
Z d , Z’ d , Z e , Z’ e : lists, initially empty
Action:
If |A| < 2 then return 
Z = A
 {This is a base case of recursion}
Else
  For i=1 to n, do
   If 
direction
(A
i
, A
n+l
), then

     append A
i
 to 
Z
 
d
 and append 
A
n+i
 to Z
e
   Else append 
A
n+i
 to Z
e
 and append A
i
 to 
Z
e
  End For
  BitonicMerge(
Z
d
, Z’
d
, direction)
  BitonicMerge(
Z
e
, Z
e
, direction)
  Concatenated, 
Z’,Z)
End Else | A| 
=
 2
End BitonicMerge
Figure 4.4: Input and output for a bitonic merge unit
Notice the strong resemblance between BitonicMerge and both MergeSort and Quicksort.
BitonicMerge is similar to MergeSort in that it requires a list of elements to be split into two
even sublists, recursively sorted, and then concatenated (the concatenation serves as a
merge step, by part c of the theorem). Be aware, though, that MergeSort takes as input an
unordered  list, which is sorted to produce an ordered list, whereas BitonicMerge takes as
input a bitonically ordered list in order to produce an ordered list.
BitonicMerge is similar to Quicksort in that it splits a list into sublists, recursively solves the
problem on the sublists, and then concatenates the sublists into the final list. In fact, notice
that in the case of BitonicMerge and Quicksort, the two intermediate sublists that are
produced both have the property that every element in one of the lists is greater than or
equal to every element in the other list.
As described, a bitonic merge unit for 2 M numbers is constructed from n com-paritors and
two M-item bitonic merge units. Two items can be merged with a single comparison unit. In
fact, n pairs of items can be simultaneously merged using one level of merge units. That is,
if L(x) is the number of levels of comparitors required to merge simultaneously x\ 2 pairs of
items, we know that the base case is L(2) =  1. In general, to merge two bitonic sequences,
each of size n, requires L(2n) = L(n) +1 = log 2 2n levels.
In terms of our analysis of running time, we assume that it takes T (1) time for a
comparison unit to perform its operation. So, each level of a sorting network contributes T
(1) time to the running time of the algorithm. Therefore, a bitonic merge unit for 2n numbers
performs a bitonic merge in T (log n) time.
Now consider implementing BitonicMerge on a sequential machine. The algorithm requires
T (log n) iterations of a procedure that makes n comparisons. Therefore, the total running
time for this merge  routine on a sequential machine is T (n log n). As a means of
comparison, recall that the time for MergeSort to merge two lists with a total of n items is T
(n ), and the time for Quicksort to partition a set of n items is, as we show later in the book,
T (n ).
In Figure 4.5  , we present a 2 n -item bitonic merge unit. It is important to note that the input
sequence, a, is bitonic and that the output sequence, c , is sorted. The boxes represent the
comparitors that accept two inputs and produce two outputs: L, which represents the low
(minimum) of the two input values, and H, which represents the high (maximum) of the two
input values.
Figure 4.5: The iterative rule for constructing a bitonic merge unit. The input sequence
<a> consists of 2n items and is bitonic. The 2n item output sequence <c> is sorted
Figures 4.6  and Figure 4.7  present examples of a four-element bitonic merge unit and an
eight-element bitonic merge unit, respectively. The input sequence <a> in both figures is
assumed to be bitonic. Further, as in Figure 4.5  , we let L denote the low (minimum) result
of the comparison, and H represents the high (maximum) result.
Bitonic merge units of 2 items apiece in a 4-item bitonic merge unit.
Figure 4.6: A 4-item bitonic merge unit. Note < a1 , a2 , a3 , a4 > is the bitonic input
sequence and < c1 , c2 , c3 , c4 > is the sorted output sequence. The number of levels
L(2n) can be determined as L(2n) = L(2 X 2) = 1 + L(n) = 1+L(2) = 2 = log 2 (2n)
An 8-item bitonic merge unit partitions the data into two 4-item bitonic merge units.
Figure 4.7: An 8-item bitonic merge unit. Note that the input sequence < a1 … , a8 > is
bitonic and the output sequence < c1 ,… , c8 > is sorted. The number of levels L(2n)
can be determined as L(2n) = L(2 X 4) = 1 + L(4) = 1 + 2 = 3 = log 2 8 = log 2 (2n)
TeamUnknown Release
0
Chapter 4 - Combinational Circuits
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
BitonicSort
BitonicSort is a sorting routine based on MergeSort. Given a list of n elements, MergeSort
can be viewed in a bottom-up fashion as first merging n single elements into n / 2 pairs of
ordered elements. The next step consists of pair-wise merging these n / 2 ordered pairs of
elements into n / 4 ordered quadruples. This process continues until the last stage, which
consists of merging two ordered groups of elements, each of size n / 2, into a single
ordered list of size n. BitonicSort works in much the same way.
Given an initial input list of random elements, notice that every pair of elements is bitonic.
Therefore, in the first stage of BitonicSort, bitonic sequences of size 2 are merged to
create ordered lists of size 2. Notice that if these lists alternate between being ordered
into increasing and decreasing order, then at the end of this first stage of merging, we
actually have n /4 bitonic sequences of size 4. In the next stage, bitonic sequences of size
4 are merged into sorted sequences of size 4, alternately into increasing and decreasing
order, so as to form n /8 bitonic sequences of size 8. Given an unordered sequence of
size 2n, notice that exactly log 2 2n stages of merging are required to produce a
completely ordered list. (We have assumed, for the sake of simplicity, that 2n = 2k , for
some positive integer k) See Figure 4.8  .

Figure 4.8: An example of BitonicSort on eight data items. Note that the input
sequence < a > is initially unordered, and the output sequence < c > is sorted into
nondecreasing order. The symbol I means that the comparison is done so that the top
output item is less than or equal to the bottom output item (increasing order if the
items are unique). The symbol D represents that the comparison is done with respect
to nonincreasing order (decreasing order if unique items)
Now consider the merging stages. Each of the log 2 2n stages of BitonicSort utilizes a
different number of comparitors. In fact, notice that in stage 1, each bitonic list of size 2 is
merged with one comparitor. In stage 2, each bitonic sequence of size 4 is merged with
two levels  of comparitors, as per our previous example. In fact, at stage i, the
BitonicMerge requires i levels of comparitors.
We now consider the total number of levels of comparitors required to sort an arbitrary  set
of 2n input items with BitonicSort. Again, there are log 2 2n stages of merging, and each
stage i requires i levels of comparisons. Therefore, the number of levels of comparitors is
given by
So, T (log2 ) levels of comparitors are required to sort completely an initially unordered list
of size 2n. That is, an input list of 2n values can be sorted in this (combinational circuit)
model with T (log2 n) delay.
Now consider how this algorithm compares to traditional sorting algorithms operating on
the sequential model. Notice that for 2n input values, each of the T (log2 n) levels of
comparitors actually uses n comparitors. That is, a total of T (® log2 n) comparitors is
required to sort 2n input items with BitonicSort. Therefore, if properly implemented in
software, this algorithm requires T (® log2 time on a sequential machine.
Subprogram BitonicSort(X)
Procedure:  Sort the list X[ 1,… ,2nX ],  using the BitonicSort algorithm.
Local variables:  integers segmentLength, i
Action:
segmentLength = 1;
Do
 For 
i =
 1 to 
n/ segmentLength
, do in parallel<
  
BitonicMerge(
     X[(2i -2)  segmentLength + 1,...,(2i -1)  segmentLength],
     X[(2i -1)  segmentLength + 1,...,2i  segmentLength],
     X[(2i -2)  segmentLength + 1,...,2i  segmentLength],
    ascending =odd (i))
  End For;
   segmentLength = 2  segmentLength ;
 While {End Do}
 End BitonicSort
There is an alternative view of sorting networks that some find easier to grasp. We
present such a view in Figure 4.9  for BitonicSort, as applied to an eight-element
unordered sequence. The input elements are given on the left of the diagram. Each line is
labeled with a unique three-bit binary number. Please do not confuse these labels with the
values that are contained on the lines (not shown in this figure). Horizontal lines are used
to represent the flow of data from left to right. A vertical line is used to illustrate a
comparitor between the elements on the endpoints of its line. The letters next to the
vertical lines indicate whether the comparison being performed is = (represented as I,
giving the intuition of increasing)  or = (represented as D, giving the intuition of
decreasing).  Note that dashed vertical lines are used to separate the 3 = log 2 8 merge
stages of the algorithm. The reader might want to draw a diagram of an eight-element
bitonic sorting network using the lines and comparitors that have been used previously in
this chapter and verify that such a diagram is consistent with this one.

Figure 4.9: A different view of BitonicSort for eight elements. The horizontal lines
represent wires and the vertical lines represent comparison-exchange elements. That
is, the vertical lines represent points in time at which two items are compared and
ordered according to the label I (increasing order) or D (decreasing order). Notice that
the log2 8 = 3  bitonic merge stages are separated by dotted vertical lines
Finally Batcher made an interesting observation in his seminal 1968 paper that included
BitonicSort and Odd-Even MergeSort. Consider the alternative view of BitonicSort just
presented. Batcher noticed that at each stage of the algorithm, the only elements ever
compared are those on lines that differ in exactly one bit of their line labels. Suppose that
we are given a parallel machine consisting of a set of 2 M processors, and we have one
item per processor that we want to sort. Batcher noted that if every processor were
connected to all other processors that differ in exactly one bit position, the sorting would
be performed in T (log2 n) time. In fact, such a model corresponds to the interconnection
of a hypercube, which will be discussed later in this book. See Table 4.1  .
000
a0
001, 010, and 100
001
a1
000, 011, and 101
010
a2
011, 000, and 110
011
a3
010, 001, and 111
100
a4
101, 110, and 00
101
a5
100, 111, and 001
110
a6
111, 100, and 010
111
a6
110, 101, and 011
Table 4.1: Processor Sorting Table
Processor
Entry
Neighbor processors
In conclusion, we note that BitonicSort will sort n items
in T (log2 n) time using a sorting network,
in T (log2 n ) time on a machine in which processors that differ in a single bit in their
unique, consecutively labeled indices, are directly connected (a hypercube),
in T (log2 n) time on a parallel machine that allows any two processors to
communicate in constant time (such as a PRAM, which is also presented later in this
book), and
in T (w log2 w ) time on a sequential machine (RAM).
 
TeamUnknown Release
0
Chapter 4 - Combinational Circuits
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we present Batcher's combinational circuits and sorting networks. These
pioneering ideas in the history of parallel computing illustrate the time efficiencies that are
possible via appropriate combination of architectures and algorithms. We illustrate
Batcher's BitonicMerge and BitonicSort algorithms on the hardware configurations that he
proposed, and we analyze their running times. We also observe that Batcher's algorithms
are easily modified to other parallel architectures that will be discussed later in the book.
TeamUnknown Release

0
Chapter 4 - Combinational Circuits
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
In 1968, Ken Batcher presented a short paper that introduced BitonicSort and Odd-Even
MergeSort, and made the insightful observation that both sorting networks would operate
efficiently on a hypercube network of processors. The work from this paper, "Sorting
networks and their applications" (K.E. Batcher, Proceedings of the AFIPS Spring. Joint
Computer Conference  32, 1968, 307-314) has been covered in traditional courses on
data structures and algorithms by many instructors in recent decades. The material has
become more integral for such courses as parallel computing has reached the
mainstream. This material has recently been incorporated into textbooks. A nice
presentation of this material can be found in Introduction to Algorithms,  by T.H. Cormen,
C.E. Leiserson, R.L. Rivest, and C. Stein (2nd ed.: MIT Press, Cambridge, MA, 2001).
TeamUnknown Release

0
Chapter 4 - Combinational Circuits
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
Define a transposition network  to be a comparison network in which comparisons
are made only between elements on adjacent lines. Prove that sorting n input
elements on a transposition network requires O (n2) comparison units.1.
What is the smallest number of elements for which you can construct a sequence
that is not bitonic? Prove your result.2.
Consider a comparison network C that takes a sequence of elements X = {x 1, x2, …,
xn } as input. Further, suppose that the output of C is the same set of n elements but
in some predetermined order. Let the output sequence be denoted as {y1 y2…, yn }.
Given a monotonically increasing function F, prove that if C is given the
sequence {F(x 1 ),F(x 2),…, F(x n)} as input, it will produce { F(y 1), F(y 2),
…,F(y n)}as output.a.
Suppose that input set X consists only of 0s and 1s. That is, the input is a set
of n bits. Further, suppose that the output produced by C consists of all the 0s
followed by all the 1s. That is, C can be used to sort any permutation of 0s and
1s. Prove that such a circuit (one that can sort an arbitrary sequence of n bits)
can correctly sort any sequence of arbitrary numbers (not necessarily 0s and
1s). This result is known as the 0-1 sorting principle.b.3.
Use the 0-1 sorting principle  to prove that the following odd-even merging network
correctly merges sorted sequences {x1, x2, …, xn } and {y1 y2…, yn }.
The odd-indexed elements of the input sequences, that is {x1, x3,…, xn-1}
and{y1 y3 yn-1}, are merged to produce a sorted  sequence { u1, u2, …, un}.
Simultaneously, the even-indexed elements of the input sequences, [y 2,
4.

y4,…,yn], are merged to produce a sorted sequence [ v 1, v2,…, vn].
Finally, the output sequence [ z1 z2…, z2n] is obtained from z1=u1, z2n = v n’ z2i =
min(u i+1Vi), Z2+i = max(u i+1, vi.), for all 1= i= n-1.
TeamUnknown Release
0
Chapter 5 - Models of Computation
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 5: Models of Computation
In this chapter, we introduce various models of computation that willbe used
throughout the book. Initially, we introduce the random access machine (RAM), which
isthetraditional sequential model of computation (also called the von Neumann
model). The RAM has been an extremely successful model in terms of the design and
analysis of algorithms targeted at traditional sequential computers. Next, we introduce
the parallel random access machine (PRAM), which is the most popular parallel
model of computation. The PRAM is ideal for the design and analysis of parallel
algorithms without concern for communication (either between processors and
memory or within sets of processors). Following our introduction to the PRAM and
various fundamental examples, we introduce parallel models of computation that rely
on specific interconnection networks, either between processors and memory or
between processors that contain on-board memory. Such models include the mesh,
tree, pyramid, mesh-of-trees, and hypercube. We also mention the coarse-grained
multicomputer, which has received much recent attention as a simple, practical model
of parallel computing. Finally, we conclude the chapter with a presentation of some
standard terminology.
RAM (Random Access Machine)
The RAM is the traditional sequential model of computation, as shown in Figure 5.1 . It
has proved to be quite successful because algorithms designed for the RAM tend to
perform as predicted on the majority of sequential (uniprocessor) machines.

Figure 5.1: The RAM (random access machine) is a traditional sequential model of
computation. It consists of a single processor and memory. The processor is able to
access any location of memory in T(1) time through the memory access unit
The RAM has the following characteristics:
Memory:  Assume that the RAM has M memory locations, where M is a (large) finite
number. Each memory location has a unique location (address) and is capable of
storing a single piece of data. The memory locations can be accessed in a random
fashion. That is, there is a constant C > 0 such that given any memory address A,
the data stored at address A can be accessed in at most C time. Thus, memory
access on a RAM is assumed to take T(1) time, regardless of the number of
memory locations or the particular location of the memory access.
Processor:  The RAM contains a single processor, which operates under the control
of a sequential algorithm. One instruction at a time is issued. Each instruction is
performed to completion before the processor continues with the next instruction.
We assume that the processor can perform a variety of fundamental operations.
These operations include loading and storing data from and to memory as well as
performing basic arithmetic and logical operations.
Memory Access Unit:  The memory access unit is used to create a path (a direct
connection) between the processor and a memory location.
Execution:  Each step of an algorithm consists of three phases: a read phase , a
compute phase , and a write phase . In the read phase, the processor can read data
from memory into one of its registers. In the compute phase, the processor can
perform basic operations on the contents of its registers. Finally, during the write
phase, the processor can send the contents of one of its registers to a specific
memory location. This is a high-level interpretation of a single step of an algorithm,
corresponding typically to several machine (or assembly language) instructions.
There is no distortion of analysis in such an interpretation, because several machine
instructions can be executed in T(1) time.
Running Time:  We need to consider the time that each of these three phases
requires. First, it is important to note that each register in the processor must be of a
size greater than or equal to log 2 M bits in order to accommodate M distinct memory
locations (the reader should verify this). Due to the fan-out of "wires" between the
processor and memory, any access to memory will require O(log M) time. Notice,
however, that it is often possible for k consecutive memory accesses to be pipelined
to run in O(k +  log M) time on a slightly enhanced model of a RAM. Based on this
analysis, and the fact that many computations are amenable to pipelining for
memory access, we assume that both the read and the write  phase of an execution
cycle require T(1) time.
Now consider the compute  phase of the execution cycle. Given a set of k-bit registers,
many of the fundamental operations can be performed in T(log k) time. The reader
unfamiliar with these results might wish to consult a basic book on computer architecture
and read about carry-lookahead adders, which provide an excellent example. Therefore,
because each register has k =T(log M) bits, the compute phase of each execution cycle
can be performed in O(log log M) time.
Typically, one assumes that every cycle of a RAM algorithm requires T(1) time. This is
because neither the O(k +  log M) time required for memory access nor the O(log log M)
time required to perform fundamental operations on registers typically affects the
comparison of running time between algorithms. Further, these two terms are relatively
small in practice, so much so that the running time of an algorithm is almost always
dominated by other considerations such as
the amount of data being processed,
the instructions executed, and
(in an approximate algorithm) the error tolerance.
On a parallel computer, the number of processors and their interconnection scheme will
also affect running time. It is important to note that this T(1) time model is the standard,
and that most authors do not go into the analysis or justification of it. However, this model
is properly referred to as the uniform analysis  variant of the RAM. This is the model that
we will assume throughout the book when we refer to the RAM, and as mentioned, it is
the model that is used in all standard algorithms and data structure books.
TeamUnknown Release
0
Chapter 5 - Models of Computation
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
PRAM (Parallel Random Acces Machine)
The PRAM is the most widely utilized parallel model of computation. When it was developed, the hope was that
it would do for parallel computing what the RAM model did for sequential computing. That is, the PRAM was
developed to provide a platform that could be used to design theoretical algorithms that would behave as
predicted by the asymptotic analysis on real parallel computers. The advantage of the PRAM is that it ignores
communication issues and allows the user to focus on the potential parallelism available in the design of an
efficient solution to the given problem. The PRAM has the following characteristics. (See Figure 5.2  .)
Processors:  The PRAM maintains n processors, P1 , P2 , … , Pn , each of which is identical to a RAM
processor. These processors are often referred to as processing elements, PEs  , or simply processors  .
Memory:  As with the RAM, there is a common (sometimes referred to as a "global" memory. It is typically
assumed that there are m= n memory locations.
Memory Access Unit:  The memory access unit of the PRAM is similar to that of the RAM in that it
assumes that every processor has T (1) time access to every memory location.

Figure 5.2: Characteristics of a PRAM (parallel random access machine). The PRAM consists of a set of
processing elements connected to a global memory through a memory access unit. All memory accesses are
assumed to take T (1 ) time
It is important to note that the processors are not directly connected to each other  . So, if two processors wish to
communicate in their effort to solve a problem, they must do so through the common memory. That is, PRAM
algorithms often treat the common memory as a blackboard (to borrow a term from artificial intelligence). For
example, suppose processor P1 maintains a critical value in one of its registers. Then, for another processor to
view or use this value, P1 must write the value to a location in the global memory. Once it is there, other
processors can then read this value.
Execution:  As with the RAM, each step of an algorithm consists of three phases: a read phase, a compute
phase, and a write phase. During the read phase, all n processors can read simultaneously a piece of data
from a memory location (not necessarily a unique one), where a copy of the data would be placed
simultaneously into a register of each and every processor. In the compute phase, every processor can
perform a fundamental operation on the contents of its registers. This phase is identical to that of the RAM,
but remember that n independent operations can be performed simultaneously (one in each processor).
During the write phase, every processor can (simultaneously) write an item from one of its registers to the
global memory. Again, the write stage is similar to the write stage of the RAM, with the exception that
simultaneous writes can occur. It is important to note that conflicts can occur during both the read and write
phases. We will consider resolutions to such conflicts shortly.
Running Time:  The analysis of running time per cycle is virtually identical to that of the RAM. Again, we
need to consider the time that each of these three phases takes. An analysis of the read and write phases
will again show that the time required for each processor to access any of the m memory locations, due to
constraints in fan-in, is O (log m ). As discussed previously, this can be improved by pipelining to allow k
consecutive requests from all n processors to be handled in O(k +  log m ) time. Similarly, every processor
can perform fundamental operations on its own k -bit registers in O (log k ) time. Finally, by assuming a
uniformaccess  model, we can assume that every cycle can be performed in T (1) time. Although this
uniform-access model is not perfect, it suits most of our needs.
Memory Access (resolving data access conflicts):  Conflicts in memory access can arise during both the
read phase and the write phase of a cycle. How should one handle this? For example, if two processors are
trying to read from the same memory location, should only one succeed? If so, which one? If two
processors are trying to write to the same memory location (the classical "race condition", which one, if
either, succeeds? Is a processor notified if it didn't succeed? We discuss the traditional variants of the
PRAM model in terms of memory access. Once the read and write access options have been defined, they
can be coupled in various ways to produce common PRAM models.
Read Conflicts:  Handling read conflicts is fairly straightforward. The following two basic models exist.
Exclusive Read (ER):  Only one processor is allowed to read from a given memory location during a
cycle. That is, it is considered an illegal instruction  (a runtime programming error, if you will) if at any
point during the execution of a procedure, two or more processors attempt to read from the same
memory location.1.
Concurrent Read (CR):  Multiple processors are allowed to read from the same memory location
during a clock cycle.2.
Write Conflicts:  Handling write conflicts is much more complex, and a variety of options exist.
Exclusive Write (EW):  The exclusive write  model allows only one processor to write to a given
memory location during a clock cycle. That is, it is considered a runtime error if a piece of code
requires two or more processors to write to the same memory location during the same clock cycle.1.
Concurrent Write (CW):  The concurrent write  model allows multiple processors to attempt to write to
the same memory location simultaneously (that is, during the same clock cycle). This brings up an
interesting point: how should one should resolve write conflicts? Various arbitration schemes have
been used in the literature. We list some of the popular ones.
Priority CW:  The priority CW  model assumes that if two or more processors attempt to write to
the same memory location during the same clock cycle, the processor with the highest priority
succeeds. In this case, it is assumed that processors have been assigned priorities in advance of
such an operation, and that the priorities are unique. Notice that there is no feedback to the
processors as to which one succeeds and which ones fail.a.
Common CW:  The common CW  model assumes that all processors attempting a simultaneous
write to a given memory location will write the same value. A runtime error occurs otherwise.b.
c.2.
b.
Arbitrary CW:  The arbitrary CW  model is quite interesting. This model assumes that if multiple
processors try to write simultaneously to a given memory location, then one of them, arbitrarily,
will succeed.c.
Combining CW:  The combining CW  model assumes that when multiple processors attempt to
write simultaneously to the same memory location, the values written by these multiple
processors are (magically) combined, and this combined value will be written to the memory
location in question. Popular operations for the combining CW model include arithmetic functions
such as SUM and PRODUCT; logical functions such as AND, OR, and XOR; and higher-level
fundamental operations such as MIN or MAX.d.
Popular PRAM Models:  Now that we have defined some of the common ways in which reads and writes
are arbitrated with the PRAM, we can discuss the three popular PRAM models:
CREW:  The CREW PRAM  is one of the most popular models because it represents an intuitively appealing
model. Namely, it assumes that concurrent reads may occur, but it forbids concurrent writes.
EREW:  The EREW PRAM  is the most restrictive form of a PRAM in that it forbids both concurrent reads
and concurrent writes. Because only exclusive reads and writes are permitted, it is much more of a
challenge to design efficient algorithms for this model. Further, due to the severe restrictions placed on the
EREW PRAM model, any algorithm designed for the EREW PRAM will run on any of the other models.
Note, however, that an optimal EREW algorithm may not be optimal on other PRAM models.
CRCW:  The CRCW PRAM  allows for both concurrent reads and concurrent writes. When we use such a
model, the details of the concurrent write must be specified. Several choices of CW were discussed earlier.
One might also consider an ERCW PRAM  to round out the obvious combinations of reads and writes. However,
this model has very little to offer and is rarely considered. Notice that intuitively, if one can assume that hardware
can perform concurrent writes, it is not very satisfying to assume that concurrent reads could not be managed.
Note
The PRAM is one of the earliest and most widely studied parallel models of computation. However, it is
important to realize that the PRAM is not a physically realizable machine. That is, although a machine with
PRAM-type characteristics can be built with relatively few processors, such a machine could not be built with an
extremely large number of processors. In part, this is due to current technological limitations in connecting
processors and memory. Regardless of the practical implications, the PRAM is a powerful model for studying the
logical structure of parallel computation under conditions that permit theoretically optimal communication.
Therefore, the PRAM offers a model for exploring the limits of parallel computation, in the sense that the
asymptotic running time of an optimal PRAM algorithm should be at least as fast as that of an optimal algorithm
on any other architecture with the same number of processors. (There are some exceptions to this last
statement, but they are outside the scope of this book.)
The great speed we claim for the PRAM is due to its fast communications, an issue that will be discussed in
greater detail later. The idea is that data may be communicated between a source and a destination processor
inT (1) time via
the source processor writing the data value to memory, followed by
the destination processor reading this data value from memory.
By contrast, parallel computers based on other architectures may require a non-constant amount of time for
communication between certain pairs of processors, because the data must be passed step-by-step between
neighboring processors until it reaches the desired destination.
Examples: Simple Algorithms
Now that we have introduced many of the critical aspects of the PRAM, it is appropriate for us to present several
simple algorithms, along with some basic analysis of time and space. The first operation we consider is that of
broadcasting  a piece of information. For example, suppose a particular processor contains a piece of information
in one of its registers that is required by all other processors. We can use a broadcast operation to distribute this
information from the given processor to all others. Broadcasting will serve as a nice, simple example to get us
started. The first broadcasting algorithm we present is targeted at the CR PRAM. Notice that the algorithm we
present exploits the fundamental CR capabilities. Therefore, it will not work on the ER models.
CR PRAM Algorithm for Broadcasting a Unit of Data
Initial Condition:  One processor, Pi , stores a value d in its register ri, j that is to be broadcast to all
processors.
Exit Condition:  All processors store the value d .
Action:
Processor Pi writes the value d from register ri,j to shared memory location X . 1.
In parallel, all processors read d from shared memory location X .
End Broadcast2.
Step 1 requires only a T (1) time exclusive write operation, assuming that all processors know whether they are
the one to be broadcasting the data (a reasonable assumption). Step 2 requires T (1) time by using a concurrent
read operation. Therefore, the running time of this algorithm is T (1), regardless of the number of processors.
Now, consider the broadcast problem for an ER PRAM. A simple modification to the previous algorithm could be
made to allow each processor, in sequence, to read the data item from the shared memory location X .
However, this would result in an algorithm that runs in time linear in the number of processors, which is less than
desirable. That is, given an ER PRAM with n processors, such an algorithm would run in T (n) time.
Alternatively, we could make multiple copies of the data, one for each processor, and then allow each processor
to read "its" copy simultaneously. We will take this approach. The algorithm follows.
ER PRAM Algorithm for Broadcasting a Unit of Data
Assumption:  The ER PRAM has n processors.
Initial Condition:  One processor, P i , has the data value d stored in its register ri,j that is to be broadcast to
all processors.
Exit Condition:  All processors store the value d .
Action:
  Processor P
i
 writes the valued from register r
i,j
 to shared memory location 
d
1
  For 
i
=1 to 
 
log
2
 n 
 
,do
         In parallel, processors 
P
j
, j 
e
{1,
…
,2
i-1
 1}, do
            read 
d
 from 
d
j
            If 
j
 + 2
i-1
=
n then 
P
j
 writes 
d
 to 
d
 
j+2
-1
         End Parallel
  End For
  End Broadcast
This is an example of a recursive doubling  procedure, in which during each generic step of the algorithm, the
number of copies of the initial data item has doubled (exactly or approximately). As is the case with many
parallel algorithms, it also implies that the number of processors that maintain a copy of the data doubles during
each successive step. Notice that this has the flavor of a binary treelike algorithm. Initially, there is one copy of
the data (at the root). After the first step, there are now two copies of the data (two children of root node). After
the second step, there are four copies of the data (there are four grandchildren of the root), and so on. Because
each step of reading and writing requires only T (1) time, regardless of the number of processors participating in
the operation, we know that an ER PRAM with n processors can perform a broadcast operation in logarithmic
time, that is, in T (log n ) time.
Next, we consider PRAM solutions to several fundamental operations involving arrays of data. Let's assume that
the input to these problems consists of an array X = [x 1 , x2 ,… ,xn ] , where each entry xi might be a record
containing multiple fields and where the array X may itself be ordered, as appropriate. When there is no
confusion, we will make references to the key fields simply by referring to an entry xi .
A semigroup operation  is a binary associative operation  . The term binary  refers to the fact that the operator  
takes two operands, say x1 and x2 , as input, and   is a well-defined operation (the result of which we denote
generically as x 1   x 2 ) for any values of its operands. The term associative  means that ( x   y)   z = x   (y  
z) . Popular semigroup operators include maximum, minimum, sum, product, OR  , and so forth. Sometimes
we find it easier to present a concrete example. Therefore, we will choose minimum  as our operator for several
of the semigroup operations that follow. We first consider an efficient algorithm on a RAM to compute the
minimum of a set X .
RAM Minimum Algorithm
Input:  Array X .
Output:  Minimum entry of X .
Local variables:  i, min_so_far
Action:
        min_so_ far = x
1
        For i = 2 to n, do
            If x
1
 < min_so_far then min_so_ far =x
i
        End For
        return min_so_far
  End Minimum
The analysis of this algorithm's running time is fairly straightforward. Given an array of size n , each entry is
examined exactly once, requiring T (1) time per entry. Therefore, the running time of the algorithm is T (n ).
Further, given an unordered set of data, this is optimal because we know that if we miss any of the n elements,
we may miss the minimal value and thus produce an incorrect result. Next, we consider the space requirements
of this algorithm. Notice that T (n ) space is used to store the array of data, and that the algorithm uses T (1)
additional space.
Now consider a semigroup operation for the PRAM. The first algorithm we present is fairly intuitive for the reader
who has studied treelike datastructures.
The algorithm uses a bottom-up, treelike computation, as shown in Figure 5.3  , computing the minimum of
disjoint pairs of items, then the minimum of disjoint pairs of these results, and so on until the global minimum has
been determined. In Figure 5.4  , we show how the processors cooperate to compute the minimum. The reader
should note that the processing presented in Figure 5.4  performs the computations that are presented in Figure
5.3 . To simplify our presentation, we assume the size of the problem, n , is a power of 2.
PRAM Minimum Algorithm (initial attempt)
Assumption:  The PRAM (CR or ER) has n /2 processors.
Input:  An array X = [x 1 , x2 ,… ,xn ] , in which the entries are drawn from a linearly ordered set.
Output:  A smallest entry of X .
Action:
  1.  Copy X to a temporary array T =[t
1
, t
2
,...,t
n
].
  2.  For 
i
 = 1 to log
2
 
n
, do
                   In parallel, processors P
j
, 
e
{1,...,2
log2,n-i
}, do
                             a)  Read t
2j-1
 and t
2j
;
                             b)  Write min{t
2j-1
, t
2j
}to
j
;
                 End Parallel
          End For
  3.  If desired, broadcast t
1
 = min{x
1
, x
2
,...,x
n
}
  End Minimum
Figure 5.3: A bottom-up treelike computation to compute the minimum of eight values. The global minimum
can be computed in three parallel steps. Each step reduces the total number of candidates by half
Figure 5.4: Another view of the minimum operation presented in Figure 5.3  . This shows the action of a set
of four processors. The data is presented as residing in a horizontal array. The processors that operate on
data are shown for each of the three time steps
Step 1 of the algorithm requires constant time because all processors Pj can, in parallel, copy two elements ( t2j-1
= x2j-1 and t2j = x 2j ) in T (1) time. Notice that if we do not care about preserving the input data, we could omit
step 1. Step 2 requires T (log n ) time to perform the bottom-up treetype operation. The broadcast operation can
be performed in O (log n ) time ( T (1) time on a CR PRAM and T (log n ) time on an ER PRAM). Thus, the
algorithm requires T (log n ) total time. However, time is not the only measure of the quality of an algorithm.
Sometimes we care about the efficient utilization of additional resources. We define a measure that considers
both running time and productivity of the processors, as follows.
Definition:  Let Tpar (n) be the time required for an algorithm on a parallel machine with n processors. The
cost of such an algorithm is defined as cost = n Tpar (n) , which represents the total number of cycles
available during the execution of the given algorithm.
Because we assume that n /2 processors are available in the preceding PRAM algorithm to determine the
minimum value of an array, the cost of the algorithm is n/2* T (log n ) = T (n log n ). That is, during the time that
the algorithm is executing, the machine has the capability of performing T (n log n ) operations, regardless of
how many operations it actually performs. Because the machine has the opportunity  to perform T (n log n )
operations, and the problem  can be solved with T (n ) operations, we know that this PRAM algorithm is not cost
optimal  .
Let's consider how we might improve this algorithm. To improve the cost of the algorithm, we must reduce either
the number of processors, the running time, or both. We might argue that with the model we have defined, we
cannot combine more than a fixed number of data values in one clock cycle. Therefore, it must take a
logarithmic number of clock cycles to combine the input data. Such an argument suggests that T (log n ) time is
required, so we might consider reducing the number of processors. So let's consider the question: how many
processors are required to obtain a cost-optimal algorithm   That is, what is the value P , representing the
number of processors, that will yield PT ( log n ) = T (n ), assuming that the T (log n ) running time does not
change? Clearly, the solution to this equation shows that if we can keep the running time at T (log n ), we want
the number of processors to be P = T (n /log n ). The algorithm that follows shows how to exploit P = T (n /log n
) processors to determine the global minimum of n values in T (log n ) time on a PRAM. The reader is referred to
Figures 5.5  and 5.6  . To simplify our presentation, we assume that n = 2k for some positive integer k ; when this
assumption is not true, minor modifications (that do not affect the asymptotic running time) in the algorithm given
next yield a correct solution.
PRAM Minimum Algorithm  (optimal  )
Assumption:  The PRAM (ER or CR) has n /log 2 n processors.
Input:  An array X = [x 1 , x2 ,… ,xn ] , drawn from a linearly ordered set.
Output:  A smallest entry of X
Action:
Conceptually partition the data into n /log 2 n disjoint sets of log 2 n items each. In parallel, every processor Pj
computes tj =min {xi }jlog2 n i=(j-1)log 2 n+1 using an optimal RAM algorithm, given previously. Because the data
set operated on by Pj has size T (log n ), this takes T (log n ) time.
Use the previous PRAM algorithm to compute min {t 1 , t2 ,… ,tn/log 2 n } with n /log 2 n processors in T (log( n
/log n )) = T (log n ) time.
End Minimum
Figure 5.5: Improving the performance of a PRAM algorithm by requiringeach of n /log n processors to be
responsible for log n data items
Figure 5.6: An algorithm for computingthe minimum of n items with n /log 2 n processors on a PRAM. Initially,
everyprocessor sequentially determines the minimum of the log 2 n items that it is responsible for. Once these
n /log 2 n results are known, the minimum of these values can be determined in T (log( n /log n )) = T (log n -
loglog n ) = T (log n ) time on a PRAM with n /log 2 n processors
The algorithm just described takes an interesting approach. We use asymptotically fewer processors than there
are data items of concern. We divide the data items over the number of processors. For example, suppose there
are P processors and D data items. Then we assume every processor has approximately D/P items. Each
processor first works on its set of D/P items in a sequential manner. After the sequential phase of the algorithm
completes, each processor has reduced its information to only one item of concern, which in this case is the
minimum of the items for which the processor is responsible. Finally, one item per processor is used as input
into the simple, nonoptimal parallel algorithm to complete the task. Notice that this final parallel operation uses P
items with P processors. Therefore, this PRAM algorithm runs in T (log n ) time on n /log 2 n processors. This
results in a cost of n / log 2 n T (log n ) = T (n ), which is optimal. Therefore, we have a cost-optimal PRAM
algorithm for computing the minimum entry of an array of size n that also runs in time-optimal T (log n ) time.
Now let's consider the problem of searching an ordered array on a PRAM. That is, given an array X = [x 1 , x2 ,…
,xn ] ;, in which the elements are in some predetermined order, construct an efficient algorithm to determine if a
given query element q is present. Without loss of generality, let's assume that our array X is given in
nondecreasing order. If q is present in X , we will return an index i such that xi = q . Notice that i is not
necessarily unique.
First, let's consider a traditional binary search on a RAM. Given an ordered set of data, we have previously
discussed (see Chapter 2  , "Induction and Recursion," or a generic introduction to computer science text) how to
perform a binary search in worst-case T (log n ) time. Given this result, we must target an algorithm with a worst-
case total cost of T (log n ). The first model we consider is the CRCW PRAM.
CRCW PRAM  Algorithm to Search an Ordered Array (initial attempt)
Assumption:  The combining CRCW PRAM has n processors and uses the combining operator minimum  .
Input:  An ordered array, X = [x 1 , x2 ,… ,xn ] , and the search_value
Output:  succeeds  , a flag indicating whether or not the search succeeds, and location  an index at which the
search succeeds (if it does)
Action:
        Processor P
1
 initialize ssucceeds = false;
        In parallel, every processor P
i
 does the following:
          Read search_value and xi {Note that CR is used to read search_value}
          If x
i
 search _value then
                
succeeds
 = 
true
;
                
location
 = i;
          End If
        End Parallel
   End Search
When this CRCW algorithm terminates, the value of the Boolean variable succeeds  will be set to true if and only
if search_value  is found in the array. In the event that the item is found, the variable location is set to a (not
necessarily unique) position in the array where search_value  exists. Now, let's consider the running time of the
algorithm. Notice that the initial concurrent read takes T (1) time. The time required for every processor
(simultaneously) to compare its element to the query element takes T (1) time. Finally, the two concurrent write
operations take T (1) time. Notice that the second concurrent write exploits the combining property of the CRCW
PRAM (using the operation of minimum). Therefore, the total running time of the algorithm is T (1).
Now we should consider the cost of the algorithm on this architecture. Because T (1) time is required on a
machine with n processors, the total cost is a less-than-wonderful T (n ). (Recall that a binary search requires T
(log n ) time on a RAM.) Next, we present an alternative algorithm that is somewhat slower but more cost
efficient than the previous algorithm.
CRCW PRAM  Algorithm to Search an Ordered Array (cost efficient)
Assumption:  The combining CRCW PRAM has ƒ (n) = O(n)  processors and uses combining operator
minimum. (For simplicity, assume that ƒ (n) is a factor of n .)
Input:  An ordered array X = [x 1 , x2 ,… ,xn ] , and search_value  , the item to search for
Action:
    Processor 
P
1
 initializes 
succeeds = false
;
    In parallel, every processor 
P
i
 conducts a binary search on the subarray
End Search
The preceding algorithm is interesting in that it presents the user with a continuum of options in terms of the
number of processors utilized and the effect that this number will have on the running time and total cost. So, if a
primary concern is minimizing cost, notice that by using one processor, the worst-case running time will be T (log
n ) and the cost will be T (log n ), which is optimal. In fact, with the number of processors set to 1, notice that this
is the RAM binary search algorithm.
Now, suppose what we care about is minimizing the running time. In this case, the more processors we use, the
better off we are, at least up to n processors. Using more than n processors has no positive effect on the running
time. In the case of an n processor system, we have already seen that the running time is T (1). In general, the
worst-case running time of this algorithm is 
 and the cost is 
 . In
particular, notice that if we use ƒ (n) = T (log n ) processors, the worst-case running time will be T (log n ), as in
the case of the RAM, but presumably with a smaller constant of proportionality. In other words, this PRAM
implementation should run significantly faster if other factors such as chip speed, optimized code, and so on, are
the same. The cost of T (log2 n ) will be very good, though not quite optimal.
TeamUnknown Release
0
Chapter 5 - Models of Computation
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Fundamental Terminology
In this section, we introduce some common terminology used to describe interconnection
networks. The terminology we present is standard in the field and will be used throughout
the book. It should be noted that, in general, we do our best to avoid technical
terminology. We try to use such terms only when they provide a more precise
presentation of material. In fact, we wait until the end of this chapter to provide a more
comprehensive set of terms.
Distributed Memory versus Shared Memory
Multiprocessor machines are constructed with some combination of shared and
distributed memory. When we discuss such memory, it is important to note that we are
discussing off-chip memory. A shared-memory machine  provides physically shared
memory for the processors, as shown on the left side of Figure 5.7 . For small shared-
memory machines, networks can be constructed so that every processor can access
every memory location in the same amount of time. Unfortunately, such machines cannot
currently scale to large numbers of processors while preserving uniformly fast access
time to memory. This topic has been discussed previously in connection with the PRAM.

Figure 5.7: In a traditional shared-memory machine, presented on the left, all
processors operate through an interconnection network and have equal unit-time
access to all memory locations. In a traditional distributed-memory machine,
presented on the right, everyprocessing element (processor and memory pair)
communicates with every other processing element through an interconnection
network
In a distributed-memory machine , each processor has access only to its own private
(local) memory, as shown on the right side of Figure 5.7 . On such machines, processors
communicate by sending messages to each other through an interconnection network.
So, for example, if processor A needs information stored in the memory of processor B,
this information must be transported from processor B to processor A. This is typically
done by having processor A initiate a request for information, which is sent to processor
B, followed by processor B sending the requested information back to processor A
However, it is often the case that the overhead and delay can be reduced if the
computation is synchronized so that B simply sends the information to A without
receiving such a request.
Distributed Address Space versus Shared Address Space
Recently, there has been an interest in creating a programming model that provides a
global (shared) address space . Such a model enables the user to program the machine
under the assumption that all processors have equal access to memory, regardless of
how the machine is physically constructed. Clearly, this arrangement presents a lot of
advantages to the programmer. However, it serves mainly to postpone the consideration
of a variety of real issues, including differences between NUMA (non-uniform memory
access ) machines and UMA (uniform memory access ) machines. That is, shared-
memory systems containing a large number of processors are typically constructed as
processor/memory modules. So although the memory may be logically shared, in terms
of algorithmic performance, the machine behaves as a distributed-memory machine. In
such a case, memory that is close to a processor can be accessed more quickly than
memory that is far from a processor.
In this book, we will focus on the design and analysis of algorithms for a variety of
physical parallel models of computation. Therefore, we now consider options for
connecting processors to each other (that is, options for the distributed-memory model).
TeamUnknown Release
0
Chapter 5 - Models of Computation
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Interconnection Networks
In this section, we consider distributed-memory machines, which are constructed as
processor-memory pairs connected to each other in a well-defined pattern. These
processor-memory pairs are often referred to as processing elements , or PEs, or
sometimes just as processors , when this term will not cause confusion. The efficient use
of an interconnection network to route data on a multiprocessor machine is often critical
in the development of an efficient parallel algorithm. The quality of an interconnection
network can be evaluated in various ways, including the following.
Degree of the Network:  The term degree  comes from graph theory. The degree of
a processor  is defined to be the number of (bidirectional) communication links
attached to the processor. That is, the degree of processor A corresponds to the
number of other processors to which processor A is directly  connected. So, if we
think of the processors as corresponding to vertices and the communication links as
corresponding to edges in an undirected graph, then the degree of a processor is
the degree of the corresponding vertex. Similarly, the degree of a network  refers to
the maximum degree of any processor in the network. Naturally, networks of high
degree become difficult to manufacture. Therefore, it is desirable to use networks of
low degree whenever possible. In fact, if we are concerned with scaling the network
to extremely large numbers of processors, a small fixed degree is highly desirable
with current technology.
Communication Diameter:  The communication diameter  of a network is defined to
be the maximum of the minimum distance between any pair of processors. That is,
the communication diameter represents the longest path between any two
processors, assuming that a best (shortest) path between processors is always
chosen. Therefore, a machine (network) with a low communication diameter is
highly desirable, in that it allows for efficient communication between arbitrary pairs
of processors.
Bisection Width:  The bisection width  of a network is defined to be the minimum

number of wires that have to be removed (severed) to disconnect the network into
two approximately equal-size subnetworks. In general, machines with a high
bisection width are difficult (more costly) to build, but they provide users with the
possibility of moving large amounts of data efficiently.
I/O Bandwidth:  The input/output bandwidth is not a primary concern in this book,
because it is often reasonable to assume that the data is already in the machine
before our algorithms are initiated. However, when considering the construction of a
real machine, I/O bandwidth is certainly important.
Running Time:  When comparing models of computation, it is often enlightening to
consider the time required to perform fundamental operations. Such operations
include semigroup computations (min, max, global sum, and so forth), prefix
computations (to be defined later), and fundamental data movement operations
such as sorting. In fact, as we introduce some of the network models in following
sections, we will consider the efficiency of such routines.
To summarize, we want to design the interconnection network of a distrib-uted-memory
machine so that it will reduce the cost of building a processor and minimize the degree of
the network. Further, to minimize the time necessary for individual messages to be sent
long distances, we want to minimize the communication diameter. Finally, to reduce the
probability of contention between multiple messages in the system, we want to maximize
the bisection width. Unfortunately, it is often difficult to balance these design criteria. In
fact, we also would prefer to use a simple design, because simplicity reduces the
hardware and software design costs. Further, we would like the machine (that is,
network) to be scalable, so that machines of various sizes can be built (and sold).
Processor Organizations
In this section, we introduce various processor organizations (that is, sets of processing
elements and their interconnection networks). These network models are characterized
by the interconnection scheme between the processors and the fact that the memory is
distributed among the processors (there is no shared memory).
In particular, it is the interconnection pattern that distinguishes these distributed-memory
architectures. As we introduce several such models, we will consider some of the
measures discussed previously. Notice, for example, that the communication diameter
often serves as a limiting factor in the running time of an algorithm. This measure serves
as an upper bound on the time required for any (arbitrary) pair of processors to exchange
information and, therefore, as a lower bound on the running time of any algorithm that
requires global exchanges of information.
Terminology:  We say that two processors in a network are neighbors  if and only if
they are directly connected by a communication link. We assume these
communication links are bidirectional . That is, if processor A and processor B are
connected by a communication link, we assume that A can send data to B and that
B can send data to A. Because sorting is a critical operation in network-based
parallel machines, we need to define what it means to sort on such architectures.
Suppose we have a list, X = [x 1, x2,…,xn], with entries stored in the processors of a
distributed-memory machine. For the members of X to be considered ordered, there
must be a meaningful ordering not only of those entries that are stored in the same
processor but also of entries in different processors. We assume that there is an
ordering of the processors. The notation R(i) is used to denote the ranking function
for the processor labeled i. We say the list X is in ascending order if the following
conditions are satisfied:
i < j Ü xi = xj and
i < j, x i is stored in Pr, xj is stored in Ps, and r   s implies R(r) < R(s).
Similar statements can be made for data stored in descending order.
Linear Array
A linear array of size n  consists of a string of n processors, P1, P2,…,Pn, where every
generic processor is connected to its two neighbors (see Figure 5.8 ). Specifically,
processor Pi is connected to its two neighbors, Pi-1 and Pi+1, for all 2 = i = n - 1 .
However, the two end processors, P1 and Pn, are each connected to only one neighbor.
Given a linear array of size n, let's consider some of the basic measures. Because n - 2
processors have degree 2 and two processors have degree 1, the degree of the network
is 2. Now consider the communication diameter, the maximum over the minimum
distances between any two processors. Consider the minimum number of communication
links that need to be traversed for processors P1 and Pn to exchange information. The
only way that a piece of data originating in P1 can reach processor Pn is by traversing
through the other n - 2 processors. Therefore, the communication diameter is T(n). This
is important in that it tells us that time linear in the number of processors is required to
compute any function for which all processors may need to know the final answer. Now
consider the minimum time required for a computation to be performed on two arbitrary
pieces of data. Notice that information from processors P1 and Pn could meet in
processor P n/2 . However, this still requires  n/2 -1 communication steps. Therefore,
time linear in the number of processors is required, even in the best case, to solve a
problem that requires arbitrary pairs of data to be combined.
Figure 5.8: A linear array of size n
Next, we consider the bisection width of a linear array of size n. The bisection width of a
linear array of size n is 1 due to the fact that when the communication link between
processors Pn/2 and P(n/2)+1  is severed, the result is two linear arrays, each of size n/2.
Now let's move on and consider some basic operations.
Assume that a set of data, X = [x 1, x2,…,xn], is distributed so that data ele-ment xi is
stored in processor Pi. First, we consider the problem of determining the minimum
element of array X This can be done in several ways. Our first approach is one in which
all the data march left in lockstep fashion, and as each data item reaches processor P1,
this leftmost processor updates the running minimum, as shown in Figure 5.9 . That is,
initially, processor P1 sets a register running_min  to x1. During the first step of the
algorithm, in lockstep fashion, processors P2,…,Pn each send their data elements to the
left. Now processor P1 can let running  _min = min{ running  _min, x2}. The procedure
continues so that after i steps, processor P1 has the value of min{ x1,…,xi+1}. Therefore,
after n - 1 steps, the minimum of X is stored in processor P1.
Figure 5.9: Computing the minimum of n items initially distributed one per processor
on a linear array of size n. Notice that the data is passed in lockstep fashion to the left
during every time step. The leftmost processor ( P1) keeps the running minimum
Suppose every processor needs to know this minimum value, which is currently stored in
processor P1. Initially, processor P1 (viewed as the leftmost processor in the linear array),
can send this value to the right (to processor P2). If this value continues to move to the
right during each step, after a total of n- 1 such steps, all n processors will know the
minimum of X Therefore, the minimum (or any other semigroup operation) can be
determined and distributed to all processors in T(n) time on a linear array of size n.
Notice that such a T(n) time algorithm on a set of n processors yields a cost of n × T(n) =
T(n2). This is not very appealing, considering that such problems can be solved easily in
T(n) time on a RAM. Therefore, we should consider whether it is possible to do better on
a linear array of size n. Notice that the running time cannot be improved because the
communication diameter is T(n).
Next, consider whether we can reduce the number of processors and arrive at a cost-
optimal algorithm. We have seen that if we use only one processor, computing the
minimum of n items requires T(n) time, which would yield an optimal cost of T(n).
However, this is not desirable if we wish to use a parallel computer, because the running
time has not been reduced over that of the RAM. So, although we have considered the
two extremes in terms of numbers of processors (both 1 and n), let's now consider some
intermediate value. What value should we consider   We would like to balance the
amount of work performed by each processor with the work performed by the network.
That is, we would like to balance the number of data elements per processor, because
the local minimum algorithm runs in time linear in the number of elements, with the
number of processors, because the communication diameter is linear in the number of
processors. Therefore, consider a linear array of size n1/2, where each processor is
responsible for n1/2 items, as shown in Figures 5.10  and 5.11. An algorithm to compute
the minimum of n data items, evenly distributed on a linear array of size n1/2, can be
constructed with two major steps. First, each processor runs the standard sequential
algorithm (which runs in time linear in the number of data elements stored in the
processor) on its own set of data. Next, the previous linear array algorithm is run on these
n1/2 partial results (one per processor) to obtain the final global result (that is, the
minimum of these n1/2 partial minima). Therefore, the running time of the algorithm is
dominated by the T(n1/2) time to perform the RAM algorithm simultaneously on all
processors, followed by the T(n1/2) time to determine the minimum of these n1/2 local
minima, distributed one per processor on a linear array of size n1/2. Hence, the running
time of the algorithm is T(n1/2), which results in an optimal cost of T(n).
Figure 5.10: Partitioning the data in preparation for computing the minimum of n
items initially distributed on a linear array of size n1/2 in such a fashion that each of
the n1/2 processors stores n1/2 items
Figure 5.11: Computing the minimum of n items initially distributed on a linear array
of size n1/2 in such a fashion that each of the n1/2 processors stores n1/2 items. In the
first step, every processor sequentially computes the minimum of the n1/2 items for
which it is responsible. In the second step, the minimum of these n1/2 minima is
computed on the linear array of size n1/2 by the typical lockstep algorithm
Suppose we have a linear array of size n, but that the data does not initially reside in the
processors. That is, suppose we have to input the data as part of the problem. For lack of
a better term, we will call this model an input-based linear array . Assume that the data is
input to the leftmost processor (processor P1) and only one piece of data can be input
per unit time. Assume that the data is input in reverse order and that at the end of the
operation, every processor Pi must know xi and the minimum of X The following
algorithm can accomplish this. In the first step, processor P1 takes as input xn and
initializes running_min  to xn. In the next step, processor P1 sends xn to processor P2,
inputs xn-1, and assigns running  _min = min {running_min, x n-1}. In general, during each
step of the algorithm, the data continues to march in lockstep fashion to the right, and the
leftmost processor continues to store the running minimum, as shown in Figure 5.12 .
After n steps, all processors have their data element, and the leftmost processor stores
the minimum of all n elements of X As before, processor P1 can broadcast the minimum
to all other processors in n- 1 additional steps. Therefore, we have an optimal T(n) time
algorithm for the input-based linear array.
Figure 5.12: Computing the minimum on an input-based linear array of size 6. During
step 1, processor P1 takes as input x6 = 5 and initializes running_min to 5. During
step 2, processor P1 sends x1 to processor P2, inputs xn-1 = 1, and assigns
running_min= min(running_min, x n-1), which is the minimum of 5 and 1, respectively.
The algorithm continues in this fashion as shown, sending data to the right in lockstep
fashion while the first processor keeps track of the minimum value of the input data
We introduced this input-based variant of the linear array so that we could extrapolate an
algorithmic strategy. Suppose we wanted to emulate this input-based linear array
algorithm on a traditional linear array of size n, in which the data is initially stored in the
array. This could be done with a tractor-tread  algorithm, where the data moves as one
might observe on the tractor-tread of many large construction vehicles. In the initial
phase, view the data as marching to the right, so that when a data element hits the right
wall, it turns around and marches to the left (see Figure 5.13 ). That is, every processor
starts by sending its data in lockstep fashion to the right (with the exception of the
rightmost processor). When the rightmost processor receives data it reverses its direction
so that during a good portion of this tractor-tread data movement, data is simultaneously
and synchronously moving to the left and right.
Figure 5.13: A tractor-tread algorithm. Data in the linear array moves to the right until
it hits the right wall, where it reverses itself and starts to march to the left. Once the
data hits the left wall, it again reverses itself. A revolution of the tractor-tread algorithm
is complete once the initial data resides in its original set of processors. Given a linear
array of size n, this algorithm allows every processor to view all n data items in T(n)
time
In general, every processor will continue to pass all the data that it receives in the
direction it is going, with the exception of the first and last processors, which emulate the
walls and serve to reverse the direction of data. So, after the initial n-1 steps, notice that
processor P1 will store a copy of xn, processor P2 will store a copy of xn-1, and so forth.
That is, the data is now positioned so that processor P1 is prepared to accept as "input"
xn, as in the input-based linear array algorithm. In fact, the input-based linear array
algorithm can now be emulated with a loss in running time of these initial n - 1 steps.
Therefore, the asymptotic running time of the algorithm remains as T(n).
Notice that this tractor-tread algorithm is quite powerful. It can be used, for example, to
rotate all of the data through all of the processors of the linear array. This gives every
processor the opportunity to view all of the data. Therefore, such an approach can be
used to allow every processor to compute the result of a semigroup operation in parallel.
Notice that we have traded off an initial setup phase for the postprocessing broadcast
phase. However, as we shall soon see, this approach is even more powerful than it might
initially appear.
Consider the problem of sorting. The communication diameter of a linear array dictates
that  (n) time is required to sort n pieces of data initially distributed in an arbitrary
fashion, one item per processor on a linear array of size n. Similarly, by considering the
bisection width, we know that in the worst case, if the n/2 items on the left side of the
linear array belong on the right side of the array, and vice versa, for n items to cross the
single middle wire,  (n /1) =  (n) time is required.
We now show how to construct such a time-optimal sorting algorithm for this model. First,
consider the input-based linear array model. Notice that the leftmost processor P1 will
view all n data items as they come in. If that processor retains the smallest data item and
never passes it to the right, at the end of the algorithm, processor P1 will store the
minimum data item. Now, if processor P2 performs the same minimum-keeping algorithm
as processor P1 does, at the end of the algorithm, processor P2 will store the minimum
data item of all n- 1 items that it viewed (see Figure 5.14 ). That is, processor P2 would
store the minimum of all items with the exception of the smallest item, which processor
P1 never passed along. Therefore, at the end of the algorithm, processor P2 would store
the second smallest data item. (This algorithm can be illustrated quite nicely in the
classroom. Each row of students can simulate this algorithm running on such a machine,
where the input comes from the instructor standing in the aisle.)
Figure 5.14: Sorting data on an input-based linear array. Every processor simply
retains the item that representsthe minimum value it has seen to date. All other data
continues to pass in lockstep fashion to the right. Notice that this is a minor
generalization of the minimum algorithm illustrated in Figure 5.12
We now have an optimal T(n) time algorithm for this model. By using the tractor-tread
method, we can emulate this algorithm to produce a time-optimal T(n) time algorithm for
a linear array of size n. As an aside, we should mention that this sorting algorithm can be
viewed as a parallel version of SelectionSort. That is, the first processor views all of the
data and selects the minimum. The next processor views all of the remaining data and
selects the minimum, and so forth.
The final algorithm we consider for the linear array is that of computing the parallel prefix
of X = [x 1, x2,…,xn]. Assume when the algorithm starts, xi is stored in processor Pi. When
the algorithm terminates, Pi must store the ith prefix, x1  …  xi, where   is a binary
associative operator. The algorithm follows.
First, we note that processor P1 initially stores x1, which is its final value. During the first
step, processor P1 sends a copy of x1 to processor P2, which computes and stores the
second prefix, x1   x2. During the second step, processor P2 sends a copy of its prefix
value to processor P3, which computes and stores the third prefix value, x1   x2   x3.
The algorithm continues in this fashion for n - 1 steps, after which every processor Pi
stores the ith prefix, as required. It is important to note that during step i, the ith prefix is
passed from processor Pi to processor Pi+1. That is, processor Pi passes a single value,
which is the result of x1   …  xi, to processor Pi+1. If processor Pi passed all of the
components of this result, x1,…,xi, to processor Pi+1, the running time for the ith step
would be T(i), and the total running time for the algorithm would therefore be
. By requiring only one data item to be passed between neighboring
processors during each iteration of the algorithm, the running time of this algorithm is
T(n). Notice that this is optimal for a linear array of size n because the data entries stored
at maximum distance must be combined (argument based on communication diameter).
In this case, no argument can be made with respect to the bisection width, because this
problem does not require large data movement.
Ring
A ring is a linear array of processors in which the two end processors are connected to
each other, as shown in Figure 5.15 . That is, a ring of size n  consists of an augmented
linear array of n processors, P1,… Pn; the augmentation (relative to a standard linear
array) is that the end processors are directly connected to each other. Specifically,
processor Pi is connected to its two neighbors, Pi-1 and Pi+1, for 2 = i = n-1, and
processors P1 and Pn are connected to each other.
Figure 5.15: A ring of size 8. All processors in a ring are connected to two neighbors
Let's examine the ring to see what advantages it has over the linear array. The degree of
both networks is 2. The communication diameter of a ring of size n is approximately n/2,
which compares favorably with the n -1 of the linear array. However, notice that this
factor of approximately one half is only a multiplicative constant. Thus, both architectures
have the same asymptotic communication diameter of T(n). Although the bisection width
does not really make sense in this model, if one assumes that the ring could be broken
and then each subring sealed back up, this would require severing/patching T(1)
communication links, which is the same as the linear array. In fact, when we consider the
ring compared to the linear array, the best we could hope for is a factor of 2 improvement
in the running time of algorithms. Because this book is concerned primarily with the
design and asymptotic  analysis of algorithms, the ring presents an uninteresting variant of
the linear array and will not be discussed further.
Mesh
We will use the term mesh  in this book to refer to a two-dimensional, checkerboard-type,
mesh-based computer. A variety of two-dimensional meshes have been proposed in the
literature. In the most traditional of meshes, each generic processor has four neighbors,
and the mesh itself is constructed either as a rectangular or square array of processors,
as shown in Figure 5.16 . A simple variant of the four-connected mesh is an eight-
connected mesh in which each generic processor is connected to its traditional north,
south, east, and west neighbors, as well as to its northeast, northwest, southwest, and
southeast neighbors. Meshes have also been proposed in which each processor has six
(hexagonal) neighbors.
Figure 5.16: A mesh of size 16. Each generic processor in a traditional mesh is
connected to its four nearest neighbors. Notice that there are no wraparound
connections and that the processors located along the edges of the mesh have fewer
than four neighbors
In this text, we restrict our attention to a traditional two-dimensional square mesh, which
will be referred to as a mesh of size n , where n = 4k for k a positive integer. Throughout
the text, we will show how to exploit a divide-and-conquer solution strategy on the mesh.
This will be done by showing how to divide a problem into two (or four) independent
subproblems, map each of these to a submesh, recursively solve the smaller problems
on each submesh, and then stitch together the results.
Now, let's consider several of the measures that we have discussed. Notice that the
interior processors of a mesh have degree 4, the four corner processors have degree 2,
and the remaining edge processors have degree 3. Therefore, the degree of a mesh of
size n is 4. That is, the mesh is a fixed-degree network. Now consider the communication
diameter, which represents the maximum distance over every pair of shortest paths in the
network. Notice that on a mesh of size n, there are n1/2 rows and n1/2 columns.
Therefore, transporting a piece of data from the northwest processor to the southeast
processor requires traversing n1/2- 1 rows and n1/2- 1 columns. That is, a message
originating in one corner of the mesh and traveling to the opposite corner of the mesh
requires traversing a minimum of 2 n1/2-2 communication links. Thus, the
communication diameter of a mesh of size n is T(n1/2). Notice that if we are interested in
combining , as opposed to exchanging, information from two processors at opposite
corners, such information could be sent to one of the middle processors in less than 2
n1/2-2 steps. Whereas the time to combine distant data may be an improvement over
the time to transmit such data, notice that the improvement is only by a constant factor.
Determining the bisection width of a mesh of size n is fairly straightforward. If one cuts
the links between the middle two columns, we are left with two (rectangular) meshes of
size n/2. If this process is not intellectually satisfying, we could sever the links between
the middle two rows and the middle two columns and be left with four square meshes,
each of size n/4. In any event, the bisection width of a mesh of size n is T(n1/2). Before
considering some fundamental operations, we should note that the bisection width can
be used to provide a lower bound on the worst-case time to sort a set of data distributed
one piece per processor. For example, suppose all the data elements initially stored in
the first n/2 columns need to move to the last n/2 columns and vice versa. Moving n
pieces of data between the middle two columns, which are joined by n1/2 communication
links, requires T(n/n1/2)= T(n1/2) time.
We will now turn our attention to some fundamental mesh operations. Because the mesh
can be viewed as a collection of linear arrays stacked one on top of the other and
interconnected in a natural fashion, we start by recalling that the mesh can implement
linear array algorithms independently in every row and/or column of the mesh. Of
immediate interest is the fact that the mesh can perform a row (column) rotation
simultaneously in every row (column), so that each processor will have the opportunity to
view all information stored in its row (column). As discussed earlier, a row rotation
consists of sending data from every processor in lockstep fashion to the right. When data
reaches the rightmost processor, it reverses itself and marches to the left until it reaches
the leftmost processor, at which point it reverses itself again and continues moving to the
right until it reaches the processor where it originated. Notice that at any point during the
algorithm, a processor is responsible for at most two pieces of data that are involved in
the rotation, one that is moving from left to right (viewed as riding the top tread of a
tractor that is moving from left to right) and the other that is moving from right to left
(viewed as riding the bottom portion of such a tractor tread). A careful analysis will show
that exactly 2 n1/2-2 steps are required to perform a complete rotation. Recall that this
operation is asymptotically optimal for the linear array.
Because a rotation allows every processor in a row (or column) to view all other pieces of
information in its row (or column), this operation can be used to solve a variety of
problems. For example, if it is required that all processors determine the result of
applying some semigroup operation (min, max, sum) to a set of values distributed over all
the processors in its row/column, a rotation can be used to provide a time-optimal
solution.
In the following examples, it is useful to refer to a processor of a mesh by the notation
Pi,j. The first subscript i typically indexes the ith row of processors, 1 = i = n1/2, where the
rows are numbered from top to bottom. Similarly, the second subscript j indexes the jth
column of processors, 1 = j = n1/2, where the columns are numbered from left to right.
See Figure 5.16 .
We now provide an algorithm for performing a semigroup operation over a set X
=[x1,…xn], initially distributed one item per processor on a mesh of size n. This operation
consists of performing a sequence of rotations. First, a row rotation is performed in every
row so that every processor knows the result of applying the operation to the data
elements in its row. Next, a column rotation is performed so that every processor can
determine the final result (which is a combination of every row-restricted result).
Mesh Semigroup Algorithm
Input:  An input set X, consisting of n elements, such that every processor Pi,j initially
stores data value xi,j.
Output:  Every processor stores the result of applying the semigroup operation
(generically denoted as  ) to all of the input values.
Action:
In parallel, every row i performs a row rotation so that every processor in row i
knows the product ri =  n1/2j=1 xi,j.
In parallel, every column y performs a column rotation so that every processor in
column y knows the product p =  n1/2i=1 ri(which is equal to the desired product
 n1/2i=1  n1/2j=1 xi,j).
End Semigroup Algorithm
This algorithm requires T(n1/2) time, which is optimal for a mesh of size n. However, on a
RAM, a simple scan through the data will solve the problem in T(n) time. Therefore, this
mesh algorithm is not cost-optimal because it allows for T(n × n1/2) = T(n3/2) operations
to be performed. Now, let's try to construct a cost-optimal algorithm of minimal running
time for a mesh. In order to balance the local computation time with communication time
based on the communication diameter, consider an n1/3 × n1/3 mesh, in which each
processor stores n1/3 of the data items. Initially, every processor can perform a sequential
semigroup operation on its set of n1/3 data items. Next, the n2/3 partial results, one per
processor on the n1/3 × n1/3 mesh, can be used as input to the fine-grained mesh
algorithm just presented. Notice that the sequential component of the algorithm, which
operates on n1/3 data items, can be performed in T(n1/3) time. The parallel semigroup
component also requires T(n1/3) time. Therefore, the algorithm is complete in T(n1/3) time
on a mesh of size n2/3, which results in an optimal cost of T(n2/3 × n1/3) = T(n).
In addition to semigroup operations, row and column rotations are important components
of data gathering and broadcasting operations for the mesh. Suppose a data item x is
stored in an arbitrary processor Pi,j of a mesh of size n, and we need to broadcast x to all
of the other n - 1 processors. Then a single row rotation, followed by n1/2 simultaneous
column rotations, can be used to solve this problem, as follows. (See Figure 5.17 .)
Mesh Broadcast Algorithm
Procedure:  Broadcast the data value x, initially stored in processor Pi,j, the
processor in row i and column j, to all processors of the mesh.
Action:
Use a row rotation in row i to broadcast x to all processors in row i.
In parallel, for all columns k e{1,2,…, n1/2}, use a column rotation to broadcast x to
every processor in column k.
End Broadcast
Figure 5.17: Broadcasting a piece of data on a mesh. First, a row rotation is
performed to broadcast the criticaldata item to all processors in its row. Next, column
rotations are perfored simultaneously in every column to broadcast the critical data
item to all remaining processors
An analysis of the running time of the broadcast operation is straightforward. It consists
of two T(n1/2) time rotations. Based on the communication diameter of a mesh of size n,
we know that the running time for the algorithm is optimal for this architecture. Now
consider the cost of this operation. In T(n1/2) time, n processors have the opportunity to
perform T(n3/2) operations. Therefore, this algorithm is not cost optimal.
As we did previously, let's consider reducing the number of processors to the point where
we balance the sequential processing time within each processor with the communication
time required by the network. Notice that if we construct an n1/3 × n1/3 mesh, each of
these n2/3 processors would store n1/3 of the data items. So, using the rotations as
described, a single piece of information could be broadcast from one processor to all n2/3
processors in T(n1/3) time. Once this is complete, each processor can make n1/3 copies
of this data item. (This might come up, for example, if it is desired to initialize every
member of an array of size n with the value that must be broadcast.) Therefore, the
algorithm is complete in T(n1/3) time on a mesh of size n2/3, which results in an optimal
cost of T(n).
Tree
A tree of base size n  is constructed as a full binary tree with n processors at the base
level. In graph terms, this is a tree with n leaves. Therefore, a tree of base size n has 2 n
- 1 total processors (see Figure 5.18 ). The root processor is connected to its two
children. Each of the n leaf processors is connected only to its parent. All other (interior)
processors are connected to three other processors, namely, one parent and two
children. Therefore, the degree of a tree network is 3. Notice that a tree with n leaves
contains nodes at 1 + log 2n levels. Thus, any processor in the tree can send a piece of
information to any other processor in the tree by traversing O(log n) communication links.
This is done by moving the piece of information along the unique path between the two
processors involving their least common ancestor. That is, information flows from one
processor up the tree to its least common ancestor and then down the tree to the other
processor. Therefore, the communication diameter of a tree of base size n is far superior
to the other network models that we have considered. Now, let's consider the bisection
width. The bisection width of a tree of base size n is T(1), because if the two links are cut
between the root and its children, a tree of base size n will be partitioned into two trees,
each of base size n/2.
Figure 5.18: A tree of base size 8. Notice that base processors have only a single
neighbor (parent processor), the root has only two neighbors (children processors),
and the remaining processors have three neighbors (one parent and two children
processors)
A tree presents a nice (low) communication diameter but a less than desirable (low)
bisection width. This leaves us with a "good news, bad news" scenario. The good news is
that fundamental semigroup operations can be performed in T(log n) time, as follows.
Assume that n pieces of data are initially distributed one per base processor. To compute
a semigroup operation (min, max, sum, and so on) over this set of data, the semigroup
operator can be applied to disjoint pairs of partial results in parallel as data moves up the
tree level by level. Notice that after T(log n) steps, the final result will be known in the
root processor. Naturally, if all processors need to know the final result, the final result
can be broadcast from the root to all processors in a straightforward top-down fashion in
T(log n) time. So semigroup, broadcast, and combine-type operations can be performed
in T(log n) time and with T(n log n) cost on a tree of base size n. Notice that the running
time of T(log n) is optimal for a tree of base size n, and that the cost of T(n log n), though
not optimal, is only a factor of T(log n) from optimal because a RAM can perform these
operations in T(n) time.
Now for the bad news. Consider the problem of sorting or any routing operation that
requires moving data from the leftmost n/2 base processors to the rightmost n/2
processors and vice versa. Unfortunately, the root serves as a bottleneck, because it can
process only a constant amount of traffic during each clock cycle. Therefore, to move n
pieces of data from one side of the tree to the other requires  (n) time.
Hence, the tree provides a major benefit over the linear array and mesh in terms of
combining information, but it is not well equipped to deal with situations that require
extensive data movement. At this point, it makes sense to consider an architecture that
combines the best features of the tree (fast broadcast, report, and semigroup operations)
with the best features of the mesh (increased numbers of communication links that
provide the capability to move large amounts of data in an efficient fashion). The pyramid
computer is such a machine.
Pyramid
A pyramid of base size n  combines the advantages of both the tree and the mesh
architectures (see Figure 5.19 ). It can be viewed as a set of processors connected as a
4-ary tree (a tree in which every generic node has four children), where at each level the
processors are connected as a two-dimensional mesh. Alternatively, the pyramid can be
thought of as a tapering array of meshes, in which each mesh level is connected to the
preceding and succeeding levels with 4-ary tree-type connections. Thus, the base level of
the pyramid of base size n is a mesh of size n, the next level up is a mesh of size n/4,
and so on until we reach the single processor at the root. A careful count of the number
of processors reveals that a pyramid of base size n contains (4 n- 1)/3 processors. The
root of a pyramid has links only to its four children. Each base processor has links to its
four base-level mesh neighbors and an additional link to a parent. In general, a generic
processor somewhere in the middle of a pyramid is connected to one parent and four
children and has four mesh-connected neighbors. Therefore, the degree of the pyramid
network is 9. The communication diameter of a pyramid of base size n is T(log n),
because a message can be sent from the northwest base processor to the southeast
base processor by traversing 2 log 4 n links, which represents a worst-case scenario.
(This can be done by sending a piece of data upward from the base to the root and then
downward from the root to the base.)
Figure 5.19: A pyramid of base size n can be viewed as a set of processors
connected as a 4-ary tree, where at each level in the pyramid, the processors at that
level are connected as a two-dimensional mesh. Alternatively, it can be thought of as
a tapering array of meshes. The root of a pyramid has links only to its four children.
Each base processor has links to its four base-level mesh neighbors and an
additional link to a parent. In general, a generic processor somewhere in the middle of
a pyramid is connected to one parent and four children and has four mesh-connected
neighbors
Consider the bisection width of a pyramid of base size n. The reader might picture a
plane (a flat geometric object) passing through the pyramid, positioned so that it passes
just next to the root and winds up severing connections between the middle two columns
of the base. We now need to count the number of links that have been broken. There are
n1/2/2 at the base, n1/2/4 at the next level, and so on up the pyramid, for a total of T(n1/2)
such links. Consider passing two planes through the root: one that passes between the
middle two rows of the base and the other that passes through the middle two columns of
the base. This action will result in four pyramids, each of base size n/4, with roots that
were originally the children of the root processor. Therefore, as with the mesh of size n,
the bisection width of a pyramid of base size n is T(n1/2).
Now consider fundamental semigroup and combination-type operations. Such operations
can be performed on a pyramid of base size n in T(log n) time by using tree-type
algorithms, as previously described. However, for algorithms that require extensive data
movement (such as moving T(n) data between halves of the pyramid), the mesh lower
bound of  (n1/2) applies. So, the pyramid combines the advantages of both the tree and
mesh architectures without a net asymptotic increase in the number of processors.
However, one of the reasons that the pyramid has not been more popular in the
commercial marketplace is that laying out a scalable  pyramid in hardware is a difficult
process.
Mesh-of-Trees
We now consider another interconnection network that combines the advantages of the
tree connections with the mesh connections. However, this architecture connects the
processors in a very different way than the pyramid does. Essentially, the mesh-of-trees
is a standard mesh computer with a tree above every row and a tree above every
column, as shown in Figure 5.20 . So, a mesh-of-trees of base size n consists of a mesh
of size n at the base with a tree above each of the n1/2 columns and a tree above each of
the n1/2 rows. Notice that these 2 n1/2 trees are completely disjoint except at the base.
That is, row tree i and column tree j only have base processor Pi,j in common. So, the
mesh-of-trees of base size n has n processors in the base mesh, 2 n1/2-1 processors in
each of the n1/2 row trees, and 2 n1/2-1 processors in each of the n1/2 column trees.
Because the n base processors appear both in the row trees and the column trees, the
mesh-of-trees has a total of 2 n1/2(2 n1/2-1) - n= 3 n - 2 n1/2 processors. Therefore, as
with the pyramid, the number of processors in the entire machine is linear in the number
of base processors.
Figure 5.20: A mesh-of-trees of base size n consists of a mesh of size n at the base,
with a tree above each of the n1/2 columns and a tree above each of the n1/2 rows.
Notice that the trees are completely disjoint except at the base. The mesh-of-trees of
base size n has n processors in the base mesh, 2n1/2-1 processors in each of the
n1/2 row trees, and 2n1/2-1 processors in each of the n1/2 column trees
First, as has been our tradition, let's consider the degree of the network. A generic base
processor is connected to four mesh neighbors, one parent in a row tree, and one parent
in a column tree. Notice that processors along the edge of the mesh have fewer mesh
connections, as previously discussed. The root processor of every tree is connected to
two children, and interior tree nodes are connected to one parent and two children. Note
that leaf processors are mesh processors, so we need not consider them again.
Therefore, the degree of the mesh-of-trees of base size n is 6.
Next, consider the communication diameter of a mesh-of-trees of base size n. Without
loss of generality, assume that base processor Pa,b needs to send a piece of information,
call it x, to base processor Pc,d. Notice that processor Pa,b can use the tree over row a to
send x to base processor Pa,d in O(log n1/2) = O(log n) time. Now, processor Pa,d can use
the tree over column d to send x to base processor Pc,d in O(log n1/2) = O(log n) time.
Therefore, any two base processors can communicate by exploiting one row tree and
one column tree in O(log n) time.
The bisection width of a mesh-of-trees can be determined by passing a plane through the
middle two rows or columns (or both) of the base mesh. The analysis is similar to the
pyramid, where the total number of links severed is T(n1/2).
Therefore, some of the objective measures of the pyramid and mesh-of-trees are similar.
The difference between the two is that in a pyramid, the apex (root of the pyramid) serves
as a bottleneck, whereas for the mesh-of-trees, there is no such bottleneck. In fact, the
mesh-of-trees offers more paths between processors. One might hope that more efficient
algorithms can be designed for the mesh-of-trees than for the pyramid. However, due to
the bisection width, we know that this is not possible for problems that require significant
data movement. For example, for problems such as sorting, in which all data on the left
half of the base mesh might need to move to the right half and vice versa, a lower bound
of  (n/n1/2) =  (n1/2) still holds. One can only hope that problems that require a
moderate amount of data movement can be solved faster than on the pyramid.
Let's first consider the problem of computing a semigroup operation on a set X = [x 1,
x2,…,xn], initially distributed one item per base processor. Within each row
(simultaneously), use the row tree to compute the operation over the set of data that
resides in the row. Once the result is known in the root of a tree, it can be passed down
to all of its leaves (the base processors in the row). In T(log n) time, every base
processor will know the result of applying the semigroup operation to the elements of X
that are stored in its row. Next, perform a semigroup operation (simultaneously) on this
data within each column by using the tree above each column. Notice that when the root
processors of the column trees have the result, they all have the identical final result,
which they can again pass back down to the leaf (base) processors. Therefore, after two
T(log n) time tree-based semigroup operations, all processors know the final answer. As
with the tree and pyramid, this is a time-optimal algorithm. However, the cost of the
algorithm is again T(n log n), which is T(log n) from optimal.
Next, we consider an interesting problem of sorting a reduced amount of data. This
problem surfaces quite often in the middle of a wide variety of algorithms. Formally, we
are given a unique set of data, D =[d 1, d2,…,dn1/2], distributed one per processor along
the first row of the base mesh in a mesh-of-trees such that processor P1,i stores di. We
wish to sort the data so that the ith largest element in D will be stored in processor P1,i.
The method we use will be that of counting sort . That is, for each element d e D, we will
count the number of elements smaller than d to determine the final position of d. To use
this counting sort, we first create a cross product of the data so that each pair ( di, dj) is
stored in some processor, as shown in Figure 5.21 . Notice that because the number of
elements in D is n1/2 we have room in the base mesh to store all n1/2 × n1/2 = n such
pairs. This cross product is created as follows (see Figure 5.22 ). First, use the column
trees to broadcast dj in column j. At the conclusion of this T(log n) time step, every base
processor Pi,j will store a copy of dj. Now, using the row trees, in every row i, broadcast
item di from processor Pi,i to all processors in row i. This operation also requires T(log n)
time. Therefore, after a row and column broadcast, every processor Pi,j will store a copy
of dj (obtained from the column broadcast) and a copy of di (obtained from the row
broadcast). At this point, the creation of the cross product is complete.
Figure 5.21: Creating a cross product of items <d1, d2, d3, d4>. Notice that processor
Pi,j will store a copy of di and dj. That is, every processor in row i will store a copy of di
and every processor in column j will store a copy of dj
Figure 5.22: Sorting a reduced set of data on a mesh-of-trees (only the base mesh is
shown). a) The initial distribution of data consists of a single row of elements. b) The
data after using the column trees to broadcast the data element in every column. c)
The result after using the row trees to broadcast the diagonal elements along every
row. At this point, a cross product of the initial data exists in the base mesh of the
mesh-of-trees. d) The result of performing row rankings of the diagonal element in
each row. This step is accomplished by performing a comparison in the base mesh
followed by a semigroup operation of every row tree. e) The result after performing
the final routing step of the diagonal elements to their proper positions according to
the rankings
Let row i be responsible for determining the rank of element di. Simultaneously, for every
processor Pi,j, set register count  to 1 if dj < d i, and to 0 otherwise. Now use the row trees
to sum the count  registers in every row. Notice that in every row i, the sum r(i)
corresponds to the rank of di, the number of elements of D that precede di. Finally, a
column broadcast is used in every column to broadcast di from processor Pi,r(i)+1  to
processor P1,r(i)+1 , completing the procedure.
The time to create the cross product is T(log n), as is the time to determine the rank of
every entry and the time to broadcast each entry to its final position. Therefore, the
running time of the algorithm is T(log n), which is worst-case optimal for the mesh-of-
trees, due to the T(log n) communication diameter and the fact that d1 and d n2 might
need to change places (processors P1,1 and P1,n1/2 might need to exchange information).
The cost of the algorithm is T(n log n). Notice that the cost is not optimal because T(n1/2)
items can be sorted in T(n1/2 log n) time on a RAM.
Hypercube
The final network model we consider is the hypercube, as shown in Figure 5.23 . The
hypercube presents a topology that provides a low communication diameter and a high
bisection width. The communication diameter is logarithmic in the number of processors,
which allows for fast semigroup and combination-based algorithms. This is the same as
for the tree, pyramid, and mesh-of-trees. However, the bisection width of the hypercube
is linear in the number of processors, which is a significant improvement over the
bisection width for the mesh, pyramid, and mesh-of-trees. Therefore, there is the
possibility of moving large amounts of data quite efficiently.
Figure 5.23: A hypercube of size16 with the processors indexed by the integers
{0,1,…,15}. Pairs of processors are connected if and only if their unique log 216 = 4 bit
strings differ in exactly one position
Formally, a hypercube of size n consists of n processors indexed by the integers
{0,1,…,n —1} , where n >;0 is an integral power of 2. Processors A and B are connected if
and only if  their unique log 2n-bit strings differ in exactly  one position. So, suppose that n
=;8. Then the processor with binary index 011 is connected to three other processors,
namely those with indices 111, 001, and 010.
It is often useful to think of constructing a hypercube in a recursive fashion, as shown in
Figure 5.24 . A hypercube of size n can be constructed from two hyper-cubes of size n/2,
which we refer to as H0 and H1, as follows. Place H0 and H1 side by side, with every
processor labeled according to its log 2(n/2)-bit string. Notice that there are now two
copies of every index: one associated with H0 and one associated with H1. We need to
resolve these conflicts and also to connect H0 and H1 to form a hypercube of size n. To
distinguish the labels of H0 from those of H1, we will add a leading 0 to every index of H0
and add a leading 1 to every index of H 1. Finally, we need to connect the corresponding
nodes of H0 and H1. That is, we need to connect those nodes that differ only in their
(new) leading bit. This completes our construction of a hypercube of size n.
Figure 5.24: Constructing a hypercube of size n from two subcubes each of size n/2.
First, attach elements of subcube A to elements of subcube B with the same index.
Then prepend a 0 to indices of subcube A, and prepend a 1 to all indices of subcube
B. Subcube A is shaded in each diagram for ease of presentation
Based on this construction scheme, the reader should note that the number of
communication links affiliated with every processor must increase as the size of the
network increases. That is, unlike the mesh, tree, pyramid, and mesh-of-trees, the
hypercube is not a fixed-degree network . Specifically, notice that a processor in a
hypercube of size n is labeled with a unique index of log 2 n bits and is, therefore,
connected to exactly log 2 n other processors. So, the degree of a hypercube of size n is
log2 n. Further, in contrast to the mesh, pyramid, tree, and mesh-of-trees, all nodes of a
hypercube are identical with respect to the number of attached neighboring nodes.
Next, we consider the communication diameter of a hypercube of size n. Notice that if
processor 011 needs to send a piece of information to processor 100, one option is for
the piece of information to traverse systematically the path from 011   111   101  
100. This traversal scheme works from the leftmost bit to the rightmost bit, correcting
each bit that differs between the current processor and the destination. Of course, one
could "correct" the logarithmic number of bits in any order. The important point is that one
can send a message from any processor to any other by visiting a sequence of nodes
that must be connected  (by definition of a hypercube) because they differ in exactly one
bit position. Therefore, the communication diameter of a hypercube of size n is log 2 n.
However, unlike the tree and pyramid, multiple minimal-length paths traverse O(log n)
communication links between many pairs of processors. This is an appealing property in
that the hypercube shows promise of avoiding some of the bottlenecks that occurred in
the previously defined network architectures.
Now we consider the bisection width. From the construction procedure described near
the beginning of this section, it is clear that any two disjoint sub-cubes of size n/2 are
connected by exactly n/2 communication links. That is, the bisection width of a hypercube
of size n is T(n). Therefore, we now have the possibility  of being able to sort n pieces of
data in T(log n) time, which would be cost optimal. In fact, in Chapter 4 , "Combinational
Circuits," a bitonic sort algorithm was presented that demonstrated that n pieces of data,
initially distributed one piece per processor on a hypercube of size n, can be sorted in
T(log2 n) time. This result represents a significant improvement over the mesh, tree,
pyramid, and mesh-of-trees. Of course, the drawback is that the hypercube is not a fixed
interconnection network, which makes it very hard to design and produce a generic
hypercube processor and to lay out the machine so that it is expandable (scalable).
We should note that the hypercube is both node- and edge-symmetric in that nodes can
be relabeled so that we can map one index scheme to a new index scheme and preserve
connectivity. This is a nice property and also means that unlike some of the other
architectures, there are no special nodes. That is, there are no special root nodes, edge
nodes, or leaf nodes, and so forth. And yet, we can often use algorithms designed for
other architectures such as meshes or trees, because if we merely ignore the existence
of some of a hypercube's interprocessor connections, we may find the remaining
connections form a mesh, tree, or other parallel architecture (or in some cases, an
"approximation" of another interesting architecture).
In terms of fundamental algorithms on the hypercube, let's consider a semigroup
operation. A description of an algorithm to perform such an operation will illustrate a
variety of algorithmic techniques for the hypercube. In this description, we will use the
term k-dimensional edge  to refer to a set of communication links in the hypercube that
connect processors that differ in the kth bit position of their indices. Without loss of
generality, suppose we want to compute the minimum of X= [x0, x1,…-,xn1], where xi is
initially stored in processor Pi (the processor with its binary label equivalent to i base 10).
The algorithm we describe makes use of the observation that by ignoring some
interprocessor connections, the remainder of the hypercube is a tree.
Consider the simple case of a hypercube of size 16, as shown in Figure 5.25 . In the first
step, we send entries from all processors with a 1 in the most significant bit to their
neighbors that have a 0 in the most significant bit. That is, we use the one-dimensional
edges to pass information. The processors that receive information compute the
minimum of the received value and their element and store this result as a running
minimum. In the next step, we send running minima from all processors with a 1 in their
next-most-significant bit and that received data during the previous step, to their
neighbors with a 0 in that bit position, using the two-dimensional edges. The receiving
processors again compute the minimum of the value received and the value stored. The
third step consists of sending data along the three-dimensional edges and determining
the minima (for processors 0001 and 0000). The final step consists of sending the
running minimum along the four-dimensional edge from processor 0001 to processor
0000, which computes the final result. Therefore, after log 2 n =; log 2 16 = 4 steps, the
final result is known in processor P0 (see Figure 5.26 ).
Figure 5.25: An example of computing a semigroup operation on a hypercube of size
n. For this example, we use minimum as the semigroup operation. In the first step, we
send entries from all processors with a 1 in the most significant bit to their neighbors
that have a 0 in the most significant bit. That is, elements from the right subcube of
size 8 are sent to their neighboring nodes in the left subcube of size 8. The receiving
processors compare the two values and keep the minimum. The algorithm continues
within the left subcube of size 8. After log 216 = 4 transmission-and-compare
operations, the minimum value (1) is known in processor 0000
Figure 5.25
Figure 5.26: Data movement in a semigroup operation on a hypercube. The links of
the hypercube of size 16 are labeled based on the step in which they are used to
move data in the semigroup operation shown in Figure 5.25
If we now wish to distribute the final result to all processors, we can simply reverse the
process, and in the ith step, send the final result along (log 2 n - i +  1)-dimensional edges
from processors with a 0 in the ith bit to those with a 1 in the ith bit. Again, this takes log 2
n =; log 2 16 =4 steps. Clearly, a generalization of this algorithm simply requires
combining data by cycling through the bits of the indices and sending data appropriately
to determine the final result. If desired, this result can be distributed to all processors by
reversing the communication mechanism just described. Therefore, semigroup, reporting,
broadcasting, and general combination-based algorithms can be performed on a
hypercube of size n in T(log n) time.
Coarse-Grained Parallel Computers
In much of the previous discussion, we have made the theoretically pleasant, but often
unrealistic, assumption that we can use as many processors as we like; for example, in
many problems, we assumed n data items were processed by n processors. Because
fine-grained processors are expensive, very few companies manufacture such machines
and relatively few organizations have such machines at their disposal.
Coarse-grained  parallel computing, in which the number of processors q is much smaller
than the number of data items n, is often a more realistic assumption than fine-grained
parallelism. However, efficient coarse-grained algorithms often combine efficient
sequential preprocessing steps with an overall fine-grained algorithmic approach.
Indeed, a common strategy for the development of efficient coarse-grained algorithms is
one we have illustrated earlier in situations for which the number of data items
significantly exceeds the number of processors. This is especially true for problems (for
example, semigroup computations) that produce a single T(1) space result. For such
situations, consider the following scenario.
Each processor runs an efficient sequential algorithm on its share of the data to
obtain a partial solution.
The processors combine their partial solutions to obtain the problem's solution.
If desired, broadcast the problem's solution to all processors.
For problems in which the first step's partial solutions consist of T(1) data per processor,
the second step can use a fine-grained algorithm to combine the partial solutions.
The coarse-grained multicomputer CGM(n,q ) is a model for coarse-grained parallel
computing that has appeared in many recent papers. This is a model of parallel
computing for processing n data items on q processors. Thus, each processor must have
 (n/q) memory locations, sufficient to store the data of the problem. It is customary to
take q=n/q (equivalently, q2 =n). This assumption facilitates many operations. For
example, a gather  operation, in which one data item from each processor is gathered into
a designated processor Pi, requires that the number of items gathered, q, not exceed the
storage capacity  (n/q) of P i.
The processors of a CGM make up a connected graph. That is, any processor can
communicate with any other processor, although exchanging data between processors
may take more than one communication step. This graph could be in the form of a linear
array, mesh, hypercube, pyramid, and so forth; the CGM model can also be realized in a
shared memory (PRAM) machine, in which case we assume that each processor is
directly connected (via the shared memory) to every other processor.
Suppose for a given problem, the best sequential solution runs in Tseq(n) time. In light of
our discussion of speedup (see the following section), the reader should conclude that an
optimal solution to this problem on a CGM(n,q ) runs in time
For many fundamental problems, CGM solutions make use of gather  and scatter
operations. As discussed previously, a gather  operation collects one data item from every
processor into one processor. That is, if the processors are P,…,P, and the data item xt is
initially stored in Pt, we gather { xqi=l.} into the processor pj by bringing copies of each xi to
Pj. A scatter  operation reverses a gather by returning each xi, from Pj to the Pi that
originally contained xi. This is useful, for example, when xi, is a record with components
that have been written into by Pj.
It is perhaps not surprising that gather and scatter operations can be performed on a
CGM (n,q ) in 0(q) time. In the following discussion, we make use of this fact, the proof of
which is beyond the scope of this book. Consider the following algorithm for a minimum
(or, more generally, semigroup) computation on a CGM (n,q ).
CGM (n,q )Minimum Algorithm
Input:  Array X, stored with the subarray 
Output:  Minimum entry of X, known to each processor.
Action:
In parallel, each processor Pj computes
 using the
sequential algorithm discussed earlier. This takes T(n/q) time.1.
Gather {mj}qj=1 into P1. This takes O(q) time. 2.
P1 computes the desired minimum value, M= min {mj}qj=1, using the sequential
algorithm discussed earlier. This takes T(q) time.3.
Broadcast M to all processors. This can be done by a "standard" broadcast
operation in 0(q) time (see Exercises) or by attaching the value of M to each mj
record in T(q) time and scattering the mj records to the processors from which
they came, in 0(q) time.4.
End Minimum
Because q =n/q, the algorithm requires T(n/q) time. This is optimal due to the fact that an
optimal sequential solution requires T(n> time.
Notice that if we assume a particular architecture—such as a PRAM, mesh, hypercube,
or other traditional models —for our CGM (n,q ), the last three steps of the previous
algorithm can be replaced by faster fine-grained analogs (not using gather/scatter
operations). For example, on a mesh, the last three steps of the algorithm can be by fine-
grained mesh semigroup and broadcast operations that run in T(q1/2) time. Doing so,
however, is likely to yield little improvement in the performance of the algorithm, which
would still run in T(n/q) time. An advantage of our previous presentation is that it covers
all parallel architectures that might be used for a CGM.
TeamUnknown Release
0
Chapter 5 - Models of Computation
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Additional Terminology
In this chapter, we have presented an introduction to the models of computation that will
be used throughout the book. We have also presented fundamental algorithms for these
models so that the reader can appreciate some of the fundamental similarities and
differences among these models. We have intentionally avoided using too much
terminology. At this point, however, we feel it is reasonable to introduce some
terminology that will be found in the scientific literature and used as appropriate
throughout the rest of this book.
Flynn's Taxonomy:  In 1966, MJ. Flynn defined a taxonomy of computer
architectures based on the concepts of both instruction stream and data stream.
Briefly, an instruction stream  is defined to be a sequence of instructions performed
by the computer, whereas a data stream  is defined to be the sequence of data items
that are operated on by the instruction stream. Flynn defines both the instruction
stream and the data stream as being either single or multiple, which leads to four
basic categories.
A single-instruction, single-data stream (SISD ) machine consists of a single
processor and a single set of data that is operated on by the processor as it
carries out the sequential set of instructions. The RAM is an SISD model, and
most serial computers fall into this category, including PCs and workstations.
This is the von Neumann model of computing.1.
A single-instruction, multiple-data stream (SIMD ) machine consists of a set of
processors (with local memory), a control unit, and an interconnection network.
The control unit stores the program and broadcasts the instructions, one per
clock cycle, to all processors simultaneously. All processors execute the same
instruction at the same time, but on the contents of their own local memory.
However, through the use of a mask , processors can be in either an active or
inactive state at any time during the execution of a program. Further, these
masks can be determined dynamically. Networks of processors, such as the2.

mesh, pyramid, and hypercube, can be built as SIMD machines. In fact, the
algorithms that we have described so far for these network models have been
described in an SIMD fashion.
A multiple-instruction, single-data stream (MISD ) machine is a model that
doesn't make much sense. One might argue that systolic arrays fall into that
category, but such a discussion is not productive within the context of this book.3.
A multiple-instruction, multiple-data stream (MIMD ) machine typically consists
of a set of processors (with local memory) and an interconnection network. In
contrast to the SIMD model, the MIMD model allows each processor to store
and execute its own program. However, in reality, for multiple processors to
cooperate to solve a given problem, these programs must at least occasionally
synchronize and cooperate. In fact, it is quite common for an algorithm to be
implemented in such a fashion that all processors execute the same program.
This is referred to as the single-program multiple-data (SPMD ) programming
style. Notice that this style is popular because it is typically infeasible to write a
large number of different programs that will be executed simultaneously on
different processors. Most commercially available multiprocessor machines fall
into the MIMD category, including departmental computers that contain multiple
processors and either a physically or virtually "shared memory." Further, most
large codes fall into the SPMD category.4.
Granularity:  Machines can also be classified according to their granularity . That is,
machines can be classified according to the number and/or complexity of their
processors. For example, a commercial machine with a dozen or so very fast (and
complex) processors would be classified as a coarse-grained machine , whereas a
machine with tens of thousands of very simple processors would be classified as a
fine-grained machine . Most commercially available multiprocessor machines fall into
the coarse-grained MIMD category. Of course, such terminology is quite subjective
and may change with time.
We now define some general performance measures. These are common terms
that the user is likely to come across while reading the scientific literature.
Throughput:  The term throughput  refers to the number of results produced per unit
time. This is a critical measure of the effectiveness of our problem-solving
environment, which includes not only our algorithm and computer but also the
quality of any queueing system and other operating system features.
Cost/Work:  Let Tpar(n) represent the length of time that an algorithm with n
processors takes to complete a problem. Then the cost of such a parallel algorithm,
as previously discussed, can be defined as C(n) = nxT (n ). That is, the cost of an
algorithm is defined as the number of potential  instructions that could  be executed
during the running time of the algorithm, which is clearly the product of the running
time and the number of processors. A related term is work , which is typically defined
to be the actual number of instructions performed.
Speedup:  We define speedup  as the ratio between the time taken for the most
efficient  sequential algorithm to perform a task and the time taken for the most
efficient  parallel algorithm to perform the same task on a machine with n processors,
which we denote as 
. The term linear speedup  refers to a speedup of
Sn = n. In general, linear speedup cannot be achieved because the coordination and
cooperation of processors to solve a given problem must take some time. A debate
continues concerning the concept of superlinear speedup , or the situation where
Sn> n.
The question of how superlinear speedup can occur is an interesting one. For
example, if we consider asymptotic analysis, it would seem that a sequential
algorithm could always be written to emulate the parallel algorithm with O(n)
slowdown, which implies that superlinear speedup is not possible. However, assume
that the algorithms are chosen in advance. Then several situations could occur.
First, in a nondeterministic search-type algorithm, a multiprocessor search might
simply get lucky and discover the solution before such an emulation of the algorithm
might. That is, the parallel algorithm has an increased probability of getting lucky in
certain situations. Second, effects of memory hierarchy might come into play. For
example, a set of very lucky or unlucky cache hits could have a drastic effect on
running time.
Efficiency:  The efficiency  of an algorithm is a measure of how well the processors
are utilized. That is, efficiency is the ratio of sequential running time and the cost on
an n-processor machine, which is equivalent to the ratio between the n-processor
speedup and n. So efficiency is given
Amdahl's Law:  While discussing speedup, one should consider Amdahls Law .
Basically, Amdahl's Law states that the maximum speedup achievable by an n-
processor machine is given by S n=1/[f + (1 — f)/n] , where f is the fraction of
operations in the computation that must be performed sequentially. So, for example,
if 5 percent of the operations in a given computation must be performed
sequentially, the speedup can never be greater than 20, regardless of how many
processors are used . That is, a small number of sequential operations can
significantly limit the speedup of an algorithm on a parallel machine. Fortunately,
what Amdahl's Law overlooks is the fact that for many algorithms, the percentage of
required sequential operations decreases as the size of the problem  increases.
Further, it is often the case that as one scales up a parallel machine, scientists often
want to solve larger and larger problems, not just the same problems more
efficiently. That is, it is common enough to find that for a given machine, a scientist
will want to solve the largest problem that fits on that machine (and complain that
the machine isn't just a bit bigger so that they could solve the problem they really
want to consider).
Scalability:  We say that an algorithm is scalable  if the level of parallelism increases
at least linearly with the problem size. We say that an architecture is scalable if the
machine continues to yield the same performance per processor as the number of
processors increases. In general, scalability is important in that it allows users to
solve larger problems in the same amount of time by purchasing a machine with
more processors.
TeamUnknown Release
0
Chapter 5 - Models of Computation
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we discuss a variety of models of computation. These include the
classical RAM model for single-processor computers, as well as several models of
parallel computation, including the PRAM, linear array, mesh, tree, pyramid, hypercube,
and others. For each model of computation, we discuss solutions to fundamental
problems and give analysis of our solutions’ running times. We also discuss, for parallel
models, factors that can limit the efficiency of the model, such as the communication
diameter and the bisection width.
TeamUnknown Release

0
Chapter 5 - Models of Computation
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
The emphasis of this chapter is on introducing the reader to a variety of parallel models
of computation. A nice, relatively concise presentation is given in "Algorithmic
Techniques for Networks of Processors," by R. Miller and Q.F. Stout in the Handbook of
Algorithms and Theory of Computation , M. Atallah, ed., CRC Press, Boca Raton, FL,
1995-1998. A general text targeted at undergraduates that covers algorithms, models,
real machines, and some applications, is Parallel Computing Theory and Practice  by MJ.
Quinn (McGraw-Hill, Inc., New York, 1994). For a book that covers PRAM algorithms at a
graduate level, the reader is referred to An Introduction to Parallel Algorithms  by J. Já Já
(Addison-Wesley, Reading, MA., 1992), whereas advanced undergraduate students or
graduate students interested primarily in mesh and pyramid algorithms might refer to
Parallel Algorithms for Regular Architectures: Meshes and Pyramids  by R. Miller and Q.F.
Stout (The MIT Press, Cambridge, MA, 1996). For the reader interested in a text devoted
to hypercube algorithms, see Hypercube Algorithms for Image Processing and Pattern
Recognition  by S. Ranka and S. Sahni (Springer-Verlag, 1990). A comprehensive parallel
algorithms book that focuses on models related to those presented in this chapter is
Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes  by FT
Leighton (Morgan Kaufmann Publishers, San Mateo, CA, 1992).
Although Amdahl's Law is discussed or mentioned in most texts on parallel algorithms,
we feel it is worth mentioning the original paper, "Validity of the Single Processor
Approach to Achieving Large Scale Computing Capabilities," by G. Amdahl, AFIPS
Conference Proceedings , vol. 30, Thompson Books, pp. 483-485, 1967. Similarly, Flynn's
taxonomy is a standard in texts devoted to parallel computing. The original articles by
Flynn are "Very High-Speed Computing Systems," by MJ. Flynn, Proceedings of the
IEEE , 54 (12), pp. 1901-09, 1966, and "Some Computer Organizations and Their
Effectiveness," by MJ. Flynn, IEEE Transactions on Computers , C-21, pp. 948-60, 1972.
The coarse-grained multicomputer, CGM (n,q ), was introduced in F Dehne, A. Fabri, and
A. Rau-Chaplin, "Scalable Parallel Geometric Algorithms for Multi-computers,"
Proceedings 9th ACM Symposium on Computational Geometry  (1993), pp. 298-307, and

has been used in many subsequent papers (see, for example, F Dehne, ed., special
edition of Algorithmica  24, no. 3-4, 1999, devoted to coarse-grained parallel algorithms).
The proof that gather and scatter algorithms can be performed on a CGM (n,q ) in O(q)
time appears in L. Boxer and R. Miller, "Coarse Grained Gather and Scatter Operations
with Applications," Journal of Parallel and Distributed Computing  64 (2004), 1297-1320.
TeamUnknown Release
0
Chapter 5 - Models of Computation
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
Consider the "star-shaped" architecture shown in Figure 5.27 , which consists of n
processors, labeled from 0 to n—;1, where processor P0 is directly connected to all
other processors, but for i,j >0,i   j, processors Pi and Pj are not directly connected.
Explain why this architecture has a "serial bottleneck" at processor P0. To do this,
consider the time required by a fundamental operation such as computing a
semigroup operation  t=0n-1xi, where xi is stored in processor Pi. Does the star-
shaped configuration seem to be a useful arrangement of processors for parallel
computation?
Figure 5.27: A star-shaped computer of size 61.
Consider an architecture of n processors partitioned into two disjoint subsets, A and
B, each with n/2 processors. Further, assume that each processor in A is joined to
each processor in B, but no pair of processors having both members in A or in B are
joined. See Figure 5.28  for an example.
Can fundamental operations be executed on this architecture faster that on the1.2.

star-shaped architecture described previously T For example, devise an
efficient parallel algorithm for computing a semigroup operation  t=0n-1xi where
xi is stored in processor Pi, on this architecture, and analyze its running time.1.
What is the bisection width of this architecture T What does this imply about the
practicality of this architecture?2.
Define an X-tree to be a tree machine in which neighboring nodes on a level are
connected. That is, each interior node has two additional links, one to each of its left
and right neighbors. Nodes on the outer edge of the tree (with the exception of the
root) have one additional link, to its neighboring node in its level.
What is the communication diameter of an X-tree? Explain. 1.
What is the bisection width of an X-tree? Explain. 2.
Give a lower bound on sorting for the X-tree. Explain. 3.3.
Suppose that we have constructed a CRCW PRAM algorithm to solve prob lem A in
O(t(n) ) time. Now when we begin to consider solutions to problem A on a CREW
PRAM, what do we already know about an upper bound on the running time to solve
this problem on a CREW PRAM? Why?4.
Suppose that we have a CREW PRAM algorithm to solve problem A in T(t (n)) time.
If we now consider a solution to this problem on an EREW PRAM, how does the
CREW PRAM algorithm help us in determining a lower bound on the running time to
solve this problem on an EREW PRAM?5.
Give an asymptotically optimal algorithm to sum n values on a three-dimensional
mesh. Discuss the running time and cost of your algorithm. Give a precise definition
of your model.6.
Give an efficient algorithm to sum n values on a hypercube. 7.
Define a linear array of size n with a bus  to be a one-dimensional mesh of size n
augmented with a single global bus. Every processor is connected to the bus, and in
each unit of time, one processor can write to the bus and all processors can read
from the bus (that is, the bus is a CREW bus).
Give an efficient algorithm to sum n values, initially distributed one per
processor. Discuss the time and cost of your algorithm.1.
Give an efficient algorithm to compute the parallel prefix of n values, initially
distributed one per processor. Discuss the time and cost of your algorithm.2.8.
9.
2.
Show that a pyramid computer with base size n contains ( 4n — ;1)/3 processors.
Hint:  Let n = 4k for integer k = 0, and use mathematical induction on k.9.
Why is it unrealistic to expect to solve an NP-complete problem on the PRAM in
polylogarithmic time using a polynomial number of processors?10.
Show that a gather operation on a linear array of q processors requires  (q)
time in the worst case.1.
Devise an algorithm to gather one data item from each processor of a linear
array of q processors into any one of the processors. Analyze the running time
of your algorithm (if it's efficient, it will take T(q) time). Note this shows that T(q)
is optimal for such an operation, and the O(q) time we have claimed for a
gather operation on a CGM (n,q ) is optimal in the worst case—that is, with
respect to the worst-case architecture.2.
Assume you have algorithms for gather and scatter operations that run in O(q) on a
CGM (n,q ). State and analyze the running time of an efficient algorithm to broadcast
a value from one processor of a CGM (n,q ) to all processors.12.
Figure 5.28: An architecture in which nproces-sors are partitioned into two disjoint
subsets of n/2 processors each
TeamUnknown Release
0
Chapter 6 - Matrix Operations
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
Chapter 6: Matrix Operations
Computational science and engineering (CS&E)  is an emerging discipline that focuses on simulation
and modeling and sits at the intersection of computer science, mathematics, and various disciplinary
areas. CS&E is already being called the third science, complementing theoretical science  and
laboratory science.  Programs in computational science, as the discipline is also referred to, are
widespread at universities and colleges and are being introduced into the K-12 curriculum.
Simulation and modeling has led to breakthroughs in numerous scientific and engineering
disciplines. In fact, numerical simulation has been used to study complex systems that would be too
expensive, time consuming, or dangerous to study by direct (physical) experimentation. The
importance of simulation can be found in "grand challenge" problems in areas such as structural
biology, materials science, high-energy physics, economics, fluid dynamics, and global climate
change, to name a few. In fact, designers of automobiles and airplanes exploit simulation in an effort
to reduce the costs of prototypes, to test models, and to provide alternatives to expensive wind
tunnels. Computational science and engineering is an interdisciplinary subject, uniting computing
(hardware, software, algorithms, and so on) with disciplinary research in biology, chemistry, physics,
and other applied and engineering fields. Because operations on matrices are central to
computational science, we consider the problems of matrix multiplication and Gaussian elimination
on various models of computation.
Matrix Multiplication
Suppose a matrix A has p rows and q columns, which we denote as Ap,q or Ap q . Given matrices Ap,q
and Bq,r , the matrix product of A and B is written informally as C = AXB  and more formally as Cp,r =
Ap,q Bp,r . The element ci, j, that is, the element of C in the ith row and jth column, for 1 = i = p and 1 = j
= r , is defined as the dot product of the ith row of A and the jth column of B, as

Notice that the number of columns of A must be the same as the number of rows of B, because each
entry of the product corresponds to the dot product  of one row of A and one column of B. In fact, to
determine the product of A and B, the dot product of every row of A with every column of B is typically
computed. See Figure 6.1  .
Figure 6.1: An example of matrix multiplication. For example, c 2,3 is the product of the second row
of A (5, 6, 7, 8) and the third column of B (2, 0, 2, 0), which yields 5X2 + 6X0 + 7X2 + 8X0 = 24
A traditional, sequential dot product of two vectors, each of length q, requires q multiplications and q— 1
additions. Therefore, such a sequential operation can be performed in T (q) time. Hence, the p r dot
products, each of length q, used to perform a traditional matrix multiplication can be computed in a
straightforward fashion in T (prq)  time on a RAM. So, the total number of operations performed in a
brute-force matrix multiplication on a RAM, as described, is T (prq).  Such an algorithm follows.
Input:  A p q matrix A and a q r matrix B.
Output:  The matrix product Cp,r = A p,q Bq,r
For 
i
 = 1 to 
p
, do                      {Loop through rows of 
A
 }
   For 
j
 = 1 to 
r
, do                {Loop through columns of 
B
 }
               {Perform the dot product of a row of 
A
 and a column of 
B
}
     C[
i,j
] = 0
     
For
 k = 1 to 
q
, do
         C[
i, j
] = C[
i, j
]+ A[
i,k
] B[
k, j
]
     
End
 For 
k
     
End
 For 
j
End
 For 
i
We now consider matrix multiplication on a variety of models of computation. For simplicity, we will
assume that all matrices are of size n n.
RAM:  A traditional sequential algorithm, as given earlier, will multiply An n Bn n to produce Cn n in T
(n3 ) time. There are better algorithms, however. In fact, due to the importance of matrix
multiplication and its relatively large running time, this problem has been the focus of research for
many years. In 1968, Strassen presented a divide-and-conquer algorithm to perform matrix
multiplication in 0(n2.81 ) time. This result was quite surprising, because it had been previously
conjectured that O (n3 ) operations were required to perform matrix multiplication. Due to the
importance of this problem, research in this area remains quite active. In fact, recently algorithms
have been presented that run in o(n2.81 ) time. Unfortunately, the details of such algorithms are
beyond the scope of this book.
PRAM:  Consider the design of an efficient matrix multiplication algorithm for a CR PRAM. Suppose
you are given a PRAM with n3 processors, where each processor has a unique label, (i, j, k),  where
1 = i,j,k = n are integers. (Notice that processors P1 , … , P 3 can be relabeled as P1,1,1 , … , Pn,n,n
in T (1) time.) One can consider this representation as associating processor Pi,j,k with ai,k. bk,j , the
kth product between the i th row of A and they jth column of B. Notice that this product is one of the
terms that contribute to Ci,j . So, suppose that initially every processor Pi,j,k. computes the result of
ai,k bk.j . After this single step, notice that all T (n3 ) multiplications have been performed. All that
remains is to compute the summation of each dot product's T (n ) terms. This can be done in T
(log n ) time by performing T (n2 ) independent semigroup operations, where the operator is
addition. So, in T (log n ) time, processors Pi,j,k, , k   {1,2,… ,n} , can perform a semigroup
operation to determine the value of cij , which can then be written into the appropriate cell in
memory in constant time. Therefore, the running time of the algorithm is T (log n ), and the total
cost is T (n3 log n ).
Unfortunately, while efficient, this algorithm is not cost-optimal. Therefore, we can consider trying to
reduce the running time by a factor of T (log n ) or the number of processors by a factor of T (log n ).
Because reducing the running time is a difficult challenge, let's consider a CR PRAM with n3 / log 2 n
processors. First, let each processor be responsible for a unique set of T (log n )multiplications. For
example, processor P1 can perform the multiplication operations that processors P1 ,… ,Plog2 n
performed in the previous algorithm, processor P2 can perform the multiplication operations that
processors P1+log 2 n , … ,P2log 2 n performed in the previous algorithm, and so on. Next, each processor
can sum the products it computed earlier in T (log n) time. Finally, in T (log n ) time, each of the n2
values ci,j could be computed by parallel semigroup operations (addition), with each semigroup
operation performed by a group of T (n/log n)  of the T (n3 /log n  ) processors associated with ci,j. The
algorithm follows.
PRAM Matrix Product Algorithm using T (n 3 /log n ) processors
Input:  A p q matrix A and a q r matrix B .
Output:  The matrix product Cp,r = A p,q Bq,r
To simplify our analysis, we assume p = q = r = n.
In parallel, each processor computes its T (log n ) products pi,j,k. = a i,j bj,k .
This takes T (log n )time.
Compute each of the n2 values 
 by parallel semigroup
(addition) operations (as described earlier). This takes T (log n ) time.
Therefore, the running time of the algorithm is T (log n ) and the cost of the algorithm is T (n3 ), which is
optimal when compared to the traditional matrix multiplication algorithm.
Finally, we consider a CR PRAM with n2 processors. The algorithm is straightforward. Every processor
simultaneously computes the result of a distinct entry in matrix C. Notice that every processor
implements a traditional sequential algorithm for multiplying a row of A by a column of B (i.e.,  a dot
product). This is done in T (n ) time, simultaneously for every row and column. Therefore, the n2 entries
of C are determined in T (n ) time with n2 processors, which results in a costoptimal T (n 3 ) operations
algorithm (with respect to the traditional matrix multiplication algorithm).
Mesh:  Consider the problem of determining C = A B,  where A, B,  and C are n n matrices, on a
mesh computer. Let's consider the case in which no processor stores more than one initial entry of
A. Similarly, we assume that no processor stores more than one initial entry of B. Further, we
assume that at the conclusion of the algorithm, no processor stores more than one entry of the
product matrix C.
Initially, we will consider a 2n 2n  mesh, where matrix A is stored in the lower-left quadrant of the mesh,
matrix B is stored in the upper-right quadrant, and matrix C will be produced in the mesh's lower-right
quadrant, as shown in Figure 6.2  . Let's consider the operations necessary to compute the entries of C
in place. That is, let's design an algorithm so that the entries of A and B flow through the lower-right
quadrant of the 2n 2n  mesh and arrive in processors where they can be of use at an appropriate time.
Figure 6.2: Matrix multiplication on a 2n 2n  mesh. Matrix An n initially resides in the lower-left
quadrant and matrix Bn n initially resides in the upper-right quadrant of the mesh. The matrix product
Cn n = A n n Bn n is stored in the lower-right quadrant of the mesh
Consider the first step of the algorithm. Notice that if all processors containing an element of the first
row of A send their entries to the right and all processors containing an entry of the first column of B
simultaneously send their entries down, the processor responsible for c 1,1 will have entries a1,n and bn,1
(see Figures 6.3a  and 6.3b  ). Because a1,n bn,1 is one of the terms necessary to compute c 1,1 , this
partial result can be used to initialize the running sum for c 1,1 in the northwest processor of the lower-
right quadrant. Notice that initially, a1,n and bn,1 represent the only pair of elements that could meet
during the first step and produce a useful result.
Figure 6.3: Data flow for matrix multiplication on a 2n 2n  mesh. The initial distribution of data is
shown in (a). (b) shows step 1, in which the first column of B starts to move down and the first row of
A starts to move right. (c), (d), and (e) show steps 2, 3, and 4, respectively, in which both columns of
B move down and both rows of A move right
Now consider the second step of such an algorithm. Notice that if the elements in row 1 of A move to
the right again, and that if the elements of column 1 of B move down again, then a1,n-1 and bn-1,1 will
meet in the processor responsible for c1,1 , which can add their product to its running sum. In addition,
notice that if the second row of A and the second column of B begin to move to the right and down,
respectively, during this second time step, then the processors responsible for entries c2,1 and c1,2 could
begin to initialize their running sums with a partial result (see Figure 6.3c  ). Continuing with this line of
thinking, notice that in general this algorithm operates so that at time i, the ith row of A and the i th
column of B initiate their journeys to the right and down, respectively. Further, at time i, rows 1 … i -1
and columns 1 … i -1 will continue on their journeys. Eventually, all of the elements of C will be
computed.
Now let's consider the running time of the algorithm. Notice that at time n, the last row of A and the last
column of B begin their journeys. During every subsequent time step, the last row of A will continue to
move one position to the right, and the last column of B will continue to move one position down. At
time 3 n - 2, elements an,1 and b1,n will finally meet in the processor responsible for computing cn,n , the
last element to be computed. Therefore, the running time for this algorithm is T (n ). Is this good?
Consider the fact that in the sequential matrix multiplication algorithm on which our current algorithm is
modeled, every pair of elements (ai,k , bkj ) must be combined. Therefore, it is easy to see that this
algorithm is asymptotically optimal in terms of running time on a mesh of size 4n2 . This is due to the T
(n ) communication diameter of a mesh of size 4n2 . Now consider the total cost of the algorithm.
Because this algorithm runs in T (n ) time on a machine with T (n 2 ) processors, the total cost of the
algorithm is T (n 3 ). Therefore, this algorithm is cost optimal with respect to the traditional sequential
algorithm.
Whereas the previous algorithm is time and cost optimal on a 2n 2n  mesh computer, let's consider a
matrix multiplication algorithm targeted at an n n mesh. Assume that processor Pi,j initially stores
element ai,j of matrix A and element bi,j of matrix B. When the algorithm terminates, processor Pi,j will
store element ci,j of the product matrix C. Because we already have an optimal algorithm for a slightly
expanded mesh, we consider adapting the algorithm just presented to an n n mesh. To do this, we
simply use row and column rotations, as we did when we adapted the selection sort algorithm from the
input-based linear array to run on the traditional linear array. Specifically, to prepare to simulate the
previous algorithm, start by performing a row rotation so that processor Pi,j contains element ai,n-j+1 of
matrix A , followed by a column rotation so that processor Pi,j contains element bn-i+1,j of matrix B (see
Figure 6.4  ).
Figure 6.4: Row and column rotations preprocessing steps for matrix multiplication on an n n matrix.
(a) shows the initial distribution of data. (b) shows the result of a row rotation of A. (c) shows the
result of a column rotation of B
At this point, the strategy described in the previous algorithm can be followed while we make the natural
adjustments to accommodate the necessary rotations to continue moving the data properly. In addition,
the data is starting in the first row and the first column. The details are left to the reader. Notice that the
additional rotations, which can be thought of as serving as a "preprocessing" step, require T (n ) time.
Therefore, the asymptotic analysis of this algorithm results in the same time- and cost-optimal results as
previously discussed.
TeamUnknown Release
0
Chapter 6 - Matrix Operations
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
Gaussian Elimination
The technique of Gaussian elimination  is used widely for such applications as finding the inverse of an n n
matrix and solving a system of n linear equations in n unknowns. In our presentation, we focus on the problem
of finding an inverse matrix.
The n n matrix In , called the identity matrix,  is the matrix in which the entry in row i and column j is
It is well known that for n n matrix A, we have A I = A  and In A = A.  We say an n n matrix A is invertible  (or
nonsingular  )if there is an n n matrix B such that A B = Bx A = I.  If such a matrix B exists, it is called the
inverse of A,  and we write B = A-1 .
We say each of the following is an elementary row operation  on an n n matrix A :
Interchange distinct rows of A (see Figure 6.5  ).
Figure 6.5: Interchange of row 1 and row 3
Multiply (or divide) a row of A by a nonzero constant. That is, for some c   0, replace each element at,j of

row i by cai,j (see Figure 6.6  ).
Figure 6.6: Replace row 1 by 0.2 row 1
Add (or subtract) a constant multiple of row i to (a different) row j . That is, for some constant c , replace
each element aj.k of row j by aj,k +ca i,k (see Figure 6.7  ).
Figure 6.7: Replace row 2 by row 2 + 5 row 1
It is well known that if a sequence s of elementary row operations applied to an n n matrix A transforms A into
In , then the same sequence s of elementary row operations applied to In transforms In into A -1 . Thus, we can
implement an algorithm to find A -1 by finding a sequence s of elementary row operations that transforms the
"augmented matrix" [ A | I n ] to [In | A -1 ].
Consider an example. Let
We can find A -1 as follows. Start with the matrix
The first phase of our procedure is the "Gaussian elimination" phase. One column at a time from left to right,
we perform elementary row operations to create entries of 1 on the diagonal  of A and 0s below the diagonal.
In this example, we use row operations to transform column 1 so that a 1,1 = 1 and a2,1 = a 3,1 =0; then we use
row operations that do not change column 1 but result in a2,2 = 1 and a2,3 = 0; then we use a row operation
that does not change columns 1 or 2 but results in a3,3 = 1. More generally, after Gaussian elimination on An n
, all a i,i = 1,1 = i = n , and all ai,j = 0, 1 = j < i = n. That is, there are 1s along the diagonal and 0s below the
diagonal, as shown in the next example.
Divide row 1 by 5 to obtain
1.
Add 3 times row 1 to row 2, and 3 times row 1 to row 3, to obtain
Notice column 1 now has the desired form. We proceed with Gaussian elimination steps on column 2.2.
Divide row 2 by 0.2 to obtain
3.
Subtract 0.2 times row 2 from row 3 to obtain
Note column 2 now has the desired form.4.
Divide row 3 by -1 to obtain5.
5.
This completes the Gaussian elimination phase of the procedure.
Now we proceed with the "back substitution" phase, in which, for one column at a time from right to left, we
use elementary row operations to eliminate nonzero entries above the diagonal. In a sense, this is more
Gaussian elimination, as we use similar techniques, now creating 0s above the diagonal. We proceed as
follows:
Subtract 0.4 times row 3 from row 1, and 1 times row 3 from row 2, to obtain
1.
Add 0.6 times row 2 to row 1, to obtain
2.
Because the left side of the augmented matrix is now I 3 , the right side is the desired inverse:
This can be verified easily by showing that the products A A -1 and A-1 A both coincide with I3 .
The previous example illustrates our general algorithm for finding the inverse of an n n matrix A. In the
algorithm presented next, we assume that array A[1… n, 1 … r ] is used to represent the matrix we wish to
invert, and the matrix I[1… n, 1… n] is initialized to represent the n n identity matrix. Next, we present a
procedure for either finding the inverse of A or determining that such an inverse does not exist.
1.
{Gaussian elimination phase: create in A an upper triangular matrix, a matrix with 1 on every diagonal
entry and 0 on every entry below the diagonal.}
For i = 1 to n, do
If A[i, i] =  0 and A[m, i] =  0 for all m > i,  conclude that A -1 does not exist and halt the algorithm.
If A[i,i] = 0  and A[m,i]   for some smallest m > i,  interchange rows i and m in the array A and in the
array I .
Now we assume A[i,i]   0. Divide row i of A and row i of I by A [i,i].  That is, let scale = A[i,i]  and then
for j = 1 to n, replace A [ij] by A[i,j] / scale.  (Actually, it suffices to make these replacements for j = i
to n, because the Gaussian elimination has caused A [ij] =  0 for j < i.)  Similarly, for j =  1 to n, replace
I[i,j] by I[ij ]/scale.  Note we now have A[k,k]  = 1 for k = i , and A[m,j]  = 0 if j = i,j = m (0 below the
diagonal in columns indexed less than i).
Now we have A[i,i] =  1. If i < n,  then for r > i, subtract A[r,j] times row i from row r in both the arrays
A and I (this zeroes out the entries in A of column i below the diagonal without destroying the 0s
below the diagonal in columns further to the left). That is,
If 
i < n
, then
  For 
row = i
 + 1 to n
     
factor 
 
 A[row,i]
     For 
col
 = 1 to 
n
      A[
row,col
]
 
A[row,col]-factor  A[i,col]
      
I[row,col] 
 
I[row,col]-factor  I[i,col]
     End For 
col
   End For 
row
End If
{Note we now have 
A[k,k]
 = 1 for 
k < i
, and 
A[m, j]
 = 0  if 
j 
=
 i, j < m
(0 below the diagonal in columns indexed 
=
 i).}
   End For 
i
1.
(Back substitution phase: eliminate the nonzero entries above the diagonal of A. We use zeroingCol  as
both a row and column index; it represents both the column we are "zeroing" off the diagonal, and the
row combined with the current row to create the desired matrix form.)2.
For 
zeroingCol =  n
 downto 2
   For 
row = zeroingCol -1
 downto 1
      
factor 
 
 A[row,zeroingCol]
     For 
col
 = 1 to 
n
       
A[row,col]
 
A[row,col]-factor  A[zeroingCol,col]
       
I[row,col] 
 
I[row,col]-factor  I[zeroingCol,col]
     End For 
col
   End For 
row
End For    
zeroingCol
We now discuss the analysis of Gaussian elimination on sequential and parallel models of computation.
RAM:  A straightforward implementation of the algorithm given earlier on a RAM requires T (n3 ) time in
the worst case, when the matrix inverse exists and is determined. The best-case running time is 0( n ),
when it is determined by examining the first column that an inverse does not exist.
Parallel Models:  We must be careful. For example, it is easy to see how some of our inner loops may
be parallelized, but some of our outer loops seem inherently sequential. Thus, on a PRAM it is easy to
see how to obtain significant speedup over the RAM but perhaps not how to obtain optimal performance.
Further, on distributed memory models such as the mesh, some of the advantages of parallelism may
seem negated by delays needed to broadcast key data values throughout rows or columns of the mesh.
Next, we discuss how the basic algorithm we have presented can be implemented efficiently on various
parallel models.
PRAM of n2 Processors:  Let's assume we are using a PRAM with the EW property. Each decision on
whether to halt, as described in the algorithm, can be performed by a semigroup operation in T (log n )
time. Now consider the situation when the decision is to continue, leading to the results that ai,i = 1 and
at,j = 0 for j < i. A row interchange can be done in T (1) time. Scalar multiplication or division of a row can
be done on a CR PRAM in T (1) time; an ER PRAM requires T (log n ) time, because a broadcast of the
scalar to all processors associated with a row is required.
Notice that the row subtraction of the last step of the Gaussian elimination phase may be done in
parallel; that is, the outer For row loop can be parallelized as there is no sequential dependence between
the rows in its operations; and the inner For col loop parallelizes. As in the scalar multiplication step, the
outer For row  loop executes its operations in T (1) time on a CR PRAM and in T (log n) time on an ER
PRAM. Thus, a straightforward implementation of the Gaussian elimination phase requires T (n log n )
time on a PRAM (CR or ER).
For the back substitution phase, we can similarly parallelize the inner and the intermediate-nested loop to
conclude this phase, which requires 0( n ) time on a CR PRAM and T (n log n ) time on an ER PRAM.
Thus, a straightforward implementation of this algorithm requires T (n log n ) time on an EW PRAM. The
total cost is T (n 3 log n ). Note that relative to the cost of our RAM implementation, the PRAM
implementation of Gaussian elimination to invert a matrix is not optimal. We leave as an exercise the
question of obtaining an optimal implementation of the algorithm on a PRAM.
Mesh of n2 Processors:  As usual, we assume entries of the arrays A and I are distributed among the
processors of the mesh so that the processor Pi,j in row i and column j of the mesh contains both A[i,j]
and I[i,j]
Several of the steps of our general algorithm require communication of data across a row or column of
the mesh. For example, scalar multiplication of a row requires communication of the scalar across the
row. If every processor in the row waits for this communication to finish, the scalar multiplication step
would take T (n ) time. It's easy to see how this would yield a running time of T (n 2 ), which is not
optimal, because the total cost is then T (n 2 n2 ) = T (n 4 ).
We obtain better mesh performance by pipelining  and pivoting.  Notice the following is true of each of the
steps of the inner loops of our algorithm. Once a processor has the data it needs to operate on, its
participation in the current step requires T (1) additional time, after which the processor can proceed to
its participation in the next step of the algorithm, regardless of whether other processors have finished
their work for the current step. (Thus, the instructions are pipelined.) Therefore, if we could be sure that
every processor experiences a total of 0(n) time waiting for data to reach it, it would follow that the
algorithm requires 0( n ) time T (n) time for waits and 0( n ) time for the "active" execution of instructions in
each processor).
However, there is one place where the algorithm as described earlier could have processors that
experience   (1) delays of 0(n) time apiece to receive data: the step that calls conditionally for
exchanging a row of A having a 0 diagonal entry with a row below it having a nonzero entry in the same
column. To ensure this situation does not cost us too much time via frequent occurrence, we modify our
algorithm using the technique of pivoting,  which we will describe now. If processor Pi,i detects that A[i,i] =
0, then Pi,i sends a message down column i to search for the first nonzero A[j,i] withy’ > i. If such a j is
found, rowy is called the pivot row,  and it plays the role similar to that otherwise played by row i: row j is
used for Gaussian elimination in the rows below it (creating 0 entries in the ith column of each such row);
rows between row i and row j (if any) have entries of 0 in column i, hence, they require no row
combination at this stage; and row j "bubbles up" to row i in a wavelike fashion (using both vertical and
horizontal pipelining), whereas row i "bubbles down" to row j, executing the row interchange.
On the other hand, if no such j is found, processor Pi,n broadcasts a message to halt throughout the
mesh.
In this fashion, we pipeline the row interchange step with the following steps of the algorithm to ensure
that each processor spends 0(n) time awaiting data. It follows, as described previously, that we can
compute the inverse of an n n matrix or decide, when appropriate, that it is not invertible, through
Gaussian elimination on an n n mesh in T (n ) time, which is optimal relative to our RAM implementation.
TeamUnknown Release
0
Chapter 6 - Matrix Operations
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Roundoff Error
It should be noted that the Gaussian elimination algorithm is sensitive to roundoff error.
Roundoff error occurs whenever an exact calculation requires more decimal places (or
binary bits) than are actually used for storage of the result. Occasionally, roundoff error
can cause an incorrect conclusion with respect to whether the input matrix has an
inverse, or with respect to which row should be the pivot row. Such a situation could be
caused by an entry that should be 0, computed as having a small nonzero absolute
value. Also, a roundoff error in a small nonzero entry could have a powerfully distorting
effect if the entry becomes a pivot element, because the pivot row is divided by the pivot
element and combined with other rows.
It is tempting to think such problems could be corrected by selecting a small positive
number e and establishing a rule that whenever a step of the algorithm computes an
entry with absolute value less than e, the value of the entry is set to 0. However, such an
approach can create other problems because a nonzero entry in the matrix with an
absolute value less than e may be correct.
Measures used to prevent major errors due to roundoff errors in Gaussian elimination are
beyond the scope of this book. However, a crude test of the accuracy of the matrix B
computed as the inverse of A is to determine the matrix products A × B  and B × A.  If all
entries of both products are sufficiently close to the respective entries of the identity
matrix In to which they correspond, then B is likely a good approximation of A-1.
TeamUnknown Release

0
Chapter 6 - Matrix Operations
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we study the implementation of the fundamental matrix operations, matrix
multiplication, and Gaussian elimination, the latter a popular technique for solving an n ×
n system of linear equations. We give algorithms to solve these problems and discuss
their implementations on several models of computation.
TeamUnknown Release

0
Chapter 6 - Matrix Operations
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
A traditional sequential algorithm to multiply An × n × B n × n runs in T(n3) time. This
algorithm is suggested by the definition of matrix multiplication. However, in 1968, the
paper "Gaussian Elimination Is Not Optimal," by V. Strassen, Numerische Mathematik
14(3), 1969, pp. 354-356, showed that a divide-and-con-quer algorithm could be
exploited to perform matrix multiplication in O(n2.81) time. The mesh matrix algorithm
presented in this chapter is derived from the one presented in Parallel Algorithms for
Regular Architectures  by R. Miller and Q.F. Stout (The MIT Press, Cambridge, Mass.,
1996).
The algorithm we present for Gaussian elimination is a traditional algorithm found in
many introductory textbooks for the mathematical discipline of linear algebra. Its
presentation is similar to that found in Parallel Algorithms for Regular Architectures.
Two additional books that concentrate on algorithms for problems in computational
science are G.S. Almasi and A. Gottlieb's Highly Parallel Computing  (The
Benjamin/Cummings Publishing Company, New York, 1994) and G.W. Stout's High
Performance Computing  (Addison Wesley Publishing Company, New York, 1995).
TeamUnknown Release

0
Chapter 6 - Matrix Operations
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
The PRAM algorithms presented in this chapter for matrix multiplication are simpler
under the assumption of the CR property. Why? In other words, in what step or
steps of our algorithms is there a computational advantage in assuming the CR
property as opposed to the ER property?1.
Give an algorithm for a CR PRAM with n processors that solves the matrix
multiplication problem in T(n2) time.2.
In this chapter, we present a mesh algorithm for computing the product of two n × n
matrices on an n × n  mesh. A somewhat different algorithm for an n × n  mesh can
be given, in which we more closely simulate the algorithm given earlier for a 2n × 2n
mesh. If we compress matrices A and B into 
 sub-meshes, it becomes easy
to simulate the 2n × 2n  mesh algorithm given in this chapter.
Give an algorithm that runs in T(n) time to compress the matrix A, where A is
initially stored so that ai,j is in processor Pi,j, 1 = i = n, 1 = j = n. At the end of
the compression, A should be stored so that processor Pi,j 1 = i = n/2, 1 = j =
n/2, stores ak,m, for k   {2i - 1, 2i}, m   {2j- 1,2j}.  Show that your algorithm is
correct.a.
Give an algorithm that runs in T(n) time to inflate the matrix C, where the initial
storage of the matrix is such that processor 
contains ck,m, for k {2i-n-1,2i-n}, m  {2j-n-  1, 2 j-n}. 2 At the end of the inflation,
processor Pi,j should store ci,j for 1 = i = n, 1 = j = n Show that your algorithm
is correct.b.3.
Show how our algorithm for Gaussian elimination to invert an n × n  matrix can be 4.
5.

implemented on a PRAM of n2/log n processors in T(n log n) time.4.
Show how the array changes (as determined by pipelining, pivoting, and
replacement computations) via our matrix inversion algorithm as implemented on a
3 × 3 mesh for the matrix
That is, you should show the appearance of A at each time step, in which a
processor performs any of the following operations:
Send a unit of data to an adjacent processor (if necessary, after a 0(1) time
decision).
Receive a unit of data from an adjacent processor (if necessary, after a 0(1)
time decision).
Calculate in 0(1) time and store a new value of its entry of A (if necessary, after
a 0(1) time decision).
5.
Devise an efficient algorithm for computing the matrix multiplication Cn × n = A n × n ×
Bn × n on a linear array of n processors, and analyze its running time. You should
make the following assumptions.
The processors P1,…,Pn of the linear array are numbered from left to right. For each
j, 1 = j = n, the jth column of A and the yth column of B are initially stored in Pj.
At the end of the algorithm, for each j, 1 = j = n, the jth column of C is stored in Pj.
Your algorithm may take advantage of the fact that addition is commutative. For
example, if n = 4, your algorithm may compute
rather than using the "usual" order
6.

TeamUnknown Release
0
Chapter 7 - Parallel Prefix
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
Chapter 7: Parallel Prefix
The focus of this chapter is on developing efficient algorithms to perform the parallel prefix computation.
Parallel prefix is a powerful operation that can be used to sum elements, find the minimum or maximum of a
set of data, broadcast values, compress (or compact) data, and much more. We willfind many uses for the
parallel prefix operation as we go through the more advanced chapters of this book. In fact, parallel prefix is
such an important operation that it has been implemented at the lowest levels on many machines and is
typically available to the user as a library call.
Parallel Prefix
First, we provide a definition of parallel prefix. Let X = {x 1 , x2 ,… ,xn } be a set of elements contained in a set Y.
Let   be a binary, associative  operator that is closed  with respect to Y. Recall that the term binary  means that
the operator   takes two operands, say x 1 and x2 , as input. The term closed  means that the result of x 1 ?x2 is
a member of Y. Recall that associative  means that the operator obeys the relation
(The reader should note that we do not  require to be commutative.  That is, we do not require xi   xj to be equal
to xj   xi .)
The result of x1   x2  … xk is referred to as the k th prefix. The computation of all n prefixes, x , x   x   , x   x
  <8> x … , x x2  …   x, is referred to as r ’1 ‘12 ’12 3 , ‘1 2 n1 the parallel prefix computation.  Another common
term for this operation is scan.  Because parallel prefix can be performed quite simply on a sequential machine
by making a single pass through the data, it is also sometimes referred to as a sweep  operation (an operation
that can be performed by sweeping  through the data).

The operator × is typically a unit-time operator, that is, an operator that requires 0(1) time to perform. Sample
operators include addition (+), multiplication (), MIN, MAX, AND, OR, and XOR.
Lower Bound:  The number of operations required to perform a complete parallel prefix is Q(n),  because
the nth prefix involves operating on all n values.
RAM Algorithm:  Let's consider a straightforward sequential algorithm for computing the n prefixes p1 , p2 ,
… , pn , where p 1 =x1 and p i+1 = p i   x i+1 for i G {1,2,… , n — 1  }. The algorithm follows.
 p
1
 = x
1
                    {A constant time assignment}
 For i = 1 to n-1, do                {A linear time scan through the elements}
   p
i+ 1
 = P
i
 
 
. X
i+1
            {A constant time operation}
 End For
Because the running time of the sequential parallel prefix algorithm is dominated by the work done within the
loop, it is easy to see that this algorithm runs in 0( n ) time. Further, this algorithm is optimal, to within a constant
factor, because Q.(n)  time is required to solve this problem (see Figures (Figure 7.1  ) and (Figure 7.2  .)
Figure 7.1: An example of parallel prefix on a set X of 6 items. The operation   is addition. The resulting
prefixsums are given in array P
Figure 7.2: An example of parallel prefix on a set X of 6 items. The operation   is addition. The resulting
prefixS are given in array P
Parallel Algorithms
When we consider parallel models of computation, we will assume that the data is stored initially in a
contiguous fashion. That is, we assume that data is stored in contiguous memory locations in the shared
memory of a PRAM or in contiguous processors on a distributed memory machine. Note that this situation is
analogous to the one just discussed for the RAM  in that we assume the input is an array  of data.
Parallel Prefix on the PRAM
The first parallel model of computation we consider is the PRAM. In this section, we will use the term segment
to refer to a nonempty subset of consecutively indexed entries of an array. We denote a segment covering
entries i through j i = j , as Si j. Using this terminology, we can say that the parallel prefix problem requires the
computation of prefix values for all n segments, S1 1 , S12 , … ,S1n .
The first algorithm we present is fairly nave. Given an input set of data, X = {x1,x 2 ,… ,xn } and a set of n
processors, P1, … , Pn , let processor Pi be associated with data item xi . The algorithm operates by recursive
doubling, that is, by combining pairs of elements, then pairs of pairs, and so forth. So, for example, S is
computed, in sequence, as x 9 , then x 8   x 9 then x6   x 7   x 8   x9 , then x 2   x , 3   x ,  x4   x 5 x 6   x 7
  x 8   x 9 , and finally as x 1   x 2   x   3   x 4   x  5   x 5   x 6   x 7   x 8 x   x 9 . Specifically, the
algorithm initially combines the elements x 8 and x 9 . In the next step, it combines pairs of two items, namely,
the result of x 8   x9 with the result of x6   x 7 . In the third step, it combines pairs of four items, namely, x 2   x
3   x 4   x 5 and x 6   x 7   x 8   x 9 , and so on.
We now consider another example. Notationally, let x i …  x j be denoted as [ Xi - x j ,]. Now consider the order
of computation for S 1,19 The computation sequence consists of x 19 , then [ x 18 —x 19 ], then [ x 16 —x 19 ], then
[x 12 —x 19 ], then [ x 4 —x 19 ], and finally [x1 - x 19 ]. The algorithm follows. (See the example shown in Figure
7.3 .)
   For i = 1 to n, do in parallel
     p
i
.prefix = x
i
;
     p
i
 first_in_segment = i;
  End For
  For i = 2 to n, do in parallel
                            {Compute the ith prefix by repeated doubling of the
                              length of the segment over which it is computed}
   While pi first_in _segment > 1, do
    j = pi .first _ in _ segment - 1;
    p
i
.prefix = p
j
.prefix
 
. p
i
.prefix;
    p
i
.first _in _segment = p
j
.first_in _segment;
   End While
 End For
Due to the recursive doubling nature of the algorithm, the running time is 0(log n). The reader is advised to go
through the algorithm carefully with an example. Notice that in every iteration of the algorithm, the number of
terms in every incomplete prefix will double. In fact, with a little work, one can determine that the ith prefix will be
complete after log 2 i iterations.
Figure 7.3: A recursive doubling algorithm to compute the parallel prefix of 11 values on a PRAM in which
each processor is responsible for one data item. The algorithm requires [log2 11 = 4 parallel steps
We now consider a version of parallel prefix with the same time and processor bounds but with different
algorithmic characteristics. The principle of this algorithm is similar to that of the combine operation in
MergeSort. Initially, we "compute" single prefix values x , x ,… , x . In the next step, we combine the single
prefix values to determine prefix values of pairs, resulting in the determination of < [X 1 —x 2 ],[x 3 -x 4 ],… ,[xh1 -
xn ]>. Next, we combine pairs of prefix values to determine prefix values of pairs of pairs, which results in the
determination of <[x 1 —x 4 ],[x 5 —x 8 ],… ,[xn-3 —xn ]>, and so forth. The algorithm continues for log2 n
iterations, at which point all prefix values have been determined for seg-ments that have lengths that are
powers of 2 or that end at x . See(Figure 7.4  ) for an example.
Figure 7.4: An example of computing parallel prefix by continually combining results of disjoint pairs of
items. The operation   used in this example is addition. Notice that the algorithm requires \log 111 = 4
steps. At the conclusion ofstep 1, we have computed \x 1 -x2 ],[x 3 -x4 2 ,[x5 -x6 ],[x 7 -x8 ],[x 9 -x10 ],x11 .At the
end of step 2, we have computed \x 1 —x 4 ],[x 5 —x 8 ],[x 9 —x 11 . At the endof step 3,we have computed\x 1
—x8 ],[x 9 —x 11 ]. At the end of step 4, we have computed x 1 -x11
In an additional   (log n ) time, in parallel each processor Pi can build up the prefix [ x 1 — xi ] by a process that
mimics the construction of the value i as a string of binary bits, from the prefix values computed in previous
steps.
Notice that the cost of either algorithm, which is a product of the running time and number of available
processors, is   (n log n). Unfortunately, this is not optimal because we know from the running time of the RAM
algorithm that this problem can be solved with   (n ) operations.
Now let's consider options for developing a time- and cost-optimal PRAM algorithm for performing a parallel
prefix. With respect to the algorithm just introduced, we can either try to reduce the running time from   (log n)
to   (1),which is unlikely, or reduce the number of processors from n to   (n /log n) while retaining the   (log n)
running time. The latter approach is the one we will take. This approach is similar to that taken earlier in the
book when we introduced a time-and cost-optimal PRAM algorithm for computing a semigroup operation. That
is, we let each processor assume responsibility for a logarithmic number of data items. Initially, each processor
sequentially computes the parallel prefix over its set of   (log n) items. A global prefix is then computed over
these 0( n /log n) final, local prefix results. Finally, each processor uses the global prefix associated with the
previous processor to update each of its   (log n) prefix values. The algorithm follows.(See the example shown
in Figure 7.5  .
Step 1:
for
 do in parallel
for j = 2 to log 2 , do
End For i ;
Comment:  After step 1,processor P1 has the correct final prefix values stored for the first log 2 n prefix
terms. Similarly, processor P2 now knows the (local) prefix values of the log 2 n entries stored in processor
P 2 , and so forth. In fact, every processor Pi stores 
 , the prefix computed over the
segment of thearray X indexed by 
 , for all j   {1,2,… ,log 2 n }.
Step 2:  Compute the global prefixes over the n/log 2 n final prefix values, currently stored one per
processor. Let
Comment:  Note that ri is a prefix over the segment of the array X indexed by1 … i log 2 n . This prefix
computation over n/log 2 n terms is computed in 0llog( n /log n )) =   (log n ) time by the previous algorithm
because the step uses one piece of data stored in each of the n/log 2 n processors.
Step 3:  The final stage of the algorithm consists of distributing, within each processor, the final prefix value
determined by the previous processor.
For i = 2 to 
 , processors P i do in parallel
For j=(i - 1) log 2 n + 1 to i log 2 n, do
pj = ri-1   pj
End For i
End Parallel
Comment:  Note that pj has the desired final value, as it is now calculated over the segment of X indexed
1, … , j.
Figure 7.5: An example of computing the parallel prefix on a PRAM with n/log n) processors. In this
example, we are given n = 16 data items, the operation is addition, there are log 2 n = 4 processors, and
each processor is responsible for n/log 2 n = 16/4 = 4 data items
Mesh
In this section, we consider the problem of computing the parallel prefix on a mesh computer. As discussed
earlier, when considering an operation that involves an ordering imposed on the data, we must first consider an
ordering of the processors. In this section, we will consider a simple row-major ordering  of the processors, as
shown in(Figure 7.6  .)Formally, the row-major index of processor 
 .
Figure 7.6: The row-major index scheme imposed on a mesh of size 16
The input to our parallel prefix problem consists of a data set X = {x 1 , x2 , … , xn },distributed one item per
processor on an n 1/2 n 1/2 mesh. That is, processor Pi (denoted by its row-major index) initially contains xi , 1=
i= n. When the algorithm terminates, processor Pi will contain the ith prefix x1  …  xi . We describe the
algorithm in terms of mesh operations that we developed earlier in the book.
First, perform a row rotation within every row. At the conclusion of this rotation, the rightmost processor in every
row knows the final prefix value of the contiguous subset of elements of X in its row. Notice that this step is
similar to step 1 of the PRAM algorithm just described, in which every processor computes the prefix of entries
initially stored in its processor. Next, using only the processors in the rightmost column, perform a column
rotation to determine the parallel prefix of these partial results. Again, note that this step is similar to step 2 of
the PRAM algorithm, which computes the global parallel prefix of the partial results determined in step 1.
At this point, notice that the rightmost processors in every row contain their correct final answers. Furthermore,
the value stored in the rightmost processor of row i (let's call this value ri ) needs to be applied to all of the
partial prefix values determined by the processors (during step 1) in the row with index i + 1.  This can be done
by first moving the appropriate prefix values ri determined at the end of step 2 down one processor (from the
rightmost processor in row i to the rightmost processor in row i + 1). Once this is done, every row (with the
exception of the first row) can perform a broadcast from the rightmost processor in the row to all other
processors in the row, so that all processors in the row i + 1 can apply ri appropriately.
Therefore, the algorithm consists of a row rotation, a column rotation, a communication step between
neighboring processors, and a final row broadcast. Each of these steps can be performed in 0(n 1/ 2 ) time on a
mesh of size n. In fact, because the rotations take   (n 1/2 ) time, the running time of the algorithm is   (n 1/2
).Of course, we are now presented with what is becoming a routine question, namely, "How good is this
algorithm?" Because the mesh of size n has a   (n 1/2 ) communication diameter, and because every pair of
data elements is required for the determination of the wth prefix, we can conclude that the running time is
optimal for this architecture. Now consider the cost. The algorithm requires   (n 1/2 ) time, using a set of   (n)
processors, which results in a cost of   (n 3/2 ). We know that only   (n) operations are required, so we can
conclude that this is not cost optimal.
This brings us to one of our favorite questions: can we design an algorithm that is more cost effective than our
current algorithm? The major limitation for the mesh, in this case, is the communication diameter. That is, there
is no inherent problem with the bisection width. To reduce the communication diameter, we must reduce the
size of the mesh. This will have the effect of increasing the number of data elements for which each processor
is responsible, including the number of input elements, the number of final results, and the number of
intermediate results.
Notice that at the extreme, we could consider a mesh of size 1,or a RAM. The algorithm would run in a very
slow   (n ) time, but it would also have an optimal cost of   (n ).However, this is not quite what we envisioned
when we thought about reducing the size of a mesh. In fact, consider keeping the cost of the mesh optimal but
improving the running time from that of a fine-grained mesh. In such a case, we want to balance the
communication diameter with the amount of work each processor must perform. Given an n 1/ 3 n 1/3 mesh,
notice that each of these n2/3 processors would store n 1/3 elements of X and would be responsible for storing n
1/3 final prefix results. This is similar to the PRAM algorithm in which we required every processor to be
responsible for   (log n) input elements and final results.
So, let's consider a mesh of size n2/3 (i.e.,  a mesh of size n 1/3 n 1/3 ), where each processor initially stores n 1/3
entries of X The algorithm follows the time-and cost-optimal PRAM algorithm presented in the previous section  ,
combined with the global operations and techniques presented in the non-optimal n 1/2 n 1/2 mesh algorithm just
presented. First, every processor computes the prefix of its n 1/3 entries in   (n 1/3 ) time by the standard
sequential (RAM) algorithm. Now, consider the final restricted prefix value in each of the n2/3 processors. The
previous (non-optimal) mesh algorithm can be applied to these n2/3 entries, stored one per processor on the n
1/3 n 1/3 mesh. Because this mesh algorithm runs in time proportional to the communication diameter of the
mesh, this step will take   (n 1/3 ) time. At the conclusion of this step, every processor will now have to obtain
the previous prefix value and go through and determine each of its final n 1/3 results, as we did in the PRAM
algorithm. Clearly, this can be done in   (n 1/3 ) time. Therefore, the running time of the algorithm is   (n 1/3 ).
This is because we balanced the time required for data movement with the time required for sequential
computing.
The algorithm runs in   (n 1/3 ) time on a machine with   (n 2/3 ) processors. Therefore, the cost of the
algorithm is   (n 1/3 n2/3 ) =   (n ), which is optimal.
Hypercube
In this section, we consider the problem of computing the parallel prefix on a hypercube. As with the mesh,
when considering an operation that involves an ordering imposed on the data, we must first consider an
ordering of the processors. In this section, we assume that the data set X = {x 0 , 1 … ,xn-1 } is distributed so that
processor Pi initially contains data item xt . Notice that we have changed the indexing of the set X from [1, … ,n],
which was used for the RAM, mesh, and PRAM, to [0,1,. … ,n—1]. The reason we did this was to accommodate
the natural indexing of a hypercube of size n, in which the log 2 M-bit indices are in the range of [0,1, … ,n — 1].
(Recall that two processors are connected if and only if  their binary addresses differ in exactly one bit.) So, we
assume that every processor Pi initially contains data item x , and at the conclusion of the algorithm, every
processor Pi will store the zth prefix, x0  …  x.
The procedure we present is similar to the recursive doubling algorithm we presented earlier in connection with
an efficient hypercube broadcasting algorithm. The algorithm operates by cycling through the log 2 n bits of the
processor indices. At iteration i, every processor determines the prefix for the subhypercube that it is in with
respect to the i least significant bits of its index. In addition, every processor uses this partial information, as
appropriate, to compute its required prefix value. The algorithm follows (see (Figure 7.7  ))
Input:  Processor Pi contains data element x i 0= i= n-1.
Output:  Processor P i contains the ith prefix x0  …  xi
In Parallel, every processor Pi does the following:
 subcube_ prefix = x
i
 {prefix for current subcube}
 processor _ prefix = x
i
 {prefix of desired result}
              {lsb=least significant bit and msb=most significant bit}
    For b = lsb to msb, do
             {In this loop, we consider the binary processor indices
                    from the rightmost bit to the leftmost bit.}
     send subcube_prefix to       b-neighbor
     receive temp_prefix from b-neighbor
     If the b
th
 bit of processor Pi   is a 1, then
      processor_prefix = temp_prefix 
 
 processor_prefix
      subcube_prefix = temp_prefix 
 
 subcube_prefix
     Else
       subcube_ prefix = subcube_ prefix 
 
temp _         prefix  {We compute
                subcube_prefix differently than in the previous case,
                         because 
 
 need not be commutative.}
     End If
Figure 7.7: An example of computing the parallel prefix on a hypercube of size 8 with the operation of
addition. The indexing of the hypercube is given in binary representation in (a).In (b),the initial set of data
items is presented. In (c),(d), and (e), we show the results after the first, second, and third steps of the
algorithm, respectively. Processor prefix values are shown large in (c),(d), and (e);subcube prefix values are
small
Analysis
The analysis of this algorithm is fairly straightforward. Notice that the n processors are uniquely indexed with
log2 n bits. The algorithm iterates over these bits, each time performing 0(1) operations (sending/receiving data
over a link and performing a fixed number of unit-time operations on the contents of local memory). Therefore,
given n elements initially distributed one per processor on a hypercube of size n, the running time of the
algorithm is   (log n). Because the communication diameter of a hypercube of size n is  (log n), the algorithm
is optimal for this architecture. However, the cost of the algorithm is   (n log n),  which is not optimal. To reduce
the cost to   (n ), we might consider reducing the number of processors from n to n/log 2 n while still
maintaining a running time of   (log n). We leave this problem as an exercise.
Coarse-Grained Multicomputer
By making use of efficient gather and scatter operations, one may modify the algorithm presented previously for
the mesh to obtain an algorithm for the parallel prefix computation on a CGM (n, q)  that runs in optimal   (n/q)
time. See the Exercises, where a more precise statement of the problem is given.
TeamUnknown Release
0
Chapter 7 - Parallel Prefix
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Application: Maximum Sum Subsequence
In this section, we consider an application of the parallel prefix computation. The problem
we consider is that of determining a subsequence  of a data set that sums to the
maximum value with respect to any subsequence of the data set. Formally, we are given
a sequence X = (x 0, x1, …, xn-1, and we are required to find (not necessarily distinct)
indices u and v, u = v, such that the subsequence (xu, xu+1,…,xv) has the largest possible
sum, xu+ xu+1+… + xv, among all possible segments of X.
We should first make an observation. Notice that if all the elements of X are nonnegative,
then the problem is trivial, because the entire sequence represents the solution. Similarly,
if all elements of X are nonpositive, an empty subsequence is the solution, because, by
convention, the sum of the elements of an empty set of numbers is 0. So, this problem is
interesting only when positive and negative values are allowed. This is the case we now
consider for several models of computation.
RAM
The lower bound to solve this problem on a RAM is O(n), because if any one element is
not examined, it is possible that an incorrect solution may be obtained. We will now
attempt to develop an optimal  (n) time solution to this problem. Consider the situation of
scanning the list from the first element to the last while maintaining some basic
information about the maximum subsequence observed and the contribution that the
current element can make to the current subsequence under investigation. A first draft of
the algorithm follows.
Solve the problem for ( x0 x1…xi-1). One can think of this as either a recursive or
iterative step.1.
Extend the solution to include the next element, xi. Notice that the maximum sum in
(x0 x1,…,xi). is the maximum of
a.2.

the sum of a maximum sum subsequence in [ x0, x1,…,xi-1),] referred to as
Global_Max,  anda.
the sum of a subsequence ending with xt, referred to as Current_Max. b.2.
The details of the algorithm are straightforward. (Also see the example presented
in(Figure 7.8 .)
  Global _ Max   x
0
  u 
 
0                  {Start index of global max subsequence}
  v 
 
0                   {End index of global max subsequence}
  Current_ Max 
 
  x
  q 
 
0                 {Initialize index of current subsequence}
 For i = 1 to n 
 
1, do                     {Traverse list}
If Current_ Max 
=
0 Then
   Current_ Max 
 
Current _ Max +x
i
  Else
   Current_ Max   
 
x
i
   q 
 
 i                 {Reset index of current subsequence}
  End Else
  If Current _ Max > Global _ Max Then
   Global _ Max 
 
Current _ Max
   u 
 
 q
   v 
 
 i
  End If
 End For
Figure 7.8: An example of the maximum sum subsequence peoblem
The five initialization steps each take  (1) time. Each pass through the For loop also
takes  (1 time. Because the loop is performed  (n) times, it follows that the running time
of the algorithm is  (n, which is optimal, as all n entries of the input array X must be
examined.
PRAM
Consider an efficient solution to the maximum sum subsequence problem for the PRAM.
Let's attempt to design a PRAM algorithm that is efficient in its running time and optimal
in its use of resources (cost optimal). Based on our previous experience with designing
cost-effective PRAM algorithms, it makes sense to target a  (log n) time algorithm on a
machine with  (n)/log n) processors. Such an algorithm would be time and cost optimal.
Suppose we first compute the parallel prefixsums S = {s 0, s1, …,sn-1} of X = {x 0, x1,…,xn-
1}, where si=x0 … xi. This can be done in  (log n) time by the cost-optimal parallel
prefix algorithm presented in the previous section . Next, compute the parallel postfix
maximum of S  so that for each index i, the maximum sj, j= i, is determined, along with the
valuey. (The parallel postfix  computation is similar to the parallel prefix computation:
given data values { y0,…,yn-1}, the parallel postfix computation using the operator  
computes the values y0 y1 … yn-1, y1 2 …  yn-1, y2 …  yn-1…, yn-2 yn-1, yn-1) So,
in this case, we compute the parallel postfix maximum of S, which, because the
maximum operator is commutative, is equivalent to computing the parallel prefix
maximum of the set S listed in reverse order, |SM _1, sn _2,…,50. Let mt denote the value
of the postfix-max at position i, and let at be the associated index (sa= max\s.,s. +1,…,sn1).
This parallel postfix is computed in  (log n) time by the algorithm presented in the
previous section .
Next, for each i, compute b i =m i.—s i.+xi.,the maximum prefix value of any-thing to the
right (in other words, with a higher index) minus the prefix sum plus the current value.
(Note that xi must be added back in because it appears in term mi as well as in term si.)
This operation can be performed in  (log n) time by having each processor (sequentially)
compute the value of b for each of its  (log n) entries. Finally, the solution corresponds to
the maximum of the bi s, where u is the index of the position where the maximum of the
bi s is found and v = a u. This final step can be computed by a semigroup operation in
 (log n)time.
Therefore, the algorithm runs in optimal  (log n) time on a PRAM with n/log 2 n
processors, which yields an optimal cost of  (n).
We now give an example for this problem. Consider the input sequence X = [—
3,5,2,—1,-4,8,10,— 2. ] The parallel prefix sum of X is S = (-3,2,4,3,-1,7,17,15).
m0=17
a0=6
b0=17-(-3)+(-3)=17
m1=17
a1=6
b1=17-2+5=20
m2=17
a2=6
b2=17-4+2=15
m3=17
a3=6
b3=17-3+(-1)=13
m4=17
a4=6
b4=17-(-1)+(-4)=14
m5=17
a5=6
b5=17-7+8=18
m6=17
a6=6
b6=17-17+10=10
m7=17
a7=6
b7=17-15+2=-2
As the example shows, we have a maximum subsequence sum of b1= 20. This
corresponds to u = 1 and v = a1 = 6,  or the subsequence (5,2,-1,-4,8,10). It is also
interesting to observe (for any doubters) that the maximum sum subsequence for this
example is a subsequence that contains positive and negative terms.
Mesh
We now consider a mesh. Notice that an optimal PRAM algorithm for solving the
maximum sum subsequence problem relies on a parallel prefix, a parallel postfix, a
semigroup operation, and some local unit-time computations. Also notice that a
semigroup computation can be implemented via a parallel prefix computation. Therefore,
the maximum sum subsequence problem can be solved via three parallel prefix
operations (one, the parallel "postfix" computation, that runs in reverse order) and some
local computations. Therefore, in designing an algorithm for the mesh, we can simply
follow the general guidelines of the PRAM algorithm while implementing the appropriate
mesh algorithms (in this case, predominantly parallel prefix) in an efficient manner. So,
we know that we can solve the maximum sum subsequence problem in  (n1/3) time on a
mesh of size n2/3 (an n1/3 × n1/3 mesh). Because this algorithm requires  (n1/3) time on a
machine with n2/3 processors, we know that the cost is  (n1/3 × n2/3) =  (n), which is
optimal. Further, as discussed previously, this is the minimal running time on a mesh for
a cost-optimal solution.
TeamUnknown Release
0
Chapter 7 - Parallel Prefix
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Array Packing
In this section, we consider an interesting problem that results in a global rearrangement
of data. The problem consists of taking an input data set, in which a subset of the items
is marked,  and rearranging the data set so that all of the marked items precede all of the
unmarked items. Formally, we are given an array X of items. Each item has an
associated label field that is initially set to one of two values: marked  or unmarked.  The
task is to pack  the items so that all of the marked  items appear before all of the
unmarked  items in the array. Notice that this problem is equivalent to sorting a set of 0s
and 1s. In fact, if we consider 0 to represent marked  and 1 to represent unmarked,  this
problem is equivalent to sorting a set of 0s and 1s into nondecreasing order (all 0s
preceding all 1s).
RAM
The first model of computation that we consider is the RAM. Because this problem is
equivalent to sorting a set of 0s and 1s, we could solve this problem quite simply in O(n
log n)  time by any one of a number of O(n log n) )-time worst-case sorting routines.
However, we know something about the data (the restricted nature of the input), so we
should consider an alternative to a general sorting routine. In this case, we know that the
keys of the items to be sorted can take on only one of two values. Using this information,
we can consider scan-based sorts such as counting sort  or radix sort.
Consider counting sort. If we are sorting an array of n entries, we could simply make one
pass through the array and count the number of 0s and the number of 1s. We could then
make another pass through and write out the appropriate number of 0s, followed by the
appropriate number of 1s. The situation is slightly more complicated if the keys are in
larger records. In such a case, we could create two linked lists (dynamic allocation) and
then traverse the array element by element. As we encounter each element in the array,
we create and initialize a record with the pertinent information and add it onto the head of
either the 0s list or the 1s list. This traversal is complete in  (n) time. We can then scan

through the 0s list, element by element, and write the pertinent information into the next
available place in the array. We then do the same with the 1s list. Again, this step takes
 (n) time, and hence the algorithm is complete in asymptotically optimal  (n) time. The
reader should observe that this algorithm is closely related to the BinSort algorithm
discussed in Chapter 1 , "Asymptotic Analysis."
Suppose we are given an array of n complex entries (that is, records), and we are
required to perform array packing in place. That is, suppose that the space requirements
in the machine are such that we cannot duplicate more than some fixed number of items.
In this case, we can use the array-based Partition routine from QuickSort (see Chapter 9 ,
"Divide and Conquer" to rearrange the items. This partition routine is implemented by
considering one index L that moves from the beginning to the end of the array and
another index R that moves from the end to the beginning of the array. Index L stops
when it encounters an unmarked  item, whereas index R stops when it encounters a
marked  item. When both L and R have found an out-of-place item, and L precedes R in
the array, the items are swapped and the search continues. When L does not precede R,
the algorithm terminates. The running time of the algorithm is linear in the number of
items in the array. That is, the running time is  (n).
PRAM
Now consider the PRAM. As with the maximum sum subsequence problem, we realize
that to obtain an efficient and cost-effective algorithm, we should try to develop an
algorithm that runs in  (log n) time using only  (n/log n) processors. This problem is
solved easily using a parallel prefix sum to determine the rank of each 0 with respect to
all 0s and the rank of each 1 with respect to all 1s. That is, suppose we first determine for
each 0, the number of 0s that precede it. Similarly, suppose we determine for each 1, the
number of 1s that precede it. Further, assume that the total number of 0s is computed as
part of the process of ranking the 0s. Then during a write stage, every 0 can be written to
its proper location, the index of which is one more than the number of 0s that precede it.
Also, during this write state, every 1 can be written to its proper location, the index of
which is one plus the number of 1s that precede it plus the number of 0s (that also
precede it).
Let's consider the running time of such an algorithm. Given a PRAM with  (n/log n)
processors, the parallel prefix computation can be performed in  (log n)  time, as
previously described. Along with this computation, the total number of 0s is easily
determined in an additional  (log n)  time. Therefore, the write stage of the algorithm can
be performed in  (log n)  time (each processor is responsible for writing out  (log n)
items). Hence, the total running time of the algorithm is  (log n),  and the cost of the
algorithm on a machine with  (n/log n)  processors is  (log n  × n/log n)  =  (n), which is
optimal. It is important to note that this algorithm can be adapted easily to sort a set of
values chosen from a constant size set. In fact, the algorithm can be adapted easily to
sort records, where each key is chosen from a set of constant size.
Network Models
Now, let's consider the problem of array packing for the general network model. Suppose
one simply cares about sorting the data set, which consists of 0s and 1s. Then the
algorithm is straightforward. Using either a semigroup operation or a parallel prefix
computation, determine the total number of 0s and 1s. These values are then broadcast
to all processors. Assume there are k 0s in the set. Then all processors Pi, i = k, record
their final result as 0, whereas all other processors record their final result as 1. This
results in all 0s appearing before all 1s in the final (sorted) list. Notice that this is a simple
implementation of the counting sort  algorithm we have used previously.
Suppose that instead of simply sorting keys, one needs the actual data to be rearranged.
That is, assume that we are performing array packing on labeled records where all
records that are marked are to appear before all records that are not marked. This is a
fundamentally different problem from sorting a set of 0s and 1s. Notice that for this
variant of the problem, it may be that all of the records are on the "wrong" half of the
machine under consideration. Therefore, the lower bound for solving the problem is a
function of the bisection width. For example, on a mesh of size n, if all n records need to
move across the links that connect the middle two columns, a lower bound on the
running time is
 a hypercube of size n, the bisection width gives us
a lower bound of
However, the communication diameter yields a better lower bound of  (log n).  The
reader should consider bounds on other machines, such as the pyramid and mesh-of-
trees.
Because the record-based variant of the array packing problem reduces to sorting, the
solution can be obtained by performing an efficient general-purpose sorting algorithm on
the architecture of interest. Such algorithms will be discussed later in this book.
TeamUnknown Release
0
Chapter 7 - Parallel Prefix
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Interval (Segment) Broadcasting
It is shown easily that parallel prefix can be used to broadcast a piece of information (see
Exercises). This is particularly useful in the ER PRAM model or on network-based
models. In this section, we consider a variant of the parallel prefix problem. Assume that
we are given a sequence of data items. Further, we assume that some subset of these
items is "marked." We can view these marked data items as separating the complete
sequence of data items into logical subsequences, where the first item of every
subsequence is a marked data item. The problem we consider is that of broadcasting a
marked data item to all of the records in its subsequence. It is important to note that in
each subsequence there is one and only one marked data item, and in fact, it is the first
item of the subsequence. For this reason, the marked data items are often referred to as
"leaders." We now give a more concise description of the problem.
Suppose we are given an array X of n data items with a subset of the elements marked
as "leaders." We then broadcast the value associated with each leader to all elements
that follow it in X up to but not including the next leader. An example follows.
The top table in ( Figure 7.9 .) gives the information before the segmented broadcast. The
leaders are those entries for which the "Leader" component is equal to 1.
Figure 7.9: An example of segmented broadcast. The top table shows the initial state
(that is, the information before the segmented broadcast). Thus, by examining the
Leader field in each processor, we know the interval leaders are processors 0, 3, 5,
and 6. In the bottom table, we show the information after the segmented broadcast.
Information from each leader has been propagated (broadcast) to all processors to
the right (higher index values) up to, but not including, the next leader

In the table at the bottom of ( Figure 7.9 .) we show the information after this segmented
broadcast. At this point, every entry knows its leader and the information broadcast from
its leader.
Solution Strategy
The interval broadcasting problem can be solved in a fairly straightforward fashion by
exploiting a parallel prefix computation, as follows. For each leader (or marked entry) xi in
X, create the record (i, xi,). For each data item x that does not correspond to a leader in
X, create the record (-1,-1). Now define our prefix operator   as
The reader should verify that our operator  is legal, as defined for parallel prefix. That is,
the reader should verify that this operator is binary, closed, and associative. Recall that
 need not be commutative. Notice that a straightforward application of a parallel prefix
will now serve to broadcast the data associated with each leader to the members of its
interval.
Analysis
Consider the RAM. A parallel prefix is implemented as a linear time scan operation,
making a single pass through the data. So given an array X of n elements, the running
time of the algorithm on a RAM is  (n), which is asymptotically optimal. Notice that the
solution to the interval broadcasting problem consists simply of a careful definition of the
prefix operator  , coupled with a straightforward implementation of parallel prefix.
Therefore, the analysis of running time, space, and cost on the PRAM, network models,
and coarse-grained multicomputer, is identical to that which has been presented earlier in
this chapter.
TeamUnknown Release
0
Chapter 7 - Parallel Prefix
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
(Simple) Point Domination Query
In this section, we consider an interesting problem from computational geometry,  a
branch of computer science concerned with designing efficient algorithms to solve
geometric problems. Such problems typically involve points, lines, polygons, and other
geometric figures. Consider a set of n data items, where each item consists of m fields.
Further, suppose that each field is drawn from some linearly ordered set. That is, within
each field, one can compare two entries and determine whether or not the first entry is
less than the second entry. To cast the problem in two dimensions (that is, m = 2), we
say that a point q1= (x1, y1) dominates  a point q 2= (x 2, y2) if and only if x 1< x2 and y1> y2.
This is an important problem in the field of databases. For example, it is often important
to determine for a given set of points Q= {q1, q2, …, qn}, all points that are not dominated
by any point in Q.
Suppose we are interested in performing a study to identify the set of students for which
no other student has both a higher grade-point average (GPA) and owns more DVDs. An
example is given in Figure 7.10 , where the x-axis represents the number of DVDs and
the y- axis represents GPA. Exactly three points from this set of nine students satisfy our
query.

Figure 7.10: An example of the point domination problem. In this example, exactly
three points have no other point both above and to the right. The remainder of the
points are dominated by at least one of these three points
Suppose that the input to our problem consists of a set of n points, Q = {q 1, q2, …, qn},
where each point qi= (x i, yi), such that no two members of Q have the same x-
coordinates or the same y-coordinates, and where Q is initially ordered with respect to
the x-coordinate of the records. Given such input, an algorithm follows to solve the point
domination query  (that is, to determine all points in Q not dominated by some other
member of Q).
Solution Strategy
Because the records are ordered initially with respect to the x-coordinate, the points can
be thought of as lying ordered along the x-axis. The first step of the algorithm is to
perform a parallel postfix  operation, where the operator is maximum-y-value.  The
maximum  operation is commutative, so this is equivalent to performing a parallel prefix
operation on the sequence of data (qn, qn-1,…,q1.Let pi denote the parallel prefix value
associated with record qi. Notice that at the conclusion of the parallel prefix algorithm, the
desired set of points consists of all qi for which i<n and pi> pi+1. Also, qn is one of the
desired points. We now consider the time and space complexity of the algorithm on the
RAM, PRAM, and network models.
RAM
Given an (ordered) array of data, a prefix operation can be performed on the n entries in
 (n) time using a constant amount of additional space. A final pass through the data can
be used to identify the desired set of records. (We should note that this second pass
could be avoided by incorporating the logic to recognize undominated points into the
parallel prefix operation.) As usual, it is easy to argue that the running time is optimal.
The only way to complete the algorithm faster would be not to examine all of the entries,
which could result in an incorrect result.
PRAM and Network Models
Notice that the solution to the two-dimensional point domination query, where the input is
given ordered by x- axis, is dominated by a parallel prefix operation. Therefore, the
running time, space, and cost analysis is consistent with the analysis of parallel prefix
given earlier in this chapter.
TeamUnknown Release
0
Chapter 7 - Parallel Prefix
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Computing Overlapping Line Segments
In this section, we consider other (simple) problems from computational geometry. These
problems involve a set of line segments that lie along the same line. We can think of this
as a set of line segments that lie along the x-axis, as shown in Figure 7.11 )where the
segments are shown raised above the x-axis for clarity. The line segments are allowed to
overlap (or not) in any possible combination.
Figure 7.11: An example of problems involving overlapping line segments. The line
segments are all assumed to lie on the x-axis, though they are drawn superimposed
for viewing purposes
Formally, we assume that the input consists of a set S = {s 1, s2,…,s } of n uniquely
labeled line segments, all of which lie along the same horizontal line. Each member of S
is represented by two records, one corresponding to each end-point. Each such record
consists of the x-coordinate of the endpoint, the label of the line segment, and a flag
indicating whether the point is the left or right end-point of the line segment. Further, we
assume that these 2n records are ordered with respect to the x-coordinate of the records,
and if there is a tie (two records with the same x-coordinate), the tie is broken by having a
record with a Left end-point precede a record with a Right endpoint.
Coverage Query:  The first problem we consider is that of determining whether or
not the x-axis is completely covered by the set S of n line segments between two
given x-coordinates, A and B, where A<B.
Solution:  We give a machine-independent solution strategy and then discuss the

analysis for a variety of models.
Determine whether or not left(s 1)= A and B =max\right{s t)nt=1.If this is the case,
then we can proceed. If not, we can halt with the answer that the coverage query is
false.1.
For each of the 2n records, create a fourth field that is set to 1 if the record
represents a left endpoint, and is set to -1 if the record represents a right endpoint.
We will refer to this field as the operand field.2.
Considering all 2n records, perform a parallel prefix sum operation on the values in
this operand field. The result of the i th prefix will be stored in a fifth field of the i th
record, for each of the 2n records.3.
Notice that any parallel prefix sum of 0 must correspond to a right endpoint.
Suppose that such a right endpoint is at x-coordinate c. Then all line segments with
a left endpoint in (— 8, c] must also have their right endpoint in (— 8, c]. Notice also
that due to the ordering of the records, in case of a tie in the x-coordinate, the left
endpoint precedes the right endpoint. This means that the record that follows must
be either a right endpoint with x-coordinate equal to c, or a left endpoint with x-
coordinate strictly greater than c. Either way, the ordered sequence cannot have a
right endpoint with an x-coordinate strictly greater than c until after another left
endpoint with x-coordinate strictly greater than c occurs in the sequence. Thus, there
is a break in the coverage of the x-axis at point c. So we determine the first record
with parallel prefix sum equal to 0.If the x-coordinate of the endpoint is greater than
or equal to B, then the answer to the coverage query is true; otherwise it is false
(see Figure 7.12 ).
Figure 7.12: Transforming the coverage query problem to the parentheses
matching problem. For this example, notice that there is a break in coverage4.
between x 6 and x 7, as indicated by the 0 in the prefix value of x 6
RAM
Consider an implementation of this algorithm on a RAM. The input consists of an array S
with 2n entries and the values of A and B. Step 1 requires the comparison of the first
element of S with the scalar quantity A and, because the records are ordered, a
comparison of B with the last point. Therefore, step 1 can be performed in  (1) time.
Step 2 is completed with a simple  (n) time scan through the array. Similarly, the parallel
prefix is performed on an array of 2n items with a scan that takes  (n) time. One final
scan can be used to determine the first break in the coverage of the line segments before
determining in  (1) time whether or not this endpoint precedes B. Therefore, the running
time of the RAM algorithm is  (n), which is optimal.
PRAM
In order to attempt to derive a cost-optimal algorithm for this problem on the PRAM, we
will consider a PRAM with  (n/log n) processors. In the first step, the values of A and B
can be broadcast to all processors in O(log n) time, even if the PRAM is ER, as shown
previously. This is followed by a  (log n) time (OR) semigroup operation to compute the
desired comparison for A and then B, and a  (1) time (CR) or  (log n) time (ER)
broadcast of the decision concerning halting. Step 2 requires  (log n) time because every
processor must examine all  (log n) of the records for which it is responsible. Step 3 is a
straightforward parallel prefix, which can be performed on a PRAM with 0( n/log n)
processors in  (log n)  time, as discussed previously. A  (log n) time semigroup operation
can be used to determine the first endpoint that breaks coverage, and a  (1) time
comparison can be used to resolve the final query. Therefore, the running time of the
algorithm is  (log n)  on a PRAM with  (n/log n) processors, resulting in an optimal cost of
 (n).
Mesh
As we have done previously when attempting to derive an algorithm with 0( n) cost on a
mesh, we consider an n 1/3× n 1/3 mesh, in which each of the n2/3 processors initially
contains the appropriate set of n 1/3 contiguous items from S. If we follow the flow of the
PRAM algorithm, as implemented on a mesh of size n2/3, we know that the broadcasts
and parallel prefix operations can be performed in 0( n1/3) time. Because these operations
dominate the running time of the algorithm, we have a 0( n1/3) time algorithm on a mesh
with n2/3 processors, which results in an optimal cost of 0( n).
Maximal Overlapping Point
The next variant of the overlapping line segments problem that we consider is the
problem of determining a point on the x-axis that is covered by the most line segments.
The input to this problem consists of the set S of 2n ordered endpoint records, as
discussed earlier.
Solution  The solution we present for the maximal overlapping point problem is similar to
the solution just presented for the coverage query problem.
For each of the 2n records, create a fourth field that is set to 1 if the record
represents a left endpoint, and is set to -1 if the record represents a right endpoint.
We will refer to this field as the operand field.1.
Considering all 2n records, perform a parallel prefix sum operation on the values in
this operand field. For each of the 2n records, the result of the i th prefix will be
stored in the fifth field of the i th record.2.
Determine the maximum value of these prefix sums, denoted as M. All points with a
prefix sum of M in the fifth field of their record correspond to points that are
overlapped by a maximal number of line segments.3.
Analysis
The analysis of this algorithm follows that of the coverage query problem quite closely.
Both problems are dominated by operations that are efficiently performed by parallel
prefix computations. Therefore, the RAM algorithm is optimal at  (n) time. A PRAM
algorithm can be constructed with  (n/log n) processors that runs in  (log n) time,
yielding an optimal cost of  (n). Finally, a mesh algorithm can be constructed with  (n2/3)
processors, running in 0( n1/3) time, which also yields an algorithm with optimal 0( n) cost.
TeamUnknown Release
0
Chapter 7 - Parallel Prefix
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we introduce parallel prefix computations. Roughly, a parallel prefix
computation on n data items x1, … xn is the result of applying a binary operator   when
we wish to preserve not only the result x1 …  xn but also the sequence of partial results
x1, x1 x2, x1 x2 3 …  xn-1. We discuss efficient to optimal implementation of parallel
prefix on various computational models. We show the power of this computation by
presenting several applications.
TeamUnknown Release

0
Chapter 7 - Parallel Prefix
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
In this chapter, we studied the implementation and application of parallel prefix, an
extremely powerful operation, especially on parallel computers. Parallel prefix-based
algorithms are presented in R. Miller's and Q.F. Stout's Parallel Algorithms for Regular
Architectures  (The MIT Press, Cambridge, 1996), to solve fundamental problems as well
as to solve application-oriented problems from fields including image processing and
computational geometry for mesh and pyramid computers. A similar treatment is
presented for the PRAM in J. Já Já's An Introduction to Parallel Algorithms  (Addison-
Wesley, Reading, MA, 1992). Parallel prefix is presented in a straightforward fashion in
the introductory text by MJ. Quinn, Parallel Computing Theory and Practice  (McGraw-Hill,
Inc., New York, 1994). Finally, the Ph. D. Thesis by G.E. Blelloch, Vector Models for
Data-Parallel Computing  (The MIT Press, Cambridge, 1990), considers a model of
computation that includes parallel prefix as a fundamental unit-time operation.
Efficient gather and scatter algorithms for coarse-grained multicomputers are
demonstrated in L. Boxer's and R. Miller's paper, "Coarse Grained Gather and Scatter
Operations with Applications," Journal of Parallel and Distributed Computing,  64 (2004),
1297-1320.The availability of these algorithms is assumed in the exercises, although
their steps are not given.
TeamUnknown Release

0
Chapter 7 - Parallel Prefix
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
Show that a hypercube with  (n/log n) processors can compute a parallel prefix
operation for a set of n data, {x0, x1,…,xn-1}, distributed  (log n)  items per processor,
in (log n)  time.1.
The interval prefix computation  is defined as performing a parallel prefix within
predefined disjoint subsequences of the data set. Give an efficient solution to this
problem for the RAM, PRAM, and mesh. Discuss the running time, space, and cost
of your algorithm.2.
Show how a parallel prefix operation can be used to broadcast 0(1) data to all the
processors of a parallel computer in the asymptotic time of a parallel prefix
operation. This should be done by providing an algorithm that can be implemented
on any parallel model, with the running time of the algorithm dominated by a parallel
prefix operation.3.
Define InsertionSort  in terms of parallel prefix operations for the RAM and PRAM.
Give an analysis of running time, space, and cost of the algorithm.4.
Give an optimal  EREW PRAM algorithm to compute the parallel prefix of n values
x1, x2…, xn5.
Give an efficient algorithm to perform Carry-Lookahead Addition  of two n bit
numbers on a PRAM. Hint:  Keep track of whether each one-bit subaddition stops (s)
a carry, propagates (p) a carry, or generates (g) a carry. See the following example.
Notice that if the i th carry indicator is p, then the ith carry is a 1 if and only if the
leftmost non-p to the right of the ith position is a g.
6.
7.

Give an efficient algorithm for computing the parallel prefix of n values, initially
distributed one per processor on a q-dimensional mesh of size n. Discuss the time
and cost of your algorithm.7.
Suppose that you are given a set of n pairwise disjoint line segments in the first
quadrant of the Euclidean plane, each of which has one of its endpoints on the x-
axis. Think of these points as representing the skyline of a city. Give an efficient
algorithm for computing the piece of each line segment that is observable from the
origin. You can assume that the viewer does not have x-ray vision. That is, the
viewer cannot see through any piece of a line segment. You may also assume the
input is ordered from left to right. Discuss the time, space, and cost complexity of
your algorithms for each of the following models of computation.
PRAMa.
Meshb.
Hypercubec.8.
Give an efficient algorithm for computing the parallel prefix of n values stored one
per processor in
the leaves of a tree machine;1.
the base of a mesh-of-trees of base size
Discuss the time and cost complexity of your algoritms2.
Consider the array packing algorithms presented in this chapter. Which of the
routines is stable? That is, given duplicate items in the initial list, which of the
routines will preserve the initial ordering with respect to duplicate items?3.
Suppose a set of n data, X = {x 0, x1,…,xn-1}, is distributed evenly among the
processors of a coarse-grained multicomputer CGM(n, q)  such that processor
in pi has the data 
 Assume there exist algorithms (that you may
use) 1 to gather  (1) data from every processor into a single processor in 0(q)
time, and scatter the gathered items (whose values may have been altered by
actions of the processor in which they were gathered) back to their original
processors in O(q) time.4.9.
Give the steps of an efficient algorithm to perform a parallel prefix computation on the
CGM(n, q),  and analyze its running time. (You should be able to obtain an algorithm that
runs in Q(n/q)  time.)
TeamUnknown Release
0
Chapter 8 - Pointer Jumping
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 8: Pointer Jumping
Overview
In this chapter, we consider algorithms for manipulating linked lists. We assume that
the linked list under consideration is arbitrarily distributed throughout the memory of
the model under consideration. Each element of the list consists of a data record and
a next field. The next field contains the address of the next element in the list. In
addition, we assume that the next field of the last entry in the list is set to null.
On a RAM, the list is distributed arbitrarily throughout the memory, and we assume
that the location of the first element is known. On a PRAM, we assume that the list is
distributed arbitrarily throughout the shared memory. Consider a linked list with n
elements distributed throughout the memory of a PRAM with n processors. In this
situation, we assume that every processor knows the location of a unique list element
and that the location of the first element in the list is known. Given a PRAM with m =
n processors, each processor will be responsible for T(n/m)  such elements.
For the network models, we assume that the list is distributed evenly in an arbitrary
fashion throughout the memory of the processing elements. Given a linked list of size
n, distributed one item per processor on a network model with n processors, every
processor will store one element. Each element consists of a data record and a next
pointer, which contains the processor ID of the next element in the list. Given a
network model with m = n processors, every processor will store approximately Tn/m
elements, and each pointer will now include the processor ID and the index of the
next element within that processor.
RAM:  A linked list and a sequential machine provide a model for traversing the
data that is inherently sequential. Therefore, given a list of size n, problems
including linked list search, linked list traversal, semigroup operation, and parallel

prefix through the list, to name a few, can be solved in a straightforward fashion
in T(n) time by a linear search.
Network Models:  Given that the data is distributed arbitrarily among the
processors, the communication diameter of a network model serves as a lower
bound on the time required for a single link to be traversed. The time for all links
to be traversed simultaneously is bounded by the bisection width. Therefore, it is
often advantageous simply to consider a linked list of data as an unordered array
of data and operate on it with high-powered data movement operations. Such
operations will be discussed later in the book.
PRAM:  The most interesting model to discuss in terms of linked list operations is
the PRAM. This is because the communication diameter is T(1) and the
bisection width of a PRAM with n processors is equivalent to T(n2). For many
years, it was believed that list-based operations were inherently sequential.
However, some clever techniques have been used to circumvent this notion. We
demonstrate some of these pointer-jumping  techniques in the context of two
problems. The problems are list ranking  and parallel prefix  (for linked lists). A
description of the problems, along with PRAM implementations and analyses,
follows.
TeamUnknown Release
0
Chapter 8 - Pointer Jumping
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
List Ranking
Suppose that we are given a linked list L of size n, and we wish to determine the distance from each
element to the end of the list. That is, for list element L(i), we want to compute the distance, call it d(i), to
the end of the list. Recall that this is a linked list of elements, so that except for the first element in the
list, the position of any element is initially unknown. We define the distance, d(i), as follows.
The PRAM algorithm we present operates by a recursive doubling procedure. Initially, every processor
finds the next element in the list, that is, the element that succeeds it in a traversal of the list from
beginning to end. In the next step, every element locates the element two places away from it (that is,
two positions closer to the end of the list). In the next step, every element locates the element four
places closer to the end of the list, and so on. Notice that in the first step, every element has a pointer to
the next element. During the course of the algorithm, these pointers are updated (otherwise, every
element would need to maintain more than a fixed number of pointers). During every step of the
algorithm, each element L(i) can determine easily the element twice as far as L(next(i))  is. Notice that the
element twice as far from L(i) as L(next(i))  is simply L(next(next(i))),  as shown in Figure 8.1  . As the
process progresses, every element needs to keep track of the number of such links traversed to
determine its distance to the end of the list. In fact, some care needs to be taken for computing
distances at the end of the list. The details follow.
Input:  A linked list L consisting of n elements, arbitrarily stored in the shared memory of a PRAM
with n processors.
Output:  For every element L(i), determine the distance d(i) from that element to the end of the list.

                                {First, initialize the distance entries}
For all 
L(i)
 do
End For all
                  {Perform pointer-jumping algorithm. The actual pointer
                                jumping step is 
next(i)
 
 
 
next(next(i))}
While there exists an 
i
 such that 
next(i)
 
 
 
null
, do
    For all 
L(i)
 do
      If 
next(i) 
 
 null
 then
        
d(i)
 
 
 
d(i)
 + 
d(next(i))
    
next(i)
 
 
 
next(next(i))
  End If
 End For all
End While
Analysis:  Given a PRAM of size n, the running time of this algorithm is T (log n). This can be seen
by the fact that the first element in the list must traverse 
 links to reach the end of the
list. Because the time for a PRAM of size n to solve the list-ranking problem for a list of size n is T
(log n), the total cost is T (n log n), which we know is suboptimal.
To reduce this cost, we can consider a PRAM with n/log 2 n processors. In this case, we can attempt
to modify the algorithm as we have done previously. That is, we can attempt to create a hybrid
algorithm in which each processor first solves the problem locally in T (log n ) time, and then the
algorithm given previously is run on this set of partial results. Finally, in T (log n) time, we can make
a final local pass through the data. However, consider this algorithm carefully. It is important to note
that if each processor were responsible for T (log n) items, there is no guarantee that these items
form a contiguous segment of the linked list. Therefore, there is no easy way to consider merging
the T (log n) items for which a processor is responsible into a single partial result that can be used
during the remainder of the computation. In this case, such a transformation fails, and we are left
with a cost-suboptimal algorithm.
Figure 8.1: An example of list ranking. Given a linked list, determine for each element the number of
elements in the list that follow it. The algorithm follows a recursive doubling procedure. Initially, every
processor finds the next element in the list. (a) shows the initial list, in which every element knows the
following element one step away. In steps (b), (c), and (d), every element locates the element 2, 4,
and 8 places away from it, respectively. Step (e) shows the final values at the end of the recursive
doubling procedure. Given a list with 10 elements, the number of iterations required is   log 2 10   = 4
 
TeamUnknown Release
0
Chapter 8 - Pointer Jumping
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Linked List Parallel Prefix
Now let's consider the parallel prefix problem. Although the problem is the same as we
have considered earlier the book, the input is significantly different. Previously, whenever
we considered the parallel prefix problem, we had the advantage of knowing that the data
was ordered in a random access structure, that is, an array. Now, we have to consider
access to the data in the form of a linked list. Notice that if we simply perform a scan on
the data, the running time will be T(n), which is equivalent to the RAM algorithm. Instead,
we consider applying techniques of pointer jumping so that we can make progress
simultaneously on multiple prefix results. For completeness, recall that we are given a set
of data X = {x 1,…xn} and a binary associative operator  , from which we are required to
compute prefix values P1, P2,…,pn, where the kth prefix is defined as
We now present an algorithm for computing the parallel prefix of a linked list of size Bon
a PRAM of size n, based on the concept of pointer jumping.
{p
i
 is used to store the 
f
th
 prefix.}
For all i, p
i
 
 
 
x
i
{Perform a pointer-jumping algorithm.}
While there exists an 
i
 such that 
next(i)
 
 
 
null
, do
For all 
x
i
, do
  If 
next(i)
 
 
 
null
, then
     
p
next(i)
 
 
 
p
i
 
p
next(i)
     
next(i) 
 
 next(next(i))
  End If
 End For all
End While

An example of this algorithm is given in Figure 8.2 , where we show the application of a
parallel prefix on a PRAM to a linked list of size 6. While going through the algorithm, it is
important to implement the update steps presented inside of the "For all" statement in
lockstep fashion across the processors.
Figure 8.2: An example of parallel prefix on a PRAM with linked list input. Given a list
of size 6, the recursive doubling procedure requires three iterations (   log26   = 3)
Analysis:  This algorithm is similar to that of the list ranking algorithm just presented. That
is, given a PRAM of size n, the running time of this algorithm is T(log n). This can be
seen by the fact that the first element in the list must traverse   log 2n   links to propagate
x 1 to all n prefix values. Because the time for a PRAM of size n to compute the parallel
prefix on a list of size n is T(log n), the total cost of the algorithm is T(n log n). As with the
list ranking algorithm, the cost of the parallel prefix computation is suboptimal.
TeamUnknown Release
0
Chapter 8 - Pointer Jumping
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we consider pointer-jumping computations on a PRAM for the linked list
data structure. The techniques presented allow us to double, in each parallel step, the
portion of a list "known" to each node of the list, so that in logarithmic time, each node
can know its relationship with all other nodes between its own position and the end of the
list. The problems we consider are those of list ranking and parallel prefix (for linked lists).
Our solutions are efficient, although not optimal.
TeamUnknown Release

0
Chapter 8 - Pointer Jumping
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
The focus of this chapter is on pointer-jumping algorithms and efficient solutions to
problems involving linked lists, an inherently sequential structure. An excellent chapter
was written on this subject by R.M. Karp and V. Ramachandran, entitled "A Survey of
Parallel Algorithms and Shared Memory Machines," which appeared in the Handbook of
Theoretical Computer Science: Algorithms and Complexity  (AJ. vanLeeuwen, ed.,
Elsevier, New York, 1990, pp. 869-941). It contains numerous techniques and
applications to interesting problems. In addition, pointer-jumping algorithms are
discussed in An Introduction to Parallel Algorithms,  by J. Já Já (Addison-Wesley,
Reading, MA, 1992).
TeamUnknown Release

0
Chapter 8 - Pointer Jumping
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
The component labeling  problem has several variants in graph theory,
computational geometry, and image analysis. Suppose we have a set
, the members of which could be points in a Euclidean
space, vertices of a graph, or pixels of a digital image. Further, suppose we have a
well-defined notion of neighboring points that is symmetric (p i and pj are neighbors if
and only if pi and pj are neighbors) and anti-reflexive  (no point is a neighbor of
itself). We say pi and p j are connected  if
pi = pj or
pi and pj are neighbors, or
there is a sequence 
 such that pi = pio, pj
= pij, and pij+1 are neighbors, 0 = j < k.
A component C  is a maximal subset of S such that all members of C are connected.
The label of C is the smallest index i (or some equivalent such as a pointer to a
unique member of C) such that pi   C. The component labeling problem is to
associate with each member pi of S the label of the component of S containing pi.
Given a set of linked lists, solve this version of the component-labeling problem.
That is, given several linked lists with a total of n elements, regard each list as a
component of the totality of links; neighbors are links that are adjacent in the same
list. Give RAM and PRAM algorithms that efficiently solve the component-labeling
problem. The RAM solution should run in T(n) time. The PRAM solution should
have a cost of T(n log n). Hint:  a PRAM solution could use the pointer-jumping1.
2.

techniques illustrated in this chapter.
Give an efficient algorithm to solve the following problem. Given a collection of
linked lists with a total of n links, let every link know how many links are in its list and
how far the link is from the front of the list (the head link is number 1, the next link is
number 2, and so on). Analyze for the RAM and the PRAM.2.
Give an efficient algorithm to solve the following problem: for a linked list with n
links, report the number of links with a given data value x. Analyze for the RAM and
the PRAM.3.
Give an efficient algorithm to solve the following problem: for a set of ordered linked
lists with a total of n links, report to each link the median value of the link's list (in an
ordered list of length k for even k, the median value can be taken either as the value
in link (k/2) or link (k/2 + 1) from the head). Do not assume that it is known at the
start of the algorithm how many links are in any of the lists. Analyze for the RAM and
the PRAM.4.
TeamUnknown Release
0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 9: Divide-and-Conquer
The phrase divide-and-conquer  is used in the study of algorithms to refer to a method
of solving a problem that typically involves partitioning the problem into smaller
subproblems, recursively solving these subproblems, and then stitching together
these partial solutions to obtain a solution to the original problem. Thus, the solution
strategy involves doing some work to partition the problem into a number of
subproblems of the same form. Each of these subproblems is then solved recursively.
Finally, the solutions to these subproblems are combined to solve the original
problem. The divide-and-conquer strategy is summarized as follows:
Divide  the problem into subproblems, each of which is of a smaller size than the
original.
Conquer  all of the subproblems. In general, this is done by recursively solving
the problem. However, when the subproblem is "small enough," the problem is
solved directly as a base case.
Combine/Stitch  the solutions to the subproblems together in order to obtain a
solution to the original problem.
MergeSort (Revisited)
The divide-and-conquer paradigm is exhibited in MergeSort , a sorting algorithm that we
have previously discussed (see Chapter 2 , "Induction and Recursion". MergeSort serves
as a nice example for a concrete discussion of divide-and-conquer. Recall that the input
to the MergeSort routine consists of an unordered list of n elements, and the output
consists of an ordered list of the n elements. A high-level description of MergeSort, in
divide-and-conquer terminology, follows:

Divide:  Divide the unordered n-element input sequence into two unordered
subsequences, each containing n/2 items.
Conquer:  Recursively sort each of the two subsequences (general case). If a
subsequence has only one item, the subsequence need not be recursively sorted,
because a single item is already sorted (base case).
Stitch:  Combine the two sorted sequences by merging  them into the sorted result.
We should point out that this is a "top-down" divide-and-conquer description of
MergeSort. This is in contrast to a "bottom-up" description that many students see in their
early courses. A bottom-up description is typically presented as follows: "Merge pairs of
(ordered) sequences of length 1 into ordered sequences of length 2. Next, merge pairs of
ordered sequences of length 2 into ordered sequences of length 4, and so on." Notice
that while these two descriptions differ significantly, the algorithm described and the work
performed is identical.
We now consider the time and space analysis of MergeSort on a variety of models of
computation.
RAM
The analysis for the RAM should be familiar to readers who have taken a traditional year-
long introduction to computer science or a course on data structures. Let's first consider a
schematic of the operations performed by the MergeSort algorithm on a RAM. The n
elements in the list are divided initially into two lists, each of which is recursively sorted.
These two sorted lists are then merged into a single ordered list. Notice that a traditional,
sequential merge  routine on n items requires T(n) time. This is true whether the ordered
lists being merged are of equal length (n/2 items apiece) or not, as long as the total of the
lengths of the input lists is n. So, regardless of the details of the data structure, and
whether or not the splitting is done in T(1) or T(n) time, the total running time required for
the initial split and the final merge is T(n). In fact, we can be a little more precise and say
that the running time for the highest-level split and merge is Cn, for some constant C.
Now, consider each list of size n/2. Again, we argue that the split and merge routines are
a function of the size of the input. That is, the running time to perform the split and merge
for each input set of size n/2 can be expressed as C1(n/2) for some constant C1. In
general, the running time of the algorithm behaves as shown in Figure 9.1 .
Figure 9.1: A recursion tree giving insight into the time required to perform a
traditional MergeSort algorithm on a RAM
The top-down description and analysis of MergeSort can be used to derive the running
time of the algorithm in the form of a recurrence T(n) = 2T(n  / 2) + T(n). From the Master
Method, we know that this recurrence has a solution of T(n) = T(n log n). This is not
surprising considering the recursion tree  presented in Figure 9.1 .
Linear Array
We now consider an implementation of MergeSort on a linear array. Assume that the
elements of the list are distributed arbitrarily one per processor on a linear array of size n,
where for the sake of presentation, we assume that n is power of 2. Let's consider the
stitch step of the algorithm. That is, assume that processors P1,…,Pn/2 contain an
ordered subset of the data and that processors 
 contain the
remaining elements in sorted order (see Figure 9.2 ). By knowing its processor ID, every
processor knows the rank of its element with respect to its subsequence of size n/2 (see
Figure 9.3 ). That is, processor Pi, 1 = i = n/2, knows that the element it currently
contains is the ith element with respect to those elements stored in processors P1,…,
Pn/2. Similarly, processor 
, knows that the element it
currently contains has a rank of i - n/2 with respect to those elements stored in
processors 
. Based on this information and knowledge of where an
element ranks in the other subsequence, every processor will know the final position of
the element it contains. That is, if the element in processor Pi, 1 = i = n/2, is such that s
elements in processors 
 are less than it, the final position for the element in
processor Pi is i + s. Similarly, if the element in processor 
 elements
in processors P1, … Pn/2 are less than it, the final position for the element in processor Pi
is i - (n/2) + t  (see Figure 9.4 ).
Figure 9.2: A snapshot of MergeSort on a linear array of size 8. The initial data is
given in (a), and the result of independently sorting both the Left and Right subarrays
is shown in (b)
Figure 9.3: A snapshot of MergeSort on a linear array of size 8, using the data from
Figure 9.2 . The snapshot shows the data and local ranks that are determined after the
independent sorts on both the Left and Right subarrays
Figure 9.4: A snapshot of MergeSort on a linear array of size 8 after the independent
sorts on both the left and right subarrays. The data, local ranks, and ranks with
respect to the opposite subarray are all given. The data is from Figure 9.2
To determine the rank of an element with respect to the other subsequence, simply
perform a rotation of the data and allow every processor to count the number of elements
from the other subsequence that are less than the one that it is currently maintaining. A
final rotation can then be used to send every element to its correct sorted position. The
running time of such an algorithm is given by the recurrence T(n) = T(n /2) + T(n), which
has a solution of T(n) =  T(n), which is optimal for this architecture. At this point, it is
instructive to make two observations.
The algorithm, as described, requires that during each recursive step a rotation is
performed only within the pairs of subsequences being merged . That is, a complete
rotation is not performed each time. If a complete T(n) time rotation were performed
during each of the T(log n) iterations, the resulting running time would be T(n log n).
Although the running time of this algorithm is asymptotically equivalent to the tractor-
tread/rotation-based sorting algorithm presented earlier for the linear array, it is clear
that the high-order constants for this MergeSort routine are significantly larger than
that of the tractor-tread algorithm. This is clear from the fact that the last iteration of
the MergeSort procedure requires two complete rotations, whereas the rotation-
based sort requires only one rotation in total.
Finally, consider the cost of the MergeSort algorithm. The running time is T(n) on a linear
array with n processors, which yields a total cost of T(n2). Notice that this is significantly
larger than the T(n log n) lower-bound result on the number of operations required for
comparison-based sorting. Consider the T(n) communication diameter of the linear array.
From this we know that it is not possible to reduce the running time of MergeSort on a
linear array of n processors. Therefore, our only reasonable option for developing a
MergeSort-based algorithm that is cost optimal on a linear array is to consider reducing
the number of processors. Notice that if we reduce the number of processors to one, the
cost-optimal RAM algorithm can be executed. Because this yields no improvement in
running time, we would like to consider a linear array with more than a fixed number of
processors but less than a linear number of processors in the size of the input, in an
asymptotic sense. We leave this problem as an exercise.
TeamUnknown Release
0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Selection
In this section, we consider the selection  problem, which requires the identification of the
kth smallest element from a list of n elements, where the integer k is given as input to the
procedure and where we assume that 1 = k = n. Notice that this problem serves as a
generalization of several important problems, which include the following.
The minimum problem  (find a minimal entry), which corresponds to k = 1.
The maximum problem  (find a maximal entry), which corresponds to k = n .
The median problem  (find the median value), which corresponds to either
.
A naïve algorithm for the selection problem consists of sorting the data, and then
reporting the entry that now resides in the kth position of this ordered list.
Assume that on the given model of computation, the running time for the sort step (step
1) dominates the running time for the report step (step 2). Given this situation, the
asymptotic running time for selection is bounded by the running time for sorting. So, on a
RAM, our naïve algorithm has a running time of O(n log n).
We know that a lower bound on a solution to the selection problem requires that every
element is examined. In fact, for the restricted problem of finding the minimum or
maximum element, we know that a more efficient solution can be obtained by a
semigroup operation, which may be implemented by a prefix-based algorithm. For
example, a simple scan through the data on a RAM provides an asymptotically optimal
T(n) time solution for determining either the minimum or the maximum element of the list.
These observations suggest the possibility of solving the more general selection problem
in o(n log n) time.
We first consider an efficient T(n time algorithm for the RAM, which is followed by a

discussion of selection on parallel machines.
RAM
The fact that a simple scan of the data will result in a T(n) time solution to the minimum
or maximum problem motivates us to consider developing a solution to the general
selection problem that does not require sorting. We now present an efficient semigroup-
based algorithm to the general selection problem. We assume that the n data items are
initially stored in arbitrary order in an array. For ease of explanation, we assume that n,
the number of elements in the array, is a multiple of 5.
The algorithm may appear to be more complex than those that have been presented
previously in this text. However, it is really quite straightforward. Initially, we take the
unordered array as input and sort disjoint strings of five items (see Figure 9.5 ). That is,
given an array S, we sort S[1…5], S[6…10], …, S[n - 4…n]. Notice that this requires the
application of n/5 sorting routines. However, each sorting routine requires only constant
time (why?). Once the array is sorted within these segments of size 5, we gather the
medians of each of these segments. So we now have a set of n/5 medians. Notice that
after the initial local sort step, the first median is in S[3] (this is the median of S[1…5]),
the next median is in S[8] (the median of S[6…10]), and so on. We now (recursively) find
the median of these n/5 median values. This median of medians, which we denote as
AM, is an approximate median of the entire set S. Once we have this approximation, we
compare all elements of S to AM and create three buckets, namely, those elements less
than AM, those elements equal to AM, and those elements greater than AM (see Figure
9.6). Finally, we determine which of these three buckets contains the kth element and
solve the problem on that bucket, recursively if necessary. (Notice that if the kth element
falls in the second bucket, then, because all elements have equal value, we have
identified the requested element.)
Function Selection ( k, S, lower, upper)
Input:  An array S, positions lower  and upper , and a value k.
Output:  The kth smallest item in S[lower … upper].
Local variables:
n, the size of the subarray;
M, used for medians of certain subarrays of S;
smallList, equalList, bigList:  lists used to partition S;
j, an index variable;
AM, an approximation of the median of S
Action:
Figure 9.5: Using the Partition routine to solve the Selection Problem. An initial input
array of size 25 is given in (a). In (b), the array is shown after independently sorting
disjoint subarrays of size 5. (Note: contrary to the algorithm presented in the chapter,
for ease of presentation we ignore the fact the algorithm should proceed differently
when its recursion reaches a subarray of size 50 or smaller.)
Figure 9.6: Creating three buckets based on AM=13, the median of the five medians
(17, 9, 8, 19, 13) given in Figure 9.5b . The data given in Figure 9.5b  is traversed from
the beginning to the end of the array, with every element less than 13 being placed in
smallList, every item equal to 13 being placed in equalList, and every item greater
than 13 being placed in bigList. Notice that the items should be placed in these lists in
a manner that allows for T(1) time insertion. So, given the order shown in this figure,
one might assume that tail pointers were maintained during the insertion process
We now present a discussion of the correctness of this algorithm, which will be followed
by an analysis of its running time. Consider the lists smallList, equal-List , and bigList .
These lists contain members of S such that if x   smallList, y    equalList , and z  
bigList , then x < y < z . Therefore,
if 
, then the entries of smallList  include the k smallest
entries of S, so the algorithm correctly returns Selection(k, smallList_array , 1,
|smallList |;
if 
, then the kh smallest entry of S
belongs to equalList , each entry of which has a key value equal to AM, so the
algorithm correctly returns AM.
if 
, then the A* smallest member of S must be the
 smallest member of bigList , so the algorithm
correctly returns 
.
Analysis of Running Time
The base case of the recursive algorithm calls for sorting a list with length of at most 50.
Therefore, the running time of the base case is T(1). This is because any polynomial time
algorithm, such as the T(n2) time SelectionSort, will run in constant time on a fixed
number of input items. We remark that the criterion value 50 is rather arbitrary; for
analysis of our algorithm, any fixed positive integer will suffice.
We now consider the remainder of the algorithm.
Step 1 clearly requires T(1) time.
Step 2 calls for sorting T(n) sublists of the input list, where each sublist has at most
five entries. Because five is a constant, we know that each sublist can be sorted in
constant time. Therefore, the time to complete these T(n) sorts, each of which
requires T(1) time, is T(n).
Step 3 gathers the medians of each sublist, which requires making a copy of
 elements, each of which can be retrieved in T(1) time. Therefore, the
running time for this step is T(n).
Step 4 requires the application of the entire procedure on an array with 
elements. Therefore, this step requires 
 time. We can simplify
notation by saying that this step requires T(n/5)  time.
Step 5 calls for the creation of a fixed number of lists, which requires T(1) time in
most modern programming languages.
Step 6 consists of copying each of the n input elements to exactly one of the three
lists created in step 4. Therefore, the running time for this step is T(n.
Step 7 determines which of the three lists needs to be inspected and, in two of the
three cases, a recursive call is performed. The running time for this step is a function
of the input value k as well as the order of the initial set of data. Due to these
complexities, analysis of the running time of this step is a bit more involved. Three
basic cases must be considered, each of which we evaluate separately. Namely, the
requested element could be in smallList, equalList , or bigList.
We first consider the case where the requested element is in smallList , which occurs
when 
. Let's consider just how large smallList  can be. That
is, what is the maximum number of elements that can be in smallList?  The maximal
size of smallList  can be determined as follows:
Consider the maximum number of elements that can be less than AM (the
median of the medians). At most 
members of M are less than AM. For simplicity, and because our analysis is
based on asymptotic behavior, let's say that at most n/10 (median) elements
are less than AM.a.
Notice that each m   M is the third smallest entry of an ordered five-element
sublist of the input list S. In the n/10 sublists for which we have m < AM ,
possibly all five members could be less than AM; however, in the n/10 sublistsb.
for which we have m = AM, at most two members apiece are less than AM.b.
Therefore, at most 
 elements of the input list S can be sent to
smallList . Thus the recursive call to Selection(k,smallList_array , 1,|smallList |)
requires at most T(7n/10) time.c.
If 
 and 
, then the required
element is in equalList , and this step requires only T(1) time, because the required
element is equal to AM. (Notice at most one of the elements in equalList  must be
examined.)
If |smallList | +|equalList| < k , then the required element is in bigList . Consider the
maximum number of elements that can appear in bigList . An argument similar to the
one given above for the size of smallList  can be used to show that bigList  has at
most 7 n/10 entries. Thus, the recursive call of the Selection  routine requires at most
T(7n/10)  time. Therefore, step 7 uses at most r(7 n/10) time.
Finally, consider the total running time T(n) for the selection algorithm we have
presented. There are positive constants c, c0 such that the running time of this
algorithm is given by
By taking C = max{ c, 10 c0}, the previous statement yields
Thus, for 1 = n = 50 we have T(n) = Cn. This statement serves as the base case
for an induction proof. Suppose we have T(n) = Cn for all positive integer values n <
m. Then we have
(by the inductive hypothesis)
Thus, our induction shows T(n) = O(n) . We also must examine every entry of the
input list (because were we to overlook an entry, it is possible that the entry
overlooked has the "answer". Therefore, any selection algorithm must take  (n)
time. Thus, our algorithm takes T(n) time on a RAM, which is optimal.
Parallel Machines
Notice that a lower bound on the time required to perform selection on a parallel machine
is based on the communication diameter of the machine, not the bisection width.
Therefore, one might hope to construct an algorithm that runs in time proportional to the
communication diameter of the given parallel model.
Consider applying the algorithm we have just presented on a PRAM. Notice that the
independent sorting of step 2 can be performed in parallel in T(1) time. Step 3 requires
that the median elements are placed in their proper positions, which can be done quite
simply on a PRAM in T(1) time. Step 4 is a recursive step that requires time proportional
to T(n/5) . Step 5 requires constant time. Step 6 is interesting: the elements can be
ordered and rearranged by performing a parallel prefix operation to number the elements
in each list, which results in identifying the locations corresponding to the destinations of
the elements. That is, for each of the elements of S, we assign (by keeping a running
count) its position in the appropriate of smallList, equalList , or bigList  and copy the
element of S into its assigned position in the appropriate one of these auxiliary lists.
Therefore, this step can be performed in O(log n) time. Now consider the recursion in
step 7. Again, the running time of this step is no more than T(7n/10) . Therefore, the
running time for the algorithm can be expressed as T(n) = T(7n/10) + T(n/5) + O(log n),
which is asymptotically equivalent to T(n) = T(7n /10) + O(log n), which resolves to T(n) =
O(log2 n). It should be noted that the running time of this algorithm can be reduced to
O(log n log log n) by applying some techniques that are outside the scope of this text. In
addition, the problem can also be solved by first sorting the elements in T(log n) time and
then selecting the required element in T(1) time. This T(log n) time sorting routine is also
outside the scope of this book. In fact, T(n) optimal-cost algorithms for the selection
problem on a PRAM are known. These algorithms are also outside the scope of this text.
Consider the selection problem on a mesh of size n. Because the communication
diameter of a mesh of size n is T(n1/2), and because it will be shown later in this chapter
that sorting can be performed on the mesh in T(n1/2) time, we know that the problem of
selection can be solved in optimal T(n1/2) time on a mesh of size n.
TeamUnknown Release
0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
QuickSort (Partition Sort)
Quicksort  is an efficient and popular sorting algorithm that was originally designed for the RAM by C A.R. Hoare. It is a
beautiful algorithm that serves as an excellent example of the divide-and-conquer paradigm. It is also a good example of an
algorithm without a deterministic running time, in the sense that its expected- and worst-case running times are not identical.
Depending on the arrangement of the n input items, QuickSort has a T (n ) best-case running time and a T (n 2 ) worst-case
running time on a RAM. However, the reason that QuickSort is so popular on the RAM is that it has a very fast T (n log n )
expected-case running time. One must take care with QuickSort, however, because it can have a rather slow T (n 2 ) running
time for important input sets, including data that is nearly ordered or nearly reverse-ordered.
The basic algorithm consists of the three standard divide-and-conquer steps:
Divide:  Divide the n input items into three lists, denoted as smallList, equalList  , and bigList  , where all items in smallList
are less than all items in equalList  , all items in equalList  have the same value, and all items in equalList  are less than all
items in bigList.
Conquer:  Recursively sort smallList  and bigList.
Stitch:  Concatenate smallList, equalList  , and bigList.
The reader should note the similarity of the Divide step with the Divide step of the Selection algorithm discussed earlier in this
chapter (see Figure 9.7  ). Also, note the Conquer step does not require processing the equalList  , because its members are
sorted (they all have the same value).

Figure 9.7: An example of QuickSort on a linked list. (a) shows the initial unsorted list. (b) shows three lists after
partitioning based on the value 5. (c) shows the same lists after smallList and bigList have been recursively sorted. (d)
shows the completion of the sorting process, after concatenation of the sorted sublists
QuickSort is naturally implemented with data arranged in queue structures. Because a queue can be efficiently and naturally
implemented as a linked list, we will compare the QuickSort and MergeSort algorithms. Consider the divide (split) step.
MergeSort requires a straightforward division of the elements into two lists of equal size, whereas QuickSort requires some
intelligent reorganization of the data. However, during the stitch step, MergeSort requires an intricate combination of the
recursively sorted sublists, but QuickSort requires merely concatenation of three lists. Thus, MergeSort is referred to as an
easy split—hard join  algorithm; QuickSort is referred to as a hard split—easy join  algorithm. That is, MergeSort is more
efficient than QuickSort in the divide stage but less efficient than QuickSort in the stitch stage.
Notice that in MergeSort, comparisons are made between items in different lists during the merge operation. In QuickSort,
however, notice that comparisons are made between elements during the divide stage. The reason that no comparisons are
made during the stitch step in QuickSort is because the divide step guarantees that if element x is sent to list smallList  ,
element y is sent to list equalList  , and element z is sent to bigList  , then x < y < z .
Typically, the input data is divided into three lists by first using a small amount of time to determine an element that has a high
probability of being a good approximation to the median element. We use the term splitValue  to refer to the element that is
selected for this purpose. This value is then used much in the same way as AM was used during the selection algorithm. Every
element is sent to one of three lists, corresponding to those elements less than splitValue  (list smallList)  , those elements
equal to splitValue  (list equalList)  , and those elements greater than splitValue  (list bigList)  . After recursively sorting bigList
and smallList  , the three lists can simply be concatenated.
We mentioned earlier that depending on the order of the input, QuickSort could turn out to be a relatively slow algorithm.
Consider the split step. Suppose that splitValue  is chosen such that only a constant number of elements are either smaller
than it or larger than it. This would create a situation where all but a few items wind up in either smallList  or bigList  ,
respectively. If this scenario continues throughout the recursion, it is easy to see that the analysis of running time would obey
the recurrence T(n) = T(n - c) + T (n) , for some constant c , which sums as an arithmetic series to T(n) = T (n2 ). Notice,
unfortunately, that this worst-case running time of T (n2 ) occurs easily if the data is nearly ordered or nearly reverse-ordered.
Therefore, the user must be very careful in applying QuickSort to data for which such situations might arise.
Naturally, we hope that the splitting item is chosen (recursively) to be close to a median. Such a choice of splitValue  would
result in a running time given by T(n) = 2T(n/2) + T (n) , which gives T(n) = T (n log n ).
We now present details of a list-based QuickSort algorithm on a RAM. We start with a top-down description of the algorithm.
Subprogram QuickSort(q)
Input:  A list q.
Output:  The list q , with the elements sorted.
Procedure:  Use QuickSort to sort the list.
Local variables:
splitValue  , key used to partition the list;
smallList, equalList, bigList  , sublists for partitioning.
Action:
If 
q
 has at least two elements, then                       {do work}
   Create empty lists 
smallList, equalList
, and 
bigList.
                                                           {Divide: Partition the list}
   
splitValue = flndSplitValue(q);
   splitList(q, 
smallList, equalList, bigList, splitValue);
                                                     {Conquer: Recursively sort sublists}
   QuickSort(
smallList
;
   QuickSort(
bigList
);
                                                           {Stitch: Concatenate sublists}
   Concatenate(
smallList, equalList, bigList, q)
 End If
End 
Sort
We reiterate that it is not necessary to sort equalList  in the "Conquer" section of the algorithm because all members of
equalList  have identical key fields. Now let's consider the running time of QuickSort.
It takes T (1) time to determine whether or not a list has at least two items. Notice that a list having fewer than two items
serves as the base case of recursion, requiring no further work because such a list is already sorted.
Constructing three empty lists requires T (1) time using a modern programming language.
Consider the time it takes to find the splitValue  . Ideally, we want this splitter to be the median element, so that smallList
and bigList  are of approximately the same size, which will serve to minimize the overall running time of the algorithm. The
splitter can be chosen in as little as T (1) time, if one just grabs an easily accessible item such as the first item of the list,
or in as much as T (n ) time, if one wants to determine the median precisely (see the selection algorithm in the previous
section  ). Initially, we will consider using a unit-time algorithm to determine the splitter. We realize that this could lead to a
bad split and, if this continues at all levels of recursion, to a very slow algorithm. Later in the chapter we will discuss
improvements in choosing the splitter and the effect that such improvements have on the overall algorithm.
Splitting the list requires T (1) time per item, which results in a T (n ) time algorithm to split the n elements into the three
aforementioned lists. The algorithm follows.
Subprogram splitList(A, smallList, equalList, bigList, splitValue)
Input:  List A , partition element splitValue.
Output:  Three sublists corresponding to items of A less than, equal to, and greater than splitValue.
Local variable:  temp  , a pointer used for dequeueing and enqueueing
Action:
While not 
empty(A)
, do
  
getflrst(A, temp);
  If 
temp.key < splitValue
, then
     
putelement(temp, smallList)
  Else If 
temp.key = splitValue
, then
           
putelement(temp, equalList)
       Else 
putelement(temp, bigList)
  End While
End 
splitList
Notice that for the sake of efficiency, it is important to be able to add an element to a list in T (1) time. Many programmers
make the mistake of adding a new element to the end of a list without keeping a tail pointer. Because elements are added to
lists without respect to order, it is critical that elements be added efficiently to the lists. In a queue, elements can be removed
quite simply from the front of the list (this operation is often called dequeueing  —we called it getfirst  earlier) or added to the
back of a list (this operation is often called enqueueing  — we called it putelement  earlier) in T (1) time per element, resulting in
a T (n )-time split procedure.
Let's resume a discussion of the running time of QuickSort, though we will defer a detailed analysis until the next section  . In
the best case  , every element of the input list goes into the equalList  , with smallList  and bigList  remaining empty. If this is the
case, the algorithm makes one pass through the data, places all of the items in a single list, has recursive calls that use T (1)
time, and concatenates the lists in T (1) time. This results in a total running time of T (n ).
Without loss of generality, let's now consider the case where all of the elements are distinct. Given this scenario, the best-case
running time will occur when an even split occurs. That is, when one item is placed in equalList  , 
 items in either
smallList  or bigList  , and 
 -1 items in bigList  or smallList  , respectively. In this situation, the running time of
the algorithm, T(n) , is given (approximately) as follows:
Recall from the presentation of MergeSort, that this recurrence results in a running time of T(n) = T (nlogn)  . So, in the best
case, the running time of Quick-Sort is asymptotically optimal. In the next section  , we will show that on average the running
time of QuickSort is T (n log n) , which has important practical implications. In fact, its T (n log n) average running time is one
of the reasons that Quick-Sort comes packaged with so many computing systems.
Now consider the worst-case scenario of QuickSort. Suppose that at every level of recursion, either the maximum or minimum
element in the list is chosen as splitValue  . Therefore, after assigning elements to the three lists, one of the lists will have n - 1
items in it, one will be empty, and equalList  will have only the splitter in it. In this case, the running time of the algorithm obeys
the recurrence T(n) = T(n - 1) + T (n ), which has a solution of T(n) = T (n2 ). That is, if one gets very unlucky at each stage of
the recursion, the running time of QuickSort could be as bad as T (n 2 ). One should be concerned about this problem in the
event that such a running time is not acceptable. Further, if one anticipates data sets that have large segments of ordered
data, one may want to avoid a straightforward implementation of QuickSort. The scenario of a bad split at every stage of the
recursion could also be realized with an input list that does not have large segments of ordered data (see the Exercises). Later
in this chapter, we discuss techniques for minimizing the possibility of a T (n 2 )-time QuickSort algorithm.
Array Implementation
In this section, we discuss the application of QuickSort to a set of data stored in an array. The astute reader might note that
with modern programming languages, one very rarely encounters a situation where the data to be sorted is maintained in a
static array. However, there are certain "dusty deck" codes that must be maintained in the original style of design and
implementation for various reasons. This includes vintage scientific software written in languages such as FORTRAN. In
addition, there are other reasons why we present this unnatural implementation of QuickSort  . The first is historic. When
algorithms texts first appeared, the major data structure was a static array. For this reason, QuickSort has been presented in
many texts predominantly from the array point of view. Although this is unfortunate, we do believe that for historic reasons, it is
also important to include an array implementation of QuickSort in this book. Finally, although the linked list implementation that
we presented in the preceding section is straightforward in its design, implementation, and analysis, the array implementation
is quite complex and counterintuitive. The advantage of this is that it allows us to present some interesting analysis techniques
and to discuss some interesting algorithmic issues in terms of optimization.
Assume that the input to the QuickSort routine consists of an array A containing n elements to be sorted. For simplicity, we will
assume that A contains only the keys of the data items. Note that the data associated with each element could more generally
be maintained in other fields if the language allows an array of records or could be maintained in other (parallel) arrays. The
latter situation was common in the 1960s and 1970s, especially with languages such as FORTRAN.
Notice that a major problem with a static array is partitioning the elements. We assume that additional data structures cannot
be allocated in a dynamic fashion. For historical reasons, let's assume that all rearrangement of data is restricted to the
array(s) that contain the initial data plus a constant number of temporary data cells. Although this situation may seem strange
to current students of computer science who have learned modern (that is, post-1980s) programming languages, we reiterate
that there are situations and languages for which this scenario is critical.
So, let's consider the basic QuickSort algorithm as implemented on an array A , where we wish to sort the elements A[left…
right]  , where left = right  are integers that serve as pointers into the array.
Input:  An array A.
Output  : The array A with elements sorted by the QuickSort method.
Subprogram QuickSort (A, left, right)
If 
left < right
, then
  
Partition(A, left, right, partitionIndex)
     
QuickSort(A
, 
left, partitionIndex)
     
QuickSort(A
, 
partitionIndex+1, right)
End If
End 
QuickSort
Notice that the basic algorithm is similar to the generic version of QuickSort presented previously. That is, we need to partition
the elements and then sort each of the subarrays. For purposes of our discussion in this section, we view the array as being
horizontal. To work more easily with an array, we will partition it into only two "subarrays" under a relaxed criterion that requires
all elements in the left subarray to be less than or equal to  all elements in the right subarray. (It is critical to note that if the keys
are not unique, copies of the split element could appear in both the left and right subarrays.) We then recursively sort the left
subarray and the right sub-array. Notice that the concatenation step comes for free because concatenating two adjacent
subarrays does not require any work. Specifically, we have the following.
Divide  : A[left… right  ] is partitioned into two nonempty  subarrays A[left… p] and A[p + 1 … right] such that all elements in
A[left… p] are less than or equal to  all elements in A[p + 1 … right]  .
Conquer  : Sort the two subarrays, A[left… p] and A[p + 1… right ], recursively.
Stitch:  Requires no work because the data is in an array that is already correctly joined.
So, given the basic algorithm, we need to fill in the algorithm only for the partition routine (see Figure 9.8  ). We need to point
out that this routine is specific to array implementations. Over the years, we have watched numerous programmers
(predominantly students) try to implement this routine on a linked list because they did not understand the fundamentals of
QuickSort and did not realize that this array implementation is unnatural. The (standard) partition routine that we are about to
present should be used only with an array.
Figure 9.8: An example of QuickSort on an array of size 9. (a) shows the initial unordered array. (b) shows the array after
partitioning with respect to the value 5. Note every member of (3,4,1,2) is less than 5, and every member of (6,7,8,9,5) is
greater than or equal to 5. (c) shows the results of sorting each of the subarrays recursively. Notice that the entire array is
now sorted
This partition routine works as follows. First, choose a partition value. Next, partition the array into two subarrays so that all
elements in the left subarray are less than or equal to the partition value, whereas all elements in the right subarray are greater
than or equal to this value. This is done by marching through the array from left to right in search of an element that is greater
than or equal to  the partition value, and similarly, from right to left in search of an element that is less than or equal to  the
partition value. In other words, we march through the array from the outside in, looking for misplaced items. If such elements
are found (in a pair-wise sense), they are swapped, and the search continues until the elements discovered are in their proper
subarrays. Refer again to Figure 9.8  . Pseudo-code follows.
Subprogram Partition( A, left, right, partitionIndex)
Input:  A subarray A[left, … ,right]  .
Output:  An index, partitionIndex  , and the subarray A[left, … ,right]  partitioned so that all elements in A[left, …
,partitionIndex]  are less than or equal to all elements in A[partitionIndex  + 1,… , right ].
Local variables:  splitValue;  indices i, j
Action:
splitValue
 
 
 
A[left]
        {A simple choice of splitter}
  
i
 
 
 
left
 -1
  
j
 
 
 
right
 +1
While 
i < j
, do
    Repeat 
i
 
i + 1
 until 
A[i]
=
splitValue
    Repeat 
j
=-
j-1
 until 
A[j]
=
splitValue
    If 
i < j
, then (
Swap(A[i], A[j])
    Else 
partitionIndex
 
 
 
j
End While
End 
Partition
We now present an example of the partition routine. Notice that the marching from left to right is accomplished by the
movement of index i , whereas the marching from right to left is accomplished by the movement of index j . It is important to
note that each is looking for an element that could be located in the other subarray. That is, i will stop at any element greater
than or equal to  the splitter element, and j will stop at any element less than or equal to  the splitter element. The reader should
note that this guarantees the algorithm will terminate without allowing either index to move off of the end of the array, so there
is no infinite loop or out-of-bounds indexing.
Example
Initially, the splitValue  is chosen to be A[1] = 5, i is set to left - 1 = 0 and j is set to right +1 = 9, as shown in Figure 9.9a  .
Because i < j , the algorithm proceeds by incrementing i until an element is found that is greater than or equal to 5. Next, j is
decremented until an element is encountered that is less than or equal to 5. At the end of this first pair of index updates, we
have i= 1 and j = 7, as shown in Figure 9.9b  .
Figure 9.9: An example of the Partition routine of QuickSort on an array of 8 items
Because i < j , we swap elements A[i] = A[1]  and A[j] = A[7]  . This results in the configuration of the array shown in Figure 9.9c
.
Because i < j , the algorithm proceeds by incrementing i until an element is found that is greater than or equal to 5. Next,_/ is
decremented until an element is encountered that is less than or equal to 5. At the end of this pair of index updates, we have i
= 4 and j  = 6, as shown in Figure 9.9d  .
Because i < j , we swap elements A[i] = A[4]  and A[j] = A[6]  . This results in the configuration of the array shown in Figure 9.9e
.
Because i < j , the algorithm continues. First, we increment i until an element (6) is found that is greater than or equal to 5.
Next, we decrement y until an element (4) is found that is less than or equal to 5. At the end of this pair of index updates, we
have i = 6 and j  = 5 (see Figure 9.9f  ).
Because i = j , the procedure terminates with the partitionlndex  set to j = 5. This means that QuickSort can be called
recursively on A [1… 5] and A 1… 8].
Analysis of QuickSort
In this section, we consider the time and space requirements for the array  version of QuickSort, as implemented on a RAM.
Time
Notice that the running time is given by T(n) = T(n L ) + T(n R ) + T (n ), where T (n ) is the time required for the partition and
concatenation operations, T(nL ) is the time required to sort recursively the left subarray of size nL , and T(nR ) is the time
required to sort recursively the right subarray of size nR , where nL + n R = n.
Consider the best-case running time. That is, consider the situation that will result in the minimum running time of the array
version of QuickSort as presented. Notice that to minimize the running time, we want T(nL ) = T (T(n))  , which occurs if nL = T
(nR ) . In fact, it is easy to see that the running time is minimized if we partition the array into two approximately equally sized
pieces at every step of the recursion. This basically results in the recurrence T(n) = 2T(n  / 2) + T (n ), which has a solution of
T(n) = T (nlogn)  . This situation will occur if every time the partition element is selected, it is the median of the elements being
sorted.
Consider the worst-case running time. Notice that the running time is maximized if either nL or nR is equal to n- 1. That is, the
running time is maximized if the partition is such that the subarrays are of size 1 and n- 1 . This would yield a recurrence of
T(n) = T(n  - 1) + T (n ), which resolves to T(n) = T (n2 ). Although this situation can occur in a variety of ways, notice that this
situation occurs easily for data that is ordered or reverse-ordered. The user should be very careful of this because sorting data
that is nearly ordered can occur frequently in a number of important situations.
Finally, consider the expected running time. As it turns out, the expected-case running time is asymptotically equivalent to the
best-case running time. That is, given a set of elements with distinct keys arbitrarily distributed throughout the array, we expect
the running time of QuickSort to be T (n log n) . The proof of this running time is a bit complex, though very interesting. We
present this proof later in this chapter.
A summary of the running times for the array version of QuickSort is presented in the table below.
Best-Case
T (n log n)
Worst-Case
T (n2 )
Expected-Case
T (n log n)
Scenario
Running Time
Space
In this section, we consider the additional space  used by the array version of QuickSort as implemented on a RAM. This may
seem like a trivial issue because the routine does not use anything more than a few local variables. That is, there are no
additional arrays, no dynamic allocation of memory, and so on. However, notice that the routine is recursive. This means that
the system will create a system stack entry for each procedure call pushed onto the system stack.
Consider the best-case space scenario. This occurs when both procedure calls are placed on the stack, the first is popped off
and immediately discarded, and the second is popped off and evaluated. In this case, there will never be more than two items
on the stack—the initial call to QuickSort and one additional recursive call. Notice that this situation occurs when the array is
split into pieces of size 1 and n - 1. Furthermore, the recursive calls must be pushed onto the system stack so that the
subarray of size 1 is sorted first. This procedure call terminates immediately because sorting an array of size 1 represents the
base case of the QuickSort routine. Next, the system stack is popped and the procedure is invoked to sort the subarray of size
n - 1 . The system stack is prevented from growing to more than two calls via a minor modification in the code that replaces a
(tail-end) recursive call by either an increment to left or a decrement to right , and a branch.
Now let's consider the worst-case space scenario. This situation is almost identical to the best-case space scenario. The only
difference is that the procedure calls are pushed onto the system stack in the reverse order. In this situation, the procedure will
first be invoked to evaluate the subarray of size n - 1 (which in turn generates other recursive procedure calls), and after that
routine is complete, the system stack will be popped, and the subarray of size 1 will be sorted. In this situation, the chain of
recursive calls generated by the call to evaluate the subarray of size n - 1 requires the system stack to store T (n) procedure
calls. Demonstration of this claim is left as an exercise.
It is interesting to note that both the best-case and worst-case space situations occur with the T (n2 ) worst-case running time.
Consider the expected-case space scenario. This occurs with the expected-case T (n log n ) running time, where no more
than T (log n) procedure calls are ever on the system stack at any one time. Again, this can be seen in conjunction with the
expected-case analysis that will follow.
A summary of space requirements for the array version of QuickSort is presented in the following table.
Best-Case
T (1)
Worst-Case
T (n)
Expected-Case
T (log n)
Scenario
Extra Space
Expected-Case Analysis of QuickSort
In this section, we consider the expected-case running time of QuickSort. The analysis is intricate and suitable only for a
reader who has a solid mathematical background. We will make a variety of assumptions, most of which serve only to simplify
the analysis. Our first major assumption is that the array consists of n distinct keys, randomly distributed. In terms of
fundamental notation, we let k(i) be the expected number of key comparisons required to sort i items. QuickSort is a
comparison-based sort, and our analysis will focus on determining the number of times QuickSort compares two elements
during the sorting procedure. The reader should note that k(0) = 0, k(1) = 0, and k(2) = 3.5. That is, an array with one (or
fewer) element(s) is already sorted and does not require any keys to be compared. An array of size 2 requires (on average)
3.5 comparisons to be sorted by the array version of QuickSort that we have presented. The reader should verify this.
We now consider some assumptions that apply to the partition routine. Assume that we are required to sort A [1… n ].
According to the partition routine, we will use A[1] as the partition element.
As we are assuming distinct keys, if the partition element (originally A[1])  is the ith largest of the n elements in A [1… n ]
and i > 1 , then at the end of the partition, the smallest i—1 elements will be stored in A[1… i— 1]. We can make a simple
modification to the code so that at the end of the partition routine, the splitter is placed in position i , and partitionlndex  is
set to i . Notice that this modification requires T (1) time.
Therefore, notice that it suffices to have the recursive calls performed on A[1… i-1] and A[i + 1 … n ].
Consider the number of comparisons that are made in the partition routine.
Notice that it takes T (n ) comparisons to partition the n elements. The reader should verity this.
Based on our notation and the recursive nature of Quicksort, we note that, on average, it takes at most k(i - 1) and k(n- i)
comparisons to sort A[l… i - 1] and A[i + l … n] , respectively.
We should point out that because we assume unique input elements and that all arrangements of the input data are equally
likely, it is equally likely that the partitionlndex  returned is any of the elements of {1, … , n }. That is, the partitionlndex  will wind
up with any value from 1 through and including n with probability l/n . Finally, we present details for determining the expected-
case running time of Quicksort.
We have 
 , where
k(n) is the expected number of key comparisons,
(n + 1) is the number of comparisons required to partition n data items, assuming that Partition is modified in such a way
to prevent i andy from crossing,
l/n is the probability of the input A[j] being the ith largest entry of A, j   {l,… n},
k(i - 1) is the expected number of key comparisons to sort A[1… i - 1], and
k(n- i) is the expected number of key comparisons to sort A[i +1 … n] .
So,
(Note that we used the fact that k(0) = 0.)
Therefore, we now have
Notice that this means that
In order to simplify the equation for k(n ), let's define
By substituting into the previous equations for k(n ) and k(n- 1), we obtain
Therefore, 
 .
So,
Hence, 

To simplify, let's define
Therefore,
So,
The reader should be able to supply an induction argument from which it can be concluded that
So, k(n) = (n + 1)X(n) = T (nlogn)  expected-case number of comparisons.
It is easily seen that the expected-case number of data moves (swaps) is O(n log n) , because the number of data moves is no
more than the number of comparisons. Therefore, the expected-case running time of QuickSort is T (n log n) . The previous
argument requires little modification to show that our queue-based implementation of QuickSort also has an expected-case
running time of T (n log n ).
TeamUnknown Release
0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Improving QuickSort
In this section, we discuss some improvements that can be made to QuickSort. First, we
consider modifications targeted at improving the running time. (It is important to note that
the modifications we suggest should be evaluated experimentally on the systems under
consideration.) One way to avoid the possibility of a bad splitter is to sample more than
one element. For example, choosing the median of some small number of keys as the
splitter might result in a small (single-digit) percentage improvement in overall running
time for large input sets. As an extreme case, one might use the selection algorithm
presented earlier in this chapter to choose the splitter as the median value in the list.
Notice that this raises the time from T(1) to T(n) to choose the splitter. However, because
the selection of a splitter is bundled into the T(n) time partition routine, this increased
running time will have no effect on the asymptotic expected-case running time of
QuickSort (of course, it will often have a significant effect on the real running time of
Quick-Sort); further, because it guarantees good splits, choosing the split value in this
fashion lowers the worst-case running time of QuickSort to T(n log n).
If one is really concerned about trying to avoid the worst-case running time of QuickSort,
it might be wise to reduce the possibility of having to sort mostly ordered or reverse-
ordered data. As strange as it may seem, a reasonable way to do this is first to
randomize the input data. That is, take the set of input data and randomly permute it .
This will have the effect of significantly reducing the possibility of taking ordered
sequences of significant length as input.
After experimentation, the reader will note that QuickSort is very fast for large values of n
but relatively slow when compared to T(n2) time algorithms such as SelectionSort,
InsertionSort, or BubbleSort, for small values of n. The reader might perform an
experiment comparing QuickSort to SelectionSort, Insertion-Sort, and other sorting
methods for various values of n. One of the reasons that QuickSort is slow for small n is
that there is significant overhead to recursion. This overhead does not exist for straight-
sorting methods, like InsertionSort and SelectionSort, which are constructed as tight,
doubly nested loops. Therefore, one might consider a hybrid  approach to QuickSort that
exploits an asymptotically inferior routine, which is applied only in a situation where it is

better in practice. Such a hybrid sort can be constructed in several ways. The most
obvious is to use Quick-Sort (recursively) only as long as right - left = m, for some
experimentally determined m. That is, one uses the basic QuickSort routine of
partitioning and calling QuickSort recursively on both the left and right subarrays.
However, the base case changes from a simple evaluation of left < right  to right - left <
m. In the case that right - left < m , then one applies the straight-sorting routine that was
used to determine the cutoff value of m. Possibilities include SelectionSort and Insertion-
Sort, with SelectionSort typically being favored.
Consider an alternative approach. Sort the data recursively, so long as right - left = m.
Whenever a partition is created such that right - left < m , however, simply ignore that
partition (that is, leave that partition in an unsorted state). Notice that, at the end of the
entire QuickSort procedure, every element will be within m places of where it really
belongs. At this point, one could run Insertion-Sort on the entire set of data. Notice that
InsertionSort runs in T(mn) time, where n is the number of elements in the array, and m
is the maximum distance any element must move. Therefore, for m small, InsertionSort is
a fast routine. In fact, for m constant, this implementation of InsertionSort requires only
T(n) time. (We should not forget the preceding in analyzing the resulting hybrid version of
Quick-Sort, which from start to finish has the same asymptotic behavior as the non-hybrid
versions discussed earlier.) Further, compared to the previous hybrid approach, this
approach has an advantage in that only one additional procedure call is made, compared
to the O(n) procedure calls that could be made if small sub-arrays are immediately
sorted. Hence, this version of a hybrid QuickSort is generally preferred.
We now consider improvements in the space requirements of QuickSort. Recall that the
major space consideration is the additional space required for the system stack. One
might consider unrolling the recursion and rewriting QuickSort in a non-recursive fashion,
which requires maintaining your own stack. This can be used to save some real space,
but it does not have a major asymptotic benefit. Another improvement we might consider
is to maintain the stack only with jobs that need to be done and not jobs representing tail-
end recursion that are simply waiting for another job to terminate. However, in terms of
saving significant space, one should consider pushing the jobs onto the stack in an
intelligent fashion. That is, one should always push the jobs onto the stack so that the
smaller job (that is, the job sorting the smaller of the two subarrays) is evaluated first.
This helps to avoid or lessen the T(n) worst-case additional space problem, which can be
quite important if you are working in a relatively small programming environment.
TeamUnknown Release
0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Modifications of QuickSort for Parallel Models
There have been numerous attempts to parallelize QuickSort for various machines and
models of computation. One parallelization that is particularly interesting is the extension
of QuickSort, by Bruce Wagar, to HyperQuickSort , a QuickSort-based algorithm targeted
at medium- and coarse-grained parallel computers. In this section, we first describe the
HyperQuickSort  algorithm for a medium-grained hypercube and then present an analysis
of its running time.
HyperQuickSort
Initially, it is assumed that the n elements are evenly distributed among the 2d
hypercube nodes so that every node contains N= n/2d elements.1.
Each node sorts its N items independently using some T(N log N) time algorithm. 2.
Node 0 determines the median of its TV elements, denoted as Med. This takes T(1)
time because the elements in the node have just been sorted.3.
Node 0 broadcasts Med to all 2d nodes in T(d) time by a straightforward hypercube
broadcast routine.4.
Every node logically partitions its local set of data into two groups, X and Y, where X
contains those elements less than or equal to Med, and Y contains those elements
greater than Med. This requires T(log N) time via a binary search for Med among
the values of the node's data.5.
Consider two disjoint subcubes of size 2d-1, denoted as L and U. For simplicity, let L
consist of all nodes with a 0 as the most significant bit of the node's address, and let
U consist of all nodes with a 1 as the most significant bit of the node's address. Note
that the union of L and U is the entire hypercube of size 2d. So every node of the
hypercube is a member of either L or U. Each node that is a member of L sends its6.

set Y to its adjacent node in U. Likewise, each node in U sends its set X to its
adjacent node in L. Notice that when this step is complete, all elements less than or
equal to Med are in L, whereas all elements greater than Med are in U. This step
requires T(N) time for the transmission of the data.
Each node now merges  the set of data just received with the one it has kept (that is,
a node in L merges its own set X with its U-neighbor's set X; a node in U merges its
own set Y with its L-neighbor's set Y). Therefore, after T(N) time for merging two
sets of data, every node again has a sorted set of data.7.
Repeat steps 3 through 7 on each of L and U simultaneously, recursively, and in parallel
until the subcubes consist of a single node, at which point the data in the entire
hypercube is sorted.
The time analysis embedded in the previous presentation is not necessarily correct
because the algorithm continues to iterate (recurse) over steps 3 through 7, because
after some time the data may become quite unbalanced. That is, pairs of processors may
require ( (N) time to transmit and merge data. As a consequence, when the algorithm
terminates, all processors may not necessarily have TV items.
Assuming that the data is initially distributed in a random fashion, Wagar has shown that
the expected-case running time of this algorithm is
The N log N term represents the sequential running time from step 2, the d(d + 1) term
represents the broadcast step used in step 4, and the dN term  represents the time
required for the exchanging and merging of the sets of elements. We leave discussion of
the efficiency of this running time as an exercise.
In the next section , we will consider a medium-grained implementation of BitonicSort. We
will see that BitonicSort offers the advantage that, throughout the algorithm, all nodes
maintain the same number of elements per processor. However, given good recursive
choices of splitting elements, HyperQuickSort offers the advantage that it is more efficient
than BitonicSort.
TeamUnknown Release
0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
BitonicSort (Revisited)
In Chapter 4 , we presented some motivation, history, and a detailed description of
BitonicSort. In addition, we presented an analysis of the algorithm for several models of
computation. To recap, given a set of n elements, we showed that BitonicSort will run in
T(log2) time on a PRAM of size n, in T(log2n) on a hypercube of size n, and in T(n log2n)
time on a RAM. In this section, we consider BitonicSort on a medium-grained hypercube,
as a means of comparison to the HyperQuickSort routine presented in the preceding
section. We then consider BitonicSort on a mesh of size n.
Our initial assumptions are the same as they were for HyperQuickSort. Assume that we
are initially given n data elements evenly distributed among the 2d processors (nodes) so
that each processor contains N = n/2d items. Suppose that each processor sorts its initial
set of data in Q(N log N)  time. Once this is done, we simply follow the data movement
and general philosophy of the fine-grained BitonicSort algorithm, as previously presented.
The major modification is to accommodate the difference between processors performing
a comparison and exchange of two items (fine-grained model), and a comparison and
exchange of 2 7V items (medium-grained model).
Suppose processor A and processor B need to order their 2N items in the medium-
grained model so that the N smaller items will reside in processor A and the N larger
items will reside in processor B. This can be accomplished as follows. In T(N) time,
processors A and B exchange data so that each processor has the complete set of 2N
items. Each processor now merges the two sets of items in T(N) time simultaneously.
Finally, processor A retains the N smallest items (discarding the N largest items) and
processor B retains the N largest items (discarding the N smallest items).
The running time of BitonicSort on a medium-grained hypercube consists of the initial
T(N log N)  sequential sort, followed by the d(d + 1)/2 steps of BitonicSort, each of which
now requires T(N) time, resulting in a total running time of

As mentioned previously, the reader should note two major differences when considering
whether to use BitonicSort or HyperQuickSort on a medium-grained hypercube. The first
difference is the improvement in running time of HyperQuickSort over BitonicSort by a
relatively small factor. The second difference concerns the placement of the data when
the algorithm terminates. In BitonicSort, the data is distributed evenly among the
processors, whereas this is not the case with HyperQuickSort.
BitonicSort on a Mesh
In this section, we present a straightforward implementation of the fine-grained
BitonicSort algorithm on a fine-grained mesh computer. After the presentation of the
algorithm, we discuss details of the implementation and the effect that such details have
on the running time of the algorithm.
Initially, let's assume that a set of n data elements is given, arbitrarily distributed one per
processor on a mesh of size n. To perform sorting on a distributed-memory parallel
machine, we must define the ordering of the processors, because the elements are
sorted with respect to the ordering of the processors. Initially, we assume that the
processors are ordered with respect to shuffled row-major  indexing scheme, as shown in
Figure 9.10 . (Note that for a machine with more than 16 processors, this ordering holds
recursively within each quadrant.)
Figure 9.10: The shuffled-row major index scheme as applied to a mesh of size 16. It
is important to note that on a mesh of size n, this indexing continues recursively within
each quadrant
At the end of this section, we will discuss a simple way to adapt BitonicSort to whatever
predefined processor ordering is required or utilized. Recall that BitonicSort is a variant of
MergeSort. Viewed in a bottom-up fashion, initially bitonic sequences of size 2 are
bitonically merged into sorted sequences of size 4. Then bitonic sequences of size 4 are
bitonically merged into sorted sequences of size 8, and so on. At each stage, the
sequences being merged are independent, and the merging is performed in parallel on
all such sequences. In addition, recall that the concatenation of an increasing sequence
with a decreasing sequence forms a bitonic sequence. Therefore, we must be careful to
note when merging a bitonic sequence into a sorted sequence whether it is merged into
an increasing or a decreasing sequence. The reader may wish to review the section on
BitonicSort before proceeding with the remainder of this section.
In the example presented next, notice that we exploit the shuffled row-major indexing
scheme. Therefore, sequences of size 2 are stored as 1 × 2 strings, sequences of size 4
are stored as 2 × 2 strings, sequences of size 8 are stored as 2 × 4 strings, and so on. A
critical observation is that if a comparison and (possible) exchange must be made
between data that reside in two processors, those processors always reside in either the
same row or the same column . This is due to the properties of the shuffled row-major
indexing scheme coupled with the fact that BitonicSort only compares entries that differ in
one bit of their indexing.
Consider the example of BitonicSort on a mesh of size 16, as presented in Figure 9.11 .
This example shows how to sort the initial set of arbitrarily distributed data into increasing
order with respect to the shuffled row-major ordering of the processors. The first matrix
shows the initial set of arbitrarily distributed data. Notice that a sequence of size 1 is, by
default, sorted into both increasing and decreasing order. Therefore, initially, there are
n/2 bitonic sequences of size 2 (in the form of 1 × 2 strings), each of which must be
bitonically merged. This is accomplished by a single comparison, representing the base
case of the BitonicSort, resulting in the second matrix. Notice that some of the sequences
are sorted into increasing order and some into decreasing order. Next, we take this
matrix and wish to merge bitonic sequences of size 4 (in the form of 2 × 2 strings) into
sorted order. This is accomplished by first performing a comparison-exchange operation
between items that are two places apart in the indexing, followed by recursively sorting
each of the 1 × 2 strings independently. The fourth matrix shows the result of this sorting.
Notice that each of the four quadrants has data in sorted order with respect to the
shuffled row-major indexing. (The northwest and southwest quadrants are in increasing
order, whereas the northeast and southeast quadrants are in decreasing order.) The
example continues, showing the details of combining 2 × 2 strings into sorted 2 × 4
strings, and finally combining the two 2 × 4 strings into the final sorted 4 × 4 string.
Figure 9.11: An example of BitonicSort on a mesh of size 16. The elements are
sorted into shuffled-row major order, as given in Figure 9.10 . The initial data is given
in the top-left matrix. After applying a comparison-exchange operation between
indicated elements (for example, 10-9, 14-2, 4-15, …), the matrix has been ordered
into disjoint 1 × 2 segments, as indicated in the next matrix. The interpretation of the
figure continues in this manner. Note up to the final stage, we have half of the sorted
sections in ascending order, and the other half are in descending order
Analysis of Running Time
Recall from the detailed analysis of BitonicSort presented in Chapter 4  that BitonicSort is
based on MergeSort. Therefore, it requires T(log n) parallel merge operations (that is,
complete passes through the data), merging lists of size 1 into lists of size 2, then lists of
size 2 into lists of size 4, and so forth. However, the merge operation is not the standard
merge routine that one learns in a second-semester computer science course but rather
the more complex bitonic merge. Further, the time for each bitonic merge requires a
slightly more complex analysis than that of determining the time for a traditional merge.
For example, merging pairs of elements into ordered lists of size 2 requires one level of
comparison-exchange operations (which can be thought of as one parallel comparison-
exchange operation). This is the base case. Merging bitonic sequences of size 2 into
ordered lists of size 4 requires an initial comparison-exchange level (that is, n/2
comparison-exchange operations), followed by applying the BitonicSort routine for
sequences of size 2 to each of the resulting subsequences. Therefore, the total number
of comparison-exchange levels is 1 + 1 = 2. The time to merge bitonic sequences of size
4 into ordered sequences of size 8 requires one comparison-exchange level to divide the
data, followed by two (parallel) comparison-exchange levels to sort each of the bitonic
subsequences of size 4. Therefore, the total number of comparison-exchange levels to
merge a bitonic sequence of size 8 into an ordered sequence is three (1 + 2 = 3). In
general, the time to merge two bitonic sequences of size n/2 into an ordered sequence of
size n is T(log 2n).
Recall that to use the bitonic merge unit to create a sorting routine/network, we apply the
basic MergeSort scenario. That is, sorting an arbitrary sequence of n items requires us
first to sort (in parallel) two subsequences of size n/2, then to perform a comparison-
exchange on items n/2 apart, and then to merge recursively each subsequence of size
n/2. Therefore, the total number of comparison-exchange levels (or parallel comparison-
exchange operations) is
The reader should refer to the section on BitonicSort for the original presentation of this
analysis.
Now, consider a mesh implementation. Suppose that each of the T(log2n) comparison-
exchange levels is implemented by a rotation (either a column rotation or a row rotation,
as appropriate). Such an implementation leads to a T(n1/2 log2n) running time on a mesh
of size n. However, if we look closely at the data movement operations that are required
to perform the comparison-exchange operations, we notice that during the first iteration,
when creating the 1 × 2 lists, the data items are only one link apart. When creating the 2
× 2 lists, the data items are again only one link apart. When creating the 2 × 4 and 4 × 4
lists, the data items are either one or two links apart, and so forth. Therefore, if we are
careful to construct modified row and column rotations that allow for simultaneous and
disjoint rotations within segments of a row or column, respectively, the running time of
BitonicSort operations can be improved significantly. With this optimized rotation scheme,
the time to sort n items on a mesh of size n is given by the recurrence T(n) = T(n/2)  +
T(n1/2), where T(n/2)  is the time to sort each of the subsequences of size n/2, and T(n1/2)
is the time required to perform a set of n/2 comparison-exchange operations (that is, one
level of comparison-exchange operations). Therefore, the running time of the BitonicSort
algorithm is T(n1/2), which is optimal for a mesh of size n, due to the communication
diameter. Although the algorithm is optimal for this architecture, notice that the cost of the
algorithm is T(n3/2), which is far from optimal. We leave as an exercise the possibility of
modifying this architecture and algorithm to achieve a cost-optimal sorting algorithm on a
mesh.
Sorting Data with Respect to Other Orderings
How would we handle the situation of sorting a set of data on a fine-grained mesh into an
ordering other than shuffled row-major? For example, given a set of n data items, initially
distributed in an arbitrary fashion one per processor on a mesh of size n, how would the
data be sorted into row-major or snakelike order? If one is concerned only about
asymptotic complexity, the answer is quite simple: perform two sorting operations. The
first operation will sort data in terms of a known sorting algorithm into the indexing order
required by that algorithm. For example, one could use BitonicSort and sort data into
shuffled row-major order. During the second sort, each processor would generate a sort
key that corresponds to the desired destination address with respect to the desired
indexing scheme (such as row major or snakelike ordering).
Suppose that one wants to sort the 16 data items from the previous example into row-
major order. One could first sort the data into shuffled row-major order and then resort
the items so that they are ordered appropriately. For example, during the second sort,
keys would be created so that processor 0 would send its data to processor 0, processor
1 would send its data to processor 1, processor 2 would send its data to processor 4,
processor 3 would send its data to processor 5, processor 4 would send its data to
processor 2, and so forth (see Figure 9.12 ). The combination of these two sorts would
result in the data being sorted according to row-major order in the same asymptotically
optimal T(n1/2) time. Notice that this algorithm assumes that the destination addresses
can be determined in O(n1/2)time, which is sufficient for most well-defined indexing
schemes.
Figure 9.12: An example of sorting data on a mesh into row-major order by two
applications of sorting into shuffled-row major order. The initial unordered set of data
is given in (a). After applying a shuffled-row major sort, the data appears as in (b).
Note that in the lower-right corner of each item is the index for where that item should
be placed with respect to shuffled-row major order so that the data will be in row-
major order. The items are then sorted into shuffled-row major order with respect to
these indices, with the results in row-major order as shown in (c)
TeamUnknown Release
0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Concurrent Read/Write
In this section, we discuss an important application of sorting that is concerned with
porting PRAM algorithms to other architectures. The PRAM is the most widely studied
parallel model of computation. As a result, a significant body of algorithmic literature
exists for that architecture. Therefore, when one considers developing an efficient
algorithm for a non-PRAM-based parallel machine, it is often constructive to consider first
the algorithm that would result from a direct simulation of the PRAM algorithm on the
target architecture. To simulate the PRAM, one must be able to simulate the concurrent
read and concurrent write capabilities of the PRAM on the target machine.
A concurrent read  (in its more general form, an associative read)  can be used in a
situation where a set of processors must obtain data associated with a set of keys, but
where there need not be a priori  knowledge as to which processor maintains the data
associated with any particular key.
For example, processor Pi might need to know the data associated with the key "blue"
but might not know which processor Pj in the system is responsible for maintaining the
information associated with the key "blue." In fact, all processors in the system might be
requesting one or more pieces of data associated with keys that are not necessarily
distinct.
A concurrent write  (in its more general form, an associative write)  can be used in a
situation where a set of processors Pi must update the data associated with a set of
keys, but again Pi does not necessarily know which processor is responsible for
maintaining the data associated with the key.
As one can see, these concurrent read/write operations generalize the CR/CW
operations of a PRAM by making them associative , in other words, by locating data with
respect to a key rather than by an address. To maintain consistency during concurrent
read and concurrent write operations, we will assume that there is at most one master
record , stored in some processor, associated with each unique key. In a concurrent read,
every processor generates one request record  corresponding to each key about which it

wishes to receive information (a small fixed number). A concurrent read permits multiple
processors to request information about the same key. A processor requesting
information about a nonexistent key will receive a null message at the end of the
operation.
Implementation of a Concurrent Read
A relatively generic implementation of a concurrent read operation on a parallel machine
with n processors follows:
Every processor creates C1 master records  of the form [Key, Return Address, data,
"MASTER"], where C1 is the maximum number of keyed master records maintained
by any processor, and Return Address is the index of the processor that is creating
the record. (Processors maintaining less than C1 master records will create dummy
records so that all processors create the same number of master records.)1.
Every processor creates C2 request records  of the form [Key, Return Address, data,
"REQUEST"], where C2 is the maximum number of request records generated by
any processor, and Return Address is the index of the processor that is creating the
record. (Processors requesting information associated with less than C2 master
records will create dummy records so that all processors create the same number of
request records.) Notice that the data fields of the request records are presently
undefined.2.
Sort all (C1 + C 2)n records together by the Key field. In case of ties, place records
with the flag "MASTER" before records with the flag "REQUEST."3.
Use a broadcast  within ordered intervals to propagate the data associated with each
master record to the request records with the same Key field. This allows all request
records to find and store their required data.4.
Return all records to their original processors by sorting  all records on the Return
Address field.5.
Therefore, the time to perform a concurrent read, as described, is bounded by the time to
perform a fixed number of sort and interval operations (see Figure 9.13 .)
Figure 9.13: An example of a concurrent read on a linear array of size 4. (a) shows
the initial data, where each processor maintains one master record (M in the fourth
field) and generates one request record (R in the fourth field). (b) shows the records
sorted by the first field, with ties broken in favor of master records. (c) shows the
result of a segmented broadcast that propagates the third field to appropriate request
records. (d) shows the data sorted by the return address (second field)
Implementation of Concurrent Write (overview)
The implementation of the concurrent write is quite similar to that of the concurrent read.
In general, it consists of a sort step to group together records with similar keys, followed
by a semigroup operation in each group to determine the value to be written to the
master record, followed by a sort step to return the records to their original processors.
Again, it is assumed that there is at most one master  record , stored in some processor,
associated with each unique key. When processors generate update records , they
specify the key of the record and the piece of information they wish to update. If two or
more update records contain the same key, a master record will be updated with the
minimum data value of these records. (In other circumstances, one could replace the
minimum operation with any other commutative, associative, binary operation.)
Therefore, one can see that the implementation of the concurrent write is nearly identical
to the implementation just described for the concurrent read.
Concurrent Read/Write on a Mesh
A mesh of size n can simulate any PRAM algorithm that works with n data items on n
processors by using a concurrent read and concurrent write to simulate every step of the
PRAM algorithm. Suppose that a given PRAM algorithm runs in T(n) time. By simulating
every read step and every write step of the PRAM algorithm in a systematic fashion by a
T(n1/2) time concurrent read and concurrent write, respectively, the running time of the
PRAM algorithm as ported to a mesh of size n will be O(T(n)n1/2),which is often quite
good. In fact, it is often not more than some polylogarithmic factor from optimal.
TeamUnknown Release
0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we examine the divide-and-conquer paradigm of solving problems
recursively. We show the power of this paradigm by illustrating its efficient usage in
several algorithms for sorting, including sequential versions of MergeSort and QuickSort
and their adaptations to several parallel models. In addition, we revisited BitonicSort and
its implementations on both a coarse-grained hypercube and on a fine-grained mesh.
Efficient to optimal divide-and-conquer algorithms for selection and for concurrent read
and write operations on parallel computers are also given.
TeamUnknown Release

0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
Divide-and-conquer is a paradigm central to the design and analysis of both parallel and
sequential algorithms. An excellent reference, particularly for sequential algorithms, is
Introduction to Algorithms  by T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein (2nd
ed.: The MIT Press, Cambridge, MA, 2001). A nice text focusing on algorithms for the
hypercube, which includes some divide-and-conquer algorithms, is Hypercube Algorithms
for Image Processing and Pattern Recognition  by S. Ranka and S. Sahni (Springer-
Verlag, New York, 1990). More general references for theoretical parallel algorithms that
exploit the divide-and-conquer paradigm are Parallel Algorithms for Regular Architectures
by R. Miller and Q.F. Stout (The MIT Press, Cambridge, MA, 1996), and Introduction to
Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes , by F.T. Leighton
(Morgan Kaufmann Publishers, San Mateo, CA, 1992). Details of advanced PRAM
algorithms, including a T(log n) time sorting algorithm, can be found in An Introduction to
Parallel Algorithms  by J. Já Já, (Addison-Wesley, Reading, MA, 1992).
Optimal-cost PRAM algorithms for the selection problem are given in RJ. Cole's paper,
"An Optimally Efficient Selection Algorithm," Information Processing Letters  26 (1987/88),
295-99.
The QuickSort algorithm was originally presented by in "QuickSort," by C.A.R. Hoare,
Computer Journal , 5(1):10-15, 1962. Wagar's HyperQuickSort algorithm was originally
presented in, "Hyperquicksort: A Fast Sorting Algorithm for Hypercubes," by B. Wagar in
Hypercube Multiprocessors 1987 , M.T. Heath, ed., SIAM, 292-99.
TeamUnknown Release

0
Chapter 9 - Divide-and-Conquer
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
We have shown that QuickSort has a T(n2) running time if its input list is sorted or
nearly sorted. Other forms of input can also produce a T(n2) running time. For
example, let n = 2k for some positive integer k and suppose
the input list has key values x1, x2, …, xn;
the subsequence O = <x1, x3, x5,…,xn-1> of odd-indexed keys is decreasing;
the subsequence E = <x2, x4, x6,…,xn> of even-indexed keys is increasing;
xn-1 > x n (therefore, every member of O is greater than every member of E);
queues are used for the lists, with the partitioning process enqueueing new
items to smallList, equalList , and bigList;  and
the split value is always taken to be the first key in the list.
Show that under these circumstances, the running time of QuickSort will be T(n2).1.
In our sequential implementation of QuickSort, the "conquer" part of the algorithm
consists of two recursive calls. The order of these calls clearly does not matter in
terms of the correctness of the algorithm. However, the order of these recursive
calls does affect the size of the stack needed to keep track of the recursion. Show
that if one always pushes the jobs onto the stack so that the larger job is processed
first, then the stack must be able to store n items.2.
Suppose that in a parallel computer with n processors, processor Pi has data value
xi, i   {1, …, n}. Further, suppose that i   j Ü xi   xj. Describe an efficient algorithm
so that each processor Pi can determine the rank of its data value xi. That is, if xi is
the kth largest member of { xj}nj=1, then processor. Pi will store the value k at the end
of the algorithm. Analyze the running time of your algorithm in terms of operations3.

discussed in this chapter. Your analysis may be quite abstract. For example, you
may express the running time of your algorithm in terms of the running times of the
operations you use.
Suppose that you implement a linked-list version of QuickSort on a RAM using
predefined ADTs (abstract data types). Further, suppose the ADT for inserting an
element into a list is actually written so that it traverses a list from the front to the
end and then inserts the new element at the end of the list. Give an analysis of the
running time of QuickSort under this situation.4.
Suppose you are given a singly linked list on a RAM and mistakenly implement the
array version of QuickSort to perform the partition step. Give the running time of the
partition step and use this result to give the running time of the resulting version of
the QuickSort algorithm.5.
Describe and analyze the running time of BitonicSort given a set of n data items
arbitrarily distributed n/p per processor on a hypercube with p processors where n
<< p (where n is much larger than p).6.
Prove that algorithm Partition is correct.7.
Modify QuickSort so that it recursively sorts as long as the size of the subarray
under consideration is greater than some constant C. Suppose that if a subarray of
size C or less is reached, the subarray is not sorted. As a final postprocessing step,
suppose that this subarray of size at most C is then sorted by
InsertionSorta.
BubbleSortb.
SelectionSortc.
Give the total running time of the modified QuickSort algorithm. Prove that the
algorithm is correct.8.
Let S be a set of n distinct real numbers and let k be a positive integer with 1 < k < n.
Give a T(n) time RAM algorithm to determine the middle k entries of S. The input
entries of S should not be assumed ordered; however, if the elements of S are such
that s1 < s 2 < … < s n, then the output of the algorithm is the (unsorted) set
. Because the running time of the algorithm should be
T(n), sorting S should not be part of the algorithm.9.
10.
Analyze the running time of the algorithm you presented in response to the previous
query as adapted in a straightforward fashion for
a PRAM andi.
for a mesh.ii.10.
Develop a version of MergeSort for a linear array of T(log n) processors to sort n
data items, initially distributed T(n/log n) items per processor. Your algorithm should
run in T(n) time (which is cost optimal). Show that it does so.11.
Analyze the running time of a concurrent read operation involving T(n) items on a
mesh of size n.12.
Given a set of n data items distributed on a mesh of size m, m = n, so that each
processor contains n/m items, what is the best lower bound to sort these items?
Justify your answer. Provide an algorithm that matches these bounds.13.
Given a set of n input elements, arbitrarily ordered, prove that any sorting network
has a depth of at least log 2 n.14.
Prove that the number of comparison units in any sorting network on n inputs is  (n
log n).15.
Suppose that you are given a sequence of arcs of a circle R = <r1, r2,…,rn), and that
you are required to find a point on the circle that has maximum overlap. That is, you
are required to determine a (not necessarily unique) point q that has a maximum
number of arcs that overlap it. Suppose that no arc is contained in any other arc,
that no two arcs share a common endpoint, that the endpoints of the arcs are given
completely sorted in clockwise order, and that the tail point of an arc appears only
following the head of its arc. Give efficient algorithms to solve this problem on the
following architectures. Discuss the time, space, and cost complexity.
RAMa.
PRAMb.
Meshc.16.
Give an efficient algorithm to compute the parallel prefix of n values, initially
distributed one per processor in the base of a pyramid computer. Discuss the time
and cost complexity of your algorithm. You may assume processors in the base
mesh are in shuffled row major order, with data distributed accordingly.17.
18.
Show that the expected time 
 of Wagar's
Hyper-QuickSort algorithm achieves the ideal 
 for a
coarse-grained hypercube. Recall p = 2d is the number of processors,
 is the initial number of data items in each processor, and in the
coarse-grained model we assume p2 = n.18.
TeamUnknown Release
0
Chapter 10 - Computational Geometry
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 10: Computational Geometry
The field of computational geometry is concerned with problems involving geometric
objects such as points, lines, and polygons. Algorithms from computational geometry
are used to solve problems in a variety of areas, including the design and analysis of
models that represent physical systems such as cars, buildings, airplanes, and so on.
In fact, in Chapter 7 , "Parallel Prefix," we presented a solution to dominance,  a
fundamental problem in computational geometry. In this chapter, we consider several
additional problems from this important and interesting field. Many of the problems in
this chapter were chosen so that we could continue our exploration of the divide-and-
conquer solution strategy.
Convex Hull
The first problem we consider is that of determining the convex hull  of a set of points in
the plane. The convex hull is an extremely important geometric structure that has been
studied extensively. The convex hull of an object can be used to solve problems in image
processing, feature extraction, layout and design, molecular biology, geographic
information systems, and so on. In fact, the convex hull of a set S of points often gives a
good approximation of S, typically accompanied by a significant reduction in the volume
of data used to approximate S. Further, the convex hull of a set S is often used as an
intermediate step to obtain additional geometrical information about S.
Definition:  A set R is convex if and only if for every pair of points x, y e R, the line
segment is contained in R (see Figure 10.1 ). Let S be a set of n points in the plane.
The convex hull of S is defined to be the smallest convex polygon P containing all n
points of S. A solution to the convex hull problem consists of determining an ordered
list of points of S that define the boundary of the convex hull of S. This ordered list of

points is referred to as hull(S).  Each point in hull(S)  is called an extreme point of the
convex hull and each pair of adjacent extreme points is referred to as an edge of the
convex hull (see Figure 10.2 ).
Figure 10.1: Examples of convex and non-convex regions. The regions in (a) are
convex. The regions in (b) are not convex, because the line segments uv and xy
are not contained in their respective regions
Figure 10.2: The convex hull. The set S of n points in the plane is represented by
circles, some of which are black and some of which are gray. The extreme points
of S are represented by the gray points. The set of such extreme points is denoted
by hull (S). Each pair of adjacent extreme points represents an edge of the convex
hull
The reader may wish to consider an intuitive construction of the convex hull. Suppose
that each of the planar points in S is represented as a (headless) nail sticking out of a
board. Now take an infinitely elastic rubber band and stretch it sufficiently to surround all
the nails. Lower the rubber band over the nails so that all the nails are enclosed within
the rubber band. Finally, let the rubber band go so that it is restricted from collapsing only
by the nails in S that it contacts. Then the polygon P, determined by the rubber band and
its interior, represents the convex hull of S. The nails that cause the rubber band to
change direction are the extreme points of the convex hull. Further, adjacent extreme
points define the edges of the convex hull.
Notice that a solution to the convex hull problem requires presenting a set of points in a
predefined order. Therefore, we first consider the relationship between the convex hull
problem and the sorting problem.
Theorem:  Sorting is linear-time transformable to the convex hull problem. That is, in
T(n) time, we can transform a problem of sorting n real numbers to a problem of
finding the convex hull of n points in the Euclidean plane.
Proof:  Given a set of n real numbers, X = {x 1,…,x}, a convex hull algorithm can be
used to sort them with only linear overhead, as follows. Corresponding to each
number xi is the point pi = (x i, xi2). Notice that these n points all lie on the parabola y
= x2. The convex hull of this set consists of a list of all the distinct points pi sorted by
x-coordinate. If there are duplicated values, for example, if pi = p j, at most one of
these appears in the listing of members of hull(X) , then the unique representative in
hull(X)  of such duplicated values can keep track of the frequency of its value's
representation in X; doing so adds O(n) time. One linear-time pass through the list
will enable us to read off the values of xi in order.
Implications of Theorem:  Based on this theorem, we know the convex hull
problem cannot be solved asymptotically faster than we can sort a set of points
presented in arbitrary order. So, given an arbitrary set of n points in the Euclidean
plane, solving the convex hull problem requires O(n log n) time on a RAM.
Graham's Scan
In this section, we present a traditional sequential solution to the convex hull problem,
known as Graham s scan . It is important to note that this solution is not based on divide-
and-conquer. For that reason, the reader interested primarily in divide-and-conquer might
wish to skip this section. For those who continue, you may note that this algorithm is
dominated by sort and scan operations. The Grahams Scan  procedure is quite simple. A
description follows (see Figure 10.3 ).
Select the lowest (and in the case of ties, leftmost) point in S and label this as point
0.1.
2.
1.
Sort the remaining n - 1 points of S by angle in [0, p) with respect to the origin
(point 0). For any angle that includes multiple points, remove all duplicates, retaining
only the point at the maximum distance from point 0. Without loss of generality, we
will proceed under the assumption that the set S has n distinct points.2.
Now consider the points [1, …, n - 1] in sequence. We build up the convex hull in
an iterative fashion. At the ith iteration, we consider point S(i). For i= 1, we have point
S(1) initially considered an "active point" (it is an extreme point of the two element
set S(0, …, 1)). For 1 < i < n, we proceed as follows. Assume the active points prior
to the ith iteration are S(0), S(j1), …, S(j k), where 0 < j 1 < … < jk, < i.
Suppose that the path from S(j k-1) to S(j k) to S(i) turns toward the left at S(jk) to
reach S(i), as shown in Figure 10.4 . Then the point S(i) is an extreme point of
the convex hull with respect to the set of points S(0, …, i), and it remains
active. Further, all of the currently active points in S(0, …, i - 1) remain active
(those points that were extreme points of S(0, …, i - 1) will remain extreme
points of S(0, …, i)).a.
Suppose that the path from S(jk-1) to S(jk) to S(i) turns toward the right at S(jk)
to reach S(i), as shown in Figure 10.5 . Then the point S(i) is an extreme point
of the convex hull with respect to the set of points S(0, …, i), and it remains
active. However, we now know that some of the currently active points in S(0,
…, i -1) are not extreme points in S(0, …, i) and must be eliminated (become
inactive). This elimination is performed by working backward through the
ordered list of currently active points and eliminating each point that continues
to cause point S(i) to be reached via a right turn with respect to the currently
active points in S(0, …, i - 1). In fact, we need work backward only through the
ordered list of currently active points until we reach an active point that is not
eliminated.b.
Suppose that S(jk-1), S(j k), and S(i) are collinear (that is, the path from S(jk-1) to
S(jk) to S(i) does not turn, or turns exactly half a revolution, at S(jk) to reach
S(i)). Then one of these three points is between the other two and can be
eliminated because it cannot be an extreme point in S(0, …, i). Indeed,
because we previously saw to it that active members of S(1, …, n) have unique
angles with respect to S(0) and the active points are ordered with respect to
these angles, the point that is eliminated is S(jk) (see Figure 10.6 ).c.
Consider the example presented earlier in Figure 10.3 . We are required to
enumerate the convex hull of S, a set consisting of 11 points. Details of the
algorithm, as applied to this example, follow:
1.3.
Scan the list of points to determine the lowest point. Label this lowest point 0.
Note: if there is more than one lowest point, choose the leftmost one.1.
Sort the remaining n - 1 points by angle with respect to a horizontal line
through point 0.
The points are now ordered in counterclockwise fashion with respect to point 0,
as shown in Figure 10.3 . Initially, all n points are candidates as extreme points
of hull(S) .2.
The point labeled 0 must be an extreme point (hull vertex), because it is the
lowest point in the set S. We proceed to visit successive points in order,
applying the "right-turn test" described in the algorithm given earlier.3.
The first stop on our tour is point number 1, which is accepted because points 0
and 1 form a convex set.4.
Now, consider point number 2. Notice that the turn from point 0 to 1 to 2 is a
left turn. Therefore, points 0, 1, and 2 are extreme points with respect to
S(0,…,2).5.
Now, consider point number 3. Notice that the turn from point 1 to 2 to 3 is a
right turn. Therefore, we begin to work backward from the preceding point. That
is, point number 2 must be eliminated. Next, consider the turn from point 0 to 1
to 3. This is a left turn. Therefore, point number 1 remains, and this backward
scan to eliminate points is complete. So points 0, 1, and 3 are the extreme
points representing the convex hull of S(0,…3).6.
Now, consider point number 4. Notice that the turn from point 1 to 3 to 4 is a
left turn. Therefore, no points are eliminated, and we know that points 0, 1, 3,
and 4 are extreme points of S(0,…,4).7.
Now, consider point number 5. Notice that the turn from point 3 to 4 to 5 is a
right turn. Therefore, we begin to work backward from the preceding point. That
is, point number 4 is eliminated. Next, consider the turn from point 1 to 3 to 5.
Notice that this is a left turn. Therefore, the points 0, 1, 3, and 5 are the
extreme points representing the convex hull of S(0,…,5).8.
Now, consider point number 6. Notice that the turn from point 3 to 5 to 6 is a
right turn. Therefore, we begin to work backward from the preceding point. That
is, point number 5 is eliminated. Next, consider the turn from point 1 to 3 to 6.
This is a left turn. Therefore, the points 0, 1, 3, and 6 are the extreme points
representing the convex hull of S(0,…,6).9.
Now, consider point number 7. Notice that the turn from point 3 to 6 to 7 is a
left turn. Therefore, no points are eliminated, and we know that points 0, 1, 3, 6,10.
11.
and 7 are extreme points of S(0,…, 7).10.
Now, consider point number 8. Notice that the turn from 6 to 7 to 8 is a right
turn. Therefore, we begin to work backward from the preceding point. That is,
point number 7 is eliminated. Now consider the turn from point 3 to 6 to 8. This
is a left turn. Therefore, the points 0, 1, 3, 6, and 8 are the extreme points
representing the convex hull of S(0,…,8).11.
Now, consider point number 9. Notice that the turn from point 6 to 8 to 9 is a
right turn. Therefore, we begin to work backward from the preceding point. That
is, point number 8 is eliminated. Now consider the turn from point 3 to 6 to 9.
This is a left turn. Therefore, the points 0, 1, 3, 6, and 9 are the extreme points
representing the convex hull of S(0,…,9).12.
Now, consider point number 10. Notice that the turn from point 6 to 9 to 10 is a
left turn. Therefore, no points are eliminated, and we know that points 0, 1, 3, 6,
9, and 10 are extreme points of S(0,…,10). The solution is now complete.13.
Figure 10.3: Graham s scan is a technique for determining the convex hull of a set of
points. The lowest point is chosen as point 0, and the remaining points are sorted in
counterclockwise order with respect to the angles they make to a horizontal line
through point 0. Grahams scan examines the points in the order listed
Figure 10.4: A path from S(jk-1) to S(jk) to S(i) that makes a left turn at S(jk)
Figure 10.5: A path from S(jk-1) to S(jk) to S(i) that makes a right turn at S(jk)
Figure 10.6: A path from S(jk-1) to S(jk) to S(i) that is straight. That is, all three points
are collinear
Analysis on a RAM
Let's consider the running time and space requirements of Graham's scan on a RAM.
The first step of the algorithm consists of determining point 0, the lowest point in the set
S (in the case of ties, the leftmost of these lowest points). Assuming that S contains n
points, the lowest point can be determined in T(n) time by a simple scan through the
data. The remaining n - 1 points of S can then be sorted with respect to point 0 (and a
horizontal line through it) in T(nlog n) time. Next, the algorithm considers the points in
order and makes decisions about eliminating points. Notice that each time a new point i
is encountered during the scan, it will be an extreme point of S(0,…,i); this is because we
are traversing the points in order according to their angles with respect to S(0), and we
have eliminated (see step 3c) all but one member of any set in S \ {S(0)} = {s eS | s  
S(0)}  that has the same angle with S(0). Each time a new point is visited, 0(1) work is
necessary to
include the new point in the data structure if it is active, and
to stop any backward search that might arise.
The remainder of the time spent in the tour is accounted for when considering the total
number of points that can be eliminated, because with a judicious choice of data
structures, such as a separate array or a stack, no point is ever considered once it has
been eliminated. It is important to consider the analysis from a global perspective. No
point is ever eliminated more than once, so the total time required for the loop in step 3 is
T(n), though the analysis is a bit different than some of the straightforward deterministic
analyses presented earlier in the book. Therefore, the running time of Graham's scan on
a RAM is a worst-case optimal T(nlog n), because the running time is dominated by the
sort performed in step 2.
Next, we consider the space required in addition to that which is necessary to maintain
the initial set of points. Notice that this algorithm does not rely on recursion, so we need
not worry about the system stack. It does, however, require a separate data structure that
in the worst case might require a copy of every point. That is, it is possible to construct
situations where T(n) points are in the convex hull, for example, when the n points
approximate a circle. Therefore, if an additional stack or array is used, the additional
space will be T(n). However, if one maintains the points in a pointer-based data
structure, it is possible to avoid making copies of the points. Of course, the penalty one
pays for this is the additional T(n) pointers.
Parallel Implementation
Consider parallel implementations of Graham's scan. Notice that steps 1 and 2, which
require computing a semigroup operation on the data as well as sorting the data, can be
done efficiently on most parallel models. However, step 3 does not appear easily
amenable to a parallel implementation. One might try to remove concave regions in
parallel and hope that (reminiscent of our pointer-jumping algorithms) the number of such
parallel removals will be polylogarithmic in the number of points. However, consider the
situation where the first n - 1 points form a convex set, but when the last point is added
to this set, T(n) points must be removed. It is not clear that such a situation can be easily
parallelized.
Jarvis' March
An alternative convex hull algorithm is Jarvis march , which works by a package wrapping
technique. To illustrate this, consider a piece of string with one end fixed at the lowest
point (point number 0). Next, wrap the string around the nails representing the points in a
counterclockwise fashion. This can be done by iteratively adding the point with the least
polar angle with respect to a horizontal line through the most recently added point.
Because all the remaining points are considered at each iteration, the total running time
of this algorithm is O(nh) , where h is the number of vertices (extreme points) on hull(S) .
Therefore, when the number of extreme points is o(log n), Jarvis’ march is asymptotically
faster than Graham's scan.
Divide-and-Conquer Solution
In this section, we focus on divide-and-conquer solutions to the convex hull problem.
Initially, we present a generic divide-and-conquer solution. The analysis is then presented
based on an implementation for the RAM and mesh. At the conclusion of this section, we
present a divide-and-conquer algorithm, complete with analysis, targeted at the PRAM.
Generic Divide-and-Conquer Solution to the Convex Hull Problem
Assume that we are required to enumerate the extreme points of a set S of n planar
points. We will enumerate the points so that the rightmost point is labeled 1 (in the case
of ties, the lowest rightmost point is labeled 1). At the conclusion of the algorithm, the
numbering of the extreme points will be given in counterclockwise fashion, starting with a
rightmost point. Notice that for algorithmic convenience, the first enumerated extreme
point determined by this algorithm differs in position from the first enumerated extreme
point derived from Graham's scan (leftmost-lowest point). A generic divide-and-conquer
algorithm to determine the extreme points of the convex hull of a set of n planar points
follows.
If n = 2, then return . In this case, both of the points are extreme points of the given
set. If n > 2, then we continue with step 2.1.
Divide  the n points by x-coordinate into two sets, A and B, each of size
approximately n/2. The division of points is done so that all points in A are to the left
of all points in B. That is, A is linearly separable from B by a vertical line (see Figure
10.7).2.
Recursively  compute hull(A)  and hull(B) . See Figure 10.8 . 3.
Stitch together hull(A)  and hull(B)  to determine hull(S) . This is done as follows (see
Figure 10.9 ).
Find the upper and lower common tangent lines (often referred to as the lines
of support)  between hull(A)  and hull(B) .a.
Discard the points inside the quadrilateral formed by the four points that
determine these two lines of support.b.
Renumber the extreme points so that they remain ordered with respect to the
defined enumeration scheme. This is necessary because the algorithm is
recursive in nature.c.4.
Figure 10.7: A set of n planar points evenly divided into two sets A and B by x-
coordinate. All points in A lie to the left of every point in B
Figure 10.8: An illustration of the situation after hull(A)  and hull(B)  have been
determined from input shown in Figure 10.7
Figure 10.9: The stitch step. To construct hull(S)  from hull(A)  and hull(B) , the upper
common tangent line and lower common tangent line between hull(A)  and hull(B)  are
determined
Notice that step 2 requires us to divide the input points into disjoint sets A and B in such
a fashion that
every point of A is left of every point of B, and
both A and B have "approximately" n/2 members.
Unfortunately, if we are overly strict in our interpretation of "approximately," these
requirements might not be met. Such a situation might occur when the median x-
coordinate is shared by a large percentage of the input points. For example, suppose five
of 100 input points have an x-coordinate less than 0, 60 input points have x-coordinate
equal to 0, and 35 input points have x-coordinate greater than 0. The requirement that
every point of A is to the left of every point of B results in either |A| = 5 and |B| = 95, or |A|
= 65 and |B| = 35. This is not really a problem because the recursion will quickly rectify
the imbalance because at most two points with the same x- coordinate can be extreme
points of a convex hull. Thus, when we determine the vertical line of separation between
A and B, we can arbitrarily assign any input points that fall on this line to A.
This algorithm is a fairly straightforward adaptation of divide-and-conquer. The interesting
step is that of determining the lines of support. It is important to note that the lines of
support are not necessarily determined by easily identified special points. For example,
the lines of support are not necessarily determined by the topmost and bottommost
points in the two convex hulls, as illustrated in Figure 10.10 . Considerable thought is
required to construct an efficient algorithm to determine these four points, hence the two
tangent lines.
Figure 10.10: An illustration of the common tangent lines between linearly separable
convex hulls. The upper common tangent line between hull(A)  and hull(B)  does not
necessarily include the topmost extreme points in either set. A similar remark can be
made about the lower common tangent line
Because the convex hulls of A and B are linearly separable by a vertical line, there are
some restrictions on possibilities of points that determine the upper tangent line. For
example, consider al, a leftmost point of A and ar, a rightmost point of A. Similarly,
consider bl, a leftmost point of B, and br, a rightmost point of B. It is then easy to show
that the upper common tangent line is determined by an extreme point of hull(A)  on or
above 
 (the edges of hull(A)  on or above 
 are referred to as the upper
envelope of A)  and an extreme point of hull(B)  on or above 
 (on the upper
envelope of B). Similarly, the lower common tangent line is determined by an extreme
point of hull(A)  on or below 
 and an extreme point of hull(B)  on or below 
.
Therefore, without loss of generality, we focus on determining the upper common tangent
line, and note that determining the lower common tangent line is similar.
The extreme point p e hull(A)  that determines the upper common tangent line has the
property that if x and y are, respectively, its left and right neighbors among the extreme
points of hull(A)  (one or both of x and y  may not exist), then every extreme point of hull(B)
lies on or below 
, whereas at least one extreme point of hull(B)  lies on or above 
(see Figure 10.11 ). Notice that the mirror image scenario is valid in terms of identifying
the right common tangent point, that is, the upper common tangent point in hull(B) .
Figure 10.11: Constructing the upper common tangent lines. The upper common
tangent line includes the extreme point p e hull(A)  with the following properties. Let
the next extreme point in counterclockwise order be called x and the previous extreme
point in counterclockwise order be called y. Then every extreme point of hull(B)  lies on
or below 
 whereas at least one extreme point of hull(B)  lies on
or above 
Convex Hull Algorithm on a RAM
In this section, we consider the implementation details and running time of the divide-
and-conquer algorithm just presented on a RAM. To partition the points with respect to x-
coordinates, a T(nlog n) time sorting procedure can be used. In fact, it is important to
notice that this single sort will serve to handle the partitioning that is required at every
level of the recursion. That is, sorting is performed only once for partitioning, not at every
level of recursion. Now let's consider the stitch step. The necessary points can be
identified in T(log n) time by a clever "teeter-totter" procedure. Basically, the procedure
performs a type of binary search in which endpoints of a line segment (one from hull(A)
and the other from hull(B))  are adjusted in a binary-type iterative fashion. Once the
extreme points are identified, then with an appropriate choice of data structures, the
points can be reordered and renumbered in T(n) time. This eliminates the points inside
the quadrilateral determined by the lines of support. Therefore, the running time of the
algorithm is given by T(n) = T(n log n) + R(n) , where T(n log n) is the time required for
the initial sort, and R(n) is the time required for the recursive procedure. Notice that R(n)
= 2R(n / 2) + T(n), where T(n) time is required to stitch two convex hulls ( T(log n) time to
identify the tangent line and T(n) time to reorder the points). Therefore, the running time
of the entire algorithm is T(nlog n), which is asymptotically optimal.
Convex Hull Algorithm on a Mesh
In this section, we discuss a mesh implementation and provide an analysis of the divide-
and-conquer solution to the convex hull problem. Specifically, given n points, arbitrarily
distributed one point per processor on a mesh of size n, we will show that the convex hull
of the set S of planar points can be determined in optimal T(n1/2) time.
The basic algorithm follows. First, sort the points into shuffled row-major order. This
results in the first n/4 points (with respect to x-coordinate ordering) being mapped to the
northwest quadrant, the next n/4 points being mapped to the northeast quadrant, and so
forth, as shown in Figure 10.12 . Notice that with this indexing scheme, the partitioning
holds recursively in each quadrant.
Because this algorithm is recursive, we now need discuss only the binary search routine.
Notice that due to the mesh environment and the way in which we have partitioned the
data, we will perform simultaneous binary searches between S1 and S2, as well as
between S3 and S4. We will then perform a binary search between S1   S2 and S3  
S4. Therefore, we need to describe the binary search only between S1 and S2, with the
others being similar. In fact, we will describe only the binary search that will determine the
upper common tangent line between S1 and S2.
Notice that it takes T(n1/2) time to broadcast a query from S1 to S2  and then report the
result back to all processors in S1. So, in T(n1/2) time, we can determine whether some
line from S1 goes above all of the points in S2 or whether there is at least one point in S2
that is above the query line. If we continue performing this binary search in a natural way,
the running time of this convex hull algorithm will be T(n1/2 log n).
Figure 10.12: Dividing the n planar points in S so that each of the four linearly
separable sets of points is stored in a different quadrant of the mesh. Notice that the
vertical slabs of points in the plane need not cover the same area of space. They
simply must contain the same number of points
However, if we first perform a query from S1 to S2, and then one from S2 to S1, notice
that half of the data from S1 and half the data from S2 can be logically eliminated. The
reader should note that while logically eliminating points during this back-and-forth binary
search, reducing the total number of points under consideration by at least half during
each iteration, the points representing the common tangent line segments remain in the
active sets.
So, if the logically active data is compressed (that is, copied into a smaller submesh)
after the binary search, which requires T(n1/2) time, each iteration of the binary search
will take time proportional to the square root of the number of items remaining. Therefore,
such a dual binary search with compression will run in B(n) = B(n/2) +  T(n1/2) = T(n1/2)
time. Therefore, the total running time of the divide-and-conquer-based binary search on
a mesh of size n is the 0( n1/2) time for the initial sort plus
time for the remainder of the algorithm. Hence, the total running time to determine the
convex hull on a mesh of size n is T(n1/2), which is optimal for this architecture.
Convex Hull Algorithm on a PRAM
In this section, we present a divide-and-conquer algorithm to solve the convex hull
problem on a PRAM. The algorithm follows the spirit of the divide-and-conquer algorithm
that we have presented; however, the individual steps have been optimized for the
PRAM. The algorithm follows.
Partition  the set S of n planar points into n1/2 sets, denoted R, R, …, Rn1/2. The
partitioning is done so that all points in region Ri are to the left of all points in region
Ri+1 for 1 = i = n1/2 - 1 (see Figure 10.13 ). This partitioning is most simply
accomplished by sorting, as previously described.1.
Recursively  (and in parallel) solve the convex hull problem for every Ri, i e {1,2,…,
n1/2}. At this point, hull(R i) is now known for every Ri.2.
Stitch  the n1/2 convex hulls together in order to determine hull(S) . This is done by the
combine  routine that we define next.3.

Figure 10.13: An illustration of partitioning the set S of n planar points into n1/2
linearly separable sets, each with n1/2 points. The sets are denoted as R1, R2,…, Rn1/2
Combine
The input to the combine routine is the set of convex hulls, hull(R 1), hull(R 2),…, hull(R 1/2),
each represented by O(n1/2) extreme points. Notice that hull(R 1) = hull(R 2) = …
lhull(R 1/2), where we use "A = B"  to mean that "all points in A are to the left of all points in
B." The combine routine will produce hull(S) . As we have done previously, we will
consider only the upper envelopes of hull(R i), 1 = i = n1/2, and we will describe an
algorithm to merge these n1/2 upper envelopes to produce the upper envelope of hull(S) .
The procedure for determining the lower envelope is analogous. The algorithm follows.
Assign n1/2 processors to each set Ri of points. For each Ri, determine the n 1/2 - 1
tangent lines between hull(R i) and every distinct hull(R j). Notice that a total of n1/2 ×
(n1/2 - 1) = O(n) such upper tangent lines are determined. These tangent lines are
computed as follows.
Let Ti, j be used to denote the (upper) common tangent line between hull(R i)
and hull(R j), i   j.a.
For each Ri, use the kth processor that was assigned to it to determine the
upper tangent line between hull(R i) and hull(R k), i   k. Each of these upper
tangent lines can be determined by a single processor in O(log n) time by
invoking the "teeter-totter" algorithm outlined earlier. In fact, all 0( n) tangent
lines can be determined simultaneously in O(log n) time on a CREW PRAM.b.1.
Let Vi be the tangent line with the smallest slope in {T i,1, Ti,2, …,Ti, i-1}. That is, with
respect to Ri, Vi represents the tangent line of minimum slope that "comes from the
left." Let vi be the poin t of con tact of Vi with hul l(R i).2.
Let Wi b e the tang ent line w ith la rgest slop e in {T i, i+1, Ti, i+2,…,Tin1/2} That is, with
respect to Ri, Wi represents the tangent line of maximum slope that "comes from the
right." Let wi be the point of contact of Wi with hull(R i).3.
Notice that both Vi and Wi can be found in O(log n) time by the n1/2 processors
assigned to Ri. This requires only that the n1/2 processors perform a minimum or
maximum operation, respectively.4.
Because neither Vi nor Wi can be vertical, they intersect and form an angle (with the
interior point upward). If this angle is = 180°, or if wi is to the left of vi, then none of
the points of the upper envelope of hull(R i) belong to hull(S);  otherwise, all points5.
from vi to wi, inclusive, belong to hull(S)  (see Figures 10.14 , 10.15 , 10.16 , and
10.17 ). Notice that this determination is performed in T(1) time.5.
Finally, compress all of the extreme points of hull(S)  into a compact region in
memory in O(log n) time by performing parallel prefix computations.6.
The running time of the combine  routine is dominated by the time required to determine
the common tangent lines and the time required to organize the final results. Therefore,
the running time for the combine routine is O(log n).
Figure 10.14: Suppose that v i is to the left of w i and that the angle above the
intersection of their tangents exceeds 180o. Then all of the extreme points of Ri
between (and including) vi and wi are extreme points of S
Figure 10.15: Suppose that vi = w i and that the angle above the intersection of their
tangents exceeds 180°. Then vi is an extreme point of S
Figure 10.16: Suppose that vi = w i and that the angle above the intersection of their
tangents does not exceed 180o. In this case, no extreme point on the upper envelope
of Ri is an extreme point of S
Figure 10.17: Suppose that wi is to the left of vi. Then no extreme point on the upper
envelope of Ri is an extreme point of S
PRAM Analysis
Although it is beyond the scope of this text, we have mentioned that sorting can be
performed on a PRAM in T(log n) time. Therefore, the running time of this convex hull
algorithm is given by T(n) = S(n) + R(n) , where S(n) = T(logn)  is the time required for the
initial sort, and R(n) = R(n1/2) + C(n) is the time required for the recursive part of the
algorithm, including the C(n) = O(logn)  time combine routine. Hence, the running time for
this convex hull algorithm is T(log n). Further, this results in an optimal total cost of T(n
log n).
TeamUnknown Release
0
Chapter 10 - Computational Geometry
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Smallest Enclosing Box
In this section, we consider the problem of determining a smallest enclosing "box" of a
set of points. That is, given a set S of n planar points, determine a (not necessarily
unique) minimum-area enclosing rectangle of S. This problem has applications in layout
and design. Because a rectangle is convex, it follows from the definition of convex hull
that any enclosing rectangle of S must enclose hull(S) . One can show that for a
minimum-area enclosing rectangle, each of its edges must intersect an extreme point of
hull(S)  and one of the edges of the rectangle must be collinear with a pair of adjacent
extreme points of hull(S)  (see Figure 10.18 ).
Figure 10.18: A smallest enclosing box of S. A (not necessarily unique) minimum-
area enclosing rectangle of S includes three edges, each of which contains an
extreme point of hull(S) , and one edge that is collinear with an edge of hull(S)
A straightforward solution to the smallest enclosing box problem consists of the following
1.

steps:
Identify the extreme points of the set S of n planar points. 1.
Consider every pair of adjacent extreme points in hull(S) . For each such pair, find
the three maximum points, as shown in Figure 10.18 , and as described below.
Given a line collinear with 
, the point E associated with 
 is the last
point of hull(S)  encountered as a line perpendicular to 
 passes through
hull(S)  from left to right.a.
Similarly, the point N associated with 
 is the last point encountered as a
line parallel to 
, originating at 
, passes through hull(S) .b.
Finally, the point W associated with 
 is the last point of hull(S)
encountered as a line perpendicular to 
 passes through hull(S)  from right
to left.c.2.
For every adjacent pair of extreme points, x and x', determine the area of the
minimum enclosing box that has an edge collinear with 
.3.
A smallest enclosing box of S is one that yields the minimum area over all of the
rectangles just determined. Therefore, identify a box that corresponds to the
minimum area with respect to those values determined in step 3.4.
RAM
We have shown that the convex hull of a set S of n planar points can be determined in
T(n log n) on a RAM. Further, given m enumerated extreme points, for each pair of
adjacent extreme points, one can determine the other three critical points by a binary
search type of procedure in T(log m) time. Therefore, the time required to determine the
m restricted minimum-area rectangles is T(m log m) . Once these m rectangles have been
determined, a minimum-area rectangle over this set can be determined in Q(m)  time by a
simple scan. Therefore, the running time for the entire algorithm on a RAM is T(nlog n +
mlog m)  = T(n log n), because m = 0(n) .
PRAM
Consider the same basic strategy as just presented for the RAM. Notice that the m
restricted minimum-area rectangles can be determined simultaneously in T(log m) time
on a PRAM. Further, a semigroup operation can be used to determine the minimum of
these in 0(log tn) time. Therefore, the running time of the entire algorithm, including the
time to determine the extreme points of the convex hull, is T(log n + log m) = T(log n) on
a PRAM.
Mesh
Given a mesh of size n, we have shown how to enumerate the m extreme points of
hull(S)  in T(n1/2) time. To arrive at an asymptotically optimal algorithm for this
architecture, we need to be able to design a T(n1/2) time algorithm to generate the m
rectangles. Once we have generated the rectangles, we know that a straightforward
T(n1/2) time semigroup operation can be used to identify one of these of minimum area.
So how do we determine all m minimum-area rectangles simultaneously in T(n1/2) time?
Recall that the extreme points of hull(S)  have been enumerated. Each point is incident on
two hull edges. Each such edge has an angle of support  that it makes with hull(S) . These
angles are all in the range of [0,27 p), where the angle (in radian measure) is viewed with
respect to the points of S (see Figure 10.19 ). Consider the situation in which every edge
 is trying to determine its point N. This corresponds to the situation in which every
edge 
 is searching for the extreme point of hull(S)  that has an angle of support that
differs from that of 
 by n. For edge 
 to determine its other two points, E and W,
it is simply searching for points bounded by hull edges with angles of support that differ
from that of 
 by p/2 and 3 p/2, respectively. Therefore, these simultaneous searches
can be performed simply by a fixed number of sort-based routines and ordered interval
broadcasts. We leave the details to the reader, though we should point out that these
operations are essentially performed by concurrent read operations. Therefore, the
running time of this algorithm, including the time to identify the extreme points of hull(S) ,
is 0(n1/2).
Figure 10.19: An illustration of angles of support. The angle of incidence of hull edge
 is p/2, of 
 is 3p/4, of
 is p and so forth. An angle of support of extreme point A is
in [p/2, 3p/4]. An angle of support of extreme point B is in [3 p/4, p], and so forth
TeamUnknown Release
0
Chapter 10 - Computational Geometry
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
All-Nearest Neighbor Problem
In this section, we consider another fundamental problem in computational geometry.
Suppose we have a set S of n planar points and for every point in S we want to know its
nearest neighbor with respect to the other points in S. That is, we are required to
determine for every point p e S, a point 
, such that dist\p,p  is the minimum dist(p, 
),p
  q,q e S. For this reason, the problem is often referred to as the all-nearest neighbor
problem .
An optimal T(n log n)-time algorithm for the RAM typically consists of constructing the
Voronoi Diagram of S  and then traversing this structure. The Voronoi Diagram  of a set of
planar points consists of a collection of n convex polygons, where each such polygon Ci
represents the region of two-dimensional space such that any point in Ci is closer to pi e
S than to any other point in S. The Voronoi Diagram is an important structure in
computational geometry. Unfortunately, a detailed discussion of its construction, either
sequentially or in parallel, is beyond the scope of this book.
In this section, we will concentrate on an interesting divide-and-conquer solution to the
all-nearest neighbor problem for the mesh. Notice that an optimal 0( n1/2)-time algorithm
on a mesh of size n carries with it a cost of T(n3/2). It therefore seems possible that we
can do better (lower cost) than a brute-force algorithm that uses T(n2) operations to
compute distances between all pairs of points.
We consider an algorithm that partitions the points into disjoint sets of points, solves the
problem recursively within each set of points, and then stitches together the partial results
in an efficient fashion. We prevent the stitching process from becoming the dominant
step by partitioning in such a way that almost all of the points in each partition know their
final answer after the recursive solution. We can accomplish this by partitioning the plane
into linearly separable vertical slabs, solving the problem recursively within each vertical
slab, then repartitioning the plane into linearly separable horizontal slabs, and solving the
problem recursively within each horizontal slab. We can then exploit a theorem from
computational geometry that states that there are no more than some fixed number of

points in each rectangle formed by the intersection of a horizontal and vertical slab that
could have a nearest neighbor somewhere other than in its horizontal or vertical slab (see
Figure 10.20 ).
Figure 10.20: The nearest neighbor of p is in neither the same horizontal nor vertical
slab as p is
We now give an outline of the algorithm.
Solve the problem recursively in vertical slabs, as follows.
Sort the n points in S by x-coordinate, creating four vertical slabs. a.
Solve the all-nearest neighbor problem recursively (steps 1 through 3) within
each vertical slab.b.1.
Solve the problem recursively in horizontal slabs, as follows.
Sort the n points in S by j-coordinate, creating four horizontal slabs. a.
Solve the all-nearest neighbor problem recursively (steps 1 through 3) within
each horizontal slab.b.2.
Sort the n points of S with respect to the identity of their boxes. The identity of a
specific box is given as the concatenation of the label of the vertical slab and the
label of the horizontal slab.
For the points in each box, it is important to note that a result from
computational geometry shows that at most two points closest to each corner
of the box could be closer to a point outside the box than to any point found so
far. Notice that there are no more than 8X16 = 128 such corner points.a.
b.3.
Each of these corner points can now be passed through the mesh so that they
can view (and be viewed by) all n points. After this traversal, each of these
corner points will know its nearest neighbor. Hence, the solution will be
complete.b.
Running Time
The running time of this algorithm on a mesh of size n is given as T(n) = 2T(n/4)  +
T(n1/2). Using the Master Method, we can determine that this recurrence has a solution
of T(n) = T(n1/2 log n), which is within a log n factor of optimal for this architecture.
TeamUnknown Release
0
Chapter 10 - Computational Geometry
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Architecture-Independent Algorithm Development
A number of interesting problems in computational geometry lend themselves to
architecture-independent algorithm development . That is, we can state an algorithm to
solve the problem that may be implemented on a variety of architectures. It should be
noted that such algorithms are often stated at a very high level because they usually
involve basic operations such as sorting, prefix, semigroup operations, computation of
the convex hull, and so forth. These are operations for which implementation details may
be quite different on different architectures. Nevertheless, these operations may be
regarded as fundamental abstract operations in the sense that efficient implementations
are known on popular models of computation. Suppose problem X can be solved by an
algorithm consisting of the computation of a convex hull, followed by a prefix
computation, followed by a semigroup operation. A straightforward implementation of this
algorithm on a given model Y results in a running time that is the sum of the times for
these three fundamental operations as implemented on model Y.
Algorithms discussed for the remainder of this chapter will be presented in an
architecture-independent style. In the exercises that follow, the reader will be asked to
analyze the running times of these algorithms on a variety of architectures.
TeamUnknown Release

0
Chapter 10 - Computational Geometry
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Line Intersection Problems
Suppose we are given a set L of n line segments in the Euclidean plane. The segments
may be arbitrary, or we may have additional knowledge, such as that every member of L
is either horizontal or vertical. Common line intersection problems  include the following:
Intersection Query:  Determine if there is at least one pair of members of L that
intersect.
Intersection Reporting:  Find and report all pairs of members of L that intersect.
An easy, though perhaps inefficient method of solving the intersection query problem is
to solve the intersection reporting problem and then observe whether any intersections
were reported. We might hope to obtain an asymptotically faster solution to the
intersection query problem that does not require us to solve the intersection reporting
problem.
An obvious approach to both problems is based on an examination of each of the T(n2)
pairs of members of L. It is easy to see how such an approach yields an 0(n2) time RAM
algorithm for the intersection query problem, and a T(n2) time RAM algorithm for the
intersection reporting problem. In fact, other solutions are more efficient:
Consider the intersection query problem:  In T(n) time, create two records for
each member of L, one for each endpoint. Let each record have an indicator as to
whether the endpoint is a left or right endpoint (lower corresponds to right in the
case of a vertical segment). Sort these records in ascending order by the x-
coordinates of their endpoints, using the left/right  switch as the secondary key, with
right < left , and y- coordinates as the tertiary key. Now, perform a plane sweep
operation, which allows us to "sweep the plane" from left to right, maintaining an
ordered data structure T of non-intersecting members of L not yet eliminated from
consideration, as possible members of an intersecting pair. Assume that T is a data
structure such as a red-black tree in which insert, retrieve, and delete operations can

be done in sequential 0(log n) time. As we move the vertical "sweep line" from left to
right and encounter a left endpoint of a member s of L, we insert s into T, then
determine if s intersects either of its at most two neighbors in T; if we find an
intersection, we report its existence and halt. As the sweep line encounters a right
endpoint of a member s of L, we remove s from T, and, as previously, determine if s
intersects either of its at most two neighbors in T. If we find an intersection, we
report its existence and halt; otherwise, we continue the plane sweep (see Figure
10.21 ).
Consider the Intersection Reporting Problem:  We can construct an algorithm
with an output-sensitive  running time for the RAM, which is asymptotically faster
under certain conditions than the straightforward T(n2) time required for the brute-
force algorithm. The term output-sensitive  refers to the fact that the amount of
output is a parameter of the running time. That is, if there are k intersections, a RAM
algorithm for this problem can be constructed to run in O((n + k)  log n) time. Thus, if
k = o(n2/log n) , such an algorithm is asymptotically faster than one that examines all
pairs. Such an algorithm can be obtained by making minor modifications to the
previous solution for the intersection query problem. The most important change is
that instead of halting upon discovering an intersection, we list the intersection and
continue the plane sweep to the right.
Figure 10.21: Illustration of a plane sweep operation to solve the intersection
query problem. The line segments are labeled by left endpoint. As a sweep of all
the endpoints is performed from left to right, when a left endpoint is encountered,
the line segment is inserted into the list at the appropriate ordered (top to bottom)
position and is tested for intersection with its neighbors in the list. The currently
active ordered list of line segments is shown beneath each endpoint. When a
right endpoint is encountered, an evaluation of an intersection is made before
removing that point from the ordering. Here, when the left endpoint of e is
encountered, the d-e intersection is detected
Overlapping Line Segments
In Chapter 7 , we examined the following:
The coverage query problem , in which we determine whether a given fixed interval
[a, b]  is covered by the union of an input set of intervals, and
The maximal overlapping point problem , where we determine a point of the real line
that is covered by the largest number of members of an input set of intervals.
Such problems fall within the scope of computational geometry. Another problem in
computational geometry that is concerned with overlapping line segments is the minimal-
cover  problem, which can be expressed as follows: Given an interval [a, b]  and a set of n
intervals S = {[a i, bi]}i=1, find a minimal-membership subset S' of S such that [a, b]  is
contained in the union of the members of S', if such a set exists, or report that no such
set exists. Another version of this problem uses a circle and circular arcs instead of
intervals.
An application of this problem is in minimizing the cost of security. The interval [a, b]
(respectively, a circle) might represent a borderline (respectively, convex perimeter) to be
guarded, and the members of S, sectors that can be watched by individual guards. A
positive solution to the problem might represent a minimal cost solution, including a listing
of the responsibilities of the individual guards, for keeping the entire borderline or
perimeter under surveillance.
Efficient solutions exist for both the interval and circular versions of these problems,
which are quite similar. The circular version seems to be the one that has appeared most
often in the literature. For the reader's convenience, however, the interval version will be
the one we work with, because some of its steps are easier to state than their analogs in
the circular version of the problem.
We discuss a greedy  algorithm, that is, an algorithm marked by steps designed to reach
as far as possible toward completion of a solution. The algorithm is greedy in that it starts
with a member of S that covers a and extends maximally to the right. (If no such member
of S exists, the algorithm terminates and reports that the requested coverage does not
exist.) Further, once a member s e S is selected, a maximal successor  for s is determined
(in other words, a member of S that intersects with s and extends maximally to the right).
This procedure continues until either b is covered (a success) or a successor cannot be
found (a failure). Thus, a high-level view of this algorithm follows.
Find a member s e S that covers a and has a maximal right endpoint. If no such
member of S exists, report failure  and halt.
While failure  has not been reported and s = [ai, bi] does not cover b, assign to s a
member of S\{s} that has a maximal right endpoint among those members of S\{s}
that contain bi. If no such member of S\{s} exists, report failure  and halt.
At the end of these steps, if failure has not been reported, the selected members of S
form a minimal-cardinality cover of [a, b] . See Figure 10.22 , in which the intervals of S
have been raised vertically in the Euclidean plane for clear viewing but should be thought
of as all belonging to the same Euclidean line.
Figure 10.22: A minimal-cardinality cover of [ a, b] consists of arcs 3, 4, 6, and 7
The preceding approach seems inherently sequential. We can make some changes so
that the resulting algorithm can be implemented in parallel, yet uses key ideas mentioned
earlier, as follows:
For each t e S, find its successor, if one exists. 1.
For each t e S, take the union of t and its successor as a chain of at most two
connected intervals. Then take the union of this chain of at most two intervals and its
final arc's successor's chain of at most two intervals to produce a chain of at most
four. Repeat this doubling until the chain starting with t either does not have a
successor chain or covers b.2.
Use a minimum operation to find a chain that covers [a, b]  with a minimal number of
intervals.3.
As is so often the case, "find" operations, including those mentioned previously, are
typically facilitated by having the data sorted appropriately. It is useful to have the
intervals ordered from left to right. However, because the data consists of intervals rather
than single values, some thought must be given to what such an ordering means. Our
primary concern is to order the intervals in such a way as to enable an efficient solution
to the problem at hand. The ordering that we use is embedded in the algorithm that
follows, which relies on a postfix operation on the ordered intervals to determine maximal
overlap of [a, b]  with a minimum number of intervals.
Sort the interval records by left endpoint, breaking ties in favor of maximal right
endpoin ts.1.
2.
1.
We o bserve that if {[a i, bi],[aj, bj]}  S and [a i, bj] [aj], then any connected chain of
members of S of minimal-cardinality among those chains that start with [ai, bi] and
cover [a, b] , will have at least as many members as a connected chain of members
of S of minimal-cardinality among those chains that start with [ aj, bj] and cover [a, b] .
Therefore, we can remove all such nonessential intervals [ai, bi] by performing a
simple prefix operation on the ordered set of interval data. Without loss of generality,
we will proceed under the assumption that no remaining member of S is a subset of
another remaining member of S.2.
For each remaining ai, bi] e S, create two records. The first set of records, called
successor records , consists of two components, namely, the index i of the interval
and the indexy of the successor of the interval. For each inter val [ai, bi] e S, we
initialize its successor record to (i, i), with the interpretation that initially every interval
is its own successor. Notice that during the procedure, the first component of these
records does not change, whereas the second component will eventually point to
the successor of interval [ai, bi]. The second set of records, referred to as
information records , contains con nectivity information. The components of the
information records include the following.
The first two components are the left and right endpoints, respectively, of the
connected union of members of S represented by the record's chain of
intervals.
The third and fourth components represent the indices of the leftmost and
rightmost members of the record's chain, respectively.
The fifth component is the index of the successor to the rightmost interval in the
record's chain (the successor to the interval indexed by the fourth component).
The sixth component is the number of members of S in the arc's chain. For
each record [ai, bi]e, S we initialize an information record to (ai, bi, i,i,i,1) .
3.
Sort the information records into ascending order by the second component.4.
In this step, we use the first four components of the information records. Determine
the successor of each member of S as follows, where ° is an operation defined as
Thus, A°B represents [ai, bi] [ak, bm], provided these arcs intersect and [ak, bm]
extends [ai, bi] to the right more than does [aj bj]; otherwise, A ° B = A . Use a parallel
postfix operation with operator ° to compute, for each information record
representing [at, bt], the transitive closure of ° on all records representing arc i up5.
through and including the information record representing arc n. Because the
intervals are ordered by their left endpoints, it follows that the fourth component of
the postfix information record representing arc [ai, bi] is the index of the successor of
the chain initiated by [ai, bi,].
For all i e{1, 2,…,n}, copy the fourth component of the postfix information record
created in the previous step, representing [ai, bi], to the second component of the
successor record representing [ai, bi], so that the successor record for [ai, bi] will
have the form (i, s i), where si is the index of the successor of [a i, bi]6.
For all i e [1,2,…,n], compute the chain of intervals vi obtained by starting with [ai, bi]
and adding successors until either b is covered or we reach an interval that is its
own successor. This can be done via a parallel postfix computation in which we
define • as
7.
A minimum operation on { vi,}ni=1, in which we seek the minimal sixth component
such that the interval determined by the first and second components contains [a, b] ,
determines whether a minimal-cardinality covering of [a, b]  by members of S exists,
and, if so, its cardinality. If j is an index such that Vj yields a minimal-cardinality
covering of [a, b]  by members of S, the members of S that make up this covering
can be listed by a parallel prefix operation that marks a succession of successors
starting with [ aj bj].8.
TeamUnknown Release
0
Chapter 10 - Computational Geometry
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we consider algorithms to solve several interesting problems from
computational geometry. Problems considered include computation of the convex hull of
a set of planar points, computation of a smallest enclosing box for a set of planar points,
the all-nearest neighbor problem, and several problems concerning line intersections and
overlaps in the Euclidean plane. Several of these problems are discussed in an
architecture-independent fashion, which allows us to obtain efficient to optimal solutions
for a variety of models of computation.
TeamUnknown Release

0
Chapter 10 - Computational Geometry
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
The focus of this chapter is on efficient sequential and parallel solutions to fundamental
problems in the field of computational geometry. The reader interested in a more
comprehensive exploration of computational geometry is referred to Computational
Geometry  by F.P. Preparata and M.I. Shamos (Springer-Verlag, 1985). In fact, the proof
that sorting is linear-time transformable to the convex hull problem comes from this
source. The reader interested in parallel implementations of solutions to problems in
computational geometry is referred to S.G. Akl and K.A. Lyons’ Parallel Computational
Geometry  (Prentice Hall, 1993).
The Graham's scan algorithm was originally presented in "An Efficient Algorithm for
Determining the Convex Hull of a Finite Planar Set," by R.L. Graham in Information
Processing Letters  1, 1972, 132-33. The Jarvis’ march algorithm was originally presented
by R.A. Jarvis in the paper "On the Identification of the Convex Hull of a Finite Set of
Points in the Plane," Information Processing Letters  2, 1973, 18-21. These algorithms are
also presented in a thorough fashion in Introduction to Algorithms  (2nd ed.: The MIT
Press, Cambridge, MA, 2001) by T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein.
The generic divide-and-conquer solution to the convex hull problem presented in this
chapter is motivated by the material presented in Parallel Algorithms for Regular
Architectures  by R. Miller and Q.F. Stout (The MIT Press, Cambridge, MA, 1996). The
"teeter-totter" binary search algorithm referred to when describing an intricate binary
search for determining common tangent lines was originally presented by M.H. Overmars
and J. van Leeuwen in "Maintenance of Configurations in the Plane," in the Journal of
Computer and Systems Sciences , vol. 23, 1981, 166-204. The interesting divide-and-
conquer algorithm for the PRAM was first presented by M. Atallah and M. Goodrich in
"Efficient Parallel Solutions to Some Geometric Problems," in the Journal of Parallel and
Distributed Computing  3, 1986, 492-507. One might note that this algorithm exploits the
CR capabilities of a CREW PRAM. We should point out that an optimal 0(log n) time
EREW PRAM algorithm to solve the convex hull problem has been presented by R. Miller
& Q.F. Stout in "Efficient Parallel Convex Hull Algorithms," in IEEE Transactions on

Computers , 37 (12), 1988. However, the presentation of the Miller and Stout algorithm is
beyond the scope of this book.
The notion of angles of support is interesting in that it allows multiple parallel searches to
be implemented by a series of sort steps. Details of the mesh convex hull algorithm that
relies on angles of support can be found in Parallel Algorithms for Regular Architectures .
The reader interested in learning more about the Voronoi Diagram and its application to
problems involving proximity might consult Computational Geometry  by F.P Preparata
and M.I. Shamos (Springer-Verlag, 1985). Details of the all-nearest neighbor algorithm
for the mesh can be found in Parallel Algorithms for Regular Architectures .
A RAM algorithm for the circular version of the cover problem was presented by C.C. Lee
and D.T. Lee in "On a Cover-Circle Minimization Problem," in Information Processing
Letters  18 (1984), 180-85. A CREW PRAM algorithm for the circular version of this
problem appears in "Parallel Circle-Cover Algorithms," by AA. Bertossi in Information
Processing Letters  27 (1988), 133-39. The algorithm by Bertossi was improved
independently in each of the following papers:
MJ. Atallah and D.Z. Chen, "An Optimal Parallel Algorithm for the Minimum Circle-
Cover Problem," Information Processing Letters  32 (1989), 159-65.
L. Boxer and R. Miller, "A Parallel Circle-Cover Minimization Algorithm," Information
Processing Letters  32 (1989), 57-60.
D. Sarkar and I. Stojmenovic, "An Optimal Parallel Circle-Cover Algorithm,"
Information Processing Letters  32 (1989), 3-6.
The exercises of this chapter, which appear in the next section , include questions
concerning the all maximal equally spaced collinear points problem . This and several
related problems were studied in the following papers:
A.B. Kahng and G. Robins, "Optimal Algorithms for Extracting Spatial Regularity in
Images," Pattern Recognition Letters  12 (1991), 757-64.
L. Boxer and R. Miller, "Parallel Algorithms for All Maximal Equally Spaced Collinear
Sets and All Maximal Regular Coplanar Lattices," Pattern Recognition Letters  14
(1993), 17-22.
G. Robins, B.L. Robinson, and B.S. Sethi, "On Detecting Spatial Regularity in Noisy
Images," Information Processing Letters  69 (1999), 189-95.
L. Boxer and R. Miller, "A Parallel Algorithm for Approximate Regularity," Information
Processing Letters  80 (2001), 311-16.
These problems have considerable practical value, because the presence of the
regularity amidst seeming or expected chaos is often meaningful. For example, the
members of S might represent points observed in an aerial or satellite photo, and the
maximal equally spaced collinear sets might represent traffic lights, military formations,
property or national boundaries in the form of fence posts, and so forth. The paper of
Kahng and Robins presents a RAM algorithm for the all maximal equally spaced collinear
sets problem that runs in optimal T(n2) time. This algorithm seems to be essentially
sequential. The 1993 Boxer and Miller paper shows how a rather different algorithm can
be implemented in efficient to optimal time on parallel architectures. These two papers
are concerned with exact solutions. The Robins et al. paper gives an approximate
sequential solution that runs in 0(n5/2) time. The asymptotically slower running time for an
approximate solution (as opposed to an exact solution) is because an approximate
solution may have more output than an exact solution; notice, however, that an
approximate solution is likely to be more useful than an exact solution, because data is
generally not exact. On the other hand, the approximate solution to this problem is
beyond the scope of this book. The algorithm of Robins et al. seems essentially
sequential. A rather different algorithm appears in the 2001 Boxer and Miller paper, giving
an efficient approximate parallel solution that can be implemented on multiple platforms.
TeamUnknown Release
0
Chapter 10 - Computational Geometry
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
Many of the exercises in this section can be considered for a variety of models of
computation.
Given a set S of n planar points, construct an efficient algorithm to determine
whether there exist three points in S that are collinear. Although there are T(n3)
triples of members of S, it is possible (and you should try) to obtain an algorithm that
runs in o(n3) sequential time.1.
Given a set of n line segments in the plane, prove that there may be as many as
T(n2) intersections.2.
Show that the algorithm sketched in this chapter to solve the intersection query
problem runs in T(nlog n) time on a RAM.3.
Given a set of n line segments in the plane that have a total of k intersections, show
that a RAM algorithm can solve the intersection reporting problem, reporting all
intersections, in O((n + k)  log n) time.4.
Given a convex polygon with n vertices, construct an efficient algorithm to determine
the area of the polygon. Input to the problem consists of the circularly ordered edges
(equivalently, vertices) of the polygon.5.
Given a polygon with n vertices, construct an efficient algorithm to determine
whether or not the polygon is simple.6.
Given two simple polygons, each consisting of n vertices, give an efficient algorithm
to determine whether or not the polygons intersect.7.
Given a simple polygon P and a point p, give an efficient algorithm to determine
whether or not p is contained in P.8.
Give an efficient algorithm to determine the convex hull of a simple polygon.9.
10.

9.
On a fine-grained parallel computer, a very different approach can be taken to the
Intersection Reporting Problem. Suppose input to a PRAM, mesh, or hypercube of n
processors consists of the n line segments in the Euclidean plane. In the case of a
mesh or hypercube, assume the segments are initially distributed one per
processor. Provide a solution to the Intersection Reporting Problem that is optimal in
the worst case, and prove the optimality, for each of these architectures. Hints:  This
can be done with an algorithm that "seems" simpler to describe than the RAM
algorithm described in the text. Also, the processors of a hypercube may be
renumbered in a circular fashion.10.
In this chapter, we sketched an algorithm to solve the following problem: for a set of
n intervals and a range [a, b] , determine a minimal-cardinality subset of the intervals
that cover [a, b]  or to show, when appropriate, that no such cover exists. Prove the
algorithm runs in
T(n log n) time on a RAM;
T(log n) time on a CREW PRAM;
T(n1/2) time on a mesh of n processors, assuming the intervals are initially
distributed one per processor.
11.
In the Graham's scan procedure given in this chapter, prove that both the point
chosen as the origin and the last point encountered in the tour must be extreme
points of the convex hull.12.
Given a set S of n planar points, prove that a pair of farthest neighbors (a pair of
points at maximum distance over all pairs of points in S) must be chosen from the
set of extreme points.13.
Given two sets of points, P and Q, give an efficient algorithm to determine whether P
and Q are linearly separable . That is, give an efficient algorithm to determine
whether or not it is possible to define a line l with the property that all points of P lie
on one side of l whereas all points of Q lie on the other side of l.14.
In this problem, we consider the all maximal equally spaced collinear points problem
in the Euclidean plane R2: given a set S of n points in R2, identify all of the maximal
equally spaced collinear subsets of S that have at least three members. A collinear
set {p1, p2,…,pk} (assume the points are numbered according to their order on their
common line) is equally spaced  if all the line segments 
, T{1,2,…,k — 1},
have the same length. Assume that we are given a set S of n points in R2, where
each point is represented by its Cartesian coordinates (see Figure 10.23 ).
a.15.
Show that O(n2) is an upper bound for the output of this problem. Hint:  show
that every pair of distinct points {p,q}   S can be a consecutive pair of at most
one maximal equally spaced collinear subset of S.a.
Show that  (n2) is a lower bound for the worst-case output of this problem.
Hint:  let n be a square and let S be the square of integer points
Let S'   S be defined by
Show that if {p,q}  S', p q, then {p,q} is a consecutive pair in a maximal
equally spaced collinear subset C of S such that |C| = 3. Together with the
previous exercise, this shows the worst-case output for this problem is T(n2).b.
Consider the following algorithm, which can be implemented on a variety of
architectures (the details of implementing some of the steps will vary with the
architecture).
Form the set P of all ordered pairs (p,q) e S such that p < q  in the
lexicographic order of points in R2. The lexicographic order is defined as
follows: if p = (p x py) and q = (q x, qy), then p < q if either px < q x or both px
= qx and p y < q y.i.
Sort the members of P in ascending order with respect to all the following
keys:
Slope of the line determined by a member of P as the primary key
(use ° for a vertical segment);
Length of the line segment determined by a member of P as the
secondary key;
Lexicographic order of the endpoints of P as the tertiary key.
ii.
Use a parallel postfix operation on P to identify all maximal equally spaced
collinear subsets of S. The operation is based on the formation of
quintuples and a binary operation specified as follows. Initial quintuples
are of the form (p,q, length, 2, true) , where the first two components are
the endpoints (members of S) in an equally spaced collinear set; the thirdiii.c.
is the length of segments that make up the current equally spaced
collinear set; the fourth component is the number of input points in the
equally spaced collinear set; and the fifth component is true or false
according to whether the first component is the first point in an equally
spaced collinear set. The binary operation is defined by
and in the former case, set v = false .
A postfix operation on the members of P is used to enumerate members
of each equally spaced collinear set of more than two points. This
operation is based on members of P with a postfix quintuple having the
fifth component true and the fourth component greater than two.iv.
Figure 10.23: The all maximal equally spaced collinear points problem. An illustration
of three equally spaced collinear line segments
Analyze the running time of this algorithm for each of a RAM, a CREW PRAM of n2
processors, and a mesh of n2 processors. In the case of the mesh, assume that the
members of S are initially distributed so that no processor has more than one member of
S. Formation of the set P can thus be done on the mesh by appropriate row and column
rotations, and/or random-access write operations. The details are left to the reader.
TeamUnknown Release
0
Chapter 11 - Image Processing
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 11: Image Processing
In this chapter, we consider some fundamental problems in image processing, an
important and challenging area of computer science. A focus of this chapter is divide-
and-conquer algorithms for the mesh. Even though this chapter focuses
predominantly on one solution strategy for one particular model of parallel
computation, we will present algorithms for the RAM, as appropriate.
Preliminaries
In this chapter, we consider the input to the problems to be an n × n digitized black-and-
white picture. That is, the input can be viewed as a matrix (mesh) of data in which every
element is either a 1 (black) or a 0 (white). These "picture elements" are typically referred
to as pixels.  The interpretation of the image is that it is a black image on a white
background, and the set of black pixels can be referred to as a digital image.  The
terminology and assumptions that we use in this chapter represent the norm in the field
of image processing.
Readers must be very careful to recalibrate their expectations. In most of the preceding
chapters, the input was of size n, whereas in this chapter the input is of size n2.
Therefore, a linear time sequential algorithm will run in T(n2) time, not in T(n) time. If the
input data is to be sorted on a RAM, an optimal worst-case comparison-based sequential
sorting algorithm will take T(n2 log n) time, not T(n log n) time.
Because we want to map the image directly onto the mesh, we assume that pixel Pi,j is
mapped to mesh processor Pi,j on a mesh of size n2. Again, we need to recalibrate. For a
mesh of size n2, the communication diameter and bisection width are both T(n). So, for
any problem that might require pixels at opposite ends of the mesh to be combined in
some way, a lower bound on the running time of an algorithm to solve such a problem is
given as  (n).

There is an important mesh result that we will use in this chapter, concerned with
determining the transitive closure of a matrix. Let G be a directed graph with n vertices,
represented by an adjacency matrix A That is, A(i, j)  = 1 if and only if there is a directed
edge in G from vertex i to vertex j. Otherwise, A(i, j)  = 0. The transitive closure of A,  which
is typically written as A*, is an n × n matrix such that A*(i, j)  = 1 if and only if there is a
(directed) path in G from vertex i to vertex j. A*(i, j)  = 0 otherwise.
It is important to note that both A and A* are binary matrices. That is, A and A* are
matrices in which all entries are either 0 or 1. Consider the effect of "multiplying" matrix A
by itself to obtain the matrix we denote as A2, where the usual method of matrix
multiplication is modified by replacing addition (+) with OR (  ) and multiplication (×) with
AND (  ). Notice that an entry A2(i, j) = 1 if and only if either
A(i, j)  = 1, or
A(i, k)  = 1 AND A(k, j)  = 1 for some k.
That is, A2(i, j) = 1 if and only if there is a path of length no more than two from vertex i to
vertex_ j. Now consider the matrix A3, which can be computed in a similar fashion from A
and A2. Notice that A3(i, j) = 1 if and only if there is a path from vertex i to vertex j that
consists of three or fewer edges. Continuing this line of thought, notice that the matrix An
is such that An (i, j)  = 1 if and only if there is a path from vertex i to vertex j that consists
of n or fewer edges (see Exercises).
That is, An contains information about the existence  of a directed path in the graph G
from vertex i to vertex j, for every pair of vertices (i, j). The matrix An, which is often
referred to as the connectivity matrix,  represents the transitive closure of A. That is, An =
A*.
Consider a sequential solution to the problem of determining the transitive closure of an n
× n matrix A. Based on the preceding discussion, it is clear that the transitive closure can
be determined by multiplying A by itself n times. Because the traditional matrix
multiplication algorithm on two n × n matrices takes T(n3) time, we know that the
transitive closure of A can be determined in O(n × n3) = O(n4) time. So the question is,
can we do better? Well, consider matrix A2. Once A2 has been determined, we can
multiply it by A to arrive at A3 or multiply it by itself (A2) to arrive at A4. If our interest is in
determining An using the least number of matrix multiplications, it is far more efficient to
produce A4, rather than A3, from A2 and all preceding matrices. In fact, notice that if we
overshoot An, it doesn't matter. It is easily verified that An+c =An for any positive integer c
(see Exercises). Therefore, if we perform 0(log n) matrix multiplication operations, each
time squaring the resulting matrix, we can reduce the natural running time of transitive
closure from T(n4) down to T(n3 log n).
In fact, we can produce the matrix An even more efficiently, as follows. Define a Boolean
matrix Ak so th at A k(i, j) = 1 if and only if there is a path from vertex i to vertex. j using no
intermediate vertex with label greater than k.  Notice that this is a nonstandard
interpretation of a Boolean matrix. Given this matrix, an algorithm can be designed that
will iteratively transform A0 = A to An = An = A*  through a series of intermediate matrix
computations Ak, 0 < k < n.  We define Ak(i, j) = 1 if and only if
there is a path from vertex i to vertex j using no intermediate vertex greater than k-
1, or
there is a path from vertex i to vertex k using no intermediate vertex greater than k-
1 and there is a path from vertex k to vertex j using no intermediate vertex greater
than k- 1.
We now present Warshalls algorithm  to determine the transitive closure of a Boolean
matrix:
Whereas the running time of Warshall's algorithm on a RAM is T(n3), notice that the
algorithm requires T(n2) additional memory. This is because at the kth iteration of the
outermost loop, it is necessary to keep the previous iteration's matrix Ak-1 in memory.
F.L. Van Scoy has shown that given an n × n adjacency A matrix mapped onto a mesh of
size n2 such that A(i, j)  is mapped to processor Pi, j, the transitive closure of A can be
determined in optimal T(n) time. Details of this algorithm are presented in Chapter 12 .
Note 
Because pixels are mapped to processors of a fine-grained mesh in a natural
fashion, we tend to think about pixels and processors as coupled when designing
mesh algorithms. Therefore, when there is no confusion, we will use the terms pixel
and processor interchangeably in describing fine-grained mesh algorithms.
TeamUnknown Release
0
Chapter 11 - Image Processing
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Component Labeling
In this section, we consider the problem of uniquely labeling every maximally connected
component in an image. Efficient algorithms to solve the component-labeling problem
serve as fundamental tools to many image-processing tasks. Given a digitized black-and-
white picture, viewed as a black image on a white background, one of the early steps in
image processing is to label uniquely each of the distinct figures (that is, components) in
the picture. Once the figures are labeled, one can process the image at a higher level to
recognize shapes and to develop relationships among objects.
It is often convenient to recast the component-labeling problem in graph theoretic terms.
Consider every black pixel to be a vertex. Consider that an edge exists between every
pair of vertices represented by neighboring black pixels. We assume that pixels are
neighbors  if and only if they are directly above, below, to the left of, or to the right of each
other. (This notion of neighbors is called 4-adjacency  in the literature.) In particular, pixels
that are diagonally adjacent are not considered neighbors for the purpose of this
presentation, though such an interpretation does not typically affect the asymptotic
analysis of component-labeling algorithms. The goal of a component-labeling algorithm is
to label uniquely every maximally connected set of pixels (vertices). Although the label
chosen for every component is irrelevant, in this book we will choose to label every
component with the minimum label over any pixel (vertex) in the figure (component). This
is a fairly standard means of labeling components (see Figure 11.1 ).
Figure 11.1: (a) A digitized 4 × 4 picture. The interpretation is that it is a black image
on a white background. (b) The same 4 × 4 picture with its maximally connected
components labeled under 4-adjacency definition of connectedness. Each component

is labeled with the pixel of minimum label in its components, where the pixel labels are
taken to be the row-major labeling with values 1, …,16
RAM
Initially, let's consider a sequential algorithm to label the maximally connected
components of an n × n digitized black-and-white picture. Suppose we use a
straightforward propagation-based algorithm. Initialize the component label  for every pixel
to nil. Initialize the vertex label  for every pixel to the concatenation of its row and column
indices. Now traverse the image in row-major order. When a black pixel is encountered
that has not previously been assigned a component label, assign that pixel's vertex label
as its component label. Next, recursively propagate this component label to all of its black
neighbors (which recursively propagate the label to all of their black neighbors, and so
on).
Let's consider the running time of this simple propagation algorithm. Every pixel is visited
once during the row-major scan. Now consider the backtracking phase of the algorithm,
in which both black and white pixels can be visited. The black pixels can be visited as the
propagation continues, and the white pixels serve as stopping points to the backtracking.
Fortunately, every component is labeled only once, and if backtracking is done properly,
every black pixel is visited only a fixed number of times during a given
backtracking/propagation phase. That is, when a black pixel p is visited, no more than
three of its neighbors need to be considered (why?). Further, during the recursion, control
returns to the pixel p three times before it returns control to its parent pixel (that is, the
black pixel visited immediately prior to visiting p for the first time). A white pixel can be
visited only by four of its neighbors during some propagation phase, each time returning
control immediately. Therefore, the running time of the algorithm is linear in the number
of pixels, which is T(n2).
Mesh
We will now consider a divide-and-conquer algorithm to solve the general component-
labeling problem on a mesh. This algorithm is traditional and exhibits an asymptotically
optimal worst-case running time. Assume that we are given an n × n digitized black-and-
white picture mapped in a natural fashion onto a mesh of size n2 so that pixel Pi,j is
mapped to processor Pi, j. The first algorithm we might consider is a direct
implementation of the sequential propagation algorithm. If we implement the algorithm
directly, then clearly the running time remains at T(n2), which is unacceptable for this
architecture. Therefore, let's consider the natural parallel variant of a propagation-type
algorithm. That is, every processor that maintains a black pixel continually exchanges its
current component label with each of its black neighbors (four at most). During each such
exchange, a processor accepts the minimum of its current label and that of its black
neighbors as its new component label. The effect is that the minimum vertex/processor
label in a component is propagated throughout the component in the minimum time
required (that is, using the minimum number of communication links required), assuming
that all messages must remain within a component. In fact, this label reaches every
processor in its component in the minimum time necessary to broadcast the label
between them, assuming that all messages must remain within the component.
Therefore, if all the (maximally) connected components (figures) are relatively small, this
is an effective algorithm. If every figure is enclosed in some k × k region, the running time
of the algorithm is O(k2). This is efficient if k2 = O(n).  In fact, if we regard k as constant,
the running time is 0(1) (see Figure 11.2 ).
Figure 11.2: Each connected component is confined to a 3 × 3  region. In such
situations, the mesh propagation algorithm will run in T(1) time
Now let's consider the worst-case running time of this parallel propagation algorithm.
Suppose we have a picture that consists of a single figure. Further, suppose that the
internal diameter (the maximum distance between two black pixels, assuming that one
travels only between pixels that are members of the figure) is large. For example,
consider Figure 11.3 , which includes a "spiral" on the left and a "snake" on the right.
Figure 11.3: Two problematic figures. A spiral is shown on the left, and a snake is
shown on the right
We see that it is easy to construct a figure that has an internal diameter of T(n2). This
propagation algorithm will run in T(n2) time on such a figure. So, our parallel propagation
algorithm has a running time of  (1) and O(n2). For many situations, we might be willing
to accept such an algorithm if we know a priori  that these troublesome situations (that is,
the worst-case running time) will not occur. There may be situations in which, even if
such an image might occur, we know that no figure of interest could have such
characteristics, and we could then modify the algorithm so that it terminates after some
more reasonable predetermined amount of time. However, there are many situations in
which we care about minimizing the general worst-case running time.
We will now consider a divide-and-conquer algorithm to solve the general component-
labeling problem on a mesh. This divide-and-conquer algorithm is fairly traditional and
exhibits an asymptotically optimal worst-case running time:
Divide  the problem into four subproblems, each of size (n/2) × (n/2). 1.
Recursively  label each of the independent subproblems. 2.
Stitch  the partial solutions together to obtain a labeled image. 3.
As with many divide-and-conquer algorithms, the stitch step is crucial. Notice that once
each (n/2) × (n/2) subproblem has been solved, there are only O(n) labels in each such
submesh that might be incorrect in a global sense. That is, for every (global) component
completely contained within its (n/2) × (n/2) region, the recursive label must be correct.
Only those local components (components of one of the (n/2) × (n/2) regions) with at
least one pixel on an edge between neighboring submeshes might need to be relabeled
(see Figure 11.4 ). Therefore, whereas the initial problem had T(n2) pieces of data
(pixels), after the recursive solutions were obtained, there are only O(n) critical pieces of
information (that is, information that is necessary to obtain the final result). We can stitch
together the partial results as follows.
Figure 11.4: An 8 × 8 image after labeling each of its 4X4 quadrants. Notice that the
component labels come from the shuffled row-major indexing scheme, starting with
processor 1 (not 0). The global components that are completely contained in a
quadrant (components 4 and 20) do not need to be considered further. The remaining
components are required for a global relabeling procedure
First, each processor P containing a black pixel on the border of one of the (n/2 ) × (n /2)
regions examines its neighbors that are located in a distinct (n/2 ) × (n /2) region. For
each such border processor P, there are either one or two such neighbors. For each
neighboring black pixel in a different region, processor P generates a record containing
the identity and current component label of both P and the neighboring pixel. Notice that
there are at most two records generated by any processor containing a border vertex.
However, also notice that for every record generated by one processor, a "mirror image"
record is generated by its neighboring processor. Next, compress these O(n) records into
an n1/2 × n 1/2 region within the n × n  mesh. In the n1/2 × n1/2 region, use these O(n)
unordered edge records to solve the component-labeling problem on the underlying
graph.
Notice that the stitch step can perform the compression operation by sorting the
necessary records in T(n) time. Once the critical data is compressed to an n1/2 × n1/2
region, we can perform a logarithmic number of iterations to merge components together
until they are maximally connected. Each such iteration involves a fixed number of sort-
based operations, including concurrent reads and writes. Therefore, each iteration is
performed in 0(n1/2) time. Hence, the time required for computing maximally connected
components within the n1/2 × n1/2 region is T(n1/2 log n). Completing the stitch step
involves a complete T(n) time concurrent read so that every pixel in the image can
determine its new label. Because the compression and concurrent read steps dominate
the running time of the Stitch routine, the running time of the algorithm is given as T(n2) =
T(n2/4) + T(which sums to T(n2) = T(n) (one can reach this conclusion, for example, by
substituting N = n2 and applying the Master Theorem to the resulting recursion, T(N) =
T(N/4) +  0(7V1/2)). Notice that this is a time-optimal algorithm for a mesh of size n2.
However, the total cost of such an algorithm is 0(n3), although the problem has a lower
bound of  (n2) total cost.
We now consider an interesting alternative to the stitch step. In the approach that we
presented, we reduced the amount of data from 0(n2) to 0(n), compressed the 0(n) data,
and then spent time working on it leisurely. Instead, we can consider creating a cross
product with the reduced amount of critical data. That is, once we have reduced the data
to 0(n) critical pieces, representing an undirected graph, we can create an adjacency
matrix. Notice that the adjacency matrix will fit easily into the n × n mesh. Once the
adjacency matrix is created, we can perform the T(n) time transitive closure algorithm of
Van Scoy mentioned at the beginning of the chapter to determine maximally connected
components. The minimum vertex label can be chosen as the label of each connected
component, and a concurrent read by all pixels can be used for the final relabeling.
Although the running time of this algorithm remains at 0(n), it is instructive to show
different approaches to dealing with a situation in which one can drastically reduce the
size of the set of data under consideration.
TeamUnknown Release
0
Chapter 11 - Image Processing
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Convex Hull
In this section, we consider the problem of marking the extreme points of the convex hull
for each labeled set of pixels in a given image. Suppose that we have a mesh of size n2
and that we associate every processor Pi,j with the lattice point (i,j). Suppose that every
processor contains a label in the range of 0… n2, where the interpretation is that 0
represents the background (a white pixel) and that values in the range of 1… n2
represent labels of foreground (non-white) pixels. Further, assume that all pixels with the
same label are members of the same set of points and that we want to determine the
convex hull for each distinctly labeled set of points.
Notice that a maximal set of points with the same label need not be a connected
component. In fact, the sets might be intertwined and their convex hulls might overlap, as
shown in Figure 11.5 .

Figure 11.5: An illustration of overlapping convex hulls of labeled (not necessarily
connected) sets of pixels
We have discussed the general convex hull problem for various models in a preceding
chapter. Clearly, the image input considered in this section can be simply and efficiently
converted to the more general form of two-dimensional point data input. From such input,
the algorithms of the previous chapter  can be invoked in a straightforward fashion. Our
goal in this section, however, is to introduce some new techniques, which will result in a
greatly simplified routine for a lattice of labeled points imposed on a mesh.
Initially, we determine the extreme points for each labeled set as restricted to each row.
Once this is done, we note that there are no more than two possible extreme points in
any row for any labeled set. Within each such set, every row-restricted extreme point can
consider all other row-restricted extreme points of its set and determine whether or not it
is contained in some triangle formed by the remaining points, in which case it is not an
extreme point. Further, if no such triangle can be found, it is an extreme point. The
algorithm follows.
Initially in every row, we wish to identify the extreme points for every labeled set. In a
given row, the extreme points of each set are simply the (at most two) outermost nonzero
points of the set. This identification can be done by a simple row rotation, simultaneously
for all rows, so that every processor can view all of the data within its row and decide
whether it is an extreme point for its labeled set.
Next, sort all of these row-restricted extreme points by label so that after the sort is
complete, elements with the same label are stored in adjacent processors. Although
there are 0(n2) such points, it is important to note that for any label, there are at most 2n
such points (at most two points per each row). Because all of the row-restricted extreme
points for a given set are now in a contiguous set of processors, we can perform rotations
within such ordered intervals. These rotations are similar to row and column rotations but
work within intervals that might cover fractions of one or more rows. Thus, simultaneously
for all intervals (that is, labeled sets), rotate the set of row-restricted extreme points.
During the rotation, suppose a processor is responsible for lattice point X Then as a new
lattice point Y arrives, the processor responsible for X performs the following operations.
If no other point is stored in the processor, then the processor stores Y.
Suppose the processor has stored one other point previously, say, U; then the
processor will store Y. However, if X, Y,  and U are on the same line, then the
processor eliminates the interior point of these three.
Suppose the processor has previously stored two other points, U and V, before 7
arrives.
1.
If X is in the triangle determined by U, V,  and Y, then the processor determines that
X is not an extreme point.1.
Otherwise, if 7 is on a line segment determined by X and either U or V, then of the
three collinear points, X is not interior (otherwise, the previous case would apply).
Discard the interior of the three collinear points, Y or U (respectively, 7 or V).2.
Otherwise, the processor should eliminate whichever of U, V,  and Y is inside the
angle formed by X and the other two, with X as the vertex of the angle. (Note the
"eliminated" point is not eliminated as a possible extreme point, just as a point that is
useful in determining whether X is an extreme point.)3.
If after the rotation, the processor responsible for row-restricted extreme point X has not
determined that X should be eliminated, then X is an extreme point.
A final concurrent read can be used to send the row-restricted extreme points back to
their originating processors (corresponding to their lattice points) and the extreme points
can then be marked.
Running Time
The analysis of running time is straightforward because no recursion is involved. The
algorithm consists of a fixed number of T(n) time rotations and sort-based operations.
Therefore, the running time of this algorithm is T(n). Notice that the cost of the algorithm
is T(n3), and we know that the problem can be solved sequentially in T(n2 log n) time by
the traditional convex hull algorithm on arbitrary point data.
TeamUnknown Release
0
Chapter 11 - Image Processing
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Distance Problems
In this section, we consider the problem of determining distances between labeled sets of
pixels. This problem is concerned with determining for every labeled set of pixels, a
nearest distinctly labeled set of pixels, where the input consists of a labeled set of (not
necessarily connected) pixels. We also consider the problem of determining distances
within connected components. Specifically, we assume that one special pixel in each
connected component is "marked" and that every pixel needs to find its minimal internal
distance to this marked pixel.
All-Nearest Neighbor between Labeled Sets
In this section, we consider the all-nearest neighbor between labeled sets problem.
Assume that the input consists of a labeled set of pixels. That is, assume that every
processor Pi,j is associated with the lattice point (i, j) on a mesh of size n2. As we did in
the previous section , assume that every processor contains a label in the range of 0…n2,
where the interpretation is that 0 represents the background (a white pixel) and that
values in the range of 1 … n2 represent labels of foreground (non-white) pixels. Recall
that pixels in the same labeled set are not necessarily connected.
The problem we are concerned with is that of determining for every labeled set of pixels,
the label of a nearest distinctly labeled set of pixels. Algorithmically, we first determine,
for every pixel, the label and distance to a nearest distinctly labeled pixel. This solves the
problem on a per pixel basis. To solve the problem for every labeled set, we then
determine the minimum of these pixels' nearest-pixel distances over all pixels within a
labeled set. Details of the algorithm follow.
The first step is to find, for every labeled processor P, a nearest distinctly labeled
processor to P. To do this, we exploit the fact that the pixels are laid out on a grid and
that we are using the Euclidean distance as a metric. Suppose that p and q are labeled
pixels that are in the same column. Further, let r be a nearest distinctly labeled pixel to p
in the same row as p, as shown in Figure 11.6 . Because we have made no assumption

about the labels of p and q (they could be identical or distinct), then with respect to p's
row, either p or r is a nearest distinctly labeled pixel to q. We refer to this observation as
"work-reducing." An algorithm to solve the all-nearest neighbor between labeled sets
problem follows.
Perform parallel row rotations of every row so that every processor Pi,j finds at most
two nearest processors in its row with distinct labels, if they exist. With respect to
processor Pi, j, we denote these nearest distinctly labeled processors as Pi,j and Pi,j,
where either j 1 or j2 is equal to j if P i,j is a labeled processor. We need two such
processors if the row has foreground pixels with distinct labels, because one of them
may have the same label as a processor in the column of p.1.
Perform parallel column rotations of every column. Every processor Pi,j sends its
information (labels and positions) and the information associated with its row-
restricted nearest distinctly labeled processors Pi,j and Pi,j. During the rotation, every
processor is able to determine its nearest distinctly labeled processor, using the
work-reducing observation.2.
Sort all of the near neighbor information by initial pixel (set) label.3.
Within every labeled set of data, perform a semigroup operation (in particular, a
minimum operation) and broadcast so that every pixel knows the label of a nearest
distinctly labeled set to its set.4.
Finally, use a concurrent read so that the initial set of pixels can determine the final
result.5.
5.
Figure 11.6: The all-nearest neighbor between labeled sets problem. Suppose p, q,
and r are labeled pixels. If r is a closest distinctly labeled pixel in row two to p, then
either p or r is a closest distinctly labeled pixel to q among those in row 2
Running Time
Given an n × n mesh, the running time of this algorithm is T(n). This is due to the fact
that the algorithm is dominated by a row rotation, column rotation, semigroup operation,
and sort-based operations. Again, the cost of the algorithm is T(n3), which is suboptimal,
because the problem can be solved in T(n2 log n) time on a RAM.
Minimum Internal Distance within Connected Components
In this section, we consider the all-points minimal internal distance problem.  The input to
this problem is a set of figures (that is, maximally connected components that have been
properly labeled) and one marked  pixel per figure. The problem requires that the
minimum internal distance be determined from every black pixel to the unique marked
pixel in its figure.
Let's first consider a simple propagation algorithm over a single figure. Assume the
marked  processor is labeled X. The processor associated with X assigns its distance to
0, because it is the marked  processor. All other processors in the figure assign their initial
distance to 8. Now every black processor exchanges distance information with its
neighboring black processors. A given processor with current distance of s to the marked
processor will receive at most four additional pieces of distance information, denoted as
a, b, c,  and d. This processor will now set s = min {s, min{a,b,c,d}  + 1}. The algorithm
continues until no distances in the figure change. Notice that the algorithm will terminate
when information from the marked  processor X reaches the processor(s) at maximal
internal distance from it. As with propagation algorithms discussed earlier, this algorithm
is quite efficient for figures with a small internal diameter, but it can be quite bad for
figures with large internal diameters. In fact, if we consider a spiral or snakelike figure, we
see that this algorithm has a running time of O(n2) on a mesh of size n2.
We now consider a significantly more complicated algorithm, based on divide-and-
conquer, which exhibits a T(n) worst-case running time. This algorithm involves both data
reduction and the application of a generalized transitive closure algorithm, which was
mentioned earlier.
The algorithm consists of two phases.  The first phase of the algorithm can be viewed as
exploiting a bottom-up divide-and-conquer strategy. During the i th stage of the first
phase, the objective is to determine correctly the internal distance from every black pixel
on the border of a 2i × 2i region
to every other border pixel, and
to the marked pixel.
The assumption in determining this information during the ith stage is that the image is
restricted to the appropriate 2i × 2i region. Notice that pixels within a figure might not
even be connected within such a 2i × 2i region, and these will result in a distance of 8
after performing the required computations during the i th stage. Further, notice that the
marked pixel of a figure can be in only one of the 2i × 2i regions. Therefore, after stage
log2 n - 1,  three of the four (n/2) × (n/2) regions will be such that every entry between a
border pixel and the marked pixel will be set to 8.
The first stage of this bottom-up phase is stage 0, in which every pixel has a distance of
8 to the marked pixel, with the exception of the marked pixel itself, which has a distance
of 0. The final stage of this phase is stage log 2 n in which the (at most) 4n- 4  outer pixels
of the n × n mesh obtain the correct internal distance from their pixel to each other, as
well as to the marked pixel.
The second phase of this algorithm consists of using the information determined during
the first phase to determine recursively the correct internal distances for all remaining
pixels. This is accomplished by a divide-and-conquer algorithm that can be viewed as
top-down. That is, the correct outer border pixel distances for the entire n × n mesh are
used to determine the correct outer border pixel distances for each of the four (n/2) ×
(n/2) regions, which can be used to determine the correct outer border pixel distances for
each of the 16 (n/4) × (n/4) regions, and so on.
Before we give details of each phase of the algorithm, we will take the unorthodox
approach of discussing the running time. It will be shown that each stage i of the first
phase can be performed in time T(2i). Hence, the running time of the first phase of the
algorithm is given by
Therefore, the running time of the first phase of the algorithm is T(n2) = T(n). We will also
show that the time for each stage of the second phase can be performed by following the
same steps as in the first phase, but with a slightly different set of input. So the running
time for the second phase, which uses the first phase as a subroutine, is given by T(n2) =
T(n2/4) + T(n). This yields T(n2) = T(n). Therefore, the algorithm that we are discussing is
asymptotically optimal for the model and input under discussion.
We now discuss some of the details of the two phases of the algorithm. First, we
consider the fth stage of the first phase. We show how to determine properly the
restricted internal distances from the outer pixels of the 2 i × 2 i region to the marked
pixel. Assume that for each of the (2/2) × (2/2) subsquares of the region, the problem for
the first phase has been solved recursively. Then we need to show how to combine the
distance results from each of the four (2 i/2) × (2 i/2) regions into the required result for
the 2 i × 2 i region. At the end of stage i- 1, we assume that each of the four subsquares
has the correct restricted internal distance not only from every outer pixel to the marked
pixel but also from every outer pixel to every other outer pixel. Notice that there is room to
store this as a matrix within each of the four (2 i/2) × (2 i/2) subsquares. The algorithm
performed at the ith phase consists simply of combining all of this internal distance
information in an appropriate way. This is done by combining the four internal distance
matrices into one distance matrix. This matrix contains restricted internal distances
between the outer border elements of the four subsquares and also to the marked pixel
(see Figure 11.7 ).
Figure 11.7: An illustration of the possible border elements in a k × k  submesh
Now, to consider the 2i X 2i region, we simply have to modify the matrix to include a
distance of 1 (instead of 8) between those outer black pixels in a (2 i/2) X (2 i/2)
subsquare that have a neighboring black pixel in an adjacent (2 i/2) X (2 i/2) subsquare.
Once the distance matrix is initialized, a generalized transitive closure algorithm can be
run to determine the necessary distances. Notice that if we define Sk(i,j) to be the
minimal internal distance from vertex i to vertex j using no intermediate vertex with label
greater than k, then Sk+1 (i, j) = min{ Sk (i, j), S k (i,k+  1) + Sk (k + 1, j)}. Notice that the
matrices can be moved into their proper location in T(2 i) time, as shown in Figure 11.8 .
Further, the necessary edges can be added in T(2 i) time, and the transitive closure and
final random access read can also be performed in T(2 i) time. Therefore, the running
time of phase 1 is as claimed.
Consider phase 2 of the algorithm. We need to show that, given the final matrices and
distances involving the outer border elements of the (n/2) X (n/2) regions (computed
while determining the final correct distances for the outer border elements of the n × n
mesh), we can continue to pass this information on down to recursively smaller and
smaller subsquares. This is fairly straightforward because all we are required to do is to
run the phase 1 algorithm on each subsquare with the final outer border distance
information included. Therefore, this phase can be completed in the time claimed.
Figure 11.8: A mapping that shows how to rearrange the distance matrices from
recursive solutions in an effort to solve the all-points minimal internal distance
problem
Hausdorff Metric for Digital Images
Let A and B be nonempty, closed, bounded subsets of a Euclidean space Rk. The
Hausdorff metric, H(A,B),  is used to measure how well the elements of each such pair of
sets approximates the other. In general, the Hausdorff metric provides the following
properties.
H(A,B)  is small if every point of A is close to some point of B and every point of B is
close to some point of A.
H(A,B)  is large if some point of A is far from every point of B, or some point of B is
far from every point of A.
Formally, we can define the Hausdorff metric as follows. Let d be the Euclidean metric for
Rk. For xeRk, f   Y   Rk, define d(x,Y)  = min {d(x,y)  | y e Y}. Let H*(A,B) = max{d(a,B)  |
aeA}, where H*(A,B)  is said to be the "oneway" or "nonsymmetric" Hausdorff distance.
Note that H*(A,B)  is not truly a "distance" in the sense of a metric function. Then the
Hausdorff metric, which is indeed a metric function when applied to sets A and B that are
nonempty, bounded, and closed, is defined by H(A,B)  = max-{ H*(A,B), H*(B,A)}.  This
definition is equivalent to the statement that H(A,B) = e if e is the minimum of all positive
numbers r for which each of A and B is contained in the r-neighborhood of the other,
where the r- neighborhood of Y in Rk is the set of all points in Rk that are less than r
distant from some point in Y. See Figure 11.9  for an example of H(A,B).
Figure 11.9: An example of the Hausdorff metric. The distances x and y respectively
mark a furthest member of A from B and a furthest member of B from A. H(A,B)  =
max{x,y}
Suppose that A and B are finite sets of points in R2 or R3. Further, suppose that these
points represent black pixels corresponding to digital images. That is, suppose A and B
represent distinct digital images in the same dimensional space. Then, to determine
whether the probability is high that A and B represent the same physical object, one
might consider the result of applying a rigid motion M (translation, rotation, and/or
reflection) to B and evaluating the result of H(A,M(B)).  If for some M, H(A,M(B))  is small,
then in certain situations there is a good chance that A and B represent the same
physical object; but if no rigid motion translates B close to A in the Hausdorff sense, it is
unlikely that A and B represent the same object.
It is interesting to note that two sets in a Euclidean space can occupy approximately the
same space, yet have very different geometric features. Although better image
recognition might result from a metric that reflects geometric as well as positional
similarity, such metrics are often much more difficult to work with, both conceptually and
computationally.
A simple, although inefficient algorithm for computing the Hausdorff metric for two digital
images A and B, each contained in an n × n digital picture, is described next. The
algorithm is a straightforward implementation of the definition of the Hausdorff metric as
applied to digital images. As we outline a more efficient algorithm in the Exercises, we will
discuss only the current algorithm's implementation for a RAM.
For every (black) pixel a e A, compute the distance d(a,b)  from a to every point b e B
and compute d(a,B)  = min {d(a,b)  | b e B}. On a RAM, this takes 0(n4) time, because
each of the 0(n2) black pixels of A is compared with each of the 0(n2) black pixels of
B.1.
Compute H* (A,B)  = max {d(a,B) | a e A} by a semigroup operation. This takes T(n2)
time on a RAM.2.
Interchange the roles of A and B and repeat steps 1 and 2. Now H* (B,A)  is known. 3.
Compute H(A,B)  = max {H* (A,B), H* (B,A)}.  This takes T(1) time. 4.
This algorithm has a running time dominated by its first step, which takes 0(n4) time on a
RAM. Clearly, the running time of the algorithm leaves much to be desired. Indeed, a
simple, more efficient algorithm for computing the Hausdorff metric between two digital
images on a RAM can be given using techniques presented in this chapter. We leave this
problem as an exercise.
We now consider metrics related to the Hausdorff metric for measuring the difference
between two fuzzy sets.  Fuzzy set generalizes the notion of a set; as implemented in a
digital picture, a fuzzy set is not necessarily a binary image. Rather, a fuzzy set is defined
to be a function f: S   [0,1] such that the domain, S, is nonempty. S is called the support
set of f.  For s e S, the value f(s) is the "degree of membership" or the "membership
value" of s in S. T   S is a crisp set,  or an "ordinary set," for the fuzzy set f if T f = -1({1})
= {s e S | f(s)  = 1} and f(S)  {0,1}. Thus, as implemented in a digital picture, a crisp set is
a digital image (the set of 1 pixels) in a support set S consisting of an n × n grid of pixels.
In a more general fuzzy set (not necessarily a binary digital image), membership values
could represent color codes for a colored picture or local physical information for a map,
such as land elevation, temperature or other meteorological data, and so on.
Let F be a family of fuzzy sets defined on the nonempty support set S with the following
properties:
S is a metric space (this is a technical requirement; for purposes of our discussion,
the reader unfamiliar with such notions can assume S is a subset of a Euclidean
space, such as a grid of pixels);
There is a finite set of membership values T = {t 1, t2, …, tm}   [0,1] such that for
every f e F, f(S)   T;
For every f e F and tke T, the set f-1([tk, 1]) = {s e S | t k = f (s) = 1] is bounded and
closed in S;
1 e T
For every f e F, there exists s eS such that f(s) = 1.
Then the formula
defines a metric. The reader should examine the preceding formula carefully. At first it
may look quite complex, but it is in fact rather simple and can be computed efficiently.
We leave it as an exercise to develop an efficient algorithm to compute this formula.
TeamUnknown Release
0
Chapter 11 - Image Processing
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we examine several fundamental problems from image processing.
Problems examined include component labeling, computation of the convex hull, and
various distance problems. Among the distance problems discussed is that of computing
the Hausdorff distance between two digital images; this problem has appeared in many
recent papers as a tool for image pattern matching. RAM solutions are presented;
because of the natural mapping of a digital image to the processors of a mesh, it is the
latter model we use for discussion of parallel solutions.
TeamUnknown Release

0
Chapter 11 - Image Processing
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
This chapter focuses on fundamental problems in image analysis for the RAM and mesh.
These problems serve as a nice vehicle to present interesting paradigms. Many of the
mesh algorithms presented in this chapter are derived from algorithms presented by R.
Miller and Q.F. Stout in Parallel Algorithms for Regular Architectures  (The MIT Press,
Cambridge, MA, 1996). These algorithms include the component-labeling algorithm, the
all-nearest neighbor between labeled sets algorithm, and the minimum internal distance
within connected components algorithm. The book by R. Miller and Q.F. Stout also
contains details of some of the data-movement operations that were presented and
utilized in this chapter, including rotation operations based on ordered intervals and so
on. The ingenious algorithm used to compute the transitive closure of an n × n matrix on
a RAM was devised by S. Warshall in his paper "A Theorem on Boolean Matrices," in the
Journal of the ACM 9  (1962), 11-12. Further, in 1980, FL. Van Scoy ("The Parallel
Recognition of Classes of Graphs," IEEE Transactions on Computers  29 (1980), 563-70)
showed that the transitive closure of an n × n matrix could be computed in T(n) time on
an n × n mesh.
For more information about the Hausdorff metric, see Hyperspaces of Sets,  by S.B.
Nadler, Jr. (Marcel Dekker, New York, 1978). The reader interested in additional
information on Hausdorff metrics for fuzzy sets is referred to the following papers:
L. Boxer, "On Hausdorff-like Metrics for Fuzzy Sets," Pattern Recognition Letters  18
(1997), 115-18;
B.B. Chaudhuri and A. Rosenfeld, "On a Metric Distance Between Fuzzy Set s,"
Pattern Recognition Letters 17 (1996), 1157-60;
M.L. Puri and D.A. Ralescu, "Differentielle d un fonction floue," Comptes Rendes
Acad. Sci. Paris, Serie 1 293 (1981), 237-39.
The paper that introduced the notion of digitally continuous functions (used in the
exercises) is: A. Rosenfeld, "‘Continuous’ Functions on Digital Pictures" in Pattern

Recognition Letters  4 (1986), 177-84.
TeamUnknown Release
0
Chapter 11 - Image Processing
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
Given an n × n digitized image, give an efficient algorithm to determine both the
number of black pixels in the image, and the number of white pixels in the image.
Present an algorithm and analysis for both the RAM and mesh.1.
Let A be the adjacency matrix of a graph G with n vertices. For integer k > 0, let Ak
be the Ath power of A, as discussed in the chapter.
Prove that for i   j, Ak(i,j) = 1  if and only if there is a path in G from vertex i to
vertex j that has at most k edges, for 1 = k = n.a.
Prove that An+C = An for any positive integer c. b.2.
Given an n × n digitized image in which each pixel is associated with a numerical
value, provide an efficient algorithm that will set to zero (0) all of the pixel values that
are below the median pixel value of the image. Present analysis for both the RAM
and mesh.3.
Given an n × n digitized image, provide an efficient algorithm that will set each pixel
to the average of itself and its eight (8) nearest neighbors. Present analysis for both
the RAM and mesh.4.
Given a labeled n × n digitized image, give an efficient algorithm to count the
number of connected components in the image. Present analysis for both the RAM
and mesh.5.
Given a labeled n × n digitized image and a single "marked" pixel somewhere in the
image, give an efficient algorithm that will mark all other pixels in the same
connected component as the "marked" pixel. Present analysis for both the RAM and
mesh.6.
Given a labeled n × n digitized image, give an efficient algorithm to determine the 7.

number of pixels in every connected component. Present analysis for both the RAM
and mesh.7.
Given a labeled n × n digitized image and one "marked" pixel per component, give
an efficient algorithm for every pixel to determine its distance to its marked pixel.
Present analysis for both the RAM and mesh.8.
Given a labeled n × n digitized image, give an efficient algorithm to determine a
minimum enclosing box of every connected component. Present analysis for both
the RAM and mesh.9.
Give an efficient algorithm for computing H(A,B),  the Hausdorff metric between A
and B, where each of A and B is an n × n digital image. Hint:  the algorithm
presented in the text can be improved on by using row and column rotations similar
to those that appeared in our algorithm for the all-nearest neighbor between labeled
sets problem, modified to allow that a pixel could belong to both A and B. Show that
your algorithm can be implemented in worst-case times of T(n2) for the RAM and
T(n) for the n × n mesh.10.
Let F be a family of fuzzy sets with support set S consisting of an n × n grid of
pixels. Present an algorithm and analysis for the RAM and mesh to compute the
distance formula D(f,g)  described earlier for members of F. Your algorithm should
run in 0(mn2) time on a RAM and in 0(mn)  time on the n × n mesh.11.
Suppose A and B are sets of black pixels for distinct n × n digital pictures. Let f: A  
B be a function, that is, for every (black) pixel a e A, f(a)  is a (black) pixel in B. Using
the 4-adjacency notion of neighboring pixels, we say f is (digitally) continuous  if for
every pair of neighboring black pixels a,a e A, either f( a0) = f(a 1) or f(a0) and f(a 1) are
neighbors in B. Prove that the following are equivalent:
f: A   B is a digitally continuous function.
For every connected subset A0 of A,  the image f(A0) is a connected subset of
B.
Using the Euclidean metric (in which four connected neighboring pixels are at
distance one apart and non-neighboring pixels are at distance greater than 1),
for every e = 1, there is a d = 1 such that if a0, a1 e A and d(a0, a1) = d, then
d[f(a 0), f(a 1) | = e.
12.
Refer to the previous exercise. Let A and B be sets of black pixels within respective
n × n digital pictures. Let f: A   B be a function. Suppose the value of f(a) can be
computed in T(1) time for every ae A. Present an algorithm to determine whether or
not the function f is digitally continuous (and, in the case of the mesh, let every13.
processor know the result of this determination), and give your analysis for the RAM
and n × n mesh. Your algorithm should take T(n2) time on a RAM and T(n) time on
an n × n mesh.
Conways Game of Life  can be regarded as a population simulation that is
implemented on an n × n digitized picture A. The focus of the "game" is the
transition between a "parent generation" and a "child generation"; the child
generation becomes the parent generation for the next transition. In one version of
the game, the transition proceeds as follows:
If in the parent generation A [i,j]  is a black pixel and exactly two or three of its
nearest 8-neighbors are black, then in the child generation A[i,j] is a black pixel
(life is propagated under "favorable" living conditions). However, if in the parent
generation A[i,j] is a black pixel with less than two black 8-neighbors
("loneliness" or more than three black 8-neighbors ("overcrowding", then in the
child generation A [i,j]  is a white pixel.
If in the parent generation A [i,j]  is a white pixel, then in the child generation
A[i,j] is a black pixel if and only if exactly three of its nearest 8-neighbors are
black.
Present and analyze an algorithm to compute the child generation matrix A from the
parent generation matrix for one transition, for the RAM and the mesh. Your
algorithm should run in T(n2) time on the RAM and in T(1) time on the mesh.14.
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 12: Graph Algorithms
Overview
In this chapter, we focus on algorithms and paradigms to solve fundamental problems
for problems in graph theory, where the input consists of data representing sets of
vertices and edges. We will present efficient solutions to problems such as
determining the connected components of a graph, constructing a minimal-cost
spanning forest, and determining shortest paths between vertices in a graph. The
algorithms will be presented for the sequential model (RAM), the PRAM, and the
mesh. In this way, we will be able to present a variety of techniques and paradigms.
Some of the material presented in this chapter will rely on algorithms presented earlier
in the book.
Many important problems can be expressed in terms of graphs, including problems
involving communications, power grids, cyberinfrastructure and grid computing,
general and special purpose networking, the scheduling or routing of airplanes, and
so on. The following is a list of tasks for which graphs are often used:
Provide a representation for a set of locations with distances or costs between
the locations. This can arise in transportation systems (airline, bus, or train
systems) where the costs can be distance, time, or money.
Provide a representation for the connectivity in networks of objects. Such
networks can be internal to devices (VLSI design of computer chips) or among
higher-level devices (communication or computer networks).
Provide a representation for problems concerned with network flow capacity,
which is important in the water, gas, and electric industries, to name a few.
Provide a representation for an ordered list of tasks. For example, one might

create an ordered list of the tasks necessary to build a guitar from instructions
and materials available on the Web.
One of the first uses of graphs dates back to 1736, when Leonhard Euler considered
the town of Königsberg, in which the Pregel River flows around the island of Kneiphof,
as shown in Figure 12.1 . Notice that the Pregel River borders on four land regions in
this area, which are connected by seven bridges, as shown in Figure 12.2 . Euler
considered the problem of whether it was possible to start on one of the four land
areas, cross every bridge exactly once, and return to the original land area. In fact, for
this situation, which is represented in the graph in Figure 12.3 , Euler was able to
prove that such a tour was not possible. The generalization of this problem has
become known as the Euler tour.  That is, an Euler tour  of a connected, directed graph
is a cycle (the path starts and ends at the same vertex) that traverses each edge of
the graph exactly once, although it may visit a vertex more than once.
Figure 12.1: In 1736, Leonhard Euler graphed the town of K nigsberg, where the
Pregel River flows around the island of Kneiphof
Figure 12.2: The seven bridges in the area of Kneiphof and the Pregel River that
Euler considered in terms of navigating the town of K nigsberg
Figure 12.3: A graph with four vertices and seven edges representing K nigsberg.
Euler considered this graph in terms of whether or not it was possible to start on
one of the four land masses (vertices), cross every bridge exactly once, and return
to the original land area. The generalization of this problem is now known as the
Euler tour problem
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Terminology
Let G = (V, E)  be a graph consisting of a set V of vertices and a set E of edges. The
edges, which connect members of V, can be either directed or undirected, resulting in
either a directed graph (digraph)  or an undirected graph,  respectively. That is, given a
directed graph G = (V, E),  an edge (a,b) e E represents a directed connection from vertex
a to vertex b, where both a,beV.  Given an undirected graph, an edge (a,b) e E represents
an undirected connection between a and b. Usually, we do not permit self-edges,  in
which an edge connects a vertex to itself, nor do we permit multiple occurrences of an
edge (resulting in a multigraph).  See Figure 12.4  for examples of directed and undirected
graphs.
Figure 12.4: Four sample graphs. (a) shows a complete undirected graph of five
vertices. (b) is a directed graph with pairs of vertices ( u, v) such that the graph has no
directed path from u to v. (c) is an undirected tree with seven vertices. (d) is an
undirected mesh of nine vertices

The number of vertices in G = (V, E)  is written as |V|, and the number of edges is written
as |E|. However, for convenience, whenever the number of vertices or number of edges
is represented inside of an asymptotic notation, we will typically avoid the vertical bars
since there is no ambiguity. For example, an algorithm that runs in time linear in the sum
of the vertices and edges will be said to run in T(V + E)  time.
In any description of a graph, we assume that there is a unique representation of the
vertices and edges. That is, no vertex will have more than one identity and no edge will
be represented more than once. In a directed graph, the maximum number of edges is
|V| (| V|— 1), whereas in an undirected graph, the maximum number of unique edges is
|V|(|V|—1)/2. Therefore, the number of edges in a graph G = (V, E)  is such that |E| =
O(V2). A complete graph G  = (V, E)  is one in which all possible edges are present. A
sparse graph  is one in which there are not very many edges, whereas a dense graph  is
one in which a high percentage of the possible edges are present. Alternately, a graph is
typically termed sparse  if |E| / |V|2 is very small, but a graph is typically referred to as
dense  if |E| / |V|2 is at least of moderate size.
Vertex b is said to be adjacent  to vertex a if and only if (a, b)  e E. At times, adjacent
vertices will be described as neighbors.  An edge (a,b) e E is said to be incident  on
vertices a and b. In a weighted graph,  every edge (a,b) e E will have an associated
weight or cost (see Figure 12.5 ).
Figure 12.5: Notice in (a) (an undirected weighted graph) that there are eight pairs of
neighboring (adjacent) vertices. Also, notice in (a) that the entire graph is connected
because there is a path between every pair of vertices. In graph (b) (a directed,
weighted graph), however, paths are not formed between every pair of vertices. In
fact, notice that vertex e is isolated in that e does not serve as the source of any
nontrivial path. Notice in (a) that a minimum-weight path from a to e is <a, c, d, e <,
which has a total weight of 3, whereas in (b) minimum-weight paths from a to e are
<a, d, e < and <a, b, f, d, e <
A path  in a graph G = (V, E)  is a sequence of vertices v1, v2,,…,(vi, vi+1) eE for all 1 = i =
k — 1. The length  of such a path is defined to be the number of edges on the path, which
in this case is k— 1. A simple path  is defined to be a path in which all vertices are unique.
A cycle  is a path of length 3 or more in which v 1 = vk. A graph is called acyclic  if it has no
cycles. A directed acyclic graph  is often referred to as a dag.
An undirected graph is called connected  if and only if there is at least one path from
every vertex to every other vertex. Given a graph G = (V, E),  a subgraph S of G is a pair
S = (V ', E'), where V '  V and E' is a subset of those edges in E that contain vertices
only in V'. The connected components  of an undirected graph G = (V, E)  correspond to
the maximally connected subgraphs of G (see Figure 12.6 ).
Figure 12.6: An undirected graph with three connected components
A directed graph is called strongly connected  if and only if there is at least one path from
every vertex to every other vertex. If a directed graph is not strongly connected but the
underlying graph in which all directed edges are replaced by undirected edges is
connected, then the original directed graph is called weakly connected  (see Figure 12.7 ).
As a point of information, note that a tree in which all edges point away from the root is a
directed acyclic graph.
Figure 12.7: A directed graph with three weakly connected components and seven
strongly connected components
Given an undirected graph, the degree of a vertex  is the number of edges incident on the
vertex, and the degree of the graph  is the maximum degree of any vertex in the graph.
Given a directed graph, the in-degree  of a vertex is the number of edges that terminate at
the vertex and the out-degree  of a vertex is the number of edges that originate at the
vertex (see Figure 12.8 ).
Figure 12.8: A directed graph. The in-degree of <a, b, c, d, e < is <2,0,1,2,2 <,
respectively, and the out-degree of <a, b, c, d, e < is <1,1,2,1,2 <, respectively
Frequently, it makes sense to assign weights to the edges or vertices in a graph. A graph
G = (V, E)  is called an edge-weighted graph  if there is a weight W(v i, vj) associated with
every edge (vi, vj)eE. In the case of edge-weighted graphs, the distance  (or shortest path)
between vertices vi and vj is defined as the sum over the edge weights in a path from vi
to vj of minimum total weight. The diameter  of such a graph is defined to be the
maximum of the distances between all pairs of vertices. Notice that for many
applications, it makes sense to consider all edges in an (otherwise) unweighted graph as
having a weight of 1.
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Representations
There are several ways to represent a graph. In this book, we will consider three of the
most common, namely,
an adjacency list,
an adjacency matrix, and
a set of arbitrarily distributed edges.
It is important to note that in some cases, the user may have a choice of representations
and can therefore choose a representation for which the computational resources may be
optimized. In other situations, the user may be given the graph in a particular form and
may need to design and implement efficient algorithms to solve problems on the
structure.
Adjacency Lists
The adjacency-list representation  of a graph G = (V, E)  typically consists of | V| linked
lists, one corresponding to each vertex vi eV. For each such vertex vi, its linked list
contains an entry corresponding to each edge ( vi, vj)eE. To navigate efficiently through a
graph, the headers of the | V | linked lists are typically stored in an array or linked list,
which we call Adj, as shown in Figure 12.9 . In this chapter, unless otherwise specified,
we will assume an array implementation of Adj so that we can refer to the adjacency list
associated with vertex v i e V as Adj(v i). It is important to note that the vertices stored in
each adjacency list, which represent the edges in the graph, are typically stored in
arbitrary order.

Figure 12.9: A directed graph and its adjacency list representation
If the graph G = (V, E)  is a directed graph, the total number of entries in all adjacency
lists is |E|, because every edge ( vi, vj) eE is represented in Adj(v i). However, if the graph
G = (V, E)  is an undirected graph, then the total number of entries in all adjacency lists is
2|E |, because every edge ( vi, vj)eE is represented in both Adj(v i) and Adj(v j). Notice that,
regardless of the type of graph, an adjacency-list representation has the feature that the
space required to store the graph is T(V + E).  Assuming that one must store some
information about every vertex and about every edge in the graph, this is an optimal
representation.
Suppose the graph G = (V, E)  is weighted. Then the elements in the individual adjacency
lists can be modified to store the weight of each edge or vertex, as appropriate. For
example, given an edge-weighted graph, an entry in Adj(v i) corresponding to edge ( vi,
vj)eE can store the identity of vj, a pointer to Adj(v j), the weight W(v i, vj), other
miscellaneous fields required for necessary operations, and a pointer to the next record
in the list.
Although the adjacency list representation is robust, in that it can be modified to support
a wide variety of graphs and is efficient in storage, it does have the drawback of not
being able to identify quickly whether or not a given edge (vi, vj) is present. In the next
section , we consider a representation that will overcome this deficiency.
Adjacency Matrix
An adjacency matrix is presented in Figure 12.10  that corresponds to the adjacency list
presented in Figure 12.9 . Given a graph G = (V, E),  the adjacency matrix A is a |V | × | V
|matrix in which entry A(i, j) =  1 if ( vi, vj)eE and A(i,j) = 0 if ( vi, vj)  E. Thus, row i of the
adjacency matrix contains all information in Adj(v i) of the corresponding adjacency list.
Notice that the matrix contains a single bit at each of the T(V2) positions. Further, if the
graph is undirected and i   j, there is no need to store both A(i, j) and A(j, i),  because
A(i,j) = A(j,i).  That is, given an undirected graph, one needs only to maintain either the
upper triangular or lower triangular portion of the adjacency matrix. Given an edge-weight
graph, each entry A(i, j)  will be set to the weight of edge (vi, vj) if the edge exists and will
be set to 0 otherwise. Given either a weighted or unweighted graph that is either directed
or undirected, the total space required by an adjacency matrix is T(V2).
Figure 12.10: An adjacency matrix representation of the graph presented in Figure
12.9
The adjacency matrix has the advantage of providing direct access to information
concerning the existence or absence of an edge. Given a dense graph, the adjacency
matrix also has the advantage that it requires only one bit per entry, as opposed to the
additional pointers required by the adjacency list representation. However, for relatively
small (typically sparse) graphs, the adjacency list has the advantage of requiring less
space and providing a relatively simplistic manner in which to traverse a graph. For an
algorithm that requires the examination of all vertices and all edges, an adjacency list
implementation can provide a sequential algorithm with running time T(V + E) , whereas
an adjacency matrix representation would result in a sequential running time of T(V2).
Thus, the algorithm based on the adjacency list might be significantly more efficient.
Unordered Edges
A third form of input that we discuss in this book is that of unordered edges, which
provides the least amount of information and structure. Given a graph G = (V, E),
unordered edge  input is such that the | E | edges are distributed in an arbitrary fashion
throughout the memory of the machine. On a sequential computer, one will typically
restructure this information to create adjacency-list or adjacency-matrix input. However,
on parallel machines, it is not always economical or feasible to perform such a
conversion.
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
Fundamental Algorithms
In this section, we consider fundamental algorithms for traversing and manipulating graphs. It is often useful to be
able to visit the vertices of a graph in some well-defined order based on the graph's topology. We first consider
sequential approaches to this concept of graph traversal.  The two major techniques we consider, breadth-first
search and depth-first search, both have the property that they begin with a specified vertex and then visit all other
vertices in a deterministic fashion. In the presentation of both of these algorithms, the reader will notice that we
keep track of the vertices as they are visited. Following the presentations of fundamental sequential traversal
methods, several fundamental techniques will be presented for the RAM, PRAM, and mesh. In particular, we
discuss an Euler tour technique for the RAM; list ranking via pointer jumping and tree contraction for the PRAM;
and the transitive closure of a Boolean matrix for the RAM, PRAM, and mesh.
Breadth-First Search
The first algorithm we consider for traversing a graph on a RAM is called breadth-first search,  which is sometimes
referred to as BFS.  The general flow of a BFS traversal is first to visit a predetermined "root" vertex r , then visit all
vertices at distance 1 from r , then visit all vertices at distance 2 from r , and so forth. This is a standard sequential
technique for traversing a graph G = (V, E).  A high-level description of this search procedure follows:
Start at a root vertex re V. 1.
Add neighboring vertices to a queue as they are encountered.2.
Process the queue in a standard FIFO (first-in, first-out) order.3.
So, initially all vertices v e V are marked as unvisited,  and the queue is initialized to contain only a root vertex re V.
The algorithm then proceeds by removing the root from the queue (the queue is now empty), determining all

neighbors of the root, and placing each of these neighbors into the queue. In general, each iteration of the
algorithm consists of
removing the next vertex v e V from the queue,
examining all neighbors of v in G to determine those that have not yet been marked (those that have not yet
been visited in the breadth-first search),
marking each of these previously unvisited neighbors as visited, and
inserting these previously unvisited neighbors of v into the queue (specifically, enqueueing them, i.e., inserting
them at the end of the queue).
This process of removing an element from the queue and inserting its previously unvisited neighbors into the
queue continues until the queue is empty. Once the queue is empty at the conclusion of a remove-explore-insert
step, all vertices reachable from the root vertex re V (that is, all vertices in the same component of G as r) have
been visited. Further, if the vertices are output as they are removed from the queue, the resulting list corresponds
to a breadth-first search tree over the graph G = (V, E)  with root re V (see Figure 12.11  ).
Figure 12.11: An example of a breadth-first search traversal. Depending on the order in which the vertices
given in graph G of (a) are stored in the associated data structure, a BFS initiated at vertex 10 could yield a
variety of breadth-first search trees. For example, the tree in (b) is associated with the traversal (10, 3, 12, 11,
9, 5, 17, 16, 2, 1, 15, 13, 14, 7, 4, 6, 18, 8, 19), though other traversals of G might also yield this tree. Similarly,
the tree in (c) is associated with the traversal (10, 9, 12, 11, 3, 17, 7, 13, 14, 15, 16, 2, 1, 5, 18, 8, 6, 4, 19) of G
We now present an algorithm that will implement a sequential breadth-first search of a graph and record the
distance from the root to every reachable vertex (see Figure 12.12  ). The reader should note that our algorithm is
presented as a graph traversal, that is, a procedure that visits every vertex of the root's component. This procedure
is modified easily to solve the query problem by returning to the calling routine with the appropriate information
when a vertex is reached that is associated with the requested key.
      BFSroutine (G, r)
      CreateEmptyQueue(Q)                            {Initialize the queue}
      For all vertices v  eV, do
        visited(v)   false                     {Initialize vertices to "unvisited"}
        dist(v)  8                                       {Initialize all distances}
        parent(v)   nil                         {Initialize parents of all vertices}
      End For
      {*} visited(r)  true                   {Initialize root vertex-it is visited,
       dist(r)  0                                    it has distance 0 from itself,
      PlaceInQueue (Q, r)                             and it goes into the queue}
      While NotEmptyQueue(Q), do
          v   RemoveFromQueue(Q)             {Take first element from queue: v}
         For all vertices w eAdj(v), do             {Examine all neighbors of v}
          If not visited(w) then                    {Process those neighbors not
                                                              previously visited}
             visited(w)    true                      {Mark neighbor as visited}
             parent(w)  v                            {The BFS parent of w is v}
             dist(w)  dist(v)+ 1          {Dist. fr. w to r is 1 more than distance
                                                         from its parent (v) to r}
             PlaceInQueue (Q, w)               {Place w at the end of the queue}
          End If
         End For
      End While

Figure 12.12: An undirected connected graph with distances from the root vertex r recorded next to the
vertices. One possible traversal of the vertices in this graph by a breadth-first search is < r, c, b, a, e, f, d, i, j, g,
h, k, l, m, n, o  <
Notice that the steps that compute the parent of a vertex v and the distance of v from the root are not necessary to
the graph traversal. We have included these steps because they are useful to other problems we discuss in
following sections. Also, note that what we have described as " v   RemoveFromQueue( Q )" may involve not only
dequeueing a node from the queue, but also processing the node as required by the graph traversal.
Given a connected undirected graph G = (V, E),  a call to BFSroutine( G, r ), for any r e V, will visit every vertex and
every edge. In fact, a careful examination shows that every edge will be visited exactly twice and that every vertex
will be considered at least once. Therefore, assuming that inserting and removing items from a queue are
performed in T (1) time, the sequential running time for this BFSroutine on a connected undirected graph is T (V +
E).
Now, suppose that the undirected graph G = (V, E)  is not necessarily connected. We can extend the BFSroutine to
visit all vertices of G. See Figure 12.13  while considering the next algorithm.
 while considering the next algorithm.
      BFS-all-undirected (G = (V, E))
      CreateEmptyQueue (Q)                                 {Initialize the queue}
      For all vertices v  eV, do
        visited(v)   false                      {Initialize vertex to "unvisited"}
        dist(v)  8                                           {Initialize distance}
        parent(v)  nil                                         {Initialize parent}
      End For
      For all v  eV, do                       {Consider all vertices in the graph}
        If not visited(v), then
         BFSroutine (G, v) at line {*}         {Perform a BFS starting at every
                                                  vertex not previously visited-
                                                       call BFSroutine, but jump
                                                        immediately to line {*}}
      End For
Figure 12.13: An undirected graph that is not connected. The two connected components can be labeled in
time linear in the number of vertices plus the number of edges by a simple extrapolation of the breadth-first
search algorithm
Notice that given an undirected graph G = (V, E),  the procedure BFS-all-undi-rected will visit all vertices and
traverse all edges in the graph in T (V + E)  time on a sequential machine.
Depth-First Search
The second algorithm we consider for traversing a graph is called depth-first search,  which is sometimes referred
to as DFS.  The philosophy of DFS is to start at a predetermined "root" vertex r and recursively  visit a previously
unvisited neighbor v of r , one by one, until all neighbors of r are marked as visited. This is a standard sequential
technique for traversing a graph. The DFS procedure follows:
Start at a root vertex re V. 1.
Consider a previously unvisited neighbor v of r. 2.
Recursively visit v . 3.
Continue with another previously unvisited neighbor of r. 4.
The algorithm is recursive in nature. Given a graph G = (V, E),  choose an initial vertex re V, which we again call the
root, and mark r as visited. Next, find a previously unvisited neighbor of r, say, v . Recursively perform a depth-first
search on v and then return to consider any other neighbors of r that have not been visited (see Figure 12.14  ). A
simple recursive presentation of this algorithm is given next.
                                             {Assume that visited(v)   false
                                for all v  eV prior to this routine being called}
DFSroutine (G, r)
 visited(r)    true                                  {Mark r   as being visited}
For all vertices v eAdj(r), do           {Consider all neighbors of r  in turn}
  If not visited(v) do               {If a given neighbor has not been visited,
     parent(v)  r                          mark its parent as r  and recursively
    DFSroutine (G, v)                   visit this neighbor. Note the recursive
                                           step causes v   to be marked visited}
  End If
End For
Figure 12.14: An example of a depth-first search traversal. Notice that the graph given in (a) is identical to the
graph G utilized in Figure 12.11  a. In (b) we see the tree associated with the traversal (10, 3, 1, 2, 15, 12, 13,
14,16, 5, 4, 6, 19, 18, 7, 8, 9, 11, 17) of G , though other traversals of G might produce the same tree. Similarly,
in (c) we see the tree associated with the traversal (10, 12, 16, 3, 17, 9, 11, 7, 18, 19, 6, 5, 4, 8, 1, 2, 15, 14,
13) of G
As in the breadth-first search graph traversal just presented, the step that computes the parent of a vertex is not
necessary to perform a depth-first search graph traversal, but it is included due to its usefulness in a number of
related problems. The step we have described as " visited(r)    true " is typically preceded or followed by steps that
process the vertex r as required by the graph traversal. Also, as with a breadth-first search, we have presented
depth-first search as a graph traversal algorithm that can be modified by the insertion of a conditional exit
instruction if a traditional search is desired that stops on realizing success.
Depth-first search is an example of a standard "backtracking" algorithm. That is, when considering a given vertex v,
the algorithm considers all of v 's "descendants" before backtracking to the parent of v in order to allow its parent to
continue with the traversal. Now, consider the analysis of DFSroutine on a sequential platform. Notice that every
vertex is initialized to unvisited and that every vertex is visited exactly once during the search. Also, notice that
every directed edge in a graph is considered exactly once. (Every undirected edge would be considered twice,
once from the point of view of each incident vertex.) Therefore, the running time of DFSroutine on a graph G = (V,
E) is T (V + E),  which is the same as the running time of BFSroutine.
Discussion of Depth-First and Breadth-First Search
A depth-first search tree T = (V,E ' ) of a graph G = (V, E)  is formed during a depth-first search of the graph G, as
follows. An edge ( u , v ) e E is a member of E ' if and only if one of its vertices is the parent of the other vertex.
Given a depth-first search tree T = (V,E ' ) of G, it should be noted that if an edge ( u , v ) e E is not in E' , then
either
u is a descendant of v in T and v is not the parent of u, or
v is a descendant of u in T and u is not the parent of v .
See Figure 12.15  .
Figure 12.15: A depth-first search tree T = (V, E ' ) of a graph G = (V, E)  . An edge (u,v) e E iss a member of E'
if and only if one of its vertices is the parent of the other vertex. Edge (u,x) e E is not in E' , corresponding to the
fact that one of its vertices is an ancestor but not the parent of the other
Each vertex v in a depth-first search tree of G can be given a time stamp corresponding to when the vertex was
first encountered and another time stamp corresponding to when the search finished examining all of v 's
neighbors. These time stamps can be used in higher-level graph algorithms to solve interesting and important
problems. Problems typically solved through a depth-first search include labeling the strongly connected
components of a directed graph, performing a topo-logical sort of a directed graph, determining articulation points
and biconnected components, and labeling connected components of undirected graphs, to name a few.
A breadth-first search tree  is similarly formed from the edges joining parent and child vertices in a BFS of a graph
G = (V, E).  Given a breadth-first search tree T = (V,E ' ) of G, it should be noted that if an edge ( ub, bv  )e E is not in
E ' , then u is not a descendant of v in T and v is not a descendant of u in T (see Figure 12.16  ).
Figure 12.16: A breadth-first search tree T = (V, E ' ) of G = (V, E)  . If an edge (u,v)e E is not in E' , then u is
not a descendant of v in T and v is not a descendant of u in T
The vertices in a breadth-first search tree T = (V,E ' ) of G = (V, E)  are at minimum distance from the root re V of
the tree. That is, the distance of u e Vin T  from r is the length of a shortest path in G from u to r. This is a useful
property when we consider certain minimal path-length problems, including the single-source shortest-path
problem.  Such searches, however, are not useful when one is considering weighted paths (as when in solving the
minimal weight spanning tree problem). A breadth-first search of a graph can be used to solve a number of
problems, including determining whether or not a graph is bipartite.
 
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
Fundamental PRAM Graph Techniques
In this section, we will present some techniques amenable to managing pointer-based graph structures on a
PRAM. The working assumption in this section is that the data structure is distributed arbitrarily throughout the
shared memory of the PRAM. We briefly review the pointer-jumping technique, which was introduced in Chapter 8
, "Pointer Jumping." We will then present the Euler tour technique, discuss the consequences of list ranking and
Euler tour, and then present a critical tree-based contraction technique.
List Ranking via Pointer Jumping
Assume that we have a linked list L stored in the shared memory of a PRAM. Let L(i) represent the contents of the
i th item and next(i)  be a pointer to L(i + 1). We assume that the last element in the list has next(i) = nil.  The list
ranking problem  requires that every element i in the list determine its distance, dist(i),  to the end of the list. The
following algorithm solves the list ranking problem via pointer jumping, where it is assumed that each of the n
processors knows the location in memory of a unique list element.
Forall L(i), do                                        {Assume there are n elements}
  If next(i) = nil, then dist(i)     0             {Initialize all distance values}
  If next(i)  nil, then dist(i)      1
     orig _ next(i)  next(i)                       {Store original next pointers}
End Forall
For    log2 n    iterations, do             {Prepare to pointer-jump until done}
  Forall L(i), do
    If next(i)  nil, then                                            {Perform the
       dist(i)  dist(i)+ dist (next(i))                         
pointer jumping
{*}     next(i) 
 
next (next(i))                            step of the algorithm}
    End If

  End Forall
End For-do
Forall L(i), do
    next(i) 
 
orig _ next(i)                   {Restore original pointer values}
End Forall
Our assumption that there are n elements is stated only to facilitate our analysis. In practice, no fixed value of n is
assumed in general, and the loop we have introduced via "For  log2  n iterations" would be introduced by
something like "In parallel, each processor Pi proceeds while next(i)    nil, as follows." The operation used in step
{*} of this algorithm (replacing a pointer by the pointer's pointer) is called the pointer-jumping step  of the algorithm.
When the algorithm terminates, dist(i)  is the rank of the i th item in the list, for all i. A proof is straightforward, and
we have previously discussed the analysis of this algorithm, which has a running time of T (log n). The cost of an
algorithm that runs in T (log n) time with T (n ) processors is T (n log n), which is suboptimal for this problem
because we know that a linear-time sequential traversal can be used to solve the problem in T (n ) time on a RAM.
We note that it is possible to construct an EREW PRAM algorithm to solve the list-ranking problem in T (log n)
time using only T (n /log n) processors. Although the algorithm is beyond the scope of this book, an outline of the
algorithm follows:
Reduce the size of the linked list L from n nodes to O(n /log n) nodes. Call the new list R. 1.
Apply the previous pointer-jumping algorithm to R in order to compute the rank of all nodes in R. Transfer the
ranks of all nodes in R to their corresponding nodes in the original list L.2.
Rank all nodes in L that do not have a rank (that were not members of R). 3.
Euler Tour Technique
Given a tree T= (V, E)  represented by an undirected graph, we let T' = (V,E ' ) be a directed graph obtained from T
in which every undirected edge ( u , v )e E is replaced by two directed edges, (u,v), (v,u)sE  ' . An Euler circuit  of T'
is a cycle of T' that traverses every directed edge exactly once. An Euler circuit of T' = (V,E ' ) can be defined by
specifying a successor function next(e)  for every edge e e E ' , so that a circuit is defined using all edges in E ' .
This can be accomplished as follows. Suppose that for a given vertex ve V, the set of neighbors D of v is
enumerated as < v 0 , v 1 ,… , vd -1 > . Then we define next ((vi , v )) = ( v, v(i  +1 ) mod d ). Notice that we do not
generally traverse all edges incident on a given vertex consecutively; an edge (u, v ) is followed by the edge
determined by the next function as determined by adj(v),  not adj(u)  (see Figure 12.17  ). It follows that an Euler
circuit of T can be listed on a RAM in T (E) time. Straightforward applications of list ranking and Euler tour include
the following:
A tree T can be rooted. That is, all vertices v can determine parent(v).
The vertices can be assigned labels corresponding to the postorder number of the vertex.
The level of every vertex can be determined.
The preorder number of every vertex can be determined.
The number of descendants of every vertex can be determined.
Figure 12.17: An undirected tree T = (V, E)  is presented in (a), along with an adjacency representation of the
graph in (b). In (c), the next edge function is given for the Euler tour of the graph; this is a function of the
adjacency representation. Because an adjacency representation is not unique, if the representation given in (b)
were changed, the next function given in (c) would be different. By starting at any directed edge in the graph T'
= (V, E ' ) (every undirected edge (u,v) e E is replaced by two directed edges, (u,v), (v,u) e E' ) , and following the
next function, an Euler tour can be achieved
Tree Contraction
In this section, we consider a procedure for contracting a tree, initially presented as a pointer-based data structure
on a PRAM. The tree contraction problem has wide applicability, including providing an efficient solution to the
expression evaluation problem. The expression evaluation problem  requires the evaluation of an expression stored
in an expression tree, where an expression tree  is typically presented as a binary tree in which every node is either
a leaf node containing a value or an internal node containing an operator (+, -, X , , and so forth), as shown in
Figure 12.18  .
Figure 12.18: An expression tree for the expression [8 + (6—4)] [4/(3-1)]
Tree contraction  consists of successively compressing leaf nodes with their respective parents until the tree
collapses into a single vertex. When considering the expression evaluation problem, notice that when a leaf is
compressed with a parent, the appropriate arithmetic operation is performed, so that partial results are provided in
an iterative fashion until the tree finally collapses, at which point the complete expression has been evaluated.
For the purpose of the PRAM algorithm that we present, we will make several assumptions about the tree that is
given as input. It should be noted that some of these assumptions are not critical and that the problem could be
solved within the same time bounds if these restrictions were removed. We impose these restrictions simply to
facilitate a clean presentation. We assume that the input is a rooted binary tree T= (V, E),  in which each vertex is
either a leaf node or an internal node with two children. The root is denoted as r. The vertices are assumed to be
labeled with integers in such a fashion that the interior leaves are labeled consecutively from left to right, as shown
in Figure 12.19  . (Do not confuse the labels of the vertices with the contents of the vertices, which are operators for
interior vertices or values for leaf vertices.) We also assume that every vertex v knows the location of parent(v),
sibling( v), left_child(v),  and right_child(v).  Notice that the root will have parent(v)  = nil, and the leaves will have
left_child(v)  = nil and right_child(v)  = nil.
Figure 12.19: Input to a tree contraction algorithm is a rooted binary tree in which each vertex has either two
children or none at all. Further, it is assumed that the leaves have been labeled consecutively from left to right,
with the exception of the leftmost and rightmost leaves
The collapse  or rake operation applied to a leaf node v consists of removing v and parent(v)  from the tree and
connecting sibling(v)  to parent(parent(v)),  as shown in Figure 12.20  . The tree-contraction algorithm consists of
collapsing leaf nodes in an iterative and parallel fashion so that approximately half of the leaf nodes disappear
each time through the loop. This results in an algorithm that runs in T (log n) time. See Figure 12.21  for an
example. The algorithm follows:
Figure 12.20: An example of a collapse operation applied to vertex number 2
Fundamental PRAM Graph Techniques 321
Given a tree T = (V, E),  assume that the m leaves are labeled consecutively from left to right, excluding the
leftmost and rightmost leaves (the exterior leaves).1.
Let Active = (1,2, … ,m) be an ordered list of the interior leaf labels. Notice that Active  does not include the
label of the leftmost or rightmost leaf.2.
For |log 2 (n - 1)J — 1 iterations, do
Apply the collapse operation to all leaf nodes with odd indexed entries in Active  that are left children.
That is, apply collapse simultaneously to nodes that are left children from the set of first, third, fifth, … ,
elements in Active.a.
Apply the collapse operation to the remaining leaf nodes that correspond to odd indexed entries in
Active.b.
Update Active  by removing the indices of the odd indexed leaves that were just collapsed and then
compressing the array Active.c.3.
End For4.
Notice that at the end of the algorithm, the input tree T= (V, E)  with root vertex r has been reduced to three
vertices, namely, the root and two children. We remark without proof that this algorithm can be implemented on an
EREW PRAM in T (log n) time.
Finally, we should note that if one is interested in compressing a tree in which a root has not been identified and
the vertices have not been labeled, efficient PRAM procedures exist to identify a root and label the vertices. The
algorithms to solve the latter two problems rely on an efficient solution to the Euler tour problem, which can be
solved by using list ranking as a subroutine. It should be noted that a solution to the Euler tour problem can also be
used to determine preorder, postorder, and inorder numbering of the vertices of a tree.
Figure 12.21: An example of tree contraction. Indices of nodes in the array Active are shown below the nodes
(these are updated following compression of Active as the steps are executed). The initial tree is given in (a).
The tree is shown in (b) after performing contraction on vertex 7 during the first iteration of the algorithm. The
tree is shown in (c) after performing contraction on vertices 1, 3, and 5 to finish the first iteration of the
algorithm. The tree is shown in (d) after performing tree contraction on vertices 2 and 6 to initiate the second
iteration of the algorithm. The tree is shown in (e) after performing tree contraction on vertex 4 to conclude the
algorithm (after the third iteration)
 
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Computing the Transitive Closure of an Adjacency Matrix
In this section, we review both the sequential and mesh implementations of a transitive
closure algorithm. The solution to this problem is critical to efficient solutions to
fundamental graph problems. Assume that an n X n  adjacency matrix representation of a
directed graph G = (V, E)  is given, where n = |V  |. In such a representation, A(i,j) = 1 if
and only if there is an edge from vi, to vj in E, and A(i,j) = 0 if ( vi, Vj) e E. The transitive
closure of A is represented as a Boolean matrix A*n×n in which A*(i,j) = 1 if and only if
there is a path in G from vi, to Vj. A*(i,j) = 0 if no such path exists. As we have discussed
previously, one way to obtain the transitive closure of an adjacency matrix A is to multiply
A by itself n times. This is not very efficient, however. Alternatively, one could perform  
log2 n   operations of squaring the matrix: AxA = A2, A2 x A2 = A4, and so on until a
matrix Am is obtained where m = n. Sequentially, this squaring procedure would result in
a T (n3 log n) time algorithm, whereas on a mesh of size n2, the procedure would run in
T(nlog n) time. The reader should verify both of these results.
Consider the Boolean matrix Ak(i, j) representing G, with the interpretation that Ak(i, j) =  1
if and only if there is a path from vt to v ,- that only uses { v1,…, vt} as intermediate
vertices. Notice that A0 = A and that An = A*.  Further, notice that there is a path from v, to
v ,- using intermediate vertices { v1,…, vt} if and only if either there is a path from v t to v ,-
using intermediate vertices {v1,…,vk-1} or there is a path from vt to v k using intermediate
vertices {v1,…,vk-1} and a path from vk to V j also using only intermediate vertices
{v1,…,vk-1}. This observation forms the foundation of Warshalls algorithm,  which can be
used to compute the transitive closure of A on a sequential machine in 0(n3) time. The
sequential algorithm follows:
For k = 1 to n, do
  For i = 1 to n, do
    For j = 1 to n, do
          A  (i, j)   A
k - 1
(i, j) 
 
 A
k-1
(i,k) 
 
 A
k-1
(k, j)
    End For j
  End For i

End For k
Now consider an implementation of Warshall's algorithm on a mesh computer. Suppose
A is stored in an n X n  mesh such that processor Pt j stores entry A(i, j).  Further, suppose
that at the end of the algorithm processor Pt j is required to store entry A*(i,j) = A (i,j).
This can be accomplished with some interesting movement of data that adheres to the
following conditions.
Entry A k(i, j) is computed in processor Pt j at time 3k+ |k- i| + |k- j| -2. 1.
For all k and i, the value of Ak(i, j) moves in a horizontal lockstep fashion (in row i)
away from processor P ik.2.
For all k and j, the value of Ak(k, j) moves in a vertical lockstep fashion (in column j)
away from processor Pk j.3.
See Figure 12.22  for an illustration of this data movement. Notice from condition 1 that
the algorithm runs in T(n) time. The reader is advised to spend some time with small
examples of the mesh implementation of Warshall's algorithm to be comfortable with the
fact that the appropriate items arrive at the appropriate processors at the precise time
that they are required. Therefore, there is no congestion or bottleneck in any of the rows
or columns.
Figure 12.22: Data movement of van Scoys implementation of Warshalls transitive
closure algorithm on a mesh. Ak(k,k) is computed at time t = 3k—  2, in processor Pk, k.
During the next time step, this value is transmitted to processors Pk,k+1, P k,k-1, Pk+1,
k, and Pk-1, k, as shown in (a). At time t + 1, the values Ak(k — 1, k), Ak(k, k + 1), Ak(k
+ 1, k), and Ak(k, k—1) are computed in processors Pk- 1, k, Pk,k+ 1, Pk+ 1, k, and Pk,k-
1, respectively, as shown in (b). The arrows displaying data movement in (b) show the
direction that this information begins to move during time step t + 2
Finally, it should be noted that the data movement associated with the mesh transitive
closure algorithm can be used to provide solutions to many recurrences of the form fk(i,j)
= g(f k1(i,j),f k1(i,k),f k1(k,j)j,  or fk(i,j) = g(f k 1(i,j), f k(i,k),f k(k,j)j.  As with the previous algorithm,
the initial value f(i,j) will be stored in processor Pi j and the final value fn(i,j) will be
computed in processor Pij.
The mesh algorithm for the (generalized) transitive closure can be used to solve a
number of important problems, including the connected component-labeling problem, the
all-pairs shortest-path problem, and the determination of whether or not a given graph is
a tree, to name a few. The first two algorithms will be discussed in more detail later in the
chapter.
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
Connected Component Labeling
In this section, we consider the problem of labeling the connected components of an undirected graph. The labeling should
be such that if vertex v is assigned a label label(v),  then all vertices to which v is connected are also assigned a
component label of label(v).
RAM
A simple sequential algorithm can be given to label all of the vertices of an undirected graph. Such an algorithm consists of
applying the breadth-first search procedure to a given vertex. During the breadth-first search, the label corresponding to
the initial vertex is propagated. Once the breadth-first search is complete, a search is made for any unlabeled vertex. If one
is found, then the BFS is repeated, labeling the next component, and so on. An algorithm follows:
Given a graph G = (V, E), where V  = { v,v,... v
n
}.
Assign 
label(v) = nil
 for all 
v 
e
V
       {Initialize labels of all vertices,
                                representing each vertex as currently unvisited}
For i = 1 to n, do
   If 
label(v
i
) =nil,
 then                       {If vertex hasn't been visited/
                                                   labeled so far, then initiate
      BFSroutine(G, vi)                               a search, during which we
                                      set  label(v) = i for every vertex visited}
   End If
End For
The algorithm is straightforward. Because the graph is undirected, every invocation of BFSroutine will visit and label all
vertices that are connected to the given vertex v ,. Due to the For loop, the algorithm will consider every connected

component. The total running time for all applications of the comparison in the If statement is T (V). Further, the running
time for the step that calls BFSroutine in aggregate is T (V+ E)  because every vertex and every edge in the graph is visited
within the context of one and only one breadth-first search. Hence, the running time of the algorithm is T (V + E),  which is
optimal in the size of the graph.
PRAM
The problem of computing the connected components of a graph G = (V, E)  is considered a fundamental problem in the
area of graph algorithms. Unfortunately, an efficient parallel strategy for performing a breadth-first search or a depth-first
search of a graph on a PRAM is not known. For this reason, a significant amount of effort has been applied to the
development of an efficient PRAM algorithm to solve the graph-based connected component problem. Several efficient
algorithms have been presented with slightly different running times and on a variety of PRAM models. The basic strategy
of these algorithms consists of processing the graph for O(log V) stages.  During each stage, the vertices are organized as
a forest of directed trees, where each vertex is in one tree and has a link (a directed edge or pointer) to its parent in that
tree. All vertices in such a tree are in the same connected component of the graph. The algorithm repeatedly combines
trees containing vertices in the same connected component. However, until the algorithm terminates, there is no guarantee
that every such tree represents a maximally connected component.
Initially, there are | V | directed trees, each consisting of a vertex pointing to itself. (Refer to the example presented in
Figure 12.23  .) During the i th stage of the algorithm, trees from stage i- 1 are hooked  or grafted  together and compressed
by a pointer-jumping operation so that the trees do not become unwieldy. Each such compressed tree is referred to as a
supervertex.  When the algorithm terminates, each supervertex corresponds to a maximally connected component in the
graph and takes the form of a star, that is, a directed tree in which all vertices point directly to the root vertex. It is the
implementation of hooking that is critical to designing an algorithm that runs in O (log V) stages. We will present an
algorithm for an arbitrary CRCW PRAM that runs in O (log V) time using T (V+ E)  processors.
Figure 12.23: A general description of a parallel component-labeling algorithm. The initial undirected graph G = (V, E )
is given in (a). In (b), the initial forest is presented. The initial forest consists of a distinct tree representing every vertex
in V . The graph presented in (c) shows the result of every vertex in V attaching to its minimum-labeled neighbor. The
graph that results from the compression of these four disjoint subgraphs is given in (d). Notice that four supervertices
are generated. The directed graph in (e) shows the result from each of these four supervertices choosing its minimum-
labeled neighbor. Finally, (f) shows the result from the final stage of the algorithm in which all vertices in the connected
graph have been compressed into a single supervertex. Note that when we present supervertices, the first vertex
(minimum label) in the list will serve as the label for the supervertex
Define index(v i ) = i to be the index of vertex vi . Define root(v i ) as a pointer to the root of the tree (or supervertex) that vi is
currently a member of. Then we can define the hooking operation hook(v i , vj) as an operation that attaches root(v i ) to
root(vj),  as shown in Figure 12.24  .
Figure 12.24: A demonstration of the hooking operation. In (a), vi and parent(v i ) are in different supervertices. In (b),
the supervertex to which v i belongs hooks to the supervertex containing parent(v i ) because root(parent(v i )) is a
minimum label over all the supervertices to which members of the supervertex labeled root(v i ) are connected. In (c),
these supervertices are merged
We can determine, for each vertex vi e V, whether or not vi belongs to a star, via the following procedure.
Determine the Boolean function star(v i ) for all vi e V, as follows: 1.
For all vertices vi , do in parallel 2.
star(vi)   true 3.
If root(v i ) > root(root(v i )), then 4.
star(v i )   false 5.
star(root(v i ))   false 6.
7.
5.
6.
star(root(root(v i )))   false 7.
End If8.
star(v i )   star(root(v i )) {*} 9.
End For10.
See Figure 12.25  for an example that shows the necessity of Step 9. It is easily seen that this procedure requires e (1)
time.
Figure 12.25: Computing the star function in parallel. Arrows represent root pointers. Step 3 initializes star(v i )  true
for all vertices. Steps 5 through 7 change star(a), star(b), star(c), and star(d) to false. However, we require step 9 to
change star(e) to false
The basic component labeling algorithm follows.
We wish to label the connected components of an undirected graph G = (V, E).
Assume that every edge between vertices vi and vj is represented by a pair of unordered edges ( vi , vj ) and ( vj , vi ).
Recall that we assume an arbitrary CRCW PRAM. That is, if there is a write conflict, one of the writes will arbitrarily
succeed.
For all v
i
 
e
V, set root(v
i
) =v
i
                              {Initialize supervertices}
For all (v
i
, v
j
)
e
E, do                                  {Loop uses arbitrary CRCWproperty}
  If index(v
i
) > index(v
j
), thenhook(v
i
, v
j
)                                   {Hook larger
                                          indexed vertices into smaller indexed vertices}
End For all edges
Repeat
    Determine star(v
i
)for all v
i
 
e
V
    For all edges (v
i
, v
j
)
e
E, do
       If v
i
  is in a star andindex(root (v
i
)) > index(root (v
j
)), then
         hook(v
i
, v
j
)                                  {Hook vertices in star to neighbors
                                                                with lower-indexed roots}
Determine star(v
i
)for all v
i
e
V
For all vertices v
i
, do
  If v
i
  is not in a star, then
       root(v
i
)
 
root(root(v
i
))                       {pointer jumping}
Until no changes are produced by the steps of the Repeat loop
Although it is beyond the scope of this book, it can be shown that the preceding algorithm is correct for an arbitrary CRCW
PRAM. Critical observations can be made, such as,
at any time during the algorithm, the structure defined by the set of root pointers corresponds to a proper (upward)
directed forest, because no vertex ever has a root with a larger index, and
when the algorithm terminates, the forest defined by the root pointers consists of stars.
Given an arbitrary CRCW PRAM with T (V + E)  processors, every computational step in the algorithm defined earlier
requires T (1) time. Therefore, we need to determine only the number of iterations required for the main loop before the
algorithm naturally terminates with stars corresponding to every connected component. It can be shown that each pass
through the loop reduces the height of a non-star tree by a fixed fraction. Therefore, the algorithm will terminate after O (log
V) steps, yielding an algorithm with total cost of T ((V+ E)  log V), which is not optimal. In fact, slightly more efficient
algorithms are possible, but they are beyond the scope of this book.
Mesh
Recall that a single step of a PRAM computation with n processors operating on a set of n data items can be simulated on
a mesh of size n in T (n1/2 ) time by a sort-based associative read and associative write operation. Therefore, given a
graph G = (V, E)  represented by a set of |E| unordered edges, distributed arbitrarily one per processor on a mesh of size
|E|, the component labeling algorithm can be solved in T (E 1/2 log E) time. Notice that this is at most a factor of T (log E)
from optimal on a mesh of size |E|. However, it is often convenient to represent dense graphs by an adjacency matrix. So
consider the situation in which a |v| |v| adjacency matrix is distributed in a natural fashion on a mesh of size |v|2 . Then, by
applying the time-optimal transitive closure algorithm followed by a simple row or column rotation, the component labeling
algorithm can be solved in T (V) time, which is optimal for this combination of architecture and graph representation.
 
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
Minimum-Cost Spanning Trees
Suppose we want to run fiber optic cable on a college campus so that there is at least one cable path between every pair of buildings. Further, suppose we want to minimize the total amount of cable that we lay. Viewing the buildings as vertices and the cables between buildings as
edges, this cabling problem is reduced to determining a spanning tree covering the buildings on campus in which the total length of cable that is laid is minimized. This leads to the definition of a minimum-cost spanning tree.
Given a connected undirected graph G = (V, E),  we define a spanning tree T= (V, E ' ), where E'   E, to be a connected acyclic graph. The reader should verify that for T to have the same vertex set as the connected graph G, and for Tnot to contain any cycles, T must contain
exactly |V| —  1 edges. Suppose that for every edge e e V, there exists a weight w(e),  where such a weight might represent, for example, the cost, length, or time required to traverse the edge. Then a minimum-cost spanning tree T  (sometimes referred to as a minimal spanning tree,
minimum-weight spanning tree, minimum spanning tree,  or MST),  is a spanning tree over G in which the weight of the tree is minimized with respect to every spanning tree of G. The weight of a tree T= (V, E ' ) is defined intuitively to be
RAM
In this section, we consider three traditional algorithms for determining a minimum-cost spanning tree of a connected, weighted, undirected graph G = (V, E)  on a RAM. All three algorithms use a greedy  approach to solve the problem. At any point during these algorithms, a set of
edges E' exists that represents a subset of some minimal spanning tree of G. At each step of these algorithms, a "best" edge is selected from those that remain, based on certain properties, and added to the working minimal spanning tree. One of the critical properties of any edge
that is added to E is that it is safe,  that is, that the updated edge set E' will continue to represent a subset of the edges of some minimal spanning tree for G.
Kruskal's Algorithm
The first algorithm we consider is Kruskals algorithm.  In this greedy algorithm, E' will always represent a forest over all vertices V in G. Furthermore, this forest will always be a subset of some minimum spanning tree. Initially, we set E' = f , which represents the forest of isolated
vertices. We also sort the edges of the graph into increasing order by weight. At each step in the algorithm, the next smallest weight edge from the ordered list is chosen and that edge is added to E ' , so long as it does not create a cycle. The algorithm follows.
Kruskal's MST Algorithm
The input consists of a connected, weighted, undirected graph G = (V, E)  with weight function w on the edges e e V.

 E'  f
For each v  eV, create Tree(v) = {v}. That is, every vertex is currently its
own tree.
Sort the edges of E into nondecreasing order by the weight function w.
While there is more than one distinct tree, consider each (u,v) eE  by
sorted order.
  If Tree(u)   Tree(v), then
     E '  E '   (u,v)
    Merge Tree(u) and Tree(v)
  End If
End While
The analysis of this algorithm depends on the data structure used to implement the graph G = (V, E),  which is critical to the time required to perform a sort operation, the time necessary to execute the function Tree(u),  and the time required for the merge operation over two trees in
the forest. Suppose that each tree is implemented as a linked list with a header element. The header element will contain the name of the tree, the number of vertices in the tree, a pointer to the first element in the list, and a pointer to the last element in the list. Assuming that the
vertices are labeled by integers in 1 … |V|, the name of a tree will correspond to the minimum vertex in the tree. Suppose that every list element contains a pointer to the next element in the list and a pointer to the head of the list. (See Figure 12.26  .) With such a data structure,
notice that Tree(u)  can be determined in T ;(1) time, and that two trees T1 and T2 can be merged in T (min{|T 1 |, |T 2 |,}) time.
Figure 12.26: A representation of a data structure that allows for an efficient implementation of Kruskals algorithm. H is a pointer to the head of the list. N is a pointer to the next element in the list
Given the data structures described, it takes z T (1) time to set E' f , T (V) time to create the initial forest of isolated elements, and T (ElogE)  time to sort the edges. The reader should verify that the union operation is invoked exactly |V | — 1 times. The difficult part of the analysis
is in determining the total time for the |V| — 1 merge operations. We leave it as an exercise to show that in the worst case, the time to perform all merge operations is T (VlogV).  Therefore, the running time of the algorithm, as described, is T (E log E), which is T (E log V).
An alternative implementation to our presentation of Kruskal's algorithm follows. Suppose that instead of initially sorting the edges into nondecreasing order by weight, we place the weighted edges into a heap, and that during each iteration of the algorithm, we simply extract the
minimum weighted edge left in the heap. Recall (perhaps from a previous course in data structures) that such a heap can be constructed in Q(ElogE) = Q(ElogV)  time, and a heap extraction can be performed in T (log E) = T (log V) time. Therefore, the heap-based (or priority-
queue-based) variant of this algorithm requires 0( E log V ) time to set up the initial heap and T (log V) time to perform the operation required during each of the T (E) iterations. Therefore, a heap-based approach results in a total running time of T (E log V ), including the merge
operations.
Prim's Algorithm
The second algorithm we consider is Prims algorithm  for determining a minimum-cost spanning forest of a weighted, connected, undirected graph G = (V, E),  with edge weight function w. The approach taken in this greedy algorithm is to add edges continually to E'   E so that E'
always represents a tree with the property that it is a subtree of some minimum spanning tree of G. Initially, an arbitrary vertex re ;Vis chosen to be the root of the tree that will be grown. Next, an edge (r, u)  is used to initialize E' , where (r, u)  has minimal weight among edges
incident on r. As the algorithm continues, an edge of minimum weight between some vertex in the current tree, represented by E ' , and some vertex not in the current tree, is chosen and added to E' . The algorithm follows.
Prim's MST Algorithm
The input consists of a connected, weighted, undirected graph G = (V E)  with weight function w on the edges e e V.
Let vertex set V  = { v
1
, v
2
,...,v
n
}.
Let the root of the tree be r = v
1
.
Initialize NotInTree = {v
2
,...,v
n
}.                            {a}
For all v 
e
NotInTree, initialize smalledge(v) 
 8
.
Set smalledge(r) 
 
0 because r is in the tree.
Set parent(r) 
 
 nil because r is the root of the tree.
For all v 
e
Adj(r), do
     parent(v) 
 
r
     smalledge(v) 
 
 w(r,v)
End For all v 
e
Adj(r)                                       {b}
While NotInTree 
 
 
f
, do                                     {c}
     u 
 
 ExtractMin(NotInTree)
  Add (u, parent(u)) to E' and remove u from NotInTree.
  For all v 
e
Adj(u) do
    If v 
e
NotInTree  and w(u,v) < smalledge(v), then
          parent(v) 
 
 u                                            {e}
          smalledge(v) 
 
 w(u,v)                                {f}
    End If
  End For
End While                                                                {d}
The structure NotlnTree  is most efficiently implemented as a priority queue because the major operations include finding a minimum weight vertex in NotlnTree  and removing it from NotlnTree.  Suppose that NotlnTree  is implemented as a heap. Then the heap can be initialized (lines
{a} through {b}) in T (FlogF) time. The While loop (lines {c} through {d}) is executed | V | —1 times. Therefore, the O (log V) time ExtractMin  operation is invoked T (V) times. Thus, the total time to perform all ExtractMin  operations is O(Vlog V).
Now consider the time required to perform the operations specified in lines {e} and {f}. Because every edge in a graph is determined by two vertices, lines {e} and {f} can be invoked at most twice for every edge. Therefore, these assignments are performed T (E) times at most.
However, notice that line {f} requires the adjustment of an entry in the priority queue, which requires O (log V) time. Therefore, the running time for the entire algorithm is 0(V log V + E  log V)), which is 0(E log V). Notice that this is the same asymptotic running time as Kruskal's
algorithm. However, by using Fibonacci heaps instead of traditional heaps, it should be noted that the time required to perform Prim's algorithm on a RAM can be reduced to T (E + V  log V).
Sollin's Algorithm
Finally, we mention Sollins algorithm.  In this greedy algorithm, E' will always represent a forest over all vertices V in G. Initially, E ' = f , which represents the forest of isolated vertices. At each step in the algorithm, every tree in the forest nominates one edge to be considered for
inclusion in E. Specifically, every tree nominates an edge of minimal weight between a vertex in its tree and a vertex in a distinct tree. So, during the f th iteration of the algorithm, the | V | —(i — 1) trees represented by E' generate | V | —(i — 1) not necessarily distinct edges to be
considered for inclusion. The minimal weight edge will then be selected from these nominees for inclusion in E' . The sequential algorithm and analysis is left as an exercise.
PRAM
In this section, we consider the problem of constructing a minimum-cost spanning tree for a connected graph represented by a weight matrix on a CREW PRAM. Given a connected graph G = (V, E),  we assume that the weights of the edges are stored in a matrix W. That is, entry
W(i, j)  corresponds to the weight of edge (i,j) e E. Because the graph is not necessarily complete, we define W(i,j) =  8 if the edge (i,j)   E. We assume that self-edges are not present in the input; therefore, we should note that W(i,i) =  8 for all 1= i= n. Notice that we use 8 to
represent nonexistent edges because the problem is one of determining a minimum-weight  spanning tree.
The algorithm we consider is based on Sollin's algorithm, as described previously. Initially, we construct a forest of isolated vertices, which are then repetitively merged into trees until a single tree (a minimum spanning tree) remains. The procedure for merging trees at a given stage
of the algorithm is to consider one candidate edge e t from every tree Tt . The candidate edge e t corresponds to an edge of minimum weight connecting a vertex of J, to a vertex in some Tj where i   j. All candidate edges are then added to the set of edges representing a minimum
weight spanning tree of G, as we have done with previously described minimum spanning tree algorithms.
During each of the merge steps, we must collapse every tree in the forest into a virtual vertex (that is, a supervertex). Throughout the algorithm, every vertex must know the identity of the tree that it is a member of so that candidate edges can be chosen in a proper fashion during
each iteration of the algorithm. We will use the component labeling technique, described earlier in this chapter, to accomplish this task.
Without loss of generality, we assume that every edge has a unique weight. Notice that in practice, ties in edge weight can be broken by appending unique edge labels to every weight. The basic algorithm follows.
The input consists of a connected, weighted, undirected graph G = (V, E)  with weight function w on the edges e e V. Let weight matrix W be used to store the weights of the edges, where W(i, j) = w(i, j).
Let vertex set V  = {v
1
,...,v
n
}.
Let G
'
 = (V,E 
'
) represent a minimum spanning tree of G that is under
construction.
Initially, set E 
'
 = 
f
.
Initially, set the forest of trees F = { T
1
,...,T
2
} where T
i
=({v
i
},
f
). That is,
every vertex is its own tree.
While |F|  > 1, do
        For all T
i
 F, determine 
Cand
i
, an edge of minimum weight between
   a vertex in T
i
  and a vertex in T
j
  where i 
 
 j.
        For all i, add Cand
i
 to E  
'
.
        Combine all trees in F                                                     that are in the same connected component with respect to the edges just added to E
'
. Assuming that r trees remain in the forest, relabel these virtual vertices (connected components) so that F = {T
1
,...,T
r
}.
        Relabel the edges in E so that the vertices correspond to the appropriate virtual vertices. This can be accomplished by reducing the weight matrix W so that it contains only information pertaining to the r virtual vertices.
End While
Combine all trees in F that are in the same connected component with respect to the edges just added to E ' . Assuming that r trees remain in the forest, relabel these virtual vertices (connected components) so that Relabel the edges in E so that the vertices correspond to the
appropriate virtual vertices. This can be accomplished by reducing the weight matrix W so that it contains only information pertaining to the r virtual vertices. End While Consider the running time of the algorithm as described. Because the graph G is connected, we know that every
time through the While loop, the number of trees in the forest will be reduced by at least half. That is, every tree in the forest will hook up with at least one other tree. Therefore, the number of iterations of the While loop is O (log V). The operations described inside of the While loop
can be performed by invoking procedures to sort edges based on vertex labels, perform parallel prefix to determine candidate edges, and apply the component-labeling algorithm to collapse connected components into virtual vertices. Because each of these procedures can be
performed in time logarithmic in the size of the input, the running time for the entire algorithm as given is O (log2 V).
Mesh
The mesh algorithm we discuss in this section is identical in spirit to that just presented for the PRAM. Our focus in this section is on the implementation of the specific steps of the algorithm. We assume that the input to the problem is a weight matrix W representing a graph G = (V,
E), where |V| = n.  Initially, W(i, j),  the weight of edge (i,j) e E, is stored in mesh processor Pi, j. Again we assume that W(i,j) = 8 if the edge does not exist or if i = j. We also assume, without loss of generality, that the edge weights are unique.
Initially, we define the forest F = {T 1 ,… ,Tn } where Ti =({vi },f ). During each of the   log 2 n   iterations of the algorithm, the number of virtual vertices (supervertices) in the forest is reduced by at least half. The reader might also note that at any point during the course of the
algorithm, only a single minimum-weight edge needs to be maintained between any two virtual vertices. We need to discuss the details of reducing the forest during a generic iteration of the algorithm. Suppose that the forest F currently has r virtual vertices. Notice that at the start of
an iteration of the While loop, as given in the previous section  , every virtual vertex is represented by a unique row and column in an r X r weight matrix W. As shown in Figure 12.27  , entry W(i, j),  1 = i,j = r, denotes the weight and identity of a minimum-weight edge between virtual
vertex i and virtual vertex j.
Figure 12.27: The r r matrix W, as distributed one entry per processor in a natural fashion on an r r submesh. Notice that each entry in processor Pi,j , 1 = i, j = r , contains the record ( Wi,j , ei,j ), which represents the minimum weight of any edge between virtual vertices (that is,
supervertices) vi and vj , as well as information about one such edge ei, j to which the weight corresponds. In this situation, the edge ei, j is actually a record containing information identifying its original vertices and its current virtual vertices
To determine the candidate edge for every virtual vertex 1= i= r, simply perform a row rotation simultaneously over all rows of W, where the rotation is restricted to the r X r region of the mesh currently storing W. The edge in E that this virtual edge represents can be conveniently
stored in the rightmost column of the r X r region because there is only one such edge per row, as shown in Figure 12.28  . Based on the virtual vertex indices of these edges being added to E ' , an adjacency matrix can be created in the r X r region that represents the connections
being formed between the current virtual vertices, as shown in Figure 12.29  . War-shall's algorithm can then be applied to this adjacency matrix to determine the connected components. That is, an application of Warshall's algorithm will determine which trees in F have just been
combined using the edges in E ' . The rows of the matrix can now be sorted according to their new virtual vertex number. Next, in a similar fashion, the columns of the matrix can be sorted with respect to the new virtual vertex numbers. Now, within every interval of rows, a minimum
weight edge can be determined to every other new virtual vertex by a combination of row and column rotations. Finally, a concurrent write can be used to compress the r X r matrix to an r' X r' matrix, as shown in Figure 12.30  .
Figure 12.28: A sample 6X6 weight matrix in which, for simplicity s sake, only the weights of the records are given. Notice that the processors in the last column also contain a minimum-weight edge and its identity after the row rotation
Figure 12.29: The 6X6 adjacency matrix corresponding to the minimum-weight edges selected by the row rotations as shown in Figure 12.28
Figure 12.30: A concurrent write is used within the r r region of the mesh to compress and update the r' rows and columns corresponding to the r' supervertices. This results in the creation of an r' r' weight matrix in the upper-left regions of the r r region so that the algorithm can
proceed to the next stage
Notice that each of the critical mesh operations working in an r X r region can be performed in O(r) time. Because the size of the matrix is reduced by at least a constant factor after every iteration, the running time of the algorithm is T (n ), which includes the time to perform a final
concurrent read to mark all of the edges in the minimum spanning tree that was determined.
 
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
Shortest-Path Problems
In this section, we consider problems involving shortest paths within graphs. Specifically, we consider two fundamental problems.
Single-Source Shortest-Path Problem:  Given a weighted, directed graph G = (V, E),  a solution to the single-source shortest-path problem  requires that we determine a
shortest (minimum-weight) path from source vertex s & V to  every other vertex v e V. Notice that the notion of a minimum-weight path generalizes that of a shortest path in that
a shortest path (a path containing a minimal number of edges) can be regarded as a minimum-weight path in a graph in which all edges have weight 1.
All-Pairs Shortest-Path Problem:  Given a weighted, directed graph G = (V, E),  a solution to the all-pairs shortest-path problem  requires the determination of a shortest
(minimum weight) path between every pair of distinct vertices u, v e V.
For problems involving shortest paths, several issues must be considered, such as whether or not negative weights and/or cycles are permitted in the input graph. It is also
important to decide whether the total weight of a minimum-weight path will be presented as the sole result or if a representation of a path that generates such a weight is also
required. Critical details such as these, which often depend on the definition of the problem, have a great effect on the algorithm that is to be developed and utilized. In the
remainder of this section, we consider representative variants of shortest-path problems as ways to introduce critical paradigms.
RAM
For the RAM, we will consider the single-source shortest-path problem in which we need to determine the weight of a shortest path from a unique source vertex to every other
vertex in the graph. Further, we assume that the result must contain a representation of an appropriate shortest path from the source vertex to every other vertex in the graph.
Assume that we are given a weighted, directed graph G = (V, E),  in which every edge e e V has an associated weight w(e).  Let s e V be  the known source vertex. The algorithm that
we present will produce a shortest-path tree T = (V ' ,E' ), rooted at s, where V   V, E'   E, V  is the set of vertices reachable from s, and for all v e V' , the unique simple path from
s to v in T is a minimum-weight path from s to v in G. It is important to emphasize that "shortest" paths (minimum-weight paths) are not necessarily unique and that shortest-path
trees (trees representing minimum-weight paths) are also not necessarily unique. See Figure 12.31  , which shows two shortest path trees for the given graph G.

Figure 12.31: A demonstration that shortest paths and shortest-path trees need not be unique. The weighted, undirected graph G is shown in (a). In (b), we see a shortest-path
tree. Notice the path (1,2,8,7) of total weight 12 chosen between source vertex 1 and sink vertex 7. A different shortest-path tree is shown in (c). Notice the path (1,6,5,7)
between vertices 1 and 7 is also of total weight 12
We consider Dijkstra s algorithm  for solving the single-source shortest-path problem on a weighted, directed graph G = (V, E)  where all of the edge weights are nonnegative. Let s e
V be the predetermined source vertex. The algorithm will create and maintain a set V of vertices that, when complete, is used to represent the final shortest-path tree T. When a
vertex v is inserted into V, it is assumed that the edge (parent(v), v)  is inserted into E.
Initially, every vertex v e V is assumed to be at distance dist(v) =  8 from the source vertex s, with the exception of all vertices directly connected to s by an edge. Let u be a
neighboring vertex of s. Then, because (s,u) e E, we initialize the distance from s to u to be dist(u) = w(s,u),  the weight of the edge originating at s and terminating at u.
The algorithm consists of continually identifying a vertex that has not been added to V, which is at minimum distance from s. Suppose the new vertex to be added to V is called x .
Then, after adding x to V, all vertices t for which (x,t) e E are examined. If the current minimum distance from s, which is maintained in dist(t),  can now be improved based on the
fact that x is in V, then dist(t)  is updated and parent(t)  is set to x (see Figure 12.32  ).
Figure 12.32: A demonstration of the progress of Dijkstras algorithm, through the iterations of its While loop, for constructing a shortest-path tree. The vertices are numbered u0
, u1 , … , in the order in which they are inserted into the tree. Arrows represent parent pointers. Dark edges are those inserted into the tree
The algorithm follows.
The algorithm takes a weighted, directed graph G = (V, E)  as input.
Initialize the vertices and edges in the shortest-path tree T = (V  ' , E' ) that this algorithm produces to be empty sets. That is, set V '   e and E'   e .
Initialize the set of available vertices to be added to V ' to be the entire set of vertices. That is, set Avail   V .
 For every vertex v  eV, do
   Set dist(v)  8. That is, the distance from every vertex to the source is initialized to be infinity.
   Set parent(v)  nil. That is, the parent of every vertex is initially assumed to be nonexistent.
 End For
 Set dist(s)  0. That is, the distance from the source to itself is 0. This step is critical to seeding the While loop that follows.
GrowingTree  true
While Avail 
 
fand 
Growing Tree
, do
  Determine u 
e
Avail
, where dist(u) is a minimum over all distances of
vertices in Avail. (Note the first pass through the loop yields u = s.)
  If 
dist(u)
 is finite, then
     V
'
 
 
V
'
 
 
{u} and Avail 
 
 Avail \{u}. That is, add u to the shortestpath tree and remove u from Avail.
     If u 
 
s, then E
'
 
 
E
'
 
 
{(parent(u),u)}. That is, add (parent(u),u)
to the edge set of T.
     For every vertex v 
e
Adj(u), do   {Check to see if neighboring vertices
                                                        should be updated.}
      If dist(v)>dist(u)+ w(u,v), then       {update distance and parent
                          information since a shorter path is now possible}
         dist(v) 
 
dist(u)+ w(u,v)
         parent(v) 
 
u
      End If dist(v)>dist(u)+ w(u,v)
     End For
  End If dist(u) is finite
  Else GrowingTree 
 
 false           {(V
'
, E
'
) is the finished component of
                                                             source vertex}
End While
The algorithm is greedy in nature in that at each step the best local choice is taken and that choice is never undone. Dijkstra's algorithm relies on an efficient implementation of a
priority queue, because the set Avail  of available vertices is continually queried in terms of minimum distance. Suppose that the priority queue of Avail  is maintained in a simple
linear array. Then a generic query to the priority queue will take T (V) time. Because there are T (V) such queries, the total time required for querying the priority queue is 0(F2 ).
Each vertex is inserted into the shortest-path tree exactly once, so this means that every edge in E is examined exactly twice in terms of trying to update distance information to
neighboring vertices. Therefore, the total time to update distance and parent information is T (E); the running time of the algorithm is T (V2 + E),  or T (V2 ), because E = 0(V2 ).
Notice that this algorithm is efficient for dense graphs. That is, if E = T (V2 ), then the algorithm has an efficient running time of T (E). However, if the graph is sparse, this
implementation is not necessarily efficient. In fact, for a sparse graph, one might implement the priority queue as a binary heap or a Fibonacci heap to achieve a slightly more
efficient running time.
PRAM and Mesh
For both of these parallel models of computation, we consider the all-pairs shortest-path problem, given a weight matrix as input. Specifically, suppose we are given a weighted,
directed graph G = (V, E)  as input, where |V| = n and every edge (u,v) e E has an associated weight w(u,v).  Further, assume that G is represented by an n X n weight matrix W,
where W(u,v) = w(u,v)  if (u,v)sE  and W(u,v) = 8 otherwise.
Let Wk (u,v)  represent the weight of a minimum-weight path from vertex u to vertex v , assuming that the intermediate vertices traversed on the path from u to v are indexed in
{1,2,… ,k}. Then the matrix Wn will contain the final weights representing a directed minimum-weight path between every pair of vertices. That is, Wn (u,v)  will contain the weight of
a minimum-weight directed path with source u and sink v , if such a path exists. Wn (u,v)  will have a value of 8 if a u   v path does not exist.
Notice that we have recast the all-pairs shortest-path problem as a variant of the transitive closure problem discussed earlier in this chapter in the section "Computing the Transitive
Closure of an Adjacency Matrix." Given a mesh of size ri 2 in which processor Pt j stores weight information concerning a path from vertex i to vertex j, we can represent the
computation of W as
Therefore, we can apply van Scoy's implementation of Warshall's algorithm, as described earlier in this chapter, to solve the problem on a mesh of size ri 2 in optimal T (n ) time.
Notice that if the graph is dense (that is, E = 0(F2 )), the weight matrix input is an efficient representation.
On a PRAM, notice that we can also implement Warshall's algorithm for computing the transitive closure of the input matrix W. Recall that two matrices can be multiplied in T (log ri
) time on a PRAM containing n3 /log n processors. Given an n X n  matrix as input on a PRAM, W' can be determined by performing T (log ri ) such matrix multiplications. Therefore,
given an n X n  weight-matrix as input, the running time to solve the all-pairs shortest-path problem on a PRAM with n3 /log n processors is 0(log2 ri ).
TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In this chapter, we study algorithms to solve a variety of problems concerned with graphs.
We present several methods of representing a graph, including an adjacency list, an
adjacency matrix, or a set of unordered edges. We introduce efficient RAM solutions to
fundamental problems such as breadth-first search, depth-first search, and Euler tour.
The PRAM algorithm for list ranking via pointer jumping, first presented in Chapter 8 , is
reviewed. Another PRAM algorithm presented is the one for tree contraction. Warshall's
efficient algorithm for computing the transitive closure of the adjacency matrix is
discussed for the RAM, and van Scoy's efficient adaptation of the algorithm to the mesh
is also presented. Connected component labeling algorithms are given for several
models of computation. Several sequential and parallel algorithms for computing
minimal-cost spanning trees are discussed. Solutions to shortest-path problems are given
for multiple models of computation.
TeamUnknown Release

0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
In this chapter, we have considered algorithms and paradigms to solve fundamental
graph problems on a RAM, PRAM, and mesh computer. For the reader interested in a
more in-depth treatment of sequential graph algorithms, please refer to the following
sources:
Graph Algorithms  by S. Even (Computer Science Press, 1979).
Data Structures and Network Algorithms  by R.E. Tarjan (Society for Industrial and
Applied Mathematics, 1983).
"Basic Graph Algorithms" by S. Khuller and B. Raghavachari, in Algorithms and
Theory of Computation Handbook,  M.J. Atallah, ed., CRC Press, Boca Raton, FL,
1999.
For the reader interested in a survey of PRAM graph algorithms, complete with an
extensive citation list, please refer to the following:
"A Survey of Parallel Algorithms and Shared Memory Machines" by R.M. Karp and
V. Ramachandran, in the Handbook of Theoretical Computer Science: Algorithms
and Complexity,  A.J. vanLeeuwen, ed. (Elsevier, New York, 1990, pp. 869-941).
The depth-first search procedure was developed by J.E. Hopcroft and R.E. Tarjan. Early
citations to this work include the following:
"Efficient Algorithms for Graph Manipulation" by J.E. Hopcroft and R.E. Tarjan,
Communications of the ACM  (16:372-378, 1973), and
"Depth-First Search and Linear Graph Algorithms" by R.E. Tarjan, SIAM Journal on
Computing,  1(2):146-60, June, 1972.

Warshall's innovative and efficient transitive closure algorithm was first presented in "A
Theorem on Boolean Matrices" by S. Warshall in the Journal of the ACM 9,  1962, 11-12.
An efficient mesh implementation of Warshall's algorithm is discussed in detail in Parallel
Algorithms for Regular Architectures  by R. Miller and Q.F Stout (The MIT Press,
Cambridge, MA, 1996).
An in-depth presentation of tree contraction for the PRAM can be found in An
Introduction to Parallel Algorithms  by J. Já Já (Addison-Wesley, Reading, MA, 1992).
This book also contains details of PRAM algorithms for additional problems discussed in
this chapter, including component labeling and minimum spanning trees. The PRAM
component-labeling algorithm presented in this chapter comes from a combination of the
algorithms presented in the following sources:
"A Survey of Parallel Algorithms and Shared Memory Machines" by R.M. Karp and
V. Ramachandran in the Handbook of Theoretical Computer Science: Algorithms
and Complexity,  AJ. vanLeeuwen, ed. (Elsevier, New York, 1990, pp. 869-941), and
"Introduction to Parallel Connectivity, List Ranking, and Euler Tour Techniques" by
S. Baase in Synthesis of Parallel Algorithms,  J.H. Reif, ed. (Morgan Kaufmann
Publishers, San Mateo, CA, 1993, pp. 61-114).
The sequential minimum spanning tree algorithm presented in this chapter combines
techniques presented in Data Structures and Algorithms in JAVA  by M.T. Goodrich and
R. Tamassia (John Wiley & Sons, Inc., New York, 1998), with those presented in
Introduction to Algorithms  by T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein (2nd
ed.: The MIT Press, Cambridge, MA, 2001). The minimum spanning tree algorithm for
the PRAM was inspired by the one presented in An Introduction to Parallel Algorithms  by
J. Já Já (Addison Wesley, 1992), whereas the MST algorithm for the mesh was inspired
by the one that appears in Parallel Algorithms for Regular Architectures  by R. Miller and
Q.F. Stout (The MIT Press, Cambridge, MA, 1996).
The reader interested in exploring additional problems involving shortest paths, as well as
techniques and algorithms for solving such problems, is referred to the following sources:
Introduction to Algorithms  by T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein
(2nd ed.: The MIT Press, Cambridge, MA, 2001).
An Introduction to Parallel Algorithms  by J. Já Já (Addison Wesley, 1992).
Parallel Algorithms for Regular Architectures  by R. Miller and Q.F. Stout (The MIT
Press, Cambridge, MA, 1996).

TeamUnknown Release
0
Chapter 12 - Graph Algorithms
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
Suppose a graph G is represented by unordered edges. Give efficient algorithms for
the following:
Construct an adjacency list representation of G. Analyze the running time of
your algorithm for the RAM and for a PRAM with |V| + | E | processors.a.
Construct an adjacency matrix representation of G. Analyze the running time of
your algorithm for the RAM, for a PRAM of T(V2) processors, and for a mesh of
T(V2) processors. For the mesh, assume an initial distribution so that no
processor has more than one edge, and include appropriate data movement
operations in your algorithm.b.1.
Give an efficient RAM algorithm to compute the height of a nonempty binary tree.
The height  is the maximum number of edges between the root node and any leaf
node. ( Hint:  recursion makes this a short problem.) What is the run ning time of your
algorithm?2.
Prove that if v0 and v1 are distinct vertices of a graph G = (V, E)  and a path exists in
G from v0 to v1, then there is a simple path in G from v0 to v 1. (Hint:  this can be
done using mathematical induction on the number of edges in a shortest path from
v0 to v 1.)3.
A graph G = (V, E)  is complete  if an edge exists between every pair of vertices.
Given an adjacency list representation of G, describe an algorithm that determines
whether or not G is complete. Analyze the algorithm for the RAM and for a CREW
PRAM with n = | V| processors.4.
Suppose the graph G = (V, E)  is represented by an adjacency matrix. Let n = V |.
Give an algorithm that determines whether or not G is complete (see the previous
exercise for the definition). Analyze the algorithm for the RAM, for an arbitrary5.

CRCW PRAM with n2 processors, and for an n X n  mesh. (For the mesh, at the end
of the algorithm, every processor should know whether or not G is complete.)5.
Let v0 and v1 be distinct vertices of a graph G = (V, E).  Suppose we want to
determine whether or not these two vertices are in the same component of G. One
way to answer this query is by executing a component-labeling algorithm, then
comparing the component with v0 and v1. However, simpler algorithms (perhaps not
asymptotically faster) can determine whether two vertices belong to the same
component. Give such an algorithm and its running time on a RAM.6.
The distance  between two vertices in the same component of a graph is the number
of edges in a shortest path connecting the vertices. The diameter  of a connected
graph is the maximum distance between a pair of vertices of the graph. Give an
algorithm to find the maximal diameter of the components of a graph. Analyze the
algorithm's running time for the PRAM and the mesh.7.
Let G = (V, E)  be a connected graph. Suppose there is a Boolean function
hasTrait(vertex)  that can be applied to any vertex of G in order to determine in 0(1)
RAM time whether or not the vertex has a certain trait.
Given a graph represented by adjacency lists, describe an efficient RAM
algorithm to determine whether or not there are adjacent vertices with the trait
tested for by this function. Give an analysis of your algorithm.a.
Suppose instead that the graph is represented by an adjacency matrix.
Describe an efficient RAM algorithm to determine whether or not there are
adjacent vertices with the trait tested for by this function. Give an analysis of
your algorithm.b.8.
A bipartite graph  is an undirected graph G = (V, E)  with nonempty subsets V0, V1 of
V such that V0  V1 = V, V 0 nV1 = f, and every member of E joins a member of V0
to a member of V1. Let T = (V,E ') be a minimum spanning tree of a connected
bipartite graph G. Show that T is also a bipartite graph.9.
Suppose G is a connected graph. Give an algorithm to determine whether or not G
is a bipartite graph (see the previous problem). Analyze the algorithm's running time
for the RAM.10.
Let S = {I i = [a i, bi]}ni=1 be a set of intervals on the real line. An interval graph G = (V,
E) is determined by S as follows. V = | v.>, and for distinct indices i andy, there is an
edge from vt to v ,- if and only if /. C| I &(p. Give an algorithm to construct an interval
graph determined by a given set S of intervals and analyze the algorithm's running
time for a RAM. Note:  there is a naïve algorithm that runs in &(n2), where n = |V |.
You should be able to give a more sophisticated algorithm that runs in T(nlogn + E)11.
12.
time.
Suppose T= (V, E)  is a tree. What is the asymptotic relationship between | E| an/*d
|V? Explain.12.
Let G = (V, E)  be a connected graph. We say e eE is a bridge  of G if the graph G
=(r, e|{ e}) is disconnected. It is easy to see that if G represents a traffic system, its
bridges represent potential bottlenecks. Thus, it is useful to be able to identify all
bridges in a graph.
A naïve (non-optimal) algorithm may be given to identify all bridge edges as
follows. Every edge e is regarded as a possible bridge, and the graph Ge is
tested for connectedness. Show that such an algorithm runs on a RAM in 0(E(V
+ E))  time.a.
Let T be a minimal spanning tree for G. Show that every bridge of G must be
an edge of T.b.
Use the result of part b to obtain an algorithm for finding all bridges of G that
runs on a RAM in 0(V2 + E log V) time. Hint:  use the result of Exercise 12.c.13.
Let G = (V, E)  be a connected graph. An articulation point  is a vertex of G whose
removal would leave the resulting graph disconnected. That is, v is an articulation
point of G if and only if the graph Gv =yV|{v},E v), where Ev = |e eE | e is not incident
on v j, is a disconnected graph. Thus, an articulation point plays a role among
vertices analogous to that of a bridge among edges.
Suppose | V |> 2.  Show that at least one vertex of a bridge of G must be an
articulation point of G.a.
Let v e V be an articulation point of G. Must there be a bridge of G incident on
v? If so, give a proof; if not, give an example.b.
Let G be a connected graph for which there is a positive number C such that no
vertex has degree greater than C. Let v eVbe a vertex of G. Give an algorithm
to determine whether or not v is an articulation point. Discuss the running time
of implementations of your algorithm on the RAM, CRCW PRAM, and mesh.c.14.
Let   be an associative binary operation that is commutative and that can be
applied to data stored in the vertices of a graph G = (V, E).  Assume a single
computation using   requires T(1) time. Suppose G is connected and repre sented
in memory by unordered edges. How can we perform an efficient RAM semigroup
computation based on  , on the vertices of G? Give the running time of your15.
16.
algorithm.
Let   be an associative binary operation that is commutative and that can be
applied to the edges of a tree T = (V, E).  Assume a single computation using  
requires T(1) time. How can we perform an efficient RAM semigroup computation
on the edges of T? Give the running time of your algorithm. (Note that your
algorithm could be used for such purposes as totaling the weights of the edges of a
weighted tree.)16.
Suppose an Euler tour of a tree starts at the root vertex. Show that for every non-
root vertex v of the tree, the tour uses the edge (parent(v),v)  before using any edge
from v to a child of v.17.
Suppose it is known that a graph G = (V, E)  is a tree with root vertex vt eV, but the
identity of the parent vertex parent(v)  is not known for v eV |{v}. How can every
vertex v determine parent(v)?  What is the running time of your algorithm on a RAM?18.
Give an efficient RAM algorithm to determine for a binary tree T= (V, E)  with root
vertex veV, the number of descendants of every vertex. What is the running time of
your algorithm?19.
Suppose T= (V, E)  is a binary tree with root vertex v e V. Let T' be the graph derived
from T as described in the Euler tour section of the chapter. Is a pre-order
(respectively, inorder  or postorder)  traversal (see Figure 12.33 ) of T' an Euler tour?
What is the running time on a RAM of a preorder (respectively, inorder or postorder)
traversal?20.
Figure 12.33: Tree traversals. Steps of each recursive algorithm are shown at
the top level of recursion; also, the order in which the vertices are processed by
each algorithm
Prove that the time required for all | V — 1 merge operations in Kruskal's algorithm,
as outlined in the text, is T(FlogF) in the worst case on a RAM.21.
Analyze the running time of Sollin's algorithm as described in the text.22.
Given a labeled n X n  digitized image, and one "marked" pixel per component,
provide an efficient algorithm to construct a minimum-distance spanning tree within
every component with respect to using the "marked" pixel as the root. Present
analysis for the RAM.23.
TeamUnknown Release
0
Chapter 13 - Numerical Problems
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter 13: Numerical Problems
With the exception of Chapter 6 , "Matrix Operations," most of this book has
beenconcerned with solutions to "non-numerical" problems. That is not to say that we
have avoided doing arithmetic. Rather, we have concentrated on problems in which
algorithms do not require the intensive use of floating-point calculations or the
unusual storage required for very large integers. It is important to realize that a stable,
accurate, and efficient use of numerically intensive calculations plays an important
role in scientific and technical computing. As we have mentioned previously, the
emerging discipline of computational science and engineering is already being called
the third science, complementing both theoretical science and laboratory science.
Computational science and engineering is an interdisciplinary discipline that unites
computing, computer science, and applied mathematics with disciplinary research in
chemistry, biology, physics, and other scientific and engineering fields. Computational
science and engineering typically focuses on solutions to problems in engineering and
science that are best served via simulation and modeling. In this chapter, we examine
algorithms for some fundamental numerical problems.
In most of our previous discussions, we have used n as a measure of the size of a
problem, in the sense of how much data is processed by an algorithm (or how much
storage is required by the data processed). This is not always the case for the
problems discussed in this chapter. For example, the value of xncan be determined
with only T(1) data items. However, the value of n will still play a role in determining
the running time and memory usage of the algorithms discussed. The focus of this
chapter is on RAM algorithms, but several of the exercises consider the design and
analysis of parallel algorithms to solve numerical problems.
Primality
Given an integer n> 1,  suppose we wish to determine if n is a prime number;  that is, if the

only positive integer factors of n are 1 and n. This problem, from the area of mathematics
known as number theory,  was once thought to be largely of theoretical interest. However,
modern data encryption techniques depend on factoring large integers, so there is
considerable practical value in the primality problem.
Our analysis of any solution to the primality problem depends in part on assumptions that
we should reexamine. For most of this book, we have assumed that operations such as
computing the quotient of two numbers or the square root of a number can be done in
T(1) time. This assumption is appropriate if we assume the operands have magnitudes
that are bounded both above and below. However, researchers are now considering the
primality problem for numbers with millions of decimal digits. For such numbers n,
computations of n/u (where u is a smaller integer) and n1/2 (with accuracy, say, to some
fixed number of decimal places) take time approximately proportional to the number of
digits in n, thus, T(log n) time. (Magnitudes of numbers considered are bounded by
available memory. However, when we allow the possibility of integers with thousands or
millions of decimal digits and observe that the time to perform arithmetic operations
depends on the number of digits in the operands, it seems more appropriate to say such
operations take T(log n) time than to say they take T(1) time.) In the following, we say " n
is bounded" if there is a positive integer C such that n < C  (hence the number of digits of
n is bounded), whereas " n is arbitrary" means n is not bounded; and we speak of
"bounded n" and "arbitrary n" models, respectively.
Recall that n is prime if and only if the only integral factorization n = u  × v of n with
integers 1 =u=v is u= 1, v = n. This naturally suggests a RAM algorithm in which we test
every integer u from 2 to n - 1 to see if u is a factor of n. Such an algorithm runs in O(n)
time under the bounded n model; O(n log n) time under the arbitrary n model. However,
we can improve our analysis by observing that any factorization n = u  ×v of n with
integers 1=u=v must satisfy 1 =u =n 1/2(otherwise, we would have n1/2 < u> v,  hence n
= n1/2 ×n1/2 < u×u =u×v = n,  yielding the contradictory conclusion that n < n).  Thus, we
obtain the following RAM algorithm:
Procedure Primality( n, nIsPrime, factor)
Input:  n, an integer greater than 1
Output:  nIsPrime,  true or false according to whether n is prime;
factor,  the smallest prime factor of n if n is not prime
Local variable:  Root_n,  integer approximation of n1/2
Action:
 factor = 2;
 Root _ n = ¢£n1 2 /   ¥¦;
  nIsPrime ¬true;
 Repeat
    If n / factor = ¢£n / factor ¥¦, then nIsPrime  ¬ false
    Else factor ¬ factor + 1;
Until (not nIsPrime) or (factor > Root _ n);
It is easily seen that this algorithm takes O(n1/2) time under the bounded n model and
O(n1/2 log n) time under the arbitrary n model. Notice that worst-case running times of
T(n1/2) under the bounded n model and T(n1/2 log n) time under the arbitrary n model are
achieved when n is prime.
Notice that exploring non-prime values of factor  in the preceding algorithm is
unnecessary, because if n is divisible by a composite integer u ×v, it follows that n is
divisible by u. Therefore, if we have in memory a list of the prime integers that are at
most n1/2 and use only these values for factor  in the preceding algorithm, we obtain a
faster algorithm. It is known that the number p(n) of prime numbers that are less than or
equal to n satisfies p(n) = T(n/log n). This follows from the Prime Number Theorem,
which states that
Thus, we can modify the previous algorithm, as follows:
Procedure Primality( n, prime, nIsPrime, factor)
Input:  n, a positive integer;
prime,  an array in which consecutive entries are successive primes including all
primes =n1/2, and the next prime
Output:  nIsPrime,  true or false according to whether n is prime; factor,  the smallest
prime factor of n if n is not prime
Local variables:  i, an index;
Root_n,  integer approximation of n1/2
Action:
  i ¬1 {set index for first entry of prime}
  Root _ n ¬  n1 2 /
            ¢£   ¥¦;
nIsPrime ¬ true;
Repeat
       factor ¬ prime[i];
   If n / factor = ¢£n / factor ¥¦, then nIsPrime  ¬ false
   Else i ¬i + 1;
Until (not nIsPrime) or (prime[i] > Root _ n);
In light of the asymptotic behavior of the function p(n) it is easily seen that this RAM
algorithm runs in 
 time under the bounded n model and in O(n1/2) time under the
arbitrary n model.
In the Exercises, the reader is asked to devise a parallel algorithm for the pri-mality
problem.
TeamUnknown Release
0
Chapter 13 - Numerical Problems
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Greatest Common Divisor
Another problem concerned with factoring integers is the greatest common divisor (gcd)
problem. Given nonnegative integers n0 and n1, we wish to find the largest positive
integer, denoted gcd ( n0,, n0,) that is a factor of both n0 and n1. We will find it useful to
define gcd(0, n)= gcd( n, 0)= n for all positive integers n.
The greatest common divisor is used in the familiar process of "reducing a fraction to its
lowest terms." This can be important in computer programming when calculations
originating with integer quantities must compute divisions without roundoff error. For
example, we would store 1/3 as the pair (1,3) rather than as 0.333 …33. In such a
representation of real numbers, for example, we would have (5,60) = (3,36), because
each of the pairs represents the fraction 1/12.
The Euclidean algorithm,  a classical solution to the gcd problem, is based on the
following observation. Suppose there are integers q and r (quotient  and remainder,
respectively) such that
Then any common factor of n0 and n1 must also be a factor of r. Therefore, if n0 = n1 and
q= n0 /n1  ,we have n1 > r =0 and
These observations give us the following recursive algorithm:
Function  gcd( n0, n1) {greatest common divisor of arguments}
Input:  nonnegative integers n0, n1
Local variables:  integer quotient, remainder
Action:

If n0 < n1, then swap(n0, n1);  {Thus, we assume n0 = n1.}
If n1= 0, return n0
Else
      quotient    n0 / n1 ;
      remainder  n0-n1× quotient;
   return gcd(n1, remainder)
End else
In terms of the variables discussed above, we easily see that the running time of this
algorithm T(n0, n1), satisfies the recursive relation
It is perhaps not immediately obvious how to solve this recursion, but we can make use
of the following.
Lamé's Theorem
The number of division operations needed to find gcd n0, n1 for integers satisfying n0 =
n1 = 0, is no more than five times the number of decimal digits of n1.
It follows that if we use the bounded n model discussed earlier for the primality problem,
our implementation of the Euclidean algorithm on a RAM requires T(n0, n1 O(log(min{ n0,
n1})) time for positive integers n0, n1
The Euclidean algorithm seems inherently sequential. In the exercises, a very different
approach is suggested that can be parallelized efficiently.
TeamUnknown Release
0
Chapter 13 - Numerical Problems
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
byRuss Miller  andLaurence Boxer
Cengage Charles River Media  2005
 
Integral Powers
Let x be a real (that is, floating-point) number and let x n be an integer. Often we consider the computation of x to be a constant-time
operation. This is a reasonable assumption to make if the absolute value of n is bounded by some constant. For example, we might
assume that the computation of x n requires T (1) time for |n| = 100. However, if we regard n as an unbounded parameter of this problem,
it is clear that the time to compute x n is likely to be related to the value of n.
We can easily reduce this problem to the assumption that n = 0 because an algorithm to compute x n for an arbitrary integer n can be
constructed by the following algorithm:
Compute temp = x |n|1.
If n = 0, return temp  else return 1/ temp. 2.
Notice that step 2 requires T (1) time. Therefore, the running time of the algorithm is dominated by the computation of a nonnegative
power. Thus, without loss of generality in the analysis of the running time of an algorithm to solve this problem, we will assume that n > 0.
A standard, brute-force, algorithm is given next for computing a simple power function on a RAM.
Function power ( x, n ) {return the value of x n
Input:  x , a real number
n, a nonnegative integer
Output:  x n
Local variables:  product,  a partial result
counter,  the current power

Action  :
product = 1;
If n > 0, then
   For counter = 1 to n, do
      product = product  x
   End For
End If
Return product
The reader should verify that the running time of the previous RAM algorithm is T (n ), and that this algorithm requires T (1) extra space.
Now, let's consider computing x 19 for any real value x . The brute-force algorithm given earlier requires 19 multiplications. However, by
exploiting the concept of recursive doubling that has been used throughout the book, observe that we can compute x 19 much more
efficiently, as follows.
Compute (and save) x 2 = x x 1.
Compute (and save) x 4 = x 2 x 22.
Compute (and save) x 8 = x 4 x 43.
Compute (and save) x 16 = x 8 x 84.
Compute and return x 19 = x 16 x 2 x 5.
Notice that this procedure requires a mere six multiplications, although we pay a (small) price in requiring extra memory.
To generalize from our example, we remark that the key to our recursive doubling algorithm is in the repeated squaring of powers of x
instead of the repeated multiplication by x . The general recursive doubling algorithm follows:
Function power( x, n) {return the value of x n }
Input:  x , a real number
n, a nonnegative integer
Output:  x n
Local variables:  product,  a partial result
counter, exponent:  integers
p [0…   log 2 n  ] , an array used for certain powers of x
q [0…   log 2 n  ] , an array used for powers of 2
Action:
Product    = 1;
If n >  0, then
     p[0] = x;
     q [0] = 1;
   For counter = 1 to            n   , do
                          log2    
           q[counter] = 2 q[counter -1]; { = 2counter }
                                                                2                     i          i
                                                                   { p[i] = xq[]  = x2}
           p[counter] = (p[counter -1])
   End For
     exponent = 0;
   For counter =                           n     downto 0, do
                                 log2    
          If exponent   +q[counter]  n  then
              exponent= exponent +q[counter];
              product = product   p[counter]
          End If exponent +q[counter]  n
   End For
End If n > 0
Return product
The reader should be able to verify that this algorithm runs in T (log n ) time on a RAM, using T (log n )extra space. The reader will be
asked to consider parallelizing this RAM algorithm as an exercise.
 
TeamUnknown Release
0
Chapter 13 - Numerical Problems
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Evaluating a Polynomial
Let f(x) be a polynomial function,
for some set of real numbers { ai}i=0n, with a n  0 if n > 0. Then n is the degree  of f(x). As
was the case in evaluating x n a straightforward algorithm for evaluating f (t), for a given
real number t, does not yield optimal performance. Consider the following naïve
algorithm.
evaluation  = 0.
For i =0 to n, do
If ai   0, then evaluation  = evaluation  + ai ×x i.
Return evaluation .
Notice that we could, instead, use an unconditional assignment in the body of the For
loop. Because the calculation of x i takes O(1) time, it is often useful to omit this
calculation when it isn't necessary ( i.e., when ai = 0).
It is clear that the For loop dominates the running time. If we use the brute-force linear
time algorithm to compute x n then the algorithm presented earlier for evaluating a
polynomial will run on a RAM in
worst-case time. Even if we use our recursive doubling T(log n) time algorithm for
computing x n this straightforward algorithm for evaluating a polynomial will run on a RAM
in

worst-case time. However, we can do better than this.
Let's consider a third-degree polynomial. We have
For example,
This illustrates a general principle, that by grouping expressions appropriately, we can
reduce the number of arithmetic operations to a number linear in n, the degree of the
polynomial. This observation is the basis for Horner s Rule  and a corresponding
algorithm, given next.
Function Evaluate( a, x)
{evaluate the polynomial represented by the coefficient array a at the input value x}
Input:  Array of real coefficients a[0… n], real number x.
Output:  
Local variables:  i, an index variable; result  to accumulate the return value
Action:
Result = a[n];
If n > 0, then
   For i = n downto 1, do
      result = result × x + a[i -1]
   End For
End If
Return result
The reader should verify that the preceding algorithm implements Horner's Rule on a
RAM in T(n) time. This polynomial evaluation method appears to be inherently
sequential, that is, it is difficult to see how Horner's method might be recognizable if
modified for efficient implementation on a fine-grained parallel computer. In the
exercises, the reader is asked to consider other approaches to constructing an efficient
parallel algorithm to evaluate a polynomial.
TeamUnknown Release
0
Chapter 13 - Numerical Problems
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Approximation by Taylor Series
Recall from calculus that a function that is sufficiently differentiable may be approximately
evaluated by using a Taylor polynomial (Taylor series).  In particular, let f(x) be
continuous everywhere on a closed interval ( a, b) and n times differentiable on the open
interval ( a, b)containing values x and x0, and let 
 be the set of polynomial
functions defined by
where f(i) denotes the ith order derivative function and i ! denotes the factorial function.
Then the error term in approximating f(x) by Pn-1(x) is
for some t between x and x0. (Actually, this quantity is the truncation error  in such a
calculation, so called because it is typically due to replacing an exact value of an infinite
computation by the approximation obtained via truncating to a finite computation. By
contrast, a roundoff error  occurs whenever an exact calculation yields more non-zero
decimal places than can be stored. In the remainder of this section, we will consider only
truncation errors.)
Often, we do not know the exact value of t in the error term. If we knew the value of t,
we could compute the error and adjust our calculation by its value to obtain a net
truncation error of 0. However, we can often obtain a useful upper bound on the
magnitude of the error. Such a bound may provide us with information regarding how
hard we must work to obtain an acceptable approximation.
For example, we may have an error tolerance e> 0. This means we wish to allow no more
than e of error in our approximation. The value of e may give us a measure of how much

work (how much computer time) is necessary to compute an acceptable approximation.
Therefore, we may wish to express our running time as a function of e. Notice that this is
significantly different from the analysis of algorithms presented in previous chapters. We
are used to the idea that the larger the value of e the larger the running time of an
algorithm. However, in a problem in which error tolerance determines running time, it is
usually the case that the smaller the value of e, the larger the running time, that is, the
smaller the error we can tolerate, the more we must work to obtain a satisfactory
approximation. It is difficult to give an analysis for large classes of functions. This is
because the rate of convergence of a Taylor series for the function f(x) that it represents
depends on the nature of f(x) and the interval [a, b]  on which the approximation is
desired. Of course, the analysis also depends on the error tolerance. Next, we present
examples to illustrate typical methods.
Example
Give a polynomial of minimal or nearly minimal degree that will approximate the
exponential function ex to d decimal places of accuracy on the interval [-1,1], for some
positive integer d.
Solution:  Let's take x0= 0 and observe that f(i) (x) = ex for all i. Our estimate of the
truncation error then becomes
Notice that ex is a positive and increasing (because its first derivative is always positive)
function. Therefore, its maximum absolute value on any interval is at the interval's right
endpoint. Thus, on the interval [-1,1], we have
(Note the choice of 2.8 as an upper bound for e is somewhat arbitrary; we could have
used 3 or 2.72 instead.) The requirement of approximation accurate to d decimal places
means we need to have | en(x<0.5 ×10-d. Therefore, it suffices to take
in order that the polynomial
approximate ex to d decimal places of accuracy on the interval [-1,1].
We would prefer to solve inequality ( 13.1) for n in terms of d, but a solution does not
appear to be straightforward. However, it is not hard to see from inequality ( 13.1) that n =
o(d)(see the Exercises), although for small values of d, this claim may not seem to be
suggested (see the following discussion). The assertion is important because we know
that on a RAM, for example, n as a measure of the degree of a polynomial is also the
measure of the running time in evaluating the polynomial (in the sense that Horner's
algorithm runs in T(n) time).
For a given value of d let nd be the smallest value of n satisfying inequality ( 13.1).Simple
calculations based on inequality ( 13.1)yield the values shown in ( Table 13.1 ).
Table 13.1:
Values of d
(decimal
places)
and nd
(number of
terms) for
the Taylor
series for
ex
expanded
about x0 =
0 on [ -1,1]
d
nd
1
5
2
6
3
8
4
9
5
10
Thus,if d =3,the desired approximating polynomial for ex on [-1,1] is

Example
Give a polynomial of minimal or nearly minimal degree that will approximate the
trigonometric function sin x to d decimal places of accuracy on the interval [- p,p] for
some positive integer d.
Solution:  Let's take x0 = 0 and observe that f (i)e {—1,0,1 } for all i. If the latter claim is not
obvious to the reader, it is a good exercise in mathematical induction. Our estimate of the
truncation error then becomes
As in the previous example, accuracy to d decimal places implies an error tolerance of | en
(x)|= 0.5 × 10-d. Hence, it suffices to take
If we take the minimal value of n that satisfies inequality ( 13.2) for a given d, we have n =
o(d)(see the Exercises), although for small values of d, this claim may not seem to be
suggested (see the following discussion).
For a given value of d, let nd be the smallest value of n satisfying inequality ( 13.2). Simple
calculations based on inequality ( 13.2) yield the values shown in ( Table 13.2 ).
Table 13.2:
Values of d
(decimal
places)
and nd
(number of
terms) for
the Taylor
series for
sin x
expanded
about x0
=0 on [-
p,p]
d
nd
1
10
2
12
3
14
4
15
5
17
Thus, for d = 2 we can approximate sin x on the interval [- p,p] to two decimal places of
accuracy by the polynomial
TeamUnknown Release
0
Chapter 13 - Numerical Problems
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Trapezoidal Integration
A fundamental theorem of calculus is that if F'(x) = f(x)for every xe[a,b], then
Unfortunately, for many important functions f(x), the corresponding anti-derivative
function i( x) is difficult to evaluate for a given value of x. As an example, consider the
function f(x) = x-1 with
For such functions, it is important to have approximation techniques to evaluate definite
integrals.
One of the best-known approximation techniques for definite integrals is Trapezoidal
Integration,  in which we use the relationship between definite integrals and the area
between the graph and the x-axis to approximate a slab of the definite integral with a
trapezoid. We will not bother to prove the following statement, because its derivation can
be found in many calculus or numerical analysis textbooks.
Theorem:  Let f(x) be a function that is twice differentiable on the interval [a,b] and let n
be a positive integer. Let
and let xi, {ie 1,2,…, n-1} be defined by xi= a + ih. Let

Then tn is an approximation to
with the error in the estimate given by
for some   (a, b).
The reader may wish to consider Figure 13.1  to recall the principles behind Trapezoidal
Integration.
Figure 13.1: Trapezoidal Integration. The dashed lines represent the tops of the
trapezoids. The area under each small arc is approximated by the area of a trapezoid.
It is often much easier to compute the area of a trapezoid than the exact area under
an arc. The total area of the trapezoids serves as an approximation to the total area
under the curve
The value of   in equation (13.3)  is often unknown to us, but an upper bound for | f ( ) is
often sufficient, as what we hope to achieve is that | en| be small.
If we assume that for xe[a,b], each value of/( x) can be computed on a RAM in T(1) time,
then it is easy to see that tn can be computed on a RAM in T(n) time (see the Exercises).
We expect that the running time of an algorithm will be a factor of the quality of the
approximation, much as was the case of computing the Taylor series to within a
predetermined error.
Example
For some positive integer d, compute ln 2 to d decimal places via trapezoidal integration.
Give an analysis of the running time of your algorithm in terms of d.
Solution:  Because
we take f(x)= x-1, f'(x) = -x-2, f (x)= 2 x-3, f(3)(x)= -6 x-4, [a, b] = [1,2]. Notice f (x)> 0 on
[1,2], and f  is a decreasing function (because its derivative, f(3)(x), is negative for all
xe[1,2]). Therefore, f  attains its maximum absolute value on [1,2] at the left endpoint. It
follows that
Because we wish to attain d decimal place accuracy, we want | en= 0.5 × 10-dd, so it
suffices to take
We leave to the reader as an exercise the computation of ln 2 accurate to a desired
number of decimal places by Trapezoidal Integration, as discussed earlier.
If we choose the smallest value of n satisfying the inequality ( 13.4), we conclude that the
running time of our approximation of ln 2 via Trapezoidal Integration as discussed
previously is exponential in the number of decimal places of accuracy, T(10d/2).
We remark that it is not unusual to find that the amount of work required is exponential in
the number of decimal places of accuracy required. In these situations, trapezoidal
integration may not be a very good technique to use for computing approximations that
are required to be extremely accurate. Another way of looking at this analysis is to
observe that using an error tolerance of e= 0.5 ×10 d, we have d = —log 10 (2e). Further, if
we substitute this into inequality ( 13.4), we conclude that the minimal value of n satisfying
the inequality is T(e-1/2).
Notice also, for example, that for d = 6 (for many purposes, a highly accurate estimate),
the minimum value of n to satisfy inequality ( 13.4) is n = 578. Although this indicates an
unreasonable amount of work for a student in a calculus class using only pencil, paper,
and a nonprogrammable calculator, it is still a small problem for a modern computer.
Other methods of "numerical integration" such as Simpson's Method tend to converge
faster (not asymptotically so) to the definite integral represented by the approximation.
Fortunately, for many purposes, only a small number of decimal places of accuracy are
required. Also, it may be that another technique, such as using a Taylor series, is more
efficient for computing the value of a logarithm.
TeamUnknown Release
0
Chapter 13 - Numerical Problems
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Summary
In contrast with most previous chapters, this chapter is concerned with numerical
computations. Many such problems have running times that do not depend on the
volume of input to be processed, but rather on the value of a constant number of
parameters, or, in some cases, on an error tolerance. The problems considered come
from branches of mathematics such as algebra and number theory, calculus, and
numerical analysis. We consider problems of prime factorization, greatest common
divisor, integral powers, evaluation of a polynomial, approximations via a Taylor series,
and trapezoidal integration. The solutions presented are all for the RAM; readers will be
asked to consider parallel models of computation in the Exercises.
TeamUnknown Release

0
Chapter 13 - Numerical Problems
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Chapter Notes
The primality problem and the greatest common divisor problem are taken from Number
Theory, a branch of mathematics devoted to fundamental properties of numbers,
particularly (although not exclusively) integers.
We have used the Prime Number Theorem concerning the asymptotic behavior of the
function p(n), the number of primes less than or equal to the positive integer n. This
theorem is discussed in the following sources:
T.M. Apostol, Introduction to Analytic Number Theory,  Springer-Verlag, New York,
2001.
W. Narkiewicz, The Development of Prime Number Theory,  Springer-Verlag, Berlin,
2000.
K.H. Rosen, Elementary Number Theory and Its Applications,  Addison-Wesley
Publishing, Reading, MA, 1993.
The latter also discusses the Euclidean algorithm for the greatest common divisor
problem and contains a proof of Lamé's Theorem.
Other problems we have discussed in this chapter are taken from numerical analysis,  an
area of applied mathematics and computing that is concerned with computationally
intensive problems involving numerical algorithms, approximation, error analysis, and
related issues. Problems in numerical analysis have applications in branches of
mathematics that derive from calculus (differential equations, probability, and statistics)
and linear algebra (matrix multiplication, solution of systems of linear equations, and
linear programming) and their application areas. For an introduction to the field, the
reader is referred to the following:
N.S. Asaithambi, Numerical Analysis: Theory and Practice,  Saunders College
Publishing, Fort Worth, 1995.

R.L. Burden and J.D. Faires, Numerical Analysis,  PWS-Kent Publishing Company,
Boston, 1993.
S. Yakowitz and Ferenc Szidarovszky, An Introduction to Numerical Computations,
Prentice Hall, Upper Saddle River, NJ, 1990.
We have discussed approximation problems with regard to the algorithmic efficiency of
our solutions in terms of error tolerance, sometimes expressed in terms of the number of
decimal places of accurate calculation. It is tempting to say this is rarely important, that
most calculations require only a small number of decimal places of accuracy. One should
note, however, that there are situations in which very large numbers of accurate decimal
places are required. As an extreme example, some mathematicians are interested in
computing the value of p to millions of decimal places. Although these examples involve
techniques beyond the scope of this book (because, for example, ordinary treatment of
real numbers allows for the storage of only a few decimal places), the point is that
interest exists in computations with more than "ordinary" accuracy.
TeamUnknown Release
0
Chapter 13 - Numerical Problems
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Exercises
Devise a parallel algorithm to solve the primality problem for the positive integer n.
At the end of the algorithm, every processor should know whether n is prime and, if
so, what the smallest prime factor of n is. Use the bounded n model and assume
your computer has n1/2 processors, but that a list of primes is not already stored in
memory. Analyze the running time of your algorithm on each of the following
platforms: CREW PRAM, EREW PRAM , mesh, and hypercube.1.
Suppose you modify the algorithm of the previous exercise as follows: assume a list
of primes p satisfying p=  n1/2  is distributed one prime per processor. How many
processors are needed? Analyze the running time of the resulting algorithm run on
each of the following platforms: CREW PRAM, EREW PRAM , mesh, and
hypercube.2.
Consider the problem of computing gcd(n 0, n1) for nonnegative integers n0, n1,
where n0 =n1. Assume a list of primes p satisfying p=  n1/2  is kept in memory (for
a parallel model of computation, assume these primes are distributed one prime per
processor). Devise an algorithm for computing gcd(n 0, n1) efficiently based on
finding, for each prime p on this list, the maximal nonnegative integer k such that pk
is a common factor of n0 and n1. Assume multiplication and division operations can
be done in T(1) time. For parallel machines, at the end of the algorithm, every
processor should have the value of gcd(n 0, n1). Analyze the running time of such an
algorithm for the RAM, CREW PRAM, EREW PRAM , mesh, and hypercube. Hint:
consider using our efficient sequential algorithm for computing xn3.
Decide whether our T(log n)-time algorithm for computing xn is effectively
parallelizable. That is, either give a version of this algorithm for a PRAM that runs in
o(log n)time and show that it does so, or argue why it is difficult or impossible to do
so.4.
Show that a RAM algorithm to evaluate a polynomial of degree n must take O(n) 5.
6.

time; hence, our T(n) time algorithm is optimal.5.
Devise an algorithm for evaluation of a polynomial of degree n on a PRAM . This will
be somewhat easier on a CREW PRAM  than on an EREW PRAM , but in either
case, you should be able to achieve T(log ri)time using T(n/log n)processors, hence
an optimal cost of T(n.6.
Modify your algorithm for the previous exercise to run on a mesh or hypercube of
size n. Assume the coefficients of the polynomial are distributed T(1) per processor.
Analyze the running time for each of these architectures.7.
Show that for any xe[-1,1], the value of ex can be computed to within 0.5 × 10 -d for
positive integer d (that is, to ( d-decimal place accuracy) in o(d) time on a RAM. You
may use inequality ( 13.1).8.
Show that inequality ( 13.2) implies n = o(d) and use this result to show that the
function sin x can be computed for any x e[-p,p] to (d-decimal place accuracy in
o(d)time on a RAM.9.
Show that if we assume the value of f(x) can be computed in T(1) time for all x e [a,
b], the Trapezoidal Integration estimate tn can be computed on a RAM  in T(n) time.10.
Analyze the running time of using Trapezoidal Integration to compute 
to d decimal places, as an asymptotic expression in d. To simplify the problem, you
may assume (possibly incorrectly) that for all x e [0,1], ex can be computed with
sufficient accuracy in T(1) time.11.
TeamUnknown Release
0
Bibliography
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Bibliography
A.V. Aho, J.E. Hopcroft, and J.D. Ullman, The Design and Analysis of Computer
Algorithms,  Addison-Wesley, 1974.1.
S.G. Akl and KA. Lyons, Parallel Computational Geometry,  Prentice Hall, 1993. 2.
G.S. Almasi and A. Gottlieb, Highly Parallel Computing,  The Benjamin/Cum-mings
Publishing Company, New York, 1994.3.
G. Amdahl, "Validity of the single processor approach to achieving large scale
computing capabilities," AFIPS Conference Proceedings,  vol. 30, Thompson Books,
1967, 483-85.4.
T.M. Apostol, Introduction to Analytic Number Theory,  Springer-Verlag, New York,
2001.5.
N.S. Asaithambi, Numerical Analysis: Theory and Practice,  Saunders College
Publishing, Fort Worth, TX, 1995.6.
MJ. Atallah, ed., Algorithms and Theory of Computation Handbook,  CRC Press,
Boca Raton, FL, 1999.7.
MJ. Atallah and D.Z. Chen, "An optimal parallel algorithm for the minimum circle-
cover problem," Information Processing Letters  32, 1989, 159-65.8.
M. Atallah and M. Goodrich, "Efficient parallel solutions to some geometric
problems," Journal of Parallel and Distributed Computing  3, 1986, 492-507.9.
S. Baase, "Introduction to parallel connectivity, list ranking, and Euler tour
techniques," in Synthesis of Parallel Algorithms,  J.H. Reif, ed., Morgan Kaufmann
Publishers, San Mateo, CA, 1993, 61-114.10.
11.

K.E. Batcher, "Sorting networks and their applications," Proc. AFIPS Spring Joint
Computer Conference  32, 1968, 307-14.11.
J.L. Bentley, D. Haken, and J.B. Saxe, "A general method for solving divide-and-
conquer recurrences," SIGACTNews,  12 (3), 1980, 36-44.12.
AA. Bertossi, "Parallel circle-cover algorithms," Information Processing Letters  27,
1988, 133-39.13.
G.E. Blelloch, Vector Models for Data-Parallel Computing,  The MIT Press,
Cambridge, MA, 1990.14.
G. Brassard and P. Bratley, Algorithmics: Theory and Practice,  Prentice Hall, 1988. 15.
L. Boxer, "On Hausdorff-like metrics for fuzzy sets," Pattern Recognition Letters  18,
1997, 115-18.16.
L. Boxer and R. Miller, "A parallel circle-cover minimization algorithm," Information
Processing Letters  32, 1989, 57-60.17.
L. Boxer and R. Miller, "Parallel algorithms for all maximal equally spaced collinear
sets and all maximal regular coplanar lattices," Pattern Recognition Letters  14,
1993, 17-22.18.
L. Boxer and R. Miller, "A parallel algorithm for approximate regularity," Information
Processing Letters  80 (2001), 311-16.19.
L. Boxer and R. Miller, "Coarse-grained gather and scatter operations with
applications," Journal of Parallel and Distributed Computing,  64 (2004), 1297-1320.20.
R.L. Burden and J.D. Faires, Numerical Analysis,  PWS-Kent Publishing Company,
Boston, 1993.21.
B.B. Chaudhuri and A. Rosenfeld, "On a metric distance between fuzzy sets,"
Pattern Recognition Letters  17, 1996, 1157-60.22.
RJ. Cole, "An optimally efficient selection algorithm," Information Processing Letters
26 (1987/88), 295-99.23.
T.H. Cormen, C.E. Leiserson, R.L. Rivest, and C. Stein, Introduction to Algorithms,
2nd ed., The MIT Press, Cambridge, MA, 2001.24.
F. Dehne, ed., special edition of Algorithmica  24, no. 3-4, 1999. 25.
F. Dehne, A. Fabri, and A. Rau-Chaplin, "Scalable parallel geometric algorithms for
multicomputers," Proceedings 9th ACM Symposium on Computational Geometry
(1993), 298-307.26.
27.
26.
S. Even, Graph Algorithms,  Computer Science Press, 1979. 27.
MJ. Flynn, "Very high-speed computing systems," Proceedings of the IEEE,  54 (12),
1966, 1901-09.28.
MJ. Flynn, "Some computer organizations and their effectiveness," IEEE
Transactions on Computers,  C-21, 1972, 948-60.29.
M.T. Goodrich and R. Tamassia, Data Structures and Algorithms in JAVA,  John
Wiley & Sons, Inc., New York, 1998.30.
R.L. Graham, "An efficient algorithm for determining the convex hull of a finite planar
set," Information Processing Letters  1, 1972, 132-33.31.
R.L. Graham, D.E. Knuth, and O. Patashnik, Concrete Mathematics,  Addison-
Wesley Publishing Company, Reading, MA, 1989.32.
C.A.R. Hoare, "Quicksort," Computer Journal,  5 (1), 1962, 10-15. 33.
J.E. Hopcroft and R.E. Tarjan, "Efficient algorithms for graph manipulation,"
Communications of the ACM  16, 1973, 372-78.34.
E. Horowitz, S. Sahni, and S. Rajasekaran, Computer Algorithms in C++,  Computer
Science Press, New York, 1997.35.
J. Já Já, An Introduction to Parallel Algorithms,  Addison-Wesley, Reading, MA,
1992.36.
RA. Jarvis, "On the identification of the convex hull of a finite set of points in the
plane," Information Processing Letters  2, 1973, 18-21.37.
A.B. Kahng and G. Robins, "Optimal algorithms for extracting spatial regularity in
images," Pattern Recognition Letters  12, 1991, 757-64.38.
R.M. Karp and V. Ramachandran, "A survey of parallel algorithms and shared
memory machines," in Handbook of Theoretical Computer Science: Algorithms and
Complexity,  AJ. vanLeeuwen, ed., Elsevier, New York, 1990, 869-941.39.
S. Khuller and B. Raghavachari, "Basic graph algorithms," in Algorithms and Theory
of Computation Handbook,  MJ. Atallah, ed., CRC Press, Boca Raton, FL, 1999.40.
D.E. Knuth, Fundamental Algorithms, Volume 1 of The Art of Computer
Programming,  Addison-Wesley, Reading, MA, 1968.41.
D.E. Knuth, Seminumerical Algorithms, Volume 2 of The Art of Computer
Programming,  Addison-Wesley, Reading, MA, 1969.42.
43.
42.
D.E. Knuth, Sorting and Searching, Volume 3 of The Art of Computer Programming,
Addison-Wesley, Reading, MA, 1973.43.
D.E. Knuth, "Big omicron and big omega and big theta," ACM SIGACTNews,  8 (2),
1976, 18-23.44.
C.C. Lee and D.T. Lee, "On a cover-circle minimization problem," Information
Processing Letters  18, 1984, 180-85.45.
F.T. Leighton, Introduction to Parallel Algorithms and Architectures: Arrays, Trees,
Hypercubes,  Morgan Kaufmann Publishers, San Mateo, CA, 1992.46.
S.B. Maurer and A. Ralston, Discrete Algorithmic Mathematics,  Addison-Wesley
Publishing Company, Reading, MA, 1991.47.
R. Miller and Q.F. Stout, "Efficient parallel convex hull algorithms," IEEE
Transactions on Computers,  37 (12), 1988.48.
R. Miller and Q.F. Stout, Parallel Algorithms for Regular Architectures: Meshes and
Pyramids,  The MIT Press, Cambridge, MA, 1996.49.
R. Miller and Q.F. Stout, "Algorithmic techniques for networks of processors," in
Algorithms and Theory of Computation Handbook,  M. Atallah, ed., CRC Press, Boca
Raton, FL, 1999.50.
S.B. Nadler, Jr., Hyperspaces of Sets,  Marcel Dekker, New York, 1978. 51.
W Narkiewicz, The Development of Prime Number Theory,  Springer-Verlag, Berlin,
2000.52.
M.H. Overmars and J. van Leeuwen, "Maintenance of configurations in the plane,"
Journal of Computer and Systems Sciences  23, 1981, 166-204.53.
M.L. Puri and DA. Ralescu, "Differentielle d'un fonction floue," Comptes Rendes
Acad. Sci. Paris, Serie  I 293, 1981, 237-39.54.
F.P. Preparata and M.I. Shamos, Computational Geometry,  Springer-Verlag, New
York, 1985.55.
MJ. Quinn, Parallel Computing Theory and Practice,  McGraw-Hill, Inc., New York,
1994.56.
S. Ranka and S. Sahni, Hypercube Algorithms for Image Processing and Pattern
Recognition,  Springer-Verlag, New York, 1990.57.
G. Robins, B.L. Robinson, and B.S. Sethi, "On detecting spatial regularity in noisy58.
59.
images," Information Processing Letters  69 (1999), 189-95.58.
K.H. Rosen, Elementary Number Theory and Its Applications,  Addison-Wesley
Publishing, Reading, MA, 1993.59.
A. Rosenfeld, "'Continuous’ functions on digital pictures," Pattern Recognition
Letters  4, 1986, 177-84.60.
D. Sarkar and I. Stojmenovic, "An optimal parallel circle-cover algorithm,"
Information Processing Letters  32, 1989, 3-6.61.
G.W. Stout, High Performance Computing,  Addison-Wesley Publishing Company,
New York, 1995.62.
V. Strassen, "Gaussian elimination is not optimal," Numerische Mathematik  14 (3),
1969, 354-56.63.
R.E. Tarjan, "Depth-first search and linear graph algorithms," SIAM Journal on
Computing,  1 (2), June 1972, 146-60.64.
R.E. Tarjan, Data Structures and Network Algorithms,  Society for Industrial and
Applied Mathematics, 1983.65.
F.L. Van Scoy, "The parallel recognition of classes of graphs," IEEE Transactions on
Computers  29, 1980, 563-70.66.
B. Wagar, "Hyperquicksort: A fast sorting algorithm for hypercubes," in Hypercube
Multiprocessors 1987,  M.T. Heath, ed., SIAM, 292-99.67.
S. Warshall, "A theorem on Boolean matrices," Journal of the ACM 9,  1962, 11-12. 68.
S. Yakowitz and Ferenc Szidarovszky, An Introduction to Numerical Computations,
Prentice Hall, Upper Saddle River, NJ, 1990.69.
TeamUnknown Release
0
Index
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
Index
Symbols
<r- (left arrow), using with values and variables, 9-10
= (equal sign), using with values and variables, 9-10
T (theta) notation
in asymptotic relationships, 11-12
example of, 6-7
and recursion, 42
T(1) time
and   operator, 166
in BinSort algorithm, 26-27
in bitonic merge, 82
and combinational circuits, 76
in CR PRAM example, 99
in CRCW PRAM example, 105
in ER PRAM example, 100, 102
executing fundamental operations in, 21-27
explanation of, 29
and Gaussian elimination, 158, 159
and HyperQuickSort, 228
and integral powers, 355
for linked lists on PRAMs, 193
memory access on RAMs in, 92, 94
in merged list example, 51
for mesh propagation algorithm, 282
on PRAMs, 94-95 , 96
and RAMs for overlapping line segments, 187
and running time for selection problem, 209
in sequential searches, 44-45

in Split algorithm, 51
for splitting lists with QuickSort, 215
T(log k) time, relationship to RAMs, 92
T(log ri) time
and array packing on PRAMs, 181
in bitonic merge, 82
and BitonicSort, 233
and convex hull on PRAMs, 259
in CRCW PRAM example, 106
and cross products for mesh-of-trees, 131
in ER PRAM example, 100, 102, 103, 104, 105
explanation of, 29
and Gaussian elimination, 158
and linked lists on PRAMs, 196
and mesh-of-trees, 128
and parallel prefix on PRAMs, 168
and PRAMs for overlapping line segments, 187-188
and pyramids, 125
relationship to matrix multiplication, 149-150
and trees, 125
T(log2 n) time
and BitonicSort, 85
and hypercubes, 133
role in BitonicSort, 87
T(logk n) time, explanation of, 29
T(n) space, using in InsertionSort routine, 23-24
T(n) time
and array packing on RAMs, 180
for convex hull in image processing, 287-288
in CRCW PRAM example, 105
and divide-and-conquer with Merge-Sort, 202
in ER PRAM example, 100, 103
explanation of, 29
and Gaussian elimination, 158
and Graham's scan on RAMs, 249-250
and linear arrays in interconnection networks, 110-111 , 112-113
for linked lists on RAMs, 193
and matrix multiplication, 153
in merged list example, 51
and meshes for overlapping line segments, 188
and parallel prefix, 166
and parallel prefix on PRAMs, 170
and point domination query, 185
and QuickSort, 212
and RAMs for overlapping line segments, 186-187
and RAMs for parallel prefix, 176-177
and RAMs for selection problem, 206
and running time for algorithm on RAMs, 183
and running time for selection problem, 209
in sequential searches, 45
in Split algorithm, 51
in tractor-tread algorithm, 115-116
and trees, 125
T(n2) time
explanation of, 29
running algorithms in, 28
T(n log n) time
and all-nearest neighbor problem, 262
and convex hull on RAMs, 255
in ER PRAM example, 102-103
and linked lists on PRAMs, 196
and parallel prefix on PRAMs, 170
T(n/log n) time
and PRAMs for parallel prefix, 177
and RAMs for overlapping line segments, 187
and trees, 125
T(n/q) time, relationship to DBM, 138
T(prq)  time, relationship to matrix multiplication, 148
T(q) time, relationship to matrix multiplication, 148
O, (omega) notation
in asymptotic relationships, 11-12
example of, 6-7
O, (n) time
and parallel prefix, 166
using with linear arrays, 116
O, (nlog ri)  worst case running times, significance of, 25
 , expressing running times as functions of, 359
p(n), asymptotic behavior of, 365-366
t, value in error terms, 359
  (little omega) notation
in asymptotic relationships, 11-12
example of, 7-8
  operator
defining for segment broadcasting, 182-183
and parallel prefix, 166
role in ER PRAM example, 100
as unit-time operator, 166
TeamUnknown Release
0
List of Figures
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
List of Figures
Chapter 1:  Asymptotic Analysis
Figure 1.1:  An illustration of the growth rate of two functions, f(n) and g(n). Notice that
for large values of n, an algorithm with an asymptotic running time of f(n) is typically
more desirable than an algorithm with an asymptotic running time of g(n). In this
illustration, large is defined as n = n0
Figure 1.2:  An illustration of T notation. f(n) = T(g(n))  because functions f(n) and g(n)
grow at the same rate for all n = n0
Figure 1.3:  An illustration of O notation. f(n) = 0(g(n))  because function f(n) is
bounded from above by g(n) for all n = n0
Figure 1.4:  An illustration of O notation. f(n) = O(g(n))  because function f(n) is
bounded from above by g(n) for all n = n0
Figure 1.5:  An illustration of o notation: f(n) = o(g(n))
Figure 1.6:  An illustration of   notation: f(n) = O(g(n))
Figure 1.7:  Graph of f(t) = 5  + sin t
Figure 1.8:  An illustration of bounding the summation 
 h(i) by the integral of
the nondecreasing function h(t). On the left, we demonstrate how to use the integral
 to derive an upper bound on the summation by aligning the rectangles to
the right. Notice that 
. On the right, we show how to use the
integral 
 to derive a lower bound on the summation by aligning the

rectangles to the left. Notice that 
. Therefore, we have
Figure 1.9:  An illustration of bounding the summation 
 for a nonincreasing
function f. For f nonincreasing, we can derive the relationship  b+1a f(t)dt =
 f(i)= ba-1 f(t)dt
Figure 1.10:  An increasing function in the [a,b] range. We have
Figure 1.11:  An example of InsertionSort. It is initially assumed that the first item (4) is
in the correct position. Then the second item (3) is placed into position with respect to
all of the items in front of it, result-ing in (3,4) being properly ordered. The algorithm
continues until the last item (2) is placed in its proper position with respect to the
items (1,3,4,5) that are in front of it
Figure 1.12:  BinSort applied to an array of 10 items chosen from [1 5]. In (a), the
initial array of data is given. In (b), the set of empty bins are created. In (c), the bins
are shown after a complete pass through the array. In (d), the array is recreated by
concatenating the bins
Figure 1.13:  An example of SelectionSort. A complete pass is made through the initial
set of data to determine the item that belongs in the front of the list (1). A swap is
performed between this minimum element and the element currently in the front of the
list. Next, a pass is made through the remaining four items to determine the minimum
(2) of these elements. This minimum element is swapped with the current second
item (3). The procedure continues until n —1  items have been properly ordered
because this forces all n items to be properly ordered
Chapter 2:  Induction and Recursion
Figure 2.1:  An example of sequential search. Given the array of data, a search for the
value 4 requires five key comparisons. A search for the value 9 requires three key
comparisons. A search for the value 1 requires seven key comparisons to determine
that the requested value is not present
Figure 2.2:  An example of binary search. Given the array of data, a search for the
value 4 requires two key comparisons (6,4). A search for the value 9 requires three
key comparisons (6,8,9). A search for the value 1 requires three key comparisons
(6,4,3) to determine that the value is not present
Figure 2.3:  Recursively sorting a set of data. Take the initial list and divide it into two
lists, each roughly half the size of the original. Recursively sort each of the sublists,
and then merge these sorted sublists to create the final sorted list
Figure 2.4:  An illustration of a linked list in a language that supports dynamic
allocation. Notice that the head of the list is simply a pointer and not a complete
record, and that the last item in the list has its next pointer set to NULL
Figure 2.5:  An example of merging two ordered lists, head1  and head2 , to create an
ordered list headMerge . Snapshots are presented at various stages of the algorithm
Chapter 3:  The Master Method
Figure 3.1:  A recursion tree representing the recurrence equation T(n) = aT(n/b) +
f(n). The number of problems to be solved at each (horizontal) level of recursion is
listed, along with the size of each problem at that level. Time is used to represent the
time per problem, not counting recursion, at each level
Figure 3.2:  A recursion tree for MergeSort, as represented by T(n) = 2T(n / 2) + T(n).
Notice that level i of the recursion tree ( i   {1,2,…,log2n}) requires a total of Tx T(n/T) =
T(n)time
Chapter 4:  Combinational Circuits
Figure 4.1:  An illustration of a comparison element, the fundamental element of a
sorting network. The comparison element receives inputs A and B and produces
min(A,B) and max( A,B)
Figure 4.2:  An illustration of a bitonic sequence <a> in which and aj is a maximal
element of <a>, where n = j = 2n
Figure 4.3:  An illustration of a bitonic sequence < a> in which an > a 2n, aj is a maximal
element of < a>, where n = j = 2n, and there exists a pivot element k such that ak-n =
ak and ak-n+1 = ak+1
Figure 4.4:  Input and output for a bitonic merge unit
Figure 4.5:  The iterative rule for constructing a bitonic merge unit. The input sequence
<a> consists of 2n items and is bitonic. The 2n item output sequence <c> is sorted
Figure 4.6:  A 4-item bitonic merge unit. Note < a1, a2, a3, a4 > is the bitonic input
sequence and < c1, c2, c3, c4 > is the sorted output sequence. The number of levels
L(2n) can be determined as L(2n) = L(2 X 2) = 1 + L(n) = 1+L(2) = 2 = log 2 (2n)
Figure 4.7:  An 8-item bitonic merge unit. Note that the input sequence < a1…, a8 > is
bitonic and the output sequence < c1,…, c8 > is sorted. The number of levels L(2n)
can be determined as L(2n) = L(2 X 4) = 1 + L(4) = 1 + 2 = 3 = log 28 = log 2 (2n)
Figure 4.8:  An example of BitonicSort on eight data items. Note that the input
sequence < a> is initially unordered, and the output sequence < c> is sorted into
nondecreasing order. The symbol I means that the comparison is done so that the top
output item is less than or equal to the bottom output item (increasing order if the
items are unique). The symbol D represents that the comparison is done with respect
to nonincreasing order (decreasing order if unique items)
Figure 4.9:  A different view of BitonicSort for eight elements. The horizontal lines
represent wires and the vertical lines represent comparison-exchange elements. That
is, the vertical lines represent points in time at which two items are compared and
ordered according to the label I (increasing order) or D (decreasing order). Notice that
the log2 8 = 3  bitonic merge stages are separated by dotted vertical lines
Chapter 5:  Models of Computation
Figure 5.1:  The RAM (random access machine) is a traditional sequential model of
computation. It consists of a single processor and memory. The processor is able to
access any location of memory in T(1) time through the memory access unit
Figure 5.2:  Characteristics of a PRAM (parallel random access machine). The PRAM
consists of a set of processing elements connected to a global memory through a
memory access unit. All memory accesses are assumed to take T(1) time
Figure 5.3:  A bottom-up treelike computation to compute the minimum of eight values.
The global minimum can be computed in three parallel steps. Each step reduces the
total number of candidates by half
Figure 5.4:  Another view of the minimum operation presented in Figure 5.3. This
shows the action of a set of four processors. The data is presented as residing in a
horizontal array. The processors that operate on data are shown for each of the three
time steps
Figure 5.5:  Improving the performance of a PRAM algorithm by requiringeach of n/log
n processors to be responsible for log n data items
Figure 5.6:  An algorithm for computingthe minimum of n items with n/log 2 n
processors on a PRAM. Initially, everyprocessor sequentially determines the minimum
of the log 2 n items that it is responsible for. Once these n/log 2 n results are known, the
minimum of these values can be determined in T(log(n/log n)) = T(log n - loglog n) =
T(log n) time on a PRAM with n/log 2 n processors
Figure 5.7:  In a traditional shared-memory machine, presented on the left, all
processors operate through an interconnection network and have equal unit-time
access to all memory locations. In a traditional distributed-memory machine,
presented on the right, everyprocessing element (processor and memory pair)
communicates with every other processing element through an interconnection
network
Figure 5.8:  A linear array of size n
Figure 5.9:  Computing the minimum of n items initially distributed one per processor
on a linear array of size n. Notice that the data is passed in lockstep fashion to the left
during every time step. The leftmost processor ( P1) keeps the running minimum
Figure 5.10:  Partitioning the data in preparation for computing the minimum of n items
initially distributed on a linear array of size n1/2 in such a fashion that each of the n1/2
processors stores n1/2 items
Figure 5.11:  Computing the minimum of n items initially distributed on a linear array of
size n1/2 in such a fashion that each of the n1/2 processors stores n1/2 items. In the
first step, every processor sequentially computes the minimum of the n1/2 items for
which it is responsible. In the second step, the minimum of these n1/2 minima is
computed on the linear array of size n1/2 by the typical lockstep algorithm
Figure 5.12:  Computing the minimum on an input-based linear array of size 6. During
step 1, processor P1 takes as input x6 = 5 and initializes running_min to 5. During
step 2, processor P1 sends x1 to processor P2, inputs xn-1 = 1, and assigns
running_min= min(running_min, x n-1), which is the minimum of 5 and 1, respectively.
The algorithm continues in this fashion as shown, sending data to the right in lockstep
fashion while the first processor keeps track of the minimum value of the input data
Figure 5.13:  A tractor-tread algorithm. Data in the linear array moves to the right until
it hits the right wall, where it reverses itself and starts to march to the left. Once the
data hits the left wall, it again reverses itself. A revolution of the tractor-tread algorithm
is complete once the initial data resides in its original set of processors. Given a linear
array of size n, this algorithm allows every processor to view all n data items in T(n)
time
Figure 5.14:  Sorting data on an input-based linear array. Every processor simply
retains the item that representsthe minimum value it has seen to date. All other data
continues to pass in lockstep fashion to the right. Notice that this is a minor
generalization of the minimum algorithm illustrated in Figure 5.12
Figure 5.15:  A ring of size 8. All processors in a ring are connected to two neighbors
Figure 5.16:  A mesh of size 16. Each generic processor in a traditional mesh is
connected to its four nearest neighbors. Notice that there are no wraparound
connections and that the processors located along the edges of the mesh have fewer
than four neighbors
Figure 5.17:  Broadcasting a piece of data on a mesh. First, a row rotation is
performed to broadcast the criticaldata item to all processors in its row. Next, column
rotations are perfored simultaneously in every column to broadcast the critical data
item to all remaining processors
Figure 5.18:  A tree of base size 8. Notice that base processors have only a single
neighbor (parent processor), the root has only two neighbors (children processors),
and the remaining processors have three neighbors (one parent and two children
processors)
Figure 5.19:  A pyramid of base size n can be viewed as a set of processors
connected as a 4-ary tree, where at each level in the pyramid, the processors at that
level are connected as a two-dimensional mesh. Alternatively, it can be thought of as
a tapering array of meshes. The root of a pyramid has links only to its four children.
Each base processor has links to its four base-level mesh neighbors and an
additional link to a parent. In general, a generic processor somewhere in the middle of
a pyramid is connected to one parent and four children and has four mesh-connected
neighbors
Figure 5.20:  A mesh-of-trees of base size n consists of a mesh of size n at the base,
with a tree above each of the n1/2 columns and a tree above each of the n1/2 rows.
Notice that the trees are completely disjoint except at the base. The mesh-of-trees of
base size n has n processors in the base mesh, 2n1/2-1 processors in each of the
n1/2 row trees, and 2n1/2-1 processors in each of the n1/2 column trees
Figure 5.21:  Creating a cross product of items <d1, d2, d3, d4>. Notice that processor
Pi,j will store a copy of di and dj. That is, every processor in row i will store a copy of di
and every processor in column j will store a copy of dj
Figure 5.22:  Sorting a reduced set of data on a mesh-of-trees (only the base mesh is
shown). a) The initial distribution of data consists of a single row of elements. b) The
data after using the column trees to broadcast the data element in every column. c)
The result after using the row trees to broadcast the diagonal elements along every
row. At this point, a cross product of the initial data exists in the base mesh of the
mesh-of-trees. d) The result of performing row rankings of the diagonal element in
each row. This step is accomplished by performing a comparison in the base mesh
followed by a semigroup operation of every row tree. e) The result after performing
the final routing step of the diagonal elements to their proper positions according to
the rankings
Figure 5.23:  A hypercube of size16 with the processors indexed by the integers
{0,1,…,15}. Pairs of processors are connected if and only if their unique log 216 = 4 bit
strings differ in exactly one position
Figure 5.24:  Constructing a hypercube of size n from two subcubes each of size n/2.
First, attach elements of subcube A to elements of subcube B with the same index.
Then prepend a 0 to indices of subcube A, and prepend a 1 to all indices of subcube
B. Subcube A is shaded in each diagram for ease of presentation
Figure 5.25:  An example of computing a semigroup operation on a hypercube of size
n. For this example, we use minimum as the semigroup operation. In the first step, we
send entries from all processors with a 1 in the most significant bit to their neighbors
that have a 0 in the most significant bit. That is, elements from the right subcube of
size 8 are sent to their neighboring nodes in the left subcube of size 8. The receiving
processors compare the two values and keep the minimum. The algorithm continues
within the left subcube of size 8. After log 216 = 4 transmission-and-compare
operations, the minimum value (1) is known in processor 0000
Figure 5.26:  Data movement in a semigroup operation on a hypercube. The links of
the hypercube of size 16 are labeled based on the step in which they are used to
move data in the semigroup operation shown in Figure 5.25
Figure 5.27:  A star-shaped computer of size 6
Figure 5.28:  An architecture in which nproces-sors are partitioned into two disjoint
subsets of n/2 processors each
Chapter 6:  Matrix Operations
Figure 6.1:  An example of matrix multiplication. For example, c 2,3 is the product of the
second row of A (5, 6, 7, 8) and the third column of B (2, 0, 2, 0), which yields 5X2 +
6X0 + 7X2 + 8X0 = 24
Figure 6.2:  Matrix multiplication on a 2n × 2n  mesh. Matrix An × n initially resides in the
lower-left quadrant and matrix Bn × n initially resides in the upper-right quadrant of the
mesh. The matrix product Cn × n = A n × n × B n × n is stored in the lower-right quadrant
of the mesh
Figure 6.3:  Data flow for matrix multiplication on a 2n × 2n  mesh. The initial
distribution of data is shown in (a). (b) shows step 1, in which the first column of B
starts to move down and the first row of A starts to move right. (c), (d), and (e) show
steps 2, 3, and 4, respectively, in which both columns of B move down and both rows
of A move right
Figure 6.4:  Row and column rotations preprocessing steps for matrix multiplication on
an n × n  matrix. (a) shows the initial distribution of data. (b) shows the result of a row
rotation of A. (c) shows the result of a column rotation of B
Figure 6.5:  Interchange of row 1 and row 3
Figure 6.6:  Replace row 1 by 0.2 × row 1
Figure 6.7:  Replace row 2 by row 2 + 5 × row 1
Chapter 7:  Parallel Prefix
Figure 7.1:  An example of parallel prefix on a set X of 6 items. The operation   is
addition. The resulting prefixsums are given in array P
Figure 7.2:  An example of parallel prefix on a set X of 6 items. The operation   is
addition. The resulting prefixS are given in array P
Figure 7.3:  A recursive doubling algorithm to compute the parallel prefix of 11 values
on a PRAM in which each processor is responsible for one data item. The algorithm
requires [log2 11 = 4 parallel steps
Figure 7.4:  An example of computing parallel prefix by continually combining results
of disjoint pairs of items. The operation   used in this example is addition. Notice that
the algorithm requires \log 111 = 4 steps. At the conclusion ofstep 1, we have
computed \x 1-x2],[x3-x42,[x5-x6],[x7-x8],[x9-x10],x11.At the end of step 2, we have
computed \x 1—x4],[x5—x8],[x9—x11. At the endof step 3,we have
computed\x 1—x8],[x9—x11]. At the end of step 4, we have computed x 1-x11
Figure 7.5:  An example of computing the parallel prefix on a PRAM with n/log n)
processors. In this example, we are given n = 16 data items, the operation is addition,
there are log 2 n = 4 processors, and each processor is responsible for n/log 2 n = 16/4
= 4 data items
Figure 7.6:  The row-major index scheme imposed on a mesh of size 16
Figure 7.7:  An example of computing the parallel prefix on a hypercube of size 8 with
the operation of addition. The indexing of the hypercube is given in binary
representation in (a).In (b),the initial set of data items is presented. In (c),(d), and (e),
we show the results after the first, second, and third steps of the algorithm,
respectively. Processor prefix values are shown large in (c),(d), and (e);subcube prefix
values are small
Figure 7.8:  An example of the maximum sum subsequence peoblem
Figure 7.9:  An example of segmented broadcast. The top table shows the initial state
(that is, the information before the segmented broadcast). Thus, by examining the
Leader field in each processor, we know the interval leaders are processors 0, 3, 5,
and 6. In the bottom table, we show the information after the segmented broadcast.
Information from each leader has been propagated (broadcast) to all processors to
the right (higher index values) up to, but not including, the next leader
Figure 7.10:  An example of the point domination problem. In this example, exactly
three points have no other point both above and to the right. The remainder of the
points are dominated by at least one of these three points
Figure 7.11:  An example of problems involving overlapping line segments. The line
segments are all assumed to lie on the x-axis, though they are drawn superimposed
for viewing purposes
Figure 7.12:  Transforming the coverage query problem to the parentheses matching
problem. For this example, notice that there is a break in coverage between x 6 and x 7,
as indicated by the 0 in the prefix value of x 6
Chapter 8:  Pointer Jumping
Figure 8.1:  An example of list ranking. Given a linked list, determine for each element
the number of elements in the list that follow it. The algorithm follows a recursive
doubling procedure. Initially, every processor finds the next element in the list. (a)
shows the initial list, in which every element knows the following element one step
away. In steps (b), (c), and (d), every element locates the element 2, 4, and 8 places
away from it, respectively. Step (e) shows the final values at the end of the recursive
doubling procedure. Given a list with 10 elements, the number of iterations required is
  log 210   = 4
Figure 8.2:  An example of parallel prefix on a PRAM with linked list input. Given a list
of size 6, the recursive doubling procedure requires three iterations (   log26   = 3)
Chapter 9:  Divide-and-Conquer
Figure 9.1:  A recursion tree giving insight into the time required to perform a
traditional MergeSort algorithm on a RAM
Figure 9.2:  A snapshot of MergeSort on a linear array of size 8. The initial data is
given in (a), and the result of independently sorting both the Left and Right subarrays
is shown in (b)
Figure 9.3:  A snapshot of MergeSort on a linear array of size 8, using the data from
Figure 9.2. The snapshot shows the data and local ranks that are determined after the
independent sorts on both the Left and Right subarrays
Figure 9.4:  A snapshot of MergeSort on a linear array of size 8 after the independent
sorts on both the left and right subarrays. The data, local ranks, and ranks with
respect to the opposite subarray are all given. The data is from Figure 9.2
Figure 9.5:  Using the Partition routine to solve the Selection Problem. An initial input
array of size 25 is given in (a). In (b), the array is shown after independently sorting
disjoint subarrays of size 5. (Note: contrary to the algorithm presented in the chapter,
for ease of presentation we ignore the fact the algorithm should proceed differently
when its recursion reaches a subarray of size 50 or smaller.)
Figure 9.6:  Creating three buckets based on AM=13, the median of the five medians
(17, 9, 8, 19, 13) given in Figure 9.5b. The data given in Figure 9.5b is traversed from
the beginning to the end of the array, with every element less than 13 being placed in
smallList, every item equal to 13 being placed in equalList, and every item greater
than 13 being placed in bigList. Notice that the items should be placed in these lists in
a manner that allows for T(1) time insertion. So, given the order shown in this figure,
one might assume that tail pointers were maintained during the insertion process
Figure 9.7:  An example of QuickSort on a linked list. (a) shows the initial unsorted list.
(b) shows three lists after partitioning based on the value 5. (c) shows the same lists
after smallList and bigList have been recursively sorted. (d) shows the completion of
the sorting process, after concatenation of the sorted sublists
Figure 9.8:  An example of QuickSort on an array of size 9. (a) shows the initial
unordered array. (b) shows the array after partitioning with respect to the value 5.
Note every member of (3,4,1,2) is less than 5, and every member of (6,7,8,9,5) is
greater than or equal to 5. (c) shows the results of sorting each of the subarrays
recursively. Notice that the entire array is now sorted
Figure 9.9:  An example of the Partition routine of QuickSort on an array of 8 items
Figure 9.10:  The shuffled-row major index scheme as applied to a mesh of size 16. It
is important to note that on a mesh of size n, this indexing continues recursively within
each quadrant
Figure 9.11:  An example of BitonicSort on a mesh of size 16. The elements are
sorted into shuffled-row major order, as given in Figure 9.10. The initial data is given
in the top-left matrix. After applying a comparison-exchange operation between
indicated elements (for example, 10-9, 14-2, 4-15, …), the matrix has been ordered
into disjoint 1 × 2 segments, as indicated in the next matrix. The interpretation of the
figure continues in this manner. Note up to the final stage, we have half of the sorted
sections in ascending order, and the other half are in descending order
Figure 9.12:  An example of sorting data on a mesh into row-major order by two
applications of sorting into shuffled-row major order. The initial unordered set of data
is given in (a). After applying a shuffled-row major sort, the data appears as in (b).
Note that in the lower-right corner of each item is the index for where that item should
be placed with respect to shuffled-row major order so that the data will be in row-
major order. The items are then sorted into shuffled-row major order with respect to
these indices, with the results in row-major order as shown in (c)
Figure 9.13:  An example of a concurrent read on a linear array of size 4. (a) shows
the initial data, where each processor maintains one master record (M in the fourth
field) and generates one request record (R in the fourth field). (b) shows the records
sorted by the first field, with ties broken in favor of master records. (c) shows the
result of a segmented broadcast that propagates the third field to appropriate request
records. (d) shows the data sorted by the return address (second field)
Chapter 10:  Computational Geometry
Figure 10.1:  Examples of convex and non-convex regions. The regions in (a) are
convex. The regions in (b) are not convex, because the line segments uv and xy are
not contained in their respective regions
Figure 10.2:  The convex hull. The set S of n points in the plane is represented by
circles, some of which are black and some of which are gray. The extreme points of S
are represented by the gray points. The set of such extreme points is denoted by
hull(S). Each pair of adjacent extreme points represents an edge of the convex hull
Figure 10.3:  Graham s scan is a technique for determining the convex hull of a set of
points. The lowest point is chosen as point 0, and the remaining points are sorted in
counterclockwise order with respect to the angles they make to a horizontal line
through point 0. Grahams scan examines the points in the order listed
Figure 10.4:  A path from S(jk-1) to S(jk) to S(i) that makes a left turn at S(jk)
Figure 10.5:  A path from S(jk-1) to S(jk) to S(i) that makes a right turn at S(jk)
Figure 10.6:  A path from S(jk-1) to S(jk) to S(i) that is straight. That is, all three points
are collinear
Figure 10.7:  A set of n planar points evenly divided into two sets A and B by x-
coordinate. All points in A lie to the left of every point in B
Figure 10.8:  An illustration of the situation after hull(A)  and hull(B)  have been
determined from input shown in Figure 10.7
Figure 10.9:  The stitch step. To construct hull(S)  from hull(A)  and hull(B) , the upper
common tangent line and lower common tangent line between hull(A)  and hull(B)  are
determined
Figure 10.10:  An illustration of the common tangent lines between linearly separable
convex hulls. The upper common tangent line between hull(A)  and hull(B)  does not
necessarily include the topmost extreme points in either set. A similar remark can be
made about the lower common tangent line
Figure 10.11:  Constructing the upper common tangent lines. The upper common
tangent line includes the extreme point p e hull(A)  with the following properties. Let
the next extreme point in counterclockwise order be called x and the previous extreme
point in counterclockwise order be called y. Then every extreme point of hull(B)  lies on
or below 
 whereas at least one extreme point of hull(B)  lies on or above 
Figure 10.12:  Dividing the n planar points in S so that each of the four linearly
separable sets of points is stored in a different quadrant of the mesh. Notice that the
vertical slabs of points in the plane need not cover the same area of space. They
simply must contain the same number of points
Figure 10.13:  An illustration of partitioning the set S of n planar points into n1/2 linearly
separable sets, each with n1/2 points. The sets are denoted as R1, R2,…, Rn1/2
Figure 10.14:  Suppose that v i is to the left of w i and that the angle above the
intersection of their tangents exceeds 180o. Then all of the extreme points of Ri
between (and including) vi and wi are extreme points of S
Figure 10.15:  Suppose that vi = w i and that the angle above the intersection of their
tangents exceeds 180°. Then vi is an extreme point of S
Figure 10.16:  Suppose that vi = w i and that the angle above the intersection of their
tangents does not exceed 180o. In this case, no extreme point on the upper envelope
of Ri is an extreme point of S
Figure 10.17:  Suppose that wi is to the left of vi. Then no extreme point on the upper
envelope of Ri is an extreme point of S
Figure 10.18:  A smallest enclosing box of S. A (not necessarily unique) minimum-area
enclosing rectangle of S includes three edges, each of which contains an extreme
point of hull(S) , and one edge that is collinear with an edge of hull(S)
Figure 10.19:  An illustration of angles of support. The angle of incidence of hull edge
 is p/2, of 
 is 3p/4, of 
 is p and so forth. An angle of support of
extreme point A is in [ p/2, 3p/4]. An angle of support of extreme point B is in [3 p/4,
p], and so forth
Figure 10.20:  The nearest neighbor of p is in neither the same horizontal nor vertical
slab as p is
Figure 10.21:  Illustration of a plane sweep operation to solve the intersection query
problem. The line segments are labeled by left endpoint. As a sweep of all the
endpoints is performed from left to right, when a left endpoint is encountered, the line
segment is inserted into the list at the appropriate ordered (top to bottom) position and
is tested for intersection with its neighbors in the list. The currently active ordered list
of line segments is shown beneath each endpoint. When a right endpoint is
encountered, an evaluation of an intersection is made before removing that point from
the ordering. Here, when the left endpoint of e is encountered, the d-e intersection is
detected
Figure 10.22:  A minimal-cardinality cover of [ a, b] consists of arcs 3, 4, 6, and 7
Figure 10.23:  The all maximal equally spaced collinear points problem. An illustration
of three equally spaced collinear line segments
Chapter 11:  Image Processing
Figure 11.1:  (a) A digitized 4 × 4 picture. The interpretation is that it is a black image
on a white background. (b) The same 4 × 4 picture with its maximally connected
components labeled under 4-adjacency definition of connectedness. Each component
is labeled with the pixel of minimum label in its components, where the pixel labels are
taken to be the row-major labeling with values 1, …,16
Figure 11.2:  Each connected component is confined to a 3 × 3  region. In such
situations, the mesh propagation algorithm will run in T(1) time
Figure 11.3:  Two problematic figures. A spiral is shown on the left, and a snake is
shown on the right
Figure 11.4:  An 8 × 8 image after labeling each of its 4X4 quadrants. Notice that the
component labels come from the shuffled row-major indexing scheme, starting with
processor 1 (not 0). The global components that are completely contained in a
quadrant (components 4 and 20) do not need to be considered further. The remaining
components are required for a global relabeling procedure
Figure 11.5:  An illustration of overlapping convex hulls of labeled (not necessarily
connected) sets of pixels
Figure 11.6:  The all-nearest neighbor between labeled sets problem. Suppose p, q,
and r are labeled pixels. If r is a closest distinctly labeled pixel in row two to p, then
either p or r is a closest distinctly labeled pixel to q among those in row 2
Figure 11.7:  An illustration of the possible border elements in a k × k  submesh
Figure 11.8:  A mapping that shows how to rearrange the distance matrices from
recursive solutions in an effort to solve the all-points minimal internal distance
problem
Figure 11.9:  An example of the Hausdorff metric. The distances x and y respectively
mark a furthest member of A from B and a furthest member of B from A. H(A,B)  =
max{x,y}
Chapter 12:  Graph Algorithms
Figure 12.1:  In 1736, Leonhard Euler graphed the town of K nigsberg, where the
Pregel River flows around the island of Kneiphof
Figure 12.2:  The seven bridges in the area of Kneiphof and the Pregel River that
Euler considered in terms of navigating the town of K nigsberg
Figure 12.3:  A graph with four vertices and seven edges representing K nigsberg.
Euler considered this graph in terms of whether or not it was possible to start on one
of the four land masses (vertices), cross every bridge exactly once, and return to the
original land area. The generalization of this problem is now known as the Euler tour
problem
Figure 12.4:  Four sample graphs. (a) shows a complete undirected graph of five
vertices. (b) is a directed graph with pairs of vertices ( u, v) such that the graph has no
directed path from u to v. (c) is an undirected tree with seven vertices. (d) is an
undirected mesh of nine vertices
Figure 12.5:  Notice in (a) (an undirected weighted graph) that there are eight pairs of
neighboring (adjacent) vertices. Also, notice in (a) that the entire graph is connected
because there is a path between every pair of vertices. In graph (b) (a directed,
weighted graph), however, paths are not formed between every pair of vertices. In
fact, notice that vertex e is isolated in that e does not serve as the source of any
nontrivial path. Notice in (a) that a minimum-weight path from a to e is <a, c, d, e <,
which has a total weight of 3, whereas in (b) minimum-weight paths from a to e are
<a, d, e < and <a, b, f, d, e <
Figure 12.6:  An undirected graph with three connected components
Figure 12.7:  A directed graph with three weakly connected components and seven
strongly connected components
Figure 12.8:  A directed graph. The in-degree of <a, b, c, d, e < is <2,0,1,2,2 <,
respectively, and the out-degree of <a, b, c, d, e < is <1,1,2,1,2 <, respectively
Figure 12.9:  A directed graph and its adjacency list representation
Figure 12.10:  An adjacency matrix representation of the graph presented in Figure
12.9
Figure 12.11:  An example of a breadth-first search traversal. Depending on the order
in which the vertices given in graph G of (a) are stored in the associated data
structure, a BFS initiated at vertex 10 could yield a variety of breadth-first search
trees. For example, the tree in (b) is associated with the traversal (10, 3, 12, 11, 9, 5,
17, 16, 2, 1, 15, 13, 14, 7, 4, 6, 18, 8, 19), though other traversals of G might also
yield this tree. Similarly, the tree in (c) is associated with the traversal (10, 9, 12, 11,
3, 17, 7, 13, 14, 15, 16, 2, 1, 5, 18, 8, 6, 4, 19) of G
Figure 12.12:  An undirected connected graph with distances from the root vertex r
recorded next to the vertices. One possible traversal of the vertices in this graph by a
breadth-first search is <r, c, b, a, e, f, d, i, j, g, h, k, l, m, n, o <
Figure 12.13:  An undirected graph that is not connected. The two connected
components can be labeled in time linear in the number of vertices plus the number of
edges by a simple extrapolation of the breadth-first search algorithm
Figure 12.14:  An example of a depth-first search traversal. Notice that the graph given
in (a) is identical to the graph G utilized in Figure 12.11 a. In (b) we see the tree
associated with the traversal (10, 3, 1, 2, 15, 12, 13, 14,16, 5, 4, 6, 19, 18, 7, 8, 9, 11,
17) of G, though other traversals of G might produce the same tree. Similarly, in (c)
we see the tree associated with the traversal (10, 12, 16, 3, 17, 9, 11, 7, 18, 19, 6, 5,
4, 8, 1, 2, 15, 14, 13) of G
Figure 12.15:  A depth-first search tree T = (V, E ') of a graph G = (V, E) . An edge (u,v)
eE iss a member of E' if and only if one of its vertices is the parent of the other vertex.
Edge (u,x) eE is not in E', corresponding to the fact that one of its vertices is an
ancestor but not the parent of the other
Figure 12.16:  A breadth-first search tree T = (V, E ') of G = (V, E) . If an edge (u,v)eE is
not in E', then u is not a descendant of v in T and v is not a descendant of u in T
Figure 12.17:  An undirected tree T = (V, E)  is presented in (a), along with an
adjacency representation of the graph in (b). In (c), the next edge function is given for
the Euler tour of the graph; this is a function of the adjacency representation.
Because an adjacency representation is not unique, if the representation given in (b)
were changed, the next function given in (c) would be different. By starting at any
directed edge in the graph T' = (V, E ') (every undirected edge (u,v) eE is replaced by
two directed edges, (u,v), (v,u) eE'), and following the next function, an Euler tour can
be achieved
Figure 12.18:  An expression tree for the expression [8 + (6—4)] × [4/(3-1)]
Figure 12.19:  Input to a tree contraction algorithm is a rooted binary tree in which
each vertex has either two children or none at all. Further, it is assumed that the
leaves have been labeled consecutively from left to right, with the exception of the
leftmost and rightmost leaves
Figure 12.20:  An example of a collapse operation applied to vertex number 2
Figure 12.21:  An example of tree contraction. Indices of nodes in the array Active are
shown below the nodes (these are updated following compression of Active as the
steps are executed). The initial tree is given in (a). The tree is shown in (b) after
performing contraction on vertex 7 during the first iteration of the algorithm. The tree
is shown in (c) after performing contraction on vertices 1, 3, and 5 to finish the first
iteration of the algorithm. The tree is shown in (d) after performing tree contraction on
vertices 2 and 6 to initiate the second iteration of the algorithm. The tree is shown in
(e) after performing tree contraction on vertex 4 to conclude the algorithm (after the
third iteration)
Figure 12.22:  Data movement of van Scoys implementation of Warshalls transitive
closure algorithm on a mesh. Ak(k,k) is computed at time t = 3k—  2, in processor Pk, k.
During the next time step, this value is transmitted to processors Pk,k+1, P k,k-1, Pk+1,
k, and Pk-1, k, as shown in (a). At time t + 1, the values Ak(k — 1, k), Ak(k, k + 1), Ak(k
+ 1, k), and Ak(k, k—1) are computed in processors Pk- 1, k, Pk,k+ 1, Pk+ 1, k, and Pk,k-
1, respectively, as shown in (b). The arrows displaying data movement in (b) show the
direction that this information begins to move during time step t + 2
Figure 12.23:  A general description of a parallel component-labeling algorithm. The
initial undirected graph G = (V, E) is given in (a). In (b), the initial forest is presented.
The initial forest consists of a distinct tree representing every vertex in V. The graph
presented in (c) shows the result of every vertex in V attaching to its minimum-labeled
neighbor. The graph that results from the compression of these four disjoint
subgraphs is given in (d). Notice that four supervertices are generated. The directed
graph in (e) shows the result from each of these four supervertices choosing its
minimum-labeled neighbor. Finally, (f) shows the result from the final stage of the
algorithm in which all vertices in the connected graph have been compressed into a
single supervertex. Note that when we present supervertices, the first vertex
(minimum label) in the list will serve as the label for the supervertex
Figure 12.24:  A demonstration of the hooking operation. In (a), vi and parent(v i) are in
different supervertices. In (b), the supervertex to which v i belongs hooks to the
supervertex containing parent(v i) because root(parent(v i)) is a minimum label over all
the supervertices to which members of the supervertex labeled root(v i) are connected.
In (c), these supervertices are merged
Figure 12.25:  Computing the star function in parallel. Arrows represent root pointers.
Step 3 initializes star(v i) true for all vertices. Steps 5 through 7 change star(a),
star(b), star(c), and star(d) to false. However, we require step 9 to change star(e) to
false
Figure 12.26:  A representation of a data structure that allows for an efficient
implementation of Kruskals algorithm. H is a pointer to the head of the list. N is a
pointer to the next element in the list
Figure 12.27:  The r × r matrix W, as distributed one entry per processor in a natural
fashion on an r × r submesh. Notice that each entry in processor Pi,j, 1 = i, j = r,
contains the record ( Wi,j, ei,j), which represents the minimum weight of any edge
between virtual vertices (that is, supervertices) vi and vj, as well as information about
one such edge ei, j to which the weight corresponds. In this situation, the edge ei, j is
actually a record containing information identifying its original vertices and its current
virtual vertices
Figure 12.28:  A sample 6X6 weight matrix in which, for simplicity s sake, only the
weights of the records are given. Notice that the processors in the last column also
contain a minimum-weight edge and its identity after the row rotation
Figure 12.29:  The 6X6 adjacency matrix corresponding to the minimum-weight edges
selected by the row rotations as shown in Figure 12.28
Figure 12.30:  A concurrent write is used within the r × r region of the mesh to
compress and update the r' rows and columns corresponding to the r' supervertices.
This results in the creation of an r' × r' weight matrix in the upper-left regions of the r ×
r region so that the algorithm can proceed to the next stage
Figure 12.31:  A demonstration that shortest paths and shortest-path trees need not
be unique. The weighted, undirected graph G is shown in (a). In (b), we see a
shortest-path tree. Notice the path (1,2,8,7) of total weight 12 chosen between source
vertex 1 and sink vertex 7. A different shortest-path tree is shown in (c). Notice the
path (1,6,5,7) between vertices 1 and 7 is also of total weight 12
Figure 12.32:  A demonstration of the progress of Dijkstras algorithm, through the
iterations of its While loop, for constructing a shortest-path tree. The vertices are
numbered u0, u1, …, in the order in which they are inserted into the tree. Arrows
represent parent pointers. Dark edges are those inserted into the tree
Figure 12.33:  Tree traversals. Steps of each recursive algorithm are shown at the top
level of recursion; also, the order in which the vertices are processed by each
algorithm
Chapter 13:  Numerical Problems
Figure 13.1:  Trapezoidal Integration. The dashed lines represent the tops of the
trapezoids. The area under each small arc is approximated by the area of a trapezoid.
It is often much easier to compute the area of a trapezoid than the exact area under
an arc. The total area of the trapezoids serves as an approximation to the total area
under the curve
TeamUnknown Release
0
List of Tables
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
List of Tables
Chapter 4:  Combinational Circuits
Table 4.1:  Processor Sorting Table
Chapter 13:  Numerical Problems
Table 13.1:  Values of d (decimal places) and nd (number of terms) for the Taylor
series for ex expanded about x0 = 0 on [ -1,1]
Table 13.2:  Values of d (decimal places) and nd (number of terms) for the Taylor
series for sin x expanded about x0 =0 on [- p,p]
TeamUnknown Release

0
List of Examples
Algorithms Sequential and Parallel: A Unified Approach, 2nd Edition
by Russ Miller  and Laurence Boxer  
Cengage Charles River Media  © 2005
List of Examples
Chapter 1:  Asymptotic Analysis
Example
Example
Example
Example
Example
Example
Example
Example
Example
Example (  INSERTIONSORT)
EXAMPLE:  BINSORT
Chapter 2:  Induction and Recursion
Example
Example
Example
Chapter 3:  The Master Method
Example
Example
Example
Example
Chapter 9:  Divide-and-Conquer

Example
Chapter 13:  Numerical Problems
Example
Example
Example
TeamUnknown Release

International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
DOI : 10.5121/ijaia.2018.9105                                                                                                                       63  
HARDWARE  DESIGN  FOR MACHINE 
LEARNING  
 
Pooja Jawandhiya 
 
School of Electrical and Electronic Engineering, 
Nanyang Technological University, Singapore 
 
ABSTRACT  
 
Things like growing volumes and varieties of availa ble data, cheaper and more powerful computational 
processing, data storage and large-value prediction s that can guide better decisions and smart actions  in 
real time without human intervention are playing cr itical role in this age. All of these require model s that 
can automatically analyse large complex data and de liver quick accurate results – even on a very large  
scale. Machine learning plays a significant role in  developing these models. The applications of machi ne 
learning range from speech and object recognition t o analysis and prediction of finance markets. Artif icial 
Neural Network is one of the important algorithms o f machine learning that is inspired by the structur e and 
functional aspects of the biological neural network s. In this paper, we discuss the purpose, represent ation 
and classification methods for developing hardware for machine learning with the main focus on neural 
networks. This paper also presents the requirements , design issues and optimization techniques for buil ding 
hardware architecture of neural networks.  
 
KEYWORDS  
 
Artificial intelligence (AI), application specific integrated circuit (ASIC), artificial neural networ k (ANN), 
central processing unit (CPU), field programmable g ate array (FPGA), graphics processing unit (GPU), 
machine learning (ML), neurochip 
 
 
1. INTRODUCTION 
 
From self-driving cars to SIRI, Artificial Intellig ence (AI) is progressing rapidly. Science fiction 
often portrays AI as robots with human characterist ics (example, Ava in Ex Machina and Skynet 
in Terminator) but the truth is that AI can encompa ss anything. The tech giants are racing to build 
their own AI software and products. We currently ha ve Google’s Tensorflow and AlphaGo, 
Nvidia’s DGX, Amazon’s Alexa, Microsoft’s Azure, IB M’s Watson and Intel’s Nervana. A 
survey by McKinsey & Company showed that the total investments in AI development tripled 
between 2013 and 2016 [1]. Most of that — $20 billi on to $30 billion — came from these tech 
giants [2]. These companies expect that machine lea rning (ML), and other AI models that 
descend from it, will be very critical to their cus tomers in the future, just like networking and 
mobility. While many ML algorithms have been used f or years, the ability to automatically apply 
complex mathematical calculations to big data – ove r and over, faster and faster – is a recent 
development. Online recommendation offers like thos e from Flipkart and Amazon, real-time 
advertisements on web pages and mobile devices, web  search results are some of the examples of 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
64 ML applications that we are familiar with. Few othe r applications are Optical Character 
Recognition (OCR) [3], speech recognition [4,5] and  pattern classification [6]. 
 
Google has become a part and parcel of our daily li ves. But we have hardly cared to think on how 
it works [7]. The millions of images uploaded to th e internet are sorted by image recognition 
which results in accurate classification and in tur n give users better search results. One of the 
breakthroughs of Google in deep learning is in imag e enhancement. It involves restoring or filling 
in details missing from images, by extrapolation, a s well as by using what it knows about other 
similar images. Google has implemented AI in langua ge processing too. Google’s Assistant 
speech recognition AI uses deep neural networks (DN N) to learn how to better understand spoken 
questions and commands. [8] Google’s Neural Machine  Translation also works in deep learning 
environment. Another way Google uses deep learning today on its services is to provide more 
useful recommendations on YoutubeTM. Google BrainTM monitors and records our viewing habits 
as we stream content from their servers. DNNs are m ade to study and learn everything about 
viewers’ habits and preferences, and work out what would keep them glued to their screens. 
 
Hence, we see that machine learning plays a signifi cant role in the advances of technology today. 
There have been literature surveys previously done on this subject. The authors in [9] present the 
opportunities and challenges in designing hardware for machine learning while the study in [10] 
specifically talks about neural networks. A detaile d survey of neural networks in hardware is 
done in [11] whereas the authors in [12] present a brief survey of FPGA implementation of neural 
networks. This paper presents the latest review of the hardware architectures for machine learning 
focussing mainly in the aspects of neural networks.  
 
The rest of the paper is organized as follows. Sect ion 2 talks about the architectural design of the 
neural networks in both software and hardware keepi ng in the contrast between them. The 
different types of hardware for ANN are discussed i n Section 3 with detailed explanation of each 
type. Section 4 finally talks about the hardware ar chitecture in detail including CPU, GPU, FPGA 
and ASIC whereas section 5 focusses on the various issues in the design architecture and related 
optimization techniques. Some other approaches and further extension in the ML hardware 
architecture as advanced technologies are discussed  in sections 6 and 7. Section 8 is a case study 
upon the Google's Tensor Processing Unit (TPU), and  the final concluding remarks are presented 
in section 9. 
 
2. A RCHITECTURE  DESIGN  
 
In recent years, there have been massive advances i n implementing ML algorithms with 
application-specific hardware (e.g., FPGA, ASIC, et c.) due to their inherent parallelism. ML 
algorithms, such as those for specialised applicati ons like image processing, speech synthesis and 
analysis, face recognition, multi-category classifi cation, and data analysis, are being developed 
that will fundamentally alter the way individuals a nd organizations live, work, and interact with 
each other. Chipmakers are racing to build hardware  for AI. Technology giants and governments 
are investing heavily in neuromorphic chips (promin ent examples include the EU’s BrainScaleS 
project, the UK’s SpiNNaker brain simulation machin e, IBM’s “synaptic chips”, DARPA’s 
SyNAPSE program, and Brain Corporation, a research company funded by Qualcomm). There is 
a timely need to map the latest learning algorithms  to physical hardware, in order to achieve 
significant improvements in speed, performance, are a and energy efficiency. However, their 
computational complexity still challenges the state -of-the-art computing platforms, especially 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
65 when the application of interest is tightly constra ined by the requirements of low power, high 
throughput, less latency, small size, etc. 
 
2.1. Hardware vs Software Implementation of ANNs 
 
Implementation of ANNs falls into two categories: s oftware implementation in conventional 
computers and hardware implementation, capable of d ramatically decreasing execution time [10]. 
ANNs are implemented in software, and are trained a nd simulated on general-purpose computers 
for emulating a wide range of neural networks model s. Software implementations offer flexibility 
and can be used to develop and debug new algorithms . However, hardware implementations are 
essential for applicability to large networks and f or taking the advantage of ANN’s inherent 
parallelism. The main purpose of building dedicated  hardware for AI is to provide a platform for 
efficient adaptive systems, capable of updating the ir parameters in the course of time. Specific-
purpose hardware implementations are dedicated to a  particular ANN model. VLSI 
implementations of ANNs provide compact architectur e and high speed in real time applications. 
 
A significant amount of work has been done in devel oping software and simulation environments 
for ANNs. Standard implementations of ML algorithms  are readily and widely available through 
libraries/packages/APIs (e.g. scikit-learn [13], Th eano [14], Spark MLlib [15], H2O [16] , 
TensorFlow [17] etc.) but applying them effectively  involves choosing a suitable model (decision 
tree, nearest neighbour, neural net, support vector  machine, etc.), a learning procedure for fitting 
the data (linear regression, gradient descent, gene tic algorithms and other model-specific 
methods), as well as understanding how hyper-parame ters affect learning [18]. Specialized 
applications have motivated the use of hardware in ANN. For example, cheap dedicated devices 
used for speech recognition in consumer products, a nd analog neuromorphic devices, such as 
silicon retinas, directly implement the desired fun ctions [11]. 
 
Generally, neural network hardware designs are of t wo types. The first is - a general, but probably 
costly, system that can be re-programmed for many k inds of tasks - such as Adaptive Solutions 
CNAPS [19]. The second is - a specialized, but rela tively cheaper, chip that does single task 
quickly and efficiently, such as IBM ZISC [20]. 
 
2.2. Measurement Units 
 
The traditional approach for quantifying ANN hardwa re performance is to measure the number of 
MAC operations performed in the unit time, i.e., Mi llions of Connections Per Second (MCPS) 
and the rate of weight updates, i.e., Millions of C onnection Update Per Second (MCUPS) [10, 
11]. These two measurements somewhat correspond to the Million Instructions per second 
(MIPS) or the Mega Floating-point Operations per Se cond (MFLOPS) measured on conventional 
systems. The common speed measurement units of toda y’s computers are GFLOPS (billions of 
flops) or TFLOPS (trillions of flops) [21]. 
 
2.3. Precision and Number Formats 
 
During the hardware implementation of ANNs, two imp ortant considerations need to be made. 
Firstly, there should be balance between the need o f reasonable precision  (number of bits) and 
the cost of logic area. Secondly, a suitable number format  should be chosen so that dynamic 
range is large enough for general-purpose applicati on [11]. So, before beginning the hardware 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
66 design of ANN model, the number format (floating po int, fixed point etc.) and precision to be 
used for inputs, weights and activation functions m ust be considered. The precision is mostly 
limited to 16-bit fixed point for the weights of AN N and 8-bit fixed point for the outputs. In case 
of the standard error backpropagation and the multi -layer perceptron learning algorithm this 
precision was shown to be sufficient in most cases [22]. Using 16 bits as precision of calculation 
instead of 32 bits results in faster computation, b ecause processors tend to have more throughput 
at lower resolution [23]. Reducing the precision al so increases the amount of available 
bandwidth, because smaller amounts of data is being  fetched for each computation. Kohonen's 
SOM algorithm can learn well with only 6-bit weight s [24]. An arithmetical precision of more 
than 16 bits may be required by recurrent neural ne tworks [25]. However, the precision cannot be 
reduced too much because then the network will not train and will never achieve the accuracy 
needed or it will become unstable. It was found tha t the discretization process degraded the 
performance of the NN algorithm in [26]. Since prec ision has great impact in the learning phase, 
it is important to keep the precision of numbers as  high as possible during training phase. 
However, propagation phase requires the use of low precision. 
 
According to researchers, it is possible to train A NNs with integer  weights [22]. The advantage 
of using integer weights is that integer multiplier s can be implemented more efficiently on 
hardware than the floating-point ones. There are so me special learning algorithms which use 
powers-of-two integers as weights [27]. The advanta ge of powers-of-two integer weight learning 
algorithms is that the required multiplications in an ANN can be reduced to a series of shift 
operations. Floating point offers the maximum dynam ic range, making it suitable for any 
application. However, floating-point operations req uire more cycles for computation than integer 
operations (unless extremely complex designs are us ed) [10]. This is why most neurocomputer 
designers consider fixed-point representations in w hich only integers are used and the position of 
the decimal point is handled by some simple additio nal circuits or software. Appropriate word 
length must be found for using such representations . The convergence of the learning algorithms 
should not be affected and enough resolution should  be provided during normal operation. The 
classification capabilities of the trained networks  depend on the length of the bit representation. 
Another method for representation called Bit-Stream arithmetic  is described in [12]. 
 
Deeper networks have improved accuracy but they gre atly increase the number of parameters and 
model sizes. This increases the storage demands and  computational memory bandwidth. As such, 
the trends have shifted towards more efficient DNNs . An emerging trend is the adoption of 
compact low precision data types , much less than 32-bits [28]. 16-bit and 8-bit dat a types are 
being used, as they are supported by DNN software f rameworks (example, TensorFlow). 
Furthermore, researchers have shown continued impro vements in accuracy for extremely low 
precision two-bit ternary DNNs where the values are  constraints to (0,+1,-1), and one-bit binary 
DNNs where the values are constraints to (+1,-1). 
 
3. C LASSIFICATION  OF NEURAL  NETWORK  HARDWARE  
 
The range of neural network hardware lies from sing le stand-alone neurochips to full-fledged 
neurocomputers. A block level architectural represe ntation for almost all neurochips and 
neurocomputer processing elements has been presente d in [11]. A variety of attributes have been 
used to classify NN hardware, such as system archit ecture, inter-processor communication 
networks, on-chip or off-chip learning, degrees of parallelism, general purpose or special purpose 
devices, and so on. NN hardware can be categorized into 4 classes by the degree of parallelism: 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
67 coarse-grained, medium-grained, fine-grained and ma ssive parallelism [11]. The number of 
processing elements yields the degree of parallelis m of a system. Parallelism increases data 
processing speed but is expensive in terms of chip area. Therefore, highly parallel systems usually 
employ simpler processing elements. Parallel proces sing elements only speed up the computation 
when they are not idle. Thus, for better system per formance it is necessary that the inter-processor 
communication network provides the processing eleme nts with sufficient data. 
 
Neurocomputers are divided into two major categorie s. Standard chips consist of sequential 
accelerators, which speed up conventional computers  like PC or workstation, and parallel 
multiprocessors, which are mostly stand alone and c an be monitored by a host computer. 
Neurochips are built from dedicated neural ASICs (A pplication Specific Integrated Circuits) and 
can be digital, analog or hybrid. 
 
3.1. Standard chips 
 
3.1.1. Accelerator Boards 
 
Accelerator boards are the most frequently used neu ral network hardware because they are widely 
available, relatively cheap, simple to connect to t he workstation, and typically provided with user-
friendly software tools. They are used to increase the speed of neural network computations. 
Accelerator boards are usually based on NN chips bu t some just use fast digital signal processors 
(DSP). A disadvantage of accelerator boards is that  they are only specialized for certain tasks, and 
thus lack flexibility. Examples of accelerator boar ds are IBM ZISC ISA and PCI Cards. Other 
accelerator systems include SAIC SIGMA-1, Neuro Tur bo, HNC, etc. Various types of neural 
network architectures have been studied and develop ed using accelerators in [29-32]. 
 
 
Figure 1. Neural network hardware categories 
 
3.1.2. Neurocomputers Built from General Purpose Pr ocessors  
 
Programmable neurocomputers were built to meet the need for high performance on large ANN 
simulations with reasonable cost/performance and fl exible software control. These architectures 
can be simple, low-cost elements or rather sophisti cated processors like transputers, which are 
unique for their parallel I/O lines or DSPs. These transputers were primarily developed for 
correlators and discrete Fourier transforms. A prob lem associated with neurocomputers is to find 
an interconnection strategy for large numbers of pr ocessors. Fortunately, knowledge about the 

International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
68 architectures of these massively parallel computers  can be directly applied in the design of neural 
architectures. 
 
Multiprocessor neurocomputers can be classified int o four categories [33]. The first category uses 
commercial digital signal processors (DSP) while th e other three categories are based on custom-
designed silicon. 
 
3.1.2.1. Commercial DSP Arrays 
 
Several neurocomputers have been built using commer cial DSP arrays. Some notable examples 
are the RAP (Ring Array Processor) [34] developed a t the International Computer Science 
Institute and the MUSIC system [35] developed at th e Swiss Federal Institute of Technology. 
Both of these arrays connect the DSPs in a unidirec tional ring topology with communication 
circuitry built from field-programmable gate arrays  (FPGAs). The RAP supports up to 40 Texas 
Instruments TMS320C30 floating-point DSPs, with a p eak performance of 32 MFLOPS per 
node. The MUSIC system connects up to 45 Motorola D SP96002 floating-point DSPs, with a 
peak performance of 60 MFLOPS per node. Both of the se systems have distributed memories. 
They are programmed using a Single Program Multiple  Data (SPMD) model, where all nodes 
operate on different portions of the data but run i dentical programs. A separate host computer 
handles data input and output and manages the overa ll program flow. 
 
3.1.2.2. SIMD Processor Arrays 
 
A popular approach in neurocomputer design is a SIM D (Single Instruction Multiple Data) array 
of processors which have limited form of processor interconnect. Instructions are broadcast by a 
common sequencer and executed simultaneously by all  processors in these designs. The 
processors in SIMD system are much simpler than tho se in SPMD system because they do not 
have to fetch and decode instructions. Examples of SIMD neurocomputers include the CNAPS 
systems [19] from Adaptive Solutions and the SNAP [ 36] system from HNC. The neurochip 
N6400 is the basic building block of the CNAPS syst em and consists of 64 processing elements 
(or processing nodes PN) connected by a broadcast b us. The HNC system is built from SNAP 
chips each of which contains four 32-bit floating-p oint multiply-add datapaths with access to 
local off-chip memory. Multiple chips can be interc onnected and controlled by the same central 
sequencer in both of these systems. 
 
3.1.2.3. Systolic Processor Arrays 
 
Several neurocomputers have been built using systol ic processor arrays that perform the matrix 
operations for most neural algorithms. A systolic p rocessor contains an array of interconnected 
pipelines through which operands flow in a regular manner. The most advanced of these systems 
is the SYNAPSE-1 [37]. The basic building block for  this neurocomputer is the Siemens' MA-16 
neurochip. It consists of eight MA-16 chips connect ed in two parallel rings controlled by two 
Motorola MC68040 processors. Systolic arrays can be  formed by cascading multiple MA-16 
chips. This ensures that inputs and outputs are pas sed from one MA-16 chip to another in a 
pipelined manner leading to an optimal throughput. 
 
 
 
 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
69 3.1.2.4. Vector or SIMD Co-processor 
 
All the neurocomputers mentioned above rely on some  form of off-chip control sequencer or host 
computer for managing the matrix computations occur ring on the parallel processor arrays. 
Alternatively, the control processor can be integra ted with the parallel execution units on the 
same die. Two examples of this type of design are t he T0 vector microprocessor [38] and the L-
Neuro 2.3 multi-DSP [39]. The T0 vector microproces sor integrates an industry-standard MIPS-II 
32-bit integer scalar RISC processor with a tightly -coupled fixed-point vector coprocessor. The 
L-Neuro 2.3 design contains a 16-bit RISC controlle r along with an array of 12 DSP datapaths. 
The DSP datapaths are controlled using a writable m icroinstruction store, indexed by the RISC 
controller macroinstructions. 
 
3.2. Neurochips 
 
For multiprocessor neurocomputers the neural functi ons are programmed on general-purpose 
processors. Neurochips contain dedicated circuits d evised in special purpose chips for the neural 
functions. This speeds up the neural iteration time  by about two orders of magnitude as compared 
to general-purpose processor implementations. Neuro chips can be designed using several 
implementation technologies. Defining a taxonomy of  neurosystems requires consideration of 
three important factors: 
 
• the kind of signals used in the network, 
• the implementation of the weights, and 
• the integration and output functions of the units . 
 
The signals transmitted through the network can be coded using an analog or a digital model [10]. 
In the analog approach, a signal is represented by the magnitude of a current or voltage difference 
whereas in the digital approach, discrete values ar e stored and transmitted. If the signals are 
represented by currents or voltages, it is easier t o implement the weights using resistances or 
transistors with a linear response function for cer tain range of values. In the case of a digital 
implementation, each transmission through one of th e network’s edges requires a digital 
multiplication. Hybrid neurocomputers are built com bining analog and digital circuits. Analog 
systems require less power and offer higher impleme ntation density on silicon. But digital 
systems offer programming flexibility, greater prec ision, and the possibility of working with 
virtual networks, that is, networks which are not p hysically mapped to the hardware, making it 
possible to deal with more units. Due to limited pr ecision, direct implementation in circuits may 
alter the exact functioning of the original (simula ted or analysed) computational elements. In 
order to build large-scale implementations many neu rochips have to be interconnected. Some 
chips are therefore supplied with special communica tion channels. Other neurochips are to be 
interconnected by specially designed communication elements. A detailed study of digital, analog 
and hybrid neurochips is given in [40]. 
 
3.2.1. Digital Neurochips 
 
Digital Neural ASICs are the most powerful neurochi ps. Digital techniques offer high 
computational precision, programmability and reliab ility. Furthermore, powerful design tools are 
available for full-custom and semi-custom design. I ts shortcoming is the relatively large circuit 
size compared to analog implementations. Synaptic w eights can be stored on or off chip which is 
determined by the trade-off between speed and size.  Two well-known digital neurochips are 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
70 CNAPS [19] and SYNAPSE-1 [37]. VLSI microprocessors , vector and signal processors, slice 
architectures, SIMD and systolic arrays can be used  for digital implementation. Greater speedup 
of linear algebraic operations can be achieved usin g systolic arrays which are regular structures of 
VLSI units, mainly one or two-dimensional, and whic h can communicate only locally [10]. 
 
3.2.2. Analog Neurochips 
 
Analog electronics have characteristics that can di rectly be used for neural network 
implementation. For instance, operational amplifier s (Opamps), are built from single transistors 
and easily perform neuron-like functions, such as i ntegration and sigmoid transfer. Some VLSI 
circuits work with field effect transistors (FETs) made of semiconductors. These are materials 
with a nonlinear voltage-current response curve whi ch makes them especially suitable for the 
implementation of digital switches. Floating gate t ransistors can be used for an analog 
implementation of the multiplication operation. The y represent the weights by statically stored 
charges or dynamically with the help of charge coup led devices (CCDs). Analog electronics are 
compact and offer high speed at low energy dissipat ion. The drawbacks of analog electronics are 
susceptibility to noise and process-parameter varia tions. Chips built according to the same design 
will never function in exactly the same way. Anothe r limitation of the applicability of analog 
circuits is the problem of representing adaptable w eights. Although analog chips will never reach 
the flexibility attainable with digital chips, thei r compactness and speed make them attractive for 
neural network research, especially when they adopt  the adaptive properties of the original NN 
paradigms. Another advantage is that they more dire ctly interface with the real, analog world, 
whereas digital implementations will always require  fast analog-to-digital converters to read in 
data and digital-to-analog converters to put their data back into the world. Extensive 
implementations have been done using analog neuroch ips in [41-44]. Intel’s ETANN is an 
example of analog neurochip [45]. 
 
3.2.3. Hybrid Neurochips 
 
Both digital and analog techniques offer unique adv antages but they also have drawbacks. The 
main shortcomings of digital techniques are the lar ge amount of silicon and power required for 
multiplication circuits and the relatively slow com putations. The shortcomings of analog 
techniques are the sensitivity to noise and suscept ibility to interference and process variations. 
Therefore, the right combination of analog and digi tal techniques for the implementation of these 
processes is advantageous. Several research groups have implemented hybrid systems in order to 
gain advantages of both techniques and avoid the ma jor drawbacks. 
 
The ANNA (Analog Neural Network Arithmetic and Logi c Unit) [46] chip can be used for a wide 
variety of ANN architectures but is optimized for l ocally connected, weight-sharing networks and 
time-delay neural networks (TDNNs). The Epsilon (Ed inburgh Pulse Stream Implementation of a 
Learning Oriented Network) chip is a hybrid neuroch ip that uses pulse coding techniques [47]. 
 
4. H ARDWARE ARCHITECTURE  
 
There are two aspects of machine learning: training  the network with massive amounts of sample 
data and then using the trained network to infer so me attributes about new data sample. Training 
is typically done in large data centres. Figure 2 l ays out the wide range of hardware targeting 
machine learning from leading vendors. 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
71  
 
Figure 2. Hardware across the Machine Learning land scape [48] 
 
Most of the applications today are hosted in public  clouds with companies like Amazon, Google, 
Microsoft, etc. These companies run their online se rvices from data centres packed with 
thousands of servers, each driven by a chip called a central processing unit, or CPU. Now these 
companies are supplementing CPUs with other process ors for using Deep Learning Networks. 
Neural networks can learn tasks by analysing huge a mounts of data, and they require more than 
just CPU power. So Google built the Tensor Processi ng Unit, or TPU while Microsoft is using a 
FPGA processor. Myriad companies employ machines eq uipped with large numbers of graphics 
processing units, or GPUs. The hardware used for ma chine learning today mainly consists of one 
or more of the following: 
 
• CPU – Central Processing Units 
• GPU – Graphic Processing Units 
• FPGA – Field Programmable Gate Arrays 
• ASIC – Application Specific Integrated Circuits 
 
Figure 3. Overview of ML hardware available today [ 49] 
 
Each step in this progression of technologies produ ces tremendous performance advantages. Each 
has its advantages for specific type of application  or data, that is being deployed and in a specific 
environment. The velocity and data complexity deter mine the amount of processing needed, 
while the environment typically determines the powe r budget and latency demands. Performance 
can be measured in a number of ways [50]: 
 
• computational capacity (or throughput) 
• energy-efficiency (computations per Joule) 
• cost-efficiency (throughput per dollar) 

International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
72 4.1. CPU  
 
Central Processing Units or CPUs are often referred  to as the “brains” of a computing system – 
whether it is for mobile, tablet, consumer (laptop/ desktop), or enterprise servers. They are 
extremely flexible in terms of programmability, and  handling workloads. They do fast 
calculations and have dynamic circuitry. However, t hey have cost and heat issues. Nearly all 
CPUs use these four steps in their operation: fetch , decode, execute, and write-back. They are 
good at fetching small amounts of memory quickly an d the best ones have about 50GB/s memory 
bandwidth. Typical consumer CPUs have <10 cores, wh ile server CPUs may go all the way up to 
28. Intel is the dominant CPU manufacturer compared  to others (ARM, AMD, IBM POWER, 
Oracle SPARC, Fujitsu). Intel’s Xeon and Xeon Phi [ 51] in datacentres and the Qualcomm 
Snapdragon in mobile devices are some examples of C PUs. Today, CPUs are mostly used for 
classic machine learning problems and sometimes for  Deep Learning Inference [49]. 
 
4.2. GPU 
 
Graphics Processing Units or GPUs are currently the  most widely used hardware option for 
machine and deep learning [50]. GPUs are designed f or high parallelism and memory bandwidth. 
They are considered to be the best option for train ing. They were originally designed to accelerate 
the large number of multiply and add computations p erformed in graphics rendering. Packaged as 
a video card attached to the PCI bus, they offloade d these numerically intensive computations 
from the CPU. As the demand for high performance gr aphics grew, so did the GPU, eventually 
becoming far more powerful than the CPU. 
 
Machine and deep learning involves lot of matrix mu ltiplications and convolutions. GPUs can 
provide an energy-efficient means of juggling the c omplex array of calculations required to train 
a neural network. This means they can train more ne ural networks with less hardware. GPU is 
good at fetching large amounts of memory. But compa nies also need chips that can rapidly 
execute neural networks through a process called in ference. Google built the TPU specifically for 
doing this job. Microsoft uses FPGAs while Baidu is  using GPUs, which are more suitable for 
training than for inference, but can do the job wit h the right software in place. 
 
CPUs contain few cores with a large cache memory, a nd each core capable of handling a few 
software threads at a time. In contrast, a GPU cont ains hundreds of cores that can handle 
thousands of threads simultaneously. For example, a  16-core CPU processor running at 3.0 GHz 
performing fused multiply-add instructions has a pe ak performance of 96 Gflops, and a 56 
processor GPU having 32 cores per processor contain ing 1792 cores and running at 1.48 GHz 
performing fused multiply-add instructions has a pe ak performance of 5300 Gflops [52]. The 
superior floating-point performance provided by GPU s is due to the large number of cores. That’s 
why the GPU can take on many multimedia tasks, such  as accelerating Adobe Flash video, 
transcoding (translating) video between different f ormats and some really hard problems to solve 
that have an inherent parallel nature – video proce ssing, image analysis, signal processing. Also, 
GPUs are now being used to accelerate computational  workloads in areas such as cutting-edge 
scientific research, oil and gas exploration, and f inancial modelling [53]. 
 
Computers may contain multiple CPUs and GPUs for ac hieving good efficiency and very high 
speed processing [54]. Popular configurations inclu de 2 CPUs and 1 to 8 GPUs. Each GPU 
provides an order of magnitude or more in performan ce over general purpose CPU processors 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
73 resulting in faster solution times and the ability to solve large problems. Also, GPUs provide an 
order of magnitude or more in processing power for the same capital cost. The efficient 
architecture of GPUs perform more floating-point op erations per watt of power consumed. 
Performance comparison of CPUs and GPUs is provided  in [55]. 
 
For training purpose, using clusters of 8-16GPUs gi ves easy parallelism leading to best 
performance, cost and energy efficiency, and memory  bandwidth [54]. For inference in data 
centres or in mobile devices (Automotive, IoT), sin gle GPUs are used. GPUs are often far away 
from the main memory of the server, thus sending al l the data to GPU takes time. This can pose a 
problem. Hence, companies like NVIDIA have come up with a faster interconnect called NVLink 
[56]. Titan X and Tegra X1 [57] are examples of GPU s. 
 
NVIDIA and AMD are expanding both the sophisticatio n of their processors and the software 
development tools for developing, porting, and debu gging GPU code [58]. NVIDIA has an 
intriguing software tool called Nexus [59] that hel ps software developers to trace and debug 
application code from the CPU running on Windows in to the GPU, including parallel applications 
on the GPU, and back to the CPU. These enhancements  mean it will be easier to get existing 
software running on GPUs, although it will still re quire a software development effort. 
 
NVIDIA’s Compute Unified Device Architecture (CUDA)  parallel computing architecture is 
developed for GPU computing. CUDA is a key to getti ng high performance out of certain 
computations that are important in engineering anal ysis and simulation. Many systems using 
GPUs and CUDA have a single industry-standard proce ssor, usually running on Windows or 
Linux. 
 
An ideal configuration is one that has one or more CPUs and a set of GPUs, known as hybrid 
computing [60], that use CUDA or similar parallel c omputation architecture thus delivering the 
best value of system performance, price, and power.  All support applications, such as word 
processing, email and web browsing use the CPU. And  with tools such as NVIDIA Nexus, 
engineering software will eventually take advantage  of both to speed up complex computations. 
 
4.3. FPGA  
 
Field Programmable Gate Arrays (FPGAs) are a type o f hardware that can be programmed and 
reconfigured using a hardware descriptive language (HDL). FPGAs have recently become a 
target appliance for machine learning researchers, and companies like Microsoft and Baidu have 
invested heavily in FPGAs. Even though they do not offer top peak floating-point performance, it 
is observed that FPGAs’ performance/watt is higher than the GPUs’. This is because they have 
much less power usage (often 10s of Watts). This me tric is important for applications in IoT and 
self-driving cars. As FPGAs can provide quick resul ts for a pre-trained machine learning model 
(stored on the FPGA memory), they are also being us ed for inference [49]. 
 
To increase the number of operations processed and hence the compute performance researchers 
are looking for ways to leverage CPU and GPU archit ectures. FPGAs are concerned with system 
performance. By controlling the data path, they acc elerate and aid the compute and connectivity 
required to collect and process the massive quantit ies of information. Also, they can directly 
receive data and process it inline without going th rough the host system. This frees the processor 
to manage other system events and provide higher re al-time system performance. AI often relies 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
74 on real-time processing to draw instantaneous concl usions and respond accurately. The work in 
[61] shows the procedure to design and develop AI h ardware using FPGA. 
 
FPGAs’ flexibility aids in delivering deterministic  low latency and high bandwidth. The authors 
in [62] demonstrate an approach that uses an on-chi p stream buffer that efficiently stores input 
and output feature maps on FPGAs and improves bandw idth efficiency. It enables users to update 
the hardware capabilities for the system without re quiring a hardware refresh as in case of CPUs 
and GPUs, thus resulting in longer lifespans of dep loyed products. FPGAs support the creation of 
custom hardware for individual solutions in an opti mal way. Regardless of the custom or standard 
data interface, topology, or precision requirement,  an FPGA can implement the exact architecture 
defined, which allows for fixed data paths and uniq ue solutions. This is also equivalent to 
excellent power efficiency and future proofing. Wit h such a dynamic technology as machine 
learning, which is evolving and changing constantly , FPGAs provide the flexibility unavailable in 
fixed devices. An FPGA has the flexibility to insta ntly support changes such as precision-drop 
from 32-bit to 8-bit and even binary/ternary networ ks. No layout, masks or other manufacturing 
steps are needed for FPGAs, thus making the time-to -market faster. The design cycle is simpler 
due to software that handles much of the routing, p lacement, and timing. The project cycle of 
FPGAs is more predictable due to elimination of pot ential re-spins, wafer capacities, etc. 
 
All FPGA implementations of ANNs try to use the re- configurability of FPGA hardware in one 
way or another. Intel and Xilinx produce FPGA that has the ability to reconfigure the hardware 
[49]. Identifying the purpose of reconfiguration hi ghlights the motivation behind different 
implementation approaches [63]. 
 
• Prototyping exploits the fact that FPGA-based hardware can be q uickly reconfigured an 
unlimited number of times. This apparent hardware f lexibility allows rapid prototyping of 
different ANN implementation strategies and learnin g algorithms for initial simulation. 
Also, due to the dynamic nature of FPGA devices the y have modifiable topologies. 
Hence, iterative construction of ANNs can be realiz ed through topology adaptation. A 
digital architecture for classification using FPGAs ’ re-programmability feature is 
described in [64]. 
 
• Density enhancement  refers to methods which increase the amount of eff ective 
functionality per unit circuit area through FPGA re configuration. This is attained by 
using FPGA run-time / partial re-configurability in  one of the two ways. Firstly, an FPGA 
chip can be time-multiplexed for each of the sequen tial steps in an ANN algorithm. 
Secondly, an FPGA chip can be time-multiplexed for each of the ANN circuits that is 
specialized with a set of constant operands at diff erent stages during execution. This 
technique is also called dynamic constant folding.  
 
A feed-forward ANN algorithm is implemented based o n the FPGA technology in [26]. A 
method of implementing a fully connected feed forwa rd network with Xilinx FPGAs for image 
processing in a way that a single processing node i s partitioned into two XC3090 chips is 
proposed in [63]. It explores the way to implement fully parallel ANN and efficiently use 32-bit 
floating-point numeric representation in FPGA-based  ANNs by making use of the features of 
SpartanIIE series FPGAs. Pedro Ferreira et. al. [65 ] proposed a hardware implementation of ANN 
using FPGA and piece-wise linear approximation. In [66], a digital hardware-implementation 
strategy for feedforward ANNs with step activation functions has been reported. The algorithm 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
75 treats each neuron as a special case of Boolean fun ctions with properties that can be exploited to 
achieve compact implementation. This is accomplishe d by means of VHDL code that can be 
easily translated into an FPGA implementation, usin g suitable electronic-design-automation 
software. The work in [67] describes the hardware i mplementation of a real-time neural network 
controller with a DSP and an FPGA for nonlinear sys tems. 
 
Recent studies say that FPGAs have outperformed GPU s in many fields and are expected to beat 
GPUs in accelerating Deep Learning [58]. The emergi ng sparse and low precision DNN 
algorithms offer orders of magnitude algorithmic ef ficiency improvement over the traditional 
dense FP32 DNNs, but they introduce custom data typ es and irregular parallelism which are 
difficult for GPUs to handle. In contrast, FPGAs ar e designed for extreme customizability when 
running irregular parallelism and custom data types . Such trends make future FPGAs a viable 
platform for running DNN, ML and AI applications. I ntel’s evaluation of various emerging 
DNNs on two generations of FPGAs (Intel Arria 10 [6 8] and Intel Stratix 10 [69] and Titan X 
GPU shows that current trends in DNN algorithms may  favour FPGAs, and that FPGAs may 
even offer superior performance than GPUs. In [70],  the authors showed the implementation of 
Binarized Neural Networks (BNN) using CPU, GPU, FPG A, and ASIC and compared their 
performances. FPGAs may be used for other irregular  applications beyond DNNs, and on latency 
sensitive applications like ADAS and industrial use s [28]. 
 
Generally, FPGA is about an order of magnitude less  efficient than ASIC. However, modern 
FPGAs contain “hardened” resources, such as DSPs fo r arithmetic operations and M20Ks (in 
Altera FPGA) for on-chip RAMs which reduce the effi ciency gap between FPGA and ASIC.  
 
Dedicated ASICs can provide a higher total cost of ownership (TCO) in the long run, and with 
such a dynamic technology, there is a higher thresh old to warrant building them, especially if 
FPGAs can meet a system’s needs. 
 
We can have a fusion of these hardware to suit the applications. This enables the hardware to be 
used to their fullest capacity and gives optimal ne twork architectures. For example, in [48] the 
accelerators do good job of running the AI inferenc e engine; sensor fusion, data pre-processing 
and post-scoring policy execution require a lot of special I/O and fast traditional logic which is 
best suited for CPUs. Hybrid hardware platforms are  offered by NVIDIA with an ARM / GPU 
combination in NVIDIA’s Jetson [71] and DrivePX2 [7 2], while Intel and Xilinx offer SoCs 
(System on Chips) that bring ARM and FPGAs into a s ingle, elegant low-power package. All of 
these products are finding their way into drones, a utomobiles and factory robots / cobots where 
the right combination of flexibility, speed and low  power demand innovative approaches. 
 
4.4. ASIC  
 
Whilst GPUs and FPGAs perform far better than CPUs,  a factor of 10 in efficiency can still be 
gained with a more specific design, via an Applicat ion-Specific Integrated Circuit (ASIC) [49]. 
ASICs are the least flexible but highest performing  hardware options. They have full custom 
capability for design since devices are manufacture d to design specifications. They are also the 
most efficient in terms of performance/dollar and p erformance/watt, but require huge investment 
and NRE (non-recurring engineering) costs that make  them cost-effective only in very high 
volume designs. They have smaller form factor since  devices are manufactured to design 
specifications. ASICs can be designed for either tr aining or inference.  
 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
76 Google is the best example of successful machine le arning ASIC deployments. The first 
generation of Google’s Tensor Processing Unit (TPU)  [73] originally focused on 8-bit integers 
for inference workloads. The newer generation ASICs  offers floating point precision and can be 
used for training, too. Unlike CPUs and GPUs, they are designed for a specific purpose (for e.g., 
mining bitcoins) and cannot be reprogrammed. Their lack of extraneous logic makes them 
extremely high in performance and economic in their  power usage – but very expensive. Intel’s 
Nervana [74], a low-latency, high-memory bandwidth chip built for deep learning, is another 
example of an ASIC. 
 
5. ISSUES  IN HARDWARE  DESIGN  AND OPTIMIZATION TECHNIQUES  
 
The key metrics for embedded machine learning are a ccuracy, energy consumption, 
throughput/latency, and cost [9]. The accuracy of t he ML algorithm should be measured on a 
sufficiently large dataset. The weights have to be updated when the application changes, thus 
making programmability important. For DNNs, the pro cessor should also be able to support 
different networks with varying number of channels,  layers, filters and filter sizes. The need for 
programmability and higher dimensionality both resu lt in an increase in data movement and 
computation. Programmability means that the weights  also need to be read and stored and higher 
dimensionality increases the amount of data generat ed. This can pose a challenge for energy-
efficiency since data movement costs more than comp utation. The throughput is dictated by the 
amount of computation, which also increases with th e dimensionality of the data. The cost is 
dictated by the amount of storage required on the c hip while maintaining low off-chip memory 
bandwidth. Finally, training requires a significant  amount of labelled data (particularly for DNNs) 
as well as computation for multiple iterations of b ack-propagation for determining the value of 
weights. Currently, state-of-the-art DNNs consume h igher energy than other forms of embedded 
processing (e.g., video compression). We must explo it opportunities at different levels of 
hardware design to address all these issues and rem ove this energy gap. 
 
When implementing ANNs, selecting weight precision is one of the important choices. Weight 
precision is used to trade-off the capabilities of the realized ANNs against the implementation 
cost. A higher weight precision results in fewer qu antization errors in the final implementations, 
while a lower precision leads to greater speed, sim pler designs, and reductions in area 
requirements and power consumption. One way of reso lving the trade-off is to determine the 
“minimum precision” required to solve a given probl em [12]. 
 
Direct implementation for non-linear sigmoid transf er functions is very expensive. There are two 
practical approaches to approximate sigmoid functio ns [12]. Piece-wise linear approximation 
describes a combination of lines in the form of y =  ax + b which is used for approximating the 
sigmoid function. The sigmoid functions can be real ized by a series of shift and add operations if 
the coefficients for the lines are chosen to be pow ers of two. The second method is lookup tables, 
in which uniform samples are taken from the centre of a sigmoid function and stored in a table for 
look up. The regions outside the centre of the sigm oid function are still approximated in a piece-
wise linear fashion. 
 
Researchers are modifying the ML algorithms to make  them more hardware-friendly while 
maintaining accuracy. The main focus lies on reduci ng computation, data movement and storage 
requirements. The optimization techniques are as fo llows: 
 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
77 5.1. Precision reduction 
 
The default size for programmable platforms such as  CPUs and GPUs is often 32 or 64 bits with 
floating-point representation during training [9]. While during inference, it is possible to use a 
fixed-point representation and substantially reduce  the bit-width for savings in energy and area, 
and increase in throughput. For instance, in [58], input and feature vectors are 4 bits while weight 
is 6 bits. 
 
5.2. Pruning 
 
For DNNs, the number of multiply and accumulate ope rations, and weights can be reduced by 
removing weights with small or minimal impact on th e output through a process called pruning. 
However, removing weights does not necessarily lead  to energy reduction. Hence, weights are 
removed based on an energy-model to directly minimi ze energy consumption. In [31], 
performance and energy efficiency are improved by a  factor of 2.7x and 2.3x respectively by 
network pruning during training. 
 
5.3. Compression 
 
Data movement and storage are important factors in both cost and energy. Feature extraction can 
result in sparse data and the weights used in class ification can also be made sparse by pruning. As 
a result, compression can be applied to exploit dat a statistics to reduce data movement and 
storage cost. 
 
6.  U SING   MIXED -SIGNAL   CIRCUITS  FOR  ML  H ARDWARE 
ARCHITECTURE  
 
Most of the data movement is in between the memory,  processing element, and sensor. Since the 
training often occurs in the digital domain, the an alog-to-digital conversion and digital-to-analog 
conversion overhead should be accounted for when ev aluating the system. While spatial 
architectures bring the memory closer to the comput ation (i.e., into the processing element), there 
have also been efforts to integrate the computation  into the memory itself. For instance, in [75] 
and [76], the classification is embedded in the SRA M. Recently, use of mixed-signal circuits to 
reduce computation cost of the MAC have been explor ed. Authors in [77] study the trade-off 
between energy and accuracy in neural networks, and  present the ways to incorporate mixed-
signal design techniques to achieve low power dissi pation in a semi-programmable ASIC 
implementation. 
 
7. A DVANCED  TECHNOLOGIES  FOR ML H ARDWARE ARCHITECTURE  
 
Conventional CPUs/GPUs, which are based on the sequ ential von Neumann architecture, are 
inadequate for fast training with large data set du e to limited power constraints and various other 
reasons. Even the computing speed of custom-designe d ASIC lags behind the requirement of real-
time online learning. Hence, to speed this process researchers are looking for ideas beyond the 
traditional CMOS designs. For example, in [78] Chen  et. al. have implemented machine learning 
algorithms on a chip using Synaptic Device Model an d Technology-design Co-optimization 
Methodologies of the Resistive Cross-point Array. R everse scaling rules have been used for 
sizing the array geometrical dimensions such as the  wire width and the cell spacing to achieve 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
78 high learning accuracy. It realizes fully parallel operations of the weighted sum and the weight 
update with the help of parallel read and write sch eme. The digital spike encoding scheme and the 
analog voltage encoding scheme have been used in te rms of learning accuracy. It achieves 103 
speed-up and 106 energy efficiency improvement, enabling real-time image feature extraction and 
learning. 
 
An analog deep learning system has been developed i n [79] which overcomes the limitations of 
conventional digital implementations by exploiting the efficiency of analog signal processing. 
Reconfigurable current-mode arithmetic realizes par allel computation. A floating-gate analog 
memory compatible with digital CMOS provides non-vo latile storage. Algorithm-level feedback 
mitigates the effects of device mismatch. System-le vel power management applies power gating 
to inactive circuits. The online cluster analysis w ith accurate parameter learning, and feature 
extraction in pattern recognition with dimension re duction by a factor of 8 has been 
demonstrated. The system features unsupervised onli ne trainability, non-volatile memory and 
good efficiency and scalability, making it a genera l-purpose feature extraction engine ideal for 
autonomous sensory applications as well as a buildi ng block for large-scale learning systems. 
 
The use of advanced memory technologies such as emb edded DRAM (eDRAM) is explored in 
[80] to reduce the energy cost in memory access of the weights in DNN. In [81], memristors are 
used to compute a 16-bit dot product operation with  8 memristors each storing 2-bits. In [82], 
ReRAM is used to compute the product of a 3-bit inp ut and 4-bit weight. Similar to the mixed-
signal circuits, the precision is limited, and the analog-to-digital conversion and digital-to-analog 
conversion overhead must be considered in the overa ll cost, especially when the weights are 
trained in the digital domain. 
 
An analog neural network suitable for building larg e scale systems has been developed using a 
learning procedure called contrastive backpropagati on learning in [83]. In [84], the components 
of VLSI implementation of a spiking neural network is presented while [85] demonstrates a 
highly configurable neuromorphic chip with integrat ed learning for a network of spiking neurons 
which can be used in pattern classification, recogn ition, and associative memory tasks. A spatial 
architecture named ‘Eyeriss’ for energy-efficient d ataflow for Convolutional Neural Networks is 
implemented in [86]. 
 
8. C ASE STUDY  
 
CNAPS [19] and SYNAPSE-1[37] have been studied exte nsively in [11]. Here, a study of the 
TPU has been presented. 
 
A Tensor Processing Unit (TPU) is an ASIC developed  by Google specifically for machine 
learning. Compared to a GPU, it is designed explici tly for a higher volume of reduced precision 
computation (e.g. 8-bit precision) with higher inpu t/output operations per second per watt. The 
chip has been specifically designed for Google's Te nsorFlowTM framework. However, Google 
still uses CPUs and GPUs for other types of machine  learning. 
 
Google has stated that its proprietary TPUs were us ed in the AlphaGo versus Lee Sedol series of 
man-machine Go games. Google has also used TPUs for  Google Street View text processing, and 
was able to find all the text in the Street View da tabase in less than five days [87]. In Google 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
79 Photos, an individual TPU can process over 100 mill ion photos a day. It is also used in 
RankBrain which is used by Google to provide search  results. 
 
8.1. First generation  
 
The first generation TPU is an 8-bit matrix multipl y engine, driven with CISC instructions. It is 
manufactured on a 28nm process with a die size ≤ 331 mm2. The clock speed is 700 MHz and has 
a thermal design power of 28-40 W. It has 28 MiB of  on chip memory, and 4 MiB of 32-bit 
accumulators taking the results of a 256x256 array of 8-bit multipliers. Within the TPU package 
is 8 GiB of dual-channel 2133 MHz DDR3 SDRAM offeri ng 34GB/s of bandwidth. Google’s 
first-generation TPUs made it dramatically faster t o run ML models that had not been trained a 
lot, but the training had to be performed separatel y. 
 
8.2. Second generation  
 
Training state-of-the-art machine learning models r equires an enormous amount of computation, 
due to which researchers, engineers, and data scien tists often wait weeks for results. To solve this 
problem, an all-new ML accelerator was designed fro m scratch, a second-generation TPU or 
Tensor Processing Unit, that can accelerate both tr aining and running ML models. The second 
generation TPU was announced in May 2017. Google st ated the first generation TPU design was 
memory bandwidth limited, and using 64 GB of high b andwidth memory in the second-
generation design increased bandwidth to 600GB/s an d performance to 45 TFLOPS. The TPUs 
are arranged into 4-chip 180 TFLOPS modules. These modules are then assembled into 256 chip 
pods (64-TPU pods) with 11.5 PFLOPS of performance.  Notably, while the first generation TPUs 
were limited to integers, the second generation TPU s can also calculate in floating point. This 
makes the second generation TPUs useful  for both t raining and inference of machine 
learning models. Google’s second-generation Cloud T PUs are even more powerful, designed to 
accelerate the training of ML models as well as run ning them. The TPU features [88] are: 
 
• The TPU is 15x to 30x faster than contemporary GPUs  and CPUs for AI workloads that 
utilize neural network inference. 
 
• As compared to conventional chips, the TPU achieves  much better energy efficiency 
gaining 30x to 80x improvement in TOPS/Watt measure  (tera-operations [trillion or 1012 
operations] of computation per Watt of energy consu med). 
 
• The neural networks powering these applications req uire a surprisingly small amount of 
code: just 100 to 1500 lines. The code is based on TensorFlow, which is an open-source 
machine learning framework. 
 
 
Figure 4. Google’s new Cloud TPUs deliver machine l earning acceleration [89] 

International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
80 9. C ONCLUSION  
 
Machine learning has covered significant areas of c omputing and information processing in 
today’s world. There is a timely need to map the la test ML algorithms to physical hardware, in 
order to achieve significant advances in performanc e, speed, area and energy efficiency. ANNs, 
one of the important learning algorithms of machine  learning, are implemented in software, and 
are trained and simulated on general-purpose comput ers for testing a wide range of neural 
networks models. The main objective of building ded icated hardware for ML is to provide a 
platform for efficient adaptive systems, capable of  updating their parameters in the course of 
time. Deep learning networks are playing a critical  role in most AI-based technologies today. 
 
Hardware implementation of ANNs is essential for ap plicability to large networks and for taking 
advantage of their inherent parallelism which is le ss efficient in their software implementation. 
While designing hardware for neural networks, caref ul consideration should be made for the 
choice of precision, number format and type of neur ocomputer. Machine learning hardware is 
also designed using CPUs, GPUs, FPGAs and ASICs dep ending on the required performance. 
The hardware built using these technologies may hav e structural and behavioural issues and 
hence optimization and careful design is necessary.  Advanced technologies such as eDRAM and 
ReRAM are being used for speed-up and to overcome c onventional design problems. As AI 
applications expand, the demand for ML-specialized devices will drive hardware into the next 
phases of evolution. It will be fascinating to expe rience the impact of these technologies applied 
in healthcare, transportation, and robotics. Many e xciting steps in the evolution of machine 
learning still remain yet to be explored. 
 
REFERENCES  
 
[1] Jacques Bughin et. al., “How Artificial Intelli gence Can Deliver Real Value to Companies”, 
McKinsey. [Online] Available: https://www.mckinsey. com/business-functions/mckinsey-analytics/ 
our-insights/how-artificial-intelligence-can-delive r-real-value-to-companies.  
[2] Kevin Fogarty, (2017, Nov. 9), “The next Phase of Machine Learning”, Semiconductor Engineering. 
[Online] Available: https://semiengineering.com/the -next-phase-of-machine-learning/. 
[3] Eduard Sackinger et. al., “Application of the A NNA Neural Network Chip to High-Speed Character 
Recognition”, IEEE Transactions on Neural Netsworks , Vol. 3, No. 3, May 1992. 
[4] Patrick Bourke, Rob A. Rutenbar, “A High-Perfor mance Hardware Speech Recognition System for 
Mobile Applications”, 2005. 
[5] Sergiu Nedevschi, Rabin K. Patra, Eric A. Brewe r, “Hardware Speech Recognition for User 
Interfaces in Low Cost, Low Power Devices”, Design Automation Conference, 2005. 
[6] B.E. Boser et al, “Hardware requirements for ne ural network pattern classifiers”, IEEE Micro 
(Volume: 12, Issue: 1, Feb. 1992), pp. 32-40. 
[7] Bernard Marr, (2017, August 8). Forbes [Online] . Available:  
 https://www.forbes.com/sites/bernardmarr/2017/08/0 8/the-amazing-ways-how-google-uses-deep-
learning-ai/#711a9ea43204. 
[8] Ryan Whitwam (2017, October 16). ExtremeTech [O nline]. Available:  
 https://www.extremetech.com/extreme/257110-deepmin ds-wavenet-voice-synthesizer-live-google-
assistant. 
[9] Vivienne Sze, Yu-Hsin Chen, Joel Emer, Amr Sule iman, Zhengdong Zhang, “Hardware for Machine 
Learning: Challenges and Opportunities”, CICC 2017.  
[10] R. Rojas, “Neural Networks”, Springer-Verlag, Berlin, 1996. 
[11] Liao, Yihua, “Neural networks in hardware: A s urvey”, Davis, CA, 2017. 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
81 [12] Jihan Zhu and Peter Sutton, “FPGA Implementati ons of Neural Networks – A Survey of a Decade of 
Progress”, Y. K. Cheung P., Constantinides G.A. (ed s) Field Programmable Logic and Application, 
FPL 2003, Lecture Notes in Computer Science, vol. 2 778. Springer, Berlin, Heidelberg.  
[13] “Scikit-learn” [Online] Available: http://scik it-learn.org/stable/, Accessed on: Dec. 19, 2017.  
[14] “Theano” [Online] Available: http://deeplearni ng.net/software/theano/, Accessed on: Dec. 19, 2017 .  
[15] “Apache Spark MLlib” [Online] Available: https ://spark.apache.org/mllib/, Accessed on: Dec. 19, 
2017.  
[16] “H2O” [Online] Available: https://www.h2o.ai/,  Accessed on: Dec. 19, 2017.  
[17] “Tensorflow” [Online] Available: https://www.t ensorflow.org/, Accessed on: Dec. 19, 2017. 
[18] Arpan Chakraborty, (2016, April 7). Udacity [O nline]. Available: https://blog.udacity.com/2016/04 /5-
skills-you-need-to-become-a-machine-learning-engine er.html. 
[19] McCartor, H., 1991, “A Highly Parallel Digital  Architecture for Neural Network Emulation”, 
Delgado-Frias, J. G. and Moore, W. R. (eds.), VLSI for Artificial Intelligence and Neural Networks, 
pp. 357- 366, Plenum Press, New York, 1991. 
[20] Lindsey, C. S., Lindblad, Th., Sekniaidze, G.,  Minerskjold, M., Szekely, S., and Eide, A., 
“Experience with the IBM ZISC Neural Network Chip”.  Proceedings of 3rd Int. Workshop on 
Software Engineering, Artificial Intelligence, and Expert Systems, for High Energy and Nuclear 
Physics, Pisa, Italy, April 3-8, 1995. 
[21] Nvidia, “Why GPUs?”. [Online] Available: http: //www.fmslib.com/mkt/gpus.html,  
Accessed on: Dec. 20, 2017. 
[22] Holt, J. and Hwang, J., “Finite Precision Erro r Analysis of the Neural Network Hardware 
Implementations”. IEEE Trans. on Computers, 42:281- 290, 1993. 
[23] Dany Bradbury, (2017, July 24), “What sort of silicon brain do you need for artificial intelligen ce?”, 
The Register. [Online]. Available:  
 https://www.theregister.co.uk/2017/07/24/ai_hardwa re_development_plans/. 
[24] Thiran, P., Peiris, V., Heim, P. and Hochet, B ., “Quantization Effects in Digitally Behaving Circ uit 
Implementations of Kohonen Networks”. IEEE Trans. o n Neural Networks, 5(3):450-458, 1994. 
[25] Strey, A. and Avellana, N., “A New Concept for  Parallel Neurocomputer Architectures”. Proceedings  
of the Euro-Par'96 Conference, Lyon (France), Sprin ger LNCS 1124, Berlin, 470-477, 1996. 
[26] E. Won, “A hardware implementation of artifici al neural networks using field programmable gate 
arrays”, Elsevier, Nuclear Instruments and Methods in Physics Research A 581 (2007) pp. 816–820, 
2007. 
[27] Marchesi, M., et al., “Fast neural networks wi thout multipliers”. IEEE Transactions on Neural 
Networks, 1993. 4(1): p. 53-62. 
[28] Linda Barney, (2017, March 21), “Can FPGAs bea t GPUs in accelerating next-generation deep 
learning?”, The Next Platform. [Online]. Available:  https://www.nextplatform.com/2017/03/21/can-
fpgas-beat-gpus-accelerating-next-generation-deep-l earning/. 
[29] Andre Xian Ming Chang, Eugenio Culurciello, “H ardware accelerators for Recurrent Neural 
Networks on FPGA”, Circuits and Systems (ISCAS), 20 17 IEEE International Symposium, ISSN: 
2379-447X, 2017. 
[30] Chao Wang, Qi Yu, Lei Gong, Xi Li, Yuan Xie, X uehai Zhou, “DLAU: A Scalable Deep Learning 
Accelerator Unit on FPGA”, IEEE Transactions on Com puter-Aided Design of Integrated Circuits 
and Systems (Volume: 36, Issue: 3, March 2017), pp.  513 – 517. 
[31] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara , Antonio Puglielli, Rangharajan Venkatesan, 
Brucek Khailany, Joel Emer, Stephen W. Keckler, Wil liam J. Dally, “SCNN: An Accelerator for 
Compressed-sparse Convolutional Neural Networks”, I SCA’17, Proceedings of the 44th Annual 
International Symposium on Computer Architecture, p p. 27-40. 
[32] Yijin Guan, Zhihang Yuan, Guangyu Sun, Jason C ong, “FPGA-based Accelerator for Long Short-
Term Memory Recurrent Neural Networks”, Design Auto mation Conference (ASP-DAC), 2017 22nd 
Asia and South Pacific, ISSN: 2153-697X, 2017. 
[33] Krste Asanovic, “Programmable Neurocomputing”,  MIT Laboratory for Computer Science, 
Cambridge, MA 02139. [Online]. Available:  
 https://people.eecs.berkeley.edu/~krste/papers/neu rocomputing.pdf, Accessed on: Sept. 26, 2017. 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
82 [34] N. Morgan, J. Beck, P. Kohn, J. Bilmes, E. All man, and J. Beer, “The Ring Array Processor (RAP): A 
multiprocessing peripheral for connectionist applic ations”, Journal of Parallel and Distributed 
Computing, 14:248–259, April 1992. 
[35] U. A. Muller, B. Baumie, P. Kohler, A. Gunzing er, and W. Guggenbuhl, “Achieving supercomputer 
performance for neural net simulation with an array  of digital signal processors”, IEEE Micro, 
12(5):55–64, October 1992. 
[36] R. Means and L. Lisenbee, “Extensible linear f loating-point SIMD neurocomputer array processor”, 
Proceedings of the International Joint Conference o n Neural Networks, pages I–587–592, New York, 
1991. IEEE Press. 
[37] Ramacher, U., Raab, W., Anlauf, J., Hachmann, U., Beichter, J., Bruls, N., Webeling, M. and 
Sicheneder, E., 1993, “Multiprocessor and Memory Ar chitecture of the Neurocomputers SYNAPSE-
1”, Proceedings of the 3rd International Conference  on Microelectronics for Neural Networks 
(MicroNeuro), pp. 227-231, 1993. 
[38] J. Wawrzynek, K. Asanovi´c, B. Kingsbury, J. B eck, D. Johnson, and N. Morgan, “Spert-II: A vector  
microprocessor syste”, IEEE Computer, 29(3):79–86, March 1996. 
[39] M. Duranto, “Image processing by neural networ ks”, IEEE Micro, 16(5):12–19, October 1996. 
[40] Fernando Morgado Dias, Ana Antunes, Alexandre Manuel Mota, “Commercial Hardware for 
Artificial Neural Networks: A Survey”, IFAC Proceed ings Volumes, Vol. 36, Issue 12, pp.189-196, 
2003. 
[41] Jung-Wook Cho and Soo-Young Lee, “Active Noise  Cancelling using Analog NeuroChip with On-
Chip Learning Capability”, NIPS Proceedings, 1998. 
[42] Mark Holler, Simon Tam, Hernan Castro, Ronald Benson, “An Electrically Trainable Artificial 
Neural Network (ETANN) with 10240 "Floating Gate" S ynapses”, Neural Networks, 1989, IJCNN., 
International Joint Conference, 1989. 
[43] Takeshi Kamio, Haruyasu Adachi, Hiroshi Ninomi ya, Hideki Asai, “A Design Method of DWT 
Analog Neuro Chip for VLSI Implementation”, Instrum entation and Measurement Technology 
Conference, 1997. IMTC/97. Proceedings. Sensing, Pr ocessing, Networking., IEEE, 1997. 
[44] Daiki Masumoto, Hiroki Ichiki, Hideki Yoshizaw a, Hideki Kato, Kazuo Asakawa, “An Analog 
Neurochip and Its Applications to Multilayered Arti ficial Neural Networks”, TOC, vol. 74, issue 9, 
pp. 92-103, 1991. 
[45] Wikichip, “ETANN - Intel”. [Online] Available:  https://en.wikichip.org/wiki/intel/etann, Accessed  
on: Oct. 19, 2017. 
[46] Eduard Sackinger, Bernhard E. Boser, Lawrence D. Jackel, “A Neurocomputer Board Based on the 
ANNA Neural Network Chip”, Advances in Neural Infor mation Processing Systems 4 (NIPS 1994), 
pp. 773-780. 
[47] Alan F. Murray et. al., “Pulse Stream VLSI Neu ral Networks”, IEEE Macro, Vol. 14, Issue 3, June 
1994, p. 29-39. 
[48] Karl Freund, (2017, March 3), “A machine learn ing landscape: where AMD, Intel, Nvidia, 
Qualcomm and Xilinx AI engines live”, Forbes. [Onli ne]. Available 
: https://www.forbes.com/sites/moorinsights/2017/03 /03/a-machine-learning-landscape-where-amd-
intel-nvidia-qualcomm-and-xilinx-ai-engines-live/#4 436108a742f. 
[49] Gaurav Nakhare, (2017, July 31), “Hardware opt ions for machine/deep learning”, MS&E 238 Blog. 
[Online]. Available: https://mse238blog.stanford.ed u/2017/07/gnakhare/hardware-options-for-
machinedeep-learning/. 
[50] Cade Metz, (2016, October 26), “How AI is shak ing up the chip market”. [Online]. Available: 
https://www.wired.com/2016/10/ai-changing-market-co mputer-chips/. 
[51] “Intel Xeon Phi Processors”. [Online] Availabl e:  
 https://www.intel.com/content/www/us/en/products/p rocessors/xeon-phi/xeon-phi-processors.html, 
Accessed on: Dec. 19, 2017.  
[52] Nvidia, “Why GPUs?”. [Online] Available: http: //www.fmslib.com/mkt/gpus.html, Accessed on: 
Dec. 20, 2017.  
[53] Kevin Krewell, (2009, December 16), “What’s th e difference between a CPU and a GPU?”. Nvivdia 
[Online]. Available: https://blogs.nvidia.com/blog/ 2009/12/16/whats-the-difference-between-a-cpu-
and-a-gpu/.  
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
83 [54] William Dally, (2015, July 12), “High performa nce hardware for machine learning”, NIPS Tutorial. 
[Online]. Available: https://media.nips.cc/Conferen ces/2015/tutorialslides/Dally-NIPS-Tutorial-
2015.pdf.  
[55] Nvidia, “Why GPUs?”. [Online] Available: http: //www.fmslib.com/mkt/gpus.html, Accessed on: 
Dec. 20, 2017. 
[56] Nvidia NVLink high-speed interconnect”, Nvidia . [Online]. Available:  
 http://www.nvidia.com/object/nvlink.html. Accessed  on: Sept. 29, 2017. 
[57] Nvidia, “Tegra Processors”. [Online] Available : http://www.nvidia.com/object/tegra-x1-
processor.html, Accessed on: Dec. 20, 2017. 
[58] Nuno Edgar Nunes Fernandes, (2017, April 3), “ FPGA chips will be the hardware future for deep 
learning and AI”, Wordpress. [Online]. Available:  
 https://theintelligenceofinformation.wordpress.com /2017/04/03/fpga-chips-will-be-the-hardware-
future-for-deep-leaning-and-ai/. 
[59] Nvidia, “Nvidia Introduces Nexus, The Industry ’s First Integrated GPU/CPU Environment for 
Developers Working with Microsoft Visual Studio”. [ Online] Available:  
 http://www.nvidia.com/object/pr_nexus_093009.html.  
[60] Kishore Kothapalli et. al., “CPU and/or GPU: R evisiting the GPU Vs. CPU Myth”. [Online] 
Available: https://arxiv.org/pdf/1303.2171.pdf. 
[61] William J., (2017, July 24), “Machine Learning  on Intel FPGAs”, Intel. [Online]. Available: 
https://software.intel.com/en-us/articles/machine-l earning-on-intel-fpgas. 
[62] Utku Aydonat, Shane O’Connell, Davor Capalija,  Andrew C. Ling, Gordon R. Chiu, “An OpenCL 
Deep Learning Accelerator on Arria 10”, 2017. 
[63] Suhap Sahin, Yasar Becerikli, Suleyman Yazici,  “Neural Network Implementation in Hardware 
Using FPGAs”, Neural Network Implementation in Hard ware Using FPGAs. In: King I., Wang J., 
Chan LW., Wang D. (eds) Neural Information Processi ng. ICONIP 2006. Lecture Notes in Computer 
Science, vol. 4234, Springer, Berlin, Heidelberg. 
[64] Cox, C.E. and E. Blanz, “GangLion - a fast fie ld-programmable gate array implementation of a 
connectionist classifier”, IEEE Journal of Solid-St ate Circuits, 1992. 28(3): pp. 288-299. 
[65] Pedro Ferreira, Pedro Ribeiro, Ana Antunes, Fe rnando Morgado Dias, “Artificial Neural Networks 
Processor - a Hardware Implementation using a FPGA” , Becker J., Platzner M., Vernalde S. (eds) 
Field Programmable Logic and Application. FPL 2004.  Lecture Notes in Computer Science, vol. 
3203, Springer, Berlin, Heidelberg. 
[66] Andrei Dinu, Marcian N. Cirstea, and Silvia E.  Cirstea, “Direct Neural-Network Hardware-
Implementation Algorithm”, IEEE Transactions on Ind ustrial Electronics (vol. 57, Issue: 5, May 
2010). 
[67] Seul Jung, Sung su Kim, “Hardware Implementati on of a Real-Time Neural Network Controller with 
a DSP and an FPGA for Nonlinear Systems”, IEEE Tran sactions on Industrial Electronics, vol. 54, 
No. 1, February 2007. 
[68] Intel FPGA and SoC, “Arria 10”. [Online] Avail able: https://www.altera.com/products/fpga/arria-
series/arria-10/overview.html. 
[69] Intel FPGA and SoC, “Stratix 10”. [Online] Ava ilable: https://www.altera.com/products/fpga/strati x-
series/stratix-10/overview.html.  
[70] Eriko Nurvitadhi et. al., “Accelerating Binari zed Neural Networks: Comparison of FPGA, CPU, 
GPU, and ASIC”, IEEE International Conference on Fi eld-Programmable Technology, 7-9 Dec., 
2016.  
[71] Nvidia, “Jetson Automotive Development Platfor m”. [Online] Available:  
 http://www.nvidia.in/object/jetson-pro-automotive- development-platform-in.html. 
[72] Nvidia, “Nvidia Drive PX”. [Online] Available:   
https://www.nvidia.com/en-us/self-driving-cars/driv e-px/.  
[73] Nicole Hemsoth (2017, April 5), “First In-dept h Look at Google’s TPU Architecture”. [Online] 
Available: https://www.nextplatform.com/2017/04/05/ first-depth-look-googles-tpu-architecture/. 
[74] Intel Nervana, [Online] Available: https://www .intelnervana.com/. 
[75] J . Zhang, Z. Wang, N. Verma, “A machine-learn ing classifier implemented in a standard 6T SRAM 
array,”, Sym. on VLSI, 2016. 
International Journal of Artificial Intelligence an d Applications (IJAIA), Vol.9, No.1, January 2018 
84 [76] Z. Wang, R. Schapire, N. Verma, “Error-adaptiv e classifier boosting (EACB): Exploiting data-drive n 
training for highly fault-tolerant hardware,”, ICAS SP, 2014. 
[77] B. Murmann, D. Bankman, E. Chai, D. Miyashita,  L. Yang, “Mixed-signal circuits for embedded 
machine-learning applications”, Signals, Systems an d Computers, 49th Asilomar Conference, 2015. 
[78] Pai-Yu Chen, Deepak Kadetotad, Zihan Xu, Abina sh Mohanty, Binbin Lin, Jieping Ye, Sarma 
Vrudhula, Jae-sun Seo, Yu Cao, Shimeng Yu, “Technol ogy-design co-optimization of resistive cross-
point array for accelerating learning algorithms on  chip”, Design, Automation & Test in Europe 
Conference & Exhibition (DATE), 2015. 
[79] Junjie Lu, Steven Young, Itamar Arel, Jeremy H olleman, “A 1 TOPS/W Analog Deep Machine-
Learning Engine with Floating-Gate Storage in 0.13 µm CMOS”, IEEE Journal of Solid-State 
Circuits (Volume: 50, Issue: 1, Jan. 2015). 
[80] Y. Chen and et al., “DaDianNao: A Machine-Lear ning Supercomputer,”, MICRO, 2014. 
[81] A. Shafiee, A. Nag, N. Muralimanohar, R. Balas ubramonian, J. P. Strachan, M. Hu, R. S. Williams, 
V. Srikumar, “ISAAC: A Convolutional Neural Network  Accelerator with In-Situ Analog Arithmetic 
in Crossbars,”, ISCA, 2016. 
[82] P. Chi, S. Li, Z. Qi, P. Gu, C. Xu, T. Zhang, J. Zhao, Y. Liu, Y. Wang, and Y. Xie, “PRIME: A Nov el 
Processing-In-Memory Architecture for Neural Networ k Computation in ReRAM-based Main 
Memory,”, ISCA, 2016. 
[83] Takashi Morie and Yoshihito Amemiya, “An All-A nalog Expandable Neural Network LSI with On-
Chip Backpropagation Learning”, IEEE Journal of Sol id-State Circuits, Vol. 29, No. 9, September, 
1994. 
[84] Arindam Basu, SunShuo, HongmingZhou, MengHiotL im, Guang-BinHuang, “Silicon spiking 
neurons for hardware implementation of extreme lear ning machines”, Neurocomputing, 102, pp.125–
134, 2013. 
[85] Jae-sun Seo et al, “A 45nm CMOS Neuromorphic C hip with a Scalable Architecture for Learning in 
Networks of Spiking Neurons”, Custom Integrated Cir cuits Conference (CICC), 2011 IEEE. 
[86] Yu-Hsin Chen, Joel Emer, Vivienne Sze, “Eyeris s: A Spatial Architecture for Energy-Efficient 
Dataflow for Convolutional Neural Networks”, Comput er Architecture (ISCA), 2016 ACM/IEEE 
43rd Annual International Symposium, 2016, ISSN: 10 63-6897. 
[87] Joe Osborne, (2016, Aug. 22), “Google’s Tensor  Processing Unit Explained: This is What the Future  
of Computing Looks Like”. Techradar [Online] Availa ble: 
 http://www.techradar.com/news/computing-components /processors/google-s-tensor-processing-unit-
explained-this-is-what-the-future-of-computing-look s-like-1326915. 
[88] Kaz Sato, (2017, May 12), “An In-depth Look at  Google’s First Tensor Processing Unit (TPU)”, 
Google Cloud Platform. [Online] Available: https:// cloud.google.com/blog/big-data/2017/05/an-in-
depth-look-at-googles-first-tensor-processing-unit- tpu. 
[89] Google AI, “Cloud TPUs”. [Online] Available: h ttps://ai.google/tools/cloud-tpus/. 
 
 
AUTHOR 
 
Pooja Jawandhiya  was born in Nagpur, India on May 2, 1995. She rece ived the 
Bachelor of Engineering degree in Electronics and T elecommunication from 
University of Mumbai in June, 2017. Currently, she is a student in Nanyang 
Technological University, Singapore and is pursuing  Master of Science (Electronics) 
from the School of Electrical and Electronic Engine ering. 


Autom Softw Eng (2017) 24:623–671
DOI 10.1007/s10515-016-0200-3
An Architecture based on interactive optimization
and machine learning applied to the next release
problem
Allysson Allex Araújo1·Matheus Paixao2·
Italo Yeltsin1·Altino Dantas1·Jerffeson Souza1
Received: 1 April 2015 / Accepted: 20 May 2016 / Published online: 6 June 2016
© Springer Science+Business Media New York 2016
Abstract The next release problem (NRP) consists of selecting which requirements
will be implemented in the next release of a software system. For many search based
software engineering approaches to the NRP, there is still a lack of capability to efﬁ-
ciently incorporate human experience and preferences in the search process. Therefore,
this paper proposes an architecture to deal with this issue, where the decision maker
(DM) and his/her tacit assessments are taken into account during the solutions eval-
uations alongside the interactive genetic algorithm. Furthermore, a learning model
is employed to avoid an overwhelming number of interactions. An empirical study
involving software engineer practitioners, different instances, and different machine
learning techniques was performed to assess the feasibility of the architecture to incor-
porate human knowledge in the overall optimization process. Obtained results indicate
the architecture can assist the DM in selecting a set of requirements that properly incor-
B Allysson Allex Araújo
allysson.araujo@uece.br
http://goes.uece.br
Matheus Paixao
matheus.paixao.14@ucl.ac.uk
http://crest.cs.ucl.ac.uk
Italo Yeltsin
italo.yeltsin@uece.br
Altino Dantas
altino.dantas@uece.br
Jerffeson Souza
jerffeson.souza@uece.br
1Optimization in Software Engineering Group, State University of Ceará, 1700,
Dr. Silas Munguba Avenue, 60.714-903 Fortaleza, Brazil
2CREST Centre, University College London, Malet Place, London WC1E 6BT, UK
123
624 Autom Softw Eng (2017) 24:623–671
porate his/her expertise, while optimizing other explicit measurable aspects equally
important to the next release planning. On a scale of 0 (very ineffective) to 5 (very
effective), all participants found the experience of interactively selecting the require-ments using the approach as a 4 (effective).
Keywords Next release problem ·Interactive optimization ·Machine learning ·
Search based software engineering
1 Introduction
During an iterative and incremental software development process, there are some
complex decisions to be made. Selecting which requirements will be implemented in
the next release of the system is one of them, given the high number of combinations,technical constraints, multiple objectives and different stakeholders. “Release” is the
term used to describe a stable and executable version of the system, which is delivered
according to stakeholders requests. Given this context, the problem of maximizingthe stakeholders satisfaction, while respecting a predeﬁned budget for the release
development, is called the next release problem (NRP) ( Bagnall et al. 2001 ). The
stakeholder is usually satisﬁed according to the requirements he/she wants the most,which are implemented in the next release.
The main goal of the search based software engineering (SBSE) ﬁeld is to refor-
mulate difﬁcult problems found in software engineering into search problems. Then,the problems are solved by the use of computational search, especially metaheuristics.
These search techniques are guided by a ﬁtness function, which is able to distinguish
good solutions from not so good ones ( Harman 2007a ). Thus, SBSE needs only two key
ingredients: (i) the choice of the representation of the problem and (ii) the deﬁnition
of the ﬁtness function ( Harman et al. 2012 ).
State of the art single objective SBSE approaches to the NRP usually select the
requirements for the next release in a fully automatic fashion, without an effective and
dynamic participation of the decision maker (DM). Most of the approaches assumethat all the required information is collected before the optimization process. However,
asking the DM to express a priori his/her knowledge might be an inconvenient task to
perform, especially because there is too much information to consider, where most ofit is actually implicit ( Ferrucci et al. 2014 ).
Moreover, when the DM is not employed in the resolution process, he/she may feel
excluded from the analysis, and present some resistance or put little conﬁdence in theﬁnal result ( Miettinen 1999 ,Shackelford 2007 ). In addition, there are several intrinsic
aspects of requirements engineering which comprehension is inherently subjective,
demanding a more effective user collaboration ( Harman 2007b ). Therefore, a user-
friendly inclusion of the human expertise during the search process can result in a
better strategy to handle the NRP, providing a decision support tool that is able to
deal with all these matters and ﬁnd a solution that successfully incorporates the DM’squalitative assessments.
The interactive evolutionary computation (IEC), which is a branch of interac-
tive optimization, is supported by two key components: (i) human evaluation and
123
Autom Softw Eng (2017) 24:623–671 625
(ii) computational search through bio-inspired evolutionary strategies ( Takagi 1998 ).
Population-based algorithms seem to be ideal for interactive optimization since a user
can directly evaluate ﬁtness values of potential solutions ( Takagi 2001 ). This optimiza-
tion strategy is often necessary when the algorithm that deﬁnes the ﬁtness function
is either too complex to specify or, in many cases, impossible to quantify and code
(Shackelford 2007 ).
Despite being widely spread in SBSE, currently used genetic algorithms (GAs) still
lack the capability of efﬁciently considering human expertise in the search process.
Thus, the interactive genetic algorithm (IGA) emerges as an interesting alternative inorder to include the DM in the GA evolution process. The IGA shares several key
concepts with a traditional GA, such as population evolution by applying selection,
crossover and mutation operators. Differently, the solution evaluation is performed
considering the DM’s knowledge, guiding the search process to more valuable areas
in subjective terms ( Cho 2002 ).
Although an intense human involvement is interesting and attractive to the search
process, it may cause one of the most critical drawbacks of interactive optimization
approaches: the human fatigue ( Takagi 2001 ). This exhaustion occurs due to repeated
requests for human judgements, which ends up distracting the focus of the user over
the algorithm execution, then jeopardizing the search trajectory ( Simons et al. 2014 ).
Dealing with this intrinsic complication can be considered a mandatory feature forany interactive optimization approach ( Shackelford 2007 ).
Based on Kamalian et al. (2006 ), this paper handles the human fatigue using a
machine learning model. The DM’s expertise is gathered when he/she provides sub-jective evaluations to each solution during the algorithm evolution. Then, after the
creation of a training dataset composed by different releases and respective human
evaluations, a learning model can be used to learn the human behavior and, eventu-ally, replace the DM in the remainder of the evolutionary process.
An initial proposal of an architecture that allows the DM to take part in a search-
based approach to the NRP has been introduced in Araújo et al. (2014 ). In this ﬁrst
work, a conceptual version of the architecture was presented and empirically evaluated.
Preliminary results were able to show that an IGA can successfully incorporate theDM preferences in the ﬁnal solution. Therefore, this paper has the main objective of
thoroughly present and evaluate the architecture to solve an interactive version of the
next release problem that allows an inclusion of human knowledge during the searchprocess using an interactive genetic algorithm.
The present paper signiﬁcantly extends the previous work in three different aspects:
(a) the learning model, previously only proposed as an idea, is now developed andconsidered in the empirical evaluation of the approach; (b) the empirical evaluation
considering artiﬁcial cases was extended with two new research questions. Further-
more, the approach has also been evaluated with software engineering practitioners;and (c) the description of the architecture has been improved and extended through
the deﬁnition of a new component, called the Interactive Module. The primary con-
tributions of this paper can be summarized as:
– The presentation of an interactive single objective formulation of the NRP, called
iNRP, that suits the explained architecture;
123
626 Autom Softw Eng (2017) 24:623–671
– A suite of proposed metrics that may contribute to the SBSE research community
for the evaluation of empirical studies involving interactive optimization;
– Analyses of the behavior of the architecture when considering both simulated and
real human subjective evaluations.
As previously mentioned, two experiments were performed in the empirical study
conducted in this work, respectively named as: (a) artiﬁcial experiment and (b)participant-based experiment. In the ﬁrst one, a simulator was employed to verify the
inﬂuence of different evaluation proﬁles in several and exhaustive scenarios, including
both artiﬁcial and real-world instances. In the second experiment, a group of softwareengineer practitioners was invited to solve a real-world instance using the proposed
approach. For each experiment, two learning techniques were used based on different
strategies of machine learning.
The remainder of this paper is organized as follows: Sect. 2explains the proposed
architecture; Sect. 3presents the interactive formulation for the NRP, while Sect. 4
exhibits and examines the empirical study designed to evaluate the proposal; Sect. 5
discusses some work related to this paper; ﬁnally, Sect. 6concludes the paper and
points out some future research directions.
2 Architecture overview
Domain knowledge can be used to guide the optimization algorithm to promising areasof the search landscape. Such an approach enables the search to simultaneously and
effortlessly adapt the solution to the business needs of the DM, while reducing the size
of the search space. It is recommended to: “wherever possible, domain knowledgeshould be incorporated into the SBSE approach” ( Harman et al. 2012 ). However,
the design of search approaches that are able to naturally incorporate the subjective
preferences into the overall optimization process is still a challenge for the SBSE
community.
Following the deﬁnitions stated in Miettinen et al. (2016 ), the role of the DM in the
solution process can be divided into four classes: (i) no-preference methods in which
there is no need of articulation of preference information; (ii) a priori methods that
require articulation of preference before the search process; (iii) a posteriori methodsin which the preference information is used after the search process and, ﬁnally, (iv)
interactive methods in which the DM introduces preference information progressively
during the search process. This proposal focuses on interactive methods.
One important assumption of this work is that, regardless of how many stakeholders
are involved in the project, there will be a speciﬁc DM to make the last call. This
decision is motivated by the need of an unbiased judgement that is able to manage thepossible conﬂicts of interests of the other stakeholders and also weight the different
characteristics of the decision-making process. Such a role should be performed by a
professional fully immersed in the high-level project particularities and with a broadview of his/her short/long time decisions. Hence, the presented approach focuses in
interacting with this only one crucial stakeholder, referred in this work as decision
maker (DM).
123
Autom Softw Eng (2017) 24:623–671 627
After deciding how the human will provide its preferences and who will be the
DM in the software project, the next step is to understand the nature of human pref-
erence. According to the work in Aljawawdeh et al. (2015 ), the preferences can be
categorized as explicit and implicit. Explicit preferences are readily articulated by the
DM and their relevance to the search is typically well understood by him/her, while
implicit preferences may be tacit and difﬁcult for humans to previously articulate. Asan example, implicit memory is a type of memory that previous experiences might
aid in the performance of a task without conscious awareness of these experiences
(Schachter 1987 ).
Therefore, this paper proposes a conceptual architecture that is able to interactively
incorporate in a step-wise manner both explicit and implicit preferences during the
search process through an interactive genetic algorithm. To overcome the need of
human intervention in all individuals evaluations, a machine learning model is used to
learn the user’s proﬁle and, consequently, be able to replace him/her when necessaryalongside the IGA. Such architecture (presented in Fig. 1), is composed of three main
components, which are generically described below:
1.Interactive genetic algorithm responsible for the optimization process, where
the candidate solutions are ﬁrstly evaluated by the Interactive Module and, whenrequired, by the learning model. Given an individual, its ﬁtness value is calculated
considering both explicit and implicit preferences. Explicit preference is repre-
sented by well deﬁned aspects of the problem, such as the quantitative measuresofscore value and the implementation effort of each requirement, as addressed
by the classical NRP ( Bagnall et al. 2001 ). Implicit preference is gathered by a
qualitative assessment of the DM regarding a certain solution, which is formallydeﬁned as subjective evaluation ( SE). The transition between using the Interac-
tive Module and the Learning Model is dynamically performed depending on how
many interactions the DM is willing to perform.
2.Interactive module responsible for the interactive interface with the DM. Thus,
each interaction consists of an individual of the IGA that receives a SEvalue from
the DM. The SEvalue of that respective individual is passed to the IGA, while
both the individual and its SEvalue are passed to the learning model. The only
constraint between the number of interactions and individuals is to have a numberof individuals that is, at least, equal or bigger than the number of interactions.
Thus, it is possible to have more individuals than interactions, but not vice-versa.
3.Learning model responsible for learning the user proﬁle through a machine learn-
ing technique, in which all details regarding the learning problem formulation and
representation have to be well-formalized. The training process occurs in this
component, according to the samples (individuals and SEvalues) provided by the
Interactive Module. After the training process be performed, the learning model
will be able to provide a SEvalue to each solution that needs to be subjectively
evaluated. Naturally, the SEvalue provided by the learning model will only be
suitable for the learnt DM’s proﬁle. In addition, the effectiveness of the learning
model will be proportional to the number of samples provided by the Interactive
Module.
123
628 Autom Softw Eng (2017) 24:623–671
Interactive Module
Interactive
Genetic
AlgorithmLearning
Model2   Stage1  Stagest
nd
SEiindividualiindividual  + SEi i
individual j
SEjSettings
Fig. 1 Conceptual architecture overview
As presented in Fig. 1, the relationship between these components are divided in
two distinct stages, deﬁned below:
(a)First stage (solid lines) before starting the search process, it is necessary for the
DM to specify two architectural settings: (i) the weight of the implicit preferencesin comparison to the explicit ones for the ﬁtness calculation. This parameter is
related to how inﬂuential the DM’s subjective knowledge will be in the search
process; (ii) a minimum number of interactions iwhich the DM is willing to
take part in. This parameter deﬁnes the ﬁrst iindividuals to be evaluated by the
DM, where each individual
iand its respective SEivalue are sent to the learning
model.
(b)Second stage (dotted lines) at this moment, the learning process is performed
using the set of samples collected in the previous stage as a training dataset.After the conclusion of this process, the learning model will be responsible for
simulating the DM behavior and evaluates the remainder of jindividuals from the
evolutionary process, in other words, providing a SE
jvalue for each individual j
until the stopping criteria is reached. At the end, the best solution found by the
IGA is presented to the DM.
Figure 2presents an activity diagram that shows the overall optimization process,
focusing on how the different components of the architecture exchange information.
As explained before, the IGA primarily follows the concepts of a traditional GA, withthe primary difference being the ﬁtness evaluation. As depicted in the Fig. 2,a f t e rt h e
architectural settings deﬁnition (weight of the subjective evaluation and number of
interactions), the population is initialized and the stopping criteria is veriﬁed. Whilethe criteria is not reached, the search process continues.
As previously mentioned, the ﬁtness value of an individual considers both explicit
preferences and the SEvalue that encapsulates the implicit preference of the DM.
The decision to stop requiring SEvalues from the Interactive Module and changing
to receive SEvalues from the learning model is deﬁned according to the minimum
number of interactions iestablished by the DM. Therefore:
123
Autom Softw Eng (2017) 24:623–671 629
Fig. 2 Activity diagram of the conceptual architecture
– If the number of interactions has not been reached, the individual is presented to the
DM, which will be responsible for giving a SE. This evaluation is then considered
in the ﬁtness calculation of individual i, and the SEivalue alongside its respective
individual iare included in the training dataset for the learning model.
– If the number of interactions hasbeen reached, it is checked whether the learning
model has been trained. If not, the training process is performed considering the
samples collected in the previous stage as training dataset. Then, the learningmodel provides a SE
jfor the individual jto be evaluated. In the case where
the learning model has already been trained, it directly provides a SE jfor each
presented individual j.
When the ﬁtness of all individuals in the population are calculated, the genetic oper-
ators are applied and, consequently, the algorithm carries on considering the subjective
123
630 Autom Softw Eng (2017) 24:623–671
evaluations provided by the learning model until the stopping criteria is reached. Upon
ﬁnishing, the best solution achieved is shown to the DM.
This conceptual architecture can be considered sufﬁciently generic to be adopted
in other software engineering scenarios tackled by SBSE, such as feature selection
in software product lines, requirements prioritization or software design problems.
However, this paper is focused on evaluating the approach to the NRP, in which allformalization required to be suited to the architecture is properly presented in the next
section.
3 An interactive next release problem formulation
In SBSE, the ﬁtness function guides the search by capturing the properties that makea given solution preferable to another one ( Zhang et al. 2008 ). Usually, the ﬁtness
function is composed of metrics that represent quality and constraint attributes fromthe software asset being measured and optimized ( Harman and Clark 2004 ). Unfor-
tunately, sometimes these attributes may not be precisely deﬁned in the early stages
of the software life cycle, or are inherently associated with comprehension activities,which require a human input to the assessment ( Harman et al. 2012 ). This kind of
problem can be properly handled by algorithms that employ a “human-in-the-loop”
ﬁtness computation, in other words, interactive optimization.
As previously mentioned, in order to apply a search based approach to a Soft-
ware Engineering problem, one needs the solution representation and ﬁtness function(Harman 2007a ). However, since this paper proposes an architecture based on SBSE,
interactive optimization and machine learning foundations, three major questions had
to be properly deﬁned: how the objectives to be optimized are mathematically mod-elled? How does the DM interact with the optimization process? And which details
are required to allow the learning process to be performed? Therefore, the problem
formulation in this work is composed of a triad of formulations, respectively namedas mathematical modeling, interactive modeling and learning modeling.
3.1 Mathematical modeling
Consider R={r
1,r2,r3,..., rN}the set of all available requirements to be selected
for the next release, where Nis the maximum number of requirements. Each require-
ment rihas an importance value viand an implementation effort ei. Consider
K={k1,k2,k3,..., kM}as the set of stakeholders to be attended by the system,
where each stakeholder kjhas a speciﬁc weight wjthat measures his/her relevance to
the software project. The interactive next release problem (iNRP) model proposed in
this work can be formalized as follows:
maximize α×score(X)+β×SE(X), (1)
subject to: ef fo rt (X)≤budget , (2)
where, score(X)=N/summationdisplay
i=1vi×xi, (3)
123
Autom Softw Eng (2017) 24:623–671 631
vi=M/summationdisplay
j=1wj×sji, (4)
ef fo rt (X)=N/summationdisplay
i=1ei×xi, (5)
where budget refers to the release available budget. A binary solution representation is
used, where the release is represented by the decision variables X={x1,x2,..., xN},
so that xi=1 implies that requirement riis included in the next release and xi=0
otherwise. The score(X)function (Eq. 3) is calculated by multiplying the sum of
the total importance ( vi) of requirement riand the decision variable xi. The total
importance ( vi) of a requirement riis given by the product of the weight ( wj) of each
stakeholder ( kj) and the speciﬁc importance ( sji) for the requirement riprovided by
each stakeholder (see Eq. 4). Generally, the score(X)function encourages the search to
achieve solutions that maximize the overall stakeholders satisfaction. Similarly to the
score(X)function, the ef fo rt (X)function (Eq. 5) represents the total implementation
ef fo rt of the release and it is calculated by the product of the sum of each requirement
ef fo rt (X)(ei) and the decision variable xi. As a constraint, the ef fo rt (X)cannot
exceed the budget . Therefore, this formulation provide the quality of a candidate
solution, also known as, the ﬁtness ( F).
In the IGA application to the iNRP, each individual is a release. However, as
explained in the Sect. 2, two kinds of information are needed to calculate the ﬁtness of
an individual: the explicit and the implicit preferences. The ﬁrst one is represented by
thescore(X)objective obtained using Eq. 3, which represents the overall stakeholders
satisfaction, while the second is encapsulated by the DM’s tacit assessment obtainedfrom SE(X). The implicit preference provided by the DM was modeled using a numer-
ical range value established as a qualitative “grade”. So, when the release fully satisﬁes
the DM, the grade is maximum. Similarly, the grade is minimal when the release is
completely different from what the DM expects. Thus, the search process will be
guided to areas that maximize both the score(X)values and the DM’s satisfaction.
To avoid any misconceptions, this paper considers “explicit preference” as the result
of the score(X)function and the “implicit preference” as the subjective evaluation
provided by the DM ( SE(X)). Thus, in some cases, the DM opinion towards the
selected requirements is not inﬂuenced by the explicit one, because the importance
assigned by a speciﬁc stakeholder to a certain requirement may not agree with the DM
strategic planning. So, it is reasonable to expect some trade-off regarding the loss inthe overall stakeholders satisfaction to achieve a better solution in subjective aspects.
Later on, it will be explained how this trade-off can be properly balanced according
to the speciﬁc needs of a certain software project.
Another important aspect to take into account under the context of next release
planning are the interdependencies between requirements. This paper considers the
functional interdependencies REQUIRE( r
i,rj) and AND( ri,rj)(Carlshamre et al.
2001 ). These two interdependencies were chosen because they are the most common
in software projects. A REQUIRE( ri,rj) dependency indicates that requirement ri
cannot be selected to the next release if rjis not selected as well. However, requirement
123
632 Autom Softw Eng (2017) 24:623–671
rjcan be included in the next release without ri. Differently, AND( ri,rj) indicates that
both requirements should be selected for the release, otherwise none of them can. Both
interdependencies are represented by a single matrix N×Ncalled “functional”. Thus,
fu n c t io n a l ij=1 indicates the existence of a dependency REQUIRE( ri,rj), while
a dependency AND( ri,rj) is represented by fu n c t io n a l ij=fu n c t io n a l ji=1.
The mathematical model used in this paper can be considered as a generalization
of the one proposed in Baker et al. (2006 ). When the weights αandβin Eq. 1
are conﬁgured to α=1 and β=0, the classical NRP is reached. In other words,
the requirements selection will consider only the score(X)function (Eq. 3). When
the weights are conﬁgured to α=0 and β=1, only the subjective evaluations
(SE(X)) will be considered in the search process. The α=1 andβ=1 conﬁguration
consider the DM’s assessment and, at the same time, select the requirements with
highest score(X). This freedom of choice to be determined before the algorithm
execution allows the proposed approach to be adapted so speciﬁc needs of differentsoftware projects. For example, if it is established to prioritize the DM’s subjective
evaluations over the stakeholders satisfaction, one can simply choose a α=1 and
β=2 conﬁguration. On the other hand, if it is decided to prioritize the importance
values assigned by the stakeholders more than the DM’s strategic opinions, a α=2
andβ=1 conﬁguration can be employed. Moreover, the ﬁtness calculation addressed
in this formulation is summarized by the following algorithm:
Algorithm 1 : Fitness calculation of a certain individual
Input: individual, SE, α,β
Output: Fitness of individual
begin
Calculate score of individual
Normalize score to the same range of SE
Fitness of individual ←(α×score )+(β×SE)
end
In this work, it was opted to normalize the score(X)value to the same interval
ofSE(X), in order to avoid a possible unbalance among these values during the
optimization process. Given this context, the only method to prioritize one function
over the other is by assigning values to the weights αandβin the ﬁtness function.
The score(X)normalization is described as follows:
normalizedScore (X)=/parenleftbiggscore(X)
score max/parenrightbigg
×SEmax, (6)
where, Xrepresents a solution, score maxis the highest value in terms of score which
a solution can have and SEmaxis the highest grade which the subjective evaluation
can assume.
3.2 Interactive modeling
There have been several studies that explore Interactive Optimization in SBSE, as will
be discussed in Sect. 5.2. In recent years there has also been an increase in the number
123
Autom Softw Eng (2017) 24:623–671 633
of empirical studies involving interactive optimization application to the Software
Engineering. As stated in Glass (2002 ), “the most important factor in software work
is not the tools and the techniques used by the programmers, but rather the quality ofthe programmers themselves”.
However, there is a challenge in designing suitable environments to exploit these
human qualities in the search process, given that the user knowledge can be expressedin a variety of forms. This paper considers the understanding of the whole process
regarding the human interaction and his/her inﬂuence in the optimization process to
be a very important issue. As discussed in Aljawawdeh et al. (2015 ), the nature of the
implicit preference and the moment in which this aspect will be exploited during the
optimization process are the main aspects to be modelled in an interactive optimization
approach.
Therefore, three main questions are proposed and, consequently discussed, to deﬁne
the Interactive Modeling for the iNRP.
1.At which moment are the preferences from the DM captured?
As previously discussed, the moment of preference incorporation in the searchprocess ranges from no-preference, a priori, a posteriori and interactively ( Miet-
tinen et al. 2016 ). To the present proposal, the DM will interactively provide
subjective evaluations for a previously deﬁned number of individuals generatedby the IGA. This number of interactions is a architectural setting deﬁned before
to start the search process, which depends on DM availability. Given a scenario
delimited by 50 interactions and a population with 50 individuals, for example,all the 50 individuals from the ﬁrst generation of the IGA will be evaluated by the
DM. On the other hand, from the ﬁrst individual of the second generation until the
end of the IGA, the individuals will be evaluated by the learning model. Similarly,in a hypothetical case of 50 human interactions and 25 individuals, the ﬁrst two
generations of the IGA will have all individuals evaluated by the DM, and the
Learning Model will start evaluating individuals from the third generation and on.
At the end of the process, the best ﬁnal solution is presented to the DM.
2.What type and which preferences are provided to the search process?
Following the assumptions detailed in Aljawawdeh et al. (2015 ), the nature of
human preferences are explicit or implicit. As stated before, this work explores
both types, being “explicit” the result of the objective measure score function and
“implicit” the subjective evaluation provided by the DM (SE). The ﬁrst one reﬂects
the overall stakeholders satisfaction, while the second encapsulates several of the
DM’s subjective concerns regarding the presented solution. This value must bewithin a predeﬁned range representing “how good” he/she thinks that selection is.
The DM is not actually looking for a precisely deﬁned numerical target but rather
for a subset of the search space that gives the general impression he/she is lookingfor. The fuzzy aspect of human subjectivity is therefore more to be taken as a basis
for robustness than as a source of trouble ( Semet 2002 ).
3.How the preferences are incorporated and inﬂuence the search process?
Human preferences can be incorporated and inﬂuence the search process in several
ways. In Aljawawdeh et al. (2015 ) six potential design pattern abstractions consid-
ering the combination between the moment and type of preferences (explicit and
123
634 Autom Softw Eng (2017) 24:623–671
implicit) are presented. One of them inspired this work, and it is called “Implicit
preference, interactive”, in which the deﬁnition states: “users are offered oppor-
tunities to input preference to metaheuristic search either as qualitative evaluationsolely or in combination with quantitative objective ﬁtness functions”. In the
present study, the subjective evaluation will be exploited as an objective alongside
the explicit metric score in the ﬁtness function, guiding the search to areas which
maximizes both the DM’s evaluations and score values. In addition, as will be fur-
ther discussed in the Related Work (Sect. 5), there are other ways to incorporate the
DM’s opinion in the search process beyond being an objective to be maximized.
The Interactive Modeling described above enables the DM to incorporate his/her
knowledge during the search process. Generally, three interesting beneﬁts arise fromthe usage of Interactive Evolutionary Computation:
(a) As the DM expresses his/her preferences during the evolutionary process, the DM
will receive some feedback from the search through the presentation of the mostpromising solutions throughout the algorithm evolution. Since the solution ﬁtness
considers both implicit and explicit preferences ( Aljawawdeh et al. 2015 ), it can
be conjectured that, as the solutions are evaluated by the DM, there will be anatural trend for the new generations to be composed of solutions which better
suits his/her subjective needs.
(b) Another interesting beneﬁt that can be highlighted is the capability to incorporate
the changes in the DM’s criteria during the search process. As stated in Miettinen
et al. (2016 ), “an important advantage of interactive methods is learning”. Given
the visualization of the solutions inﬂuenced by the DM opinion, he/she will pro-
gressively gain more consciousness of how attainable or feasible the preferences
are. Consequently, he/she may get some insights about the problem, and evenadapt the decision criteria. The proposed approach is able to deal with this aspect.
This beneﬁt reinforce one of the major goals of search based requirements opti-
mization, that is to provide meaningful insights to the DM ( Zhang et al. 2008 ). In
the case of a change in the DM decision criteria using an a priori approach, the
algorithm would have to be executed again to cope with the new preferences.
(c) At last, besides incorporating human knowledge, another important aspect of
the interactive approach is human engagement. Intuitive interaction with domain
specialists is a key factor in industrial applicability, since it makes the system more
usable and more easily accepted in an industrial setting ( Marculescu et al. 2015 ).
As presented in the Introduction, when the DM is not employed in the resolution
process, he/she may feel excluded in the analysis, which may cause resistance and
lack of conﬁdence in the ﬁnal result ( Miettinen 1999 ,Shackelford 2007 ).
3.3 Learning modeling
Machine Learning deals with the issue of how to build programs that improve their
performance at some task through experience ( Mitchell 1997 ). The ﬁeld of Software
Engineering turns out to be a fertile ground where many software development and
maintenance tasks could be formulated as learning problems and approached in terms
of learning algorithms ( Zhang and Tsai 2003 ).
123
Autom Softw Eng (2017) 24:623–671 635
According to the work in Witten and Frank (2005 ), there are basically four different
styles of learning. In classiﬁcation learning , the learning scheme is presented with a set
of classiﬁed examples from which it is expected to learn a way of classifying unseenexamples. In association learning , any association among features is sought, not only
the ones that predict a particular class value. In clustering , groups of examples that
belong together are sought. In numeric prediction , the outcome to be predicted is not
a discrete class but a numeric quantity.
This paper presents the idea of gathering the DM’s implicit preferences during
the algorithm evolution through subjectively evaluating a certain number of individ-uals, which in the NRP perspective are the releases. A machine learning model is
used alongside the search to learn the human behavior and, eventually, replace it in
the remainder of the process. In other words, classifying unseen examples following
the learnt user proﬁle. This strategy enables the system to absorb the user’s implicit
knowledge without requiring the user to formalize this information a priori ( Shack-
elford 2007 ). However, as discussed in Zhang and Tsai (2003 ), user modeling poses a
number of challenges for machine learning that have hindered its application in Soft-
ware Engineering, including: the need for large data sets; the need for labeled data;concept drift; and computational complexity.
Similarly to the Interactive Modeling, two major questions are discussed aiming to
deﬁne the learning modeling for the proposed iNRP:
1.Problem formulation
An important ﬁrst step is to formulate the problem in such a way that it conformsto the framework of a particular learning method chosen for the task. As stated
inMitchell (1997 ), “a computer program is said to learn from experience Ewith
respect to some class of tasks Tand performance measure P, if its performance
at tasks in T, as measured by P, improves with experience E”. Thus, to have a
well-deﬁned learning problem, one must identity these three features:
–T a s k T: evaluate a release considering the human preferences;
– Performance measure P: number of interactions;
– Training experience E: evaluating releases provided throughout the optimiza-
tion process.
2.Problem representation
The next step is to deﬁne a representation for both training data and knowledge tobe learned. The representation of the (a) input ,( b ) attributes and (c) output in the
learning task is often problem-speciﬁc and formalism-dependent ( Zhang and Tsai
2003 .
Following the concepts deﬁned in Witten and Frank (2005 ), the input to a machine
learning scheme is a set of samples. These samples are the things that need to
be classiﬁed, associated or clustered. Each sample is an individual, independentexample of the concept to be learned. The samples that provide the input to the
machine learning model are characterized by its values on a ﬁxed, predeﬁned set
of attributes. Also, it is important to highlight the presence of a special attributecalled class , which describes the goal to be learned.
Each training dataset is represented as a matrix of samples versus attributes, as
depicted in Fig. 3. As can be seen in such ﬁgure, there are three samples, with
123
636 Autom Softw Eng (2017) 24:623–671
r1r2r3r4 SEr5
11 1 83 0 0
34 01 01 0
74 11 01 0Attributes
SamplesClass
distinct values
Fig. 3 Example of training dataset
each row representing a possible release and each column representing a spe-
ciﬁc requirement (attribute). The exception is the last column at the right, which
represents the class obtained through the subjective evaluation provided by the
DM. As explained in Section 3.1,ri=0 implies that requirement riis included
in release and, ri=0 otherwise. For example, the ﬁrst solution represented by
X={1,1,0,1,0}is composed by requirements r1,r2and r4, and received a
SE(X)=83 from the DM. As explained in Sect. 2, this dataset will be iteratively
constructed until the number of interactions deﬁned by the DM in the architectural
settings is reached. After concluding, the training process is performed considering
the training dataset previously collected.
The output usually takes the form of predictions about new examples or clas-
siﬁcation of unknown examples. In the present context, the task is to evaluatereleases according to the human preferences. Thus, considering that the Learning
Model already learnt the user proﬁle, the output will be a subjective evaluation
to an unseen solution generated by the optimization process. As discussed above,both input and output follow a numerical range value established as a qualitative
“grade”. It is important to point out that the quality and the quantity of the data
needed are dependent on both the selected machine learning technique and thenumber of provided training samples.
At last, an important aspect to be distinguished is the type of learning, which can be
generally categorized as supervised and unsupervised. The ﬁrst one is basically asynonym for classiﬁcation, in which the learning will come from the labeled sam-
ples with a class in the training dataset, while the second is essentially a synonym
for clustering, and is unsupervised since the input samples are not labeled ( Han
et al. 2011 ). Thus, the proposed learning model follows the supervised learning
principles where each release is labeled with the respective class, deﬁned as the SE.
4 Empirical study
The following sections present all the details regarding the empirical study. As previ-ously mentioned, two experiments were performed, respectively named as (a) artiﬁcial
experiment and (b) participant-based experiment. Firstly, the metrics developed to
123
Autom Softw Eng (2017) 24:623–671 637
evaluate the outcome of these experiments are explained. Next, general settings used
in the experiments are presented, including the instances conﬁgurations, machine
learning techniques and IGA parameters, speciﬁcations of each experiment and theresearch questions proposed to be answered. Then, with all these details being properly
presented, the analyses and discussion of the achieved results are conducted. Finally,
threats that may affect the validity of the experiments are also discussed.
4.1 Metrics
In order to promote meaningful analyses, three metrics were developed to clarify the
results. It is believed such metrics are generic enough to be used in other research
projects that explore the concepts of interactive optimization in SBSE.
4.1.1 Similarity degree
The Similarity degree (SD) indicates a percentage of how similar a candidate solution
is when compared to the target solution. It is reasonable to consider that this metric
directly represents a subjective satisfaction, given that the closer a candidate solution
is from the target solution, the higher the subjective evaluation. Consider a set of
6 requirements with a target solution represented by P={1,0,0,0,1,1}and a
candidate solution represented by X={1,1,0,1,1,0}, for example. As one can see,
x
1,x3andx5are equal in both Pand X, thus, this candidate solution Xwould have a
SD(X,P)=3
6×100 % =50 %. This result is obtained by the following equation:
SD(X,P)=/parenleftBigg/summationtextN
i|1≤i≤N∧xi=pi1
N/parenrightBigg
×100 % , (7)
where Xis a candidate solution for which one wants to calculate its SD(X,P)in
relation to a target solution P.Nis the number of requirements, xiindicates whether
the requirement riis present in the solution or not, and piindicates if the requirement
ribelongs to the target solution or not.
4.1.2 Similarity factor
The Similarity factor (SF) indicates the proportional gain in SDwhen comparing a
solution with human inﬂuence and another one without human inﬂuence. For exam-
ple, consider two solutions Yand X. The ﬁrst solution with human intervention
has a SD(Y,P)=85.33, while The second one, without human inﬂuence, has a
SD(X,P)=54.27. Thus, the gain in SFachieved by Yover Xis 57.2 %. This value
is given by:
SF(Y,X,P)=/parenleftbiggSD(Y,P)
SD(X,P)−1/parenrightbigg
×100 % , (8)
123
638 Autom Softw Eng (2017) 24:623–671
where Yis the interactively generated solution, i.e., the search process is inﬂuenced
by implicit preferences, Xis the solution generated without considering subjective
evaluations and Pis the target solution.
4.1.3 Price of preference
The Price of preference (PP) shows how much is lost in explicit preference in order to
incorporate implicit preferences from the DM through SE. Consider two solutions Y
and X, for example. The ﬁrst one generated with human inﬂuence has a score(Y)=
99.89, and the second one, without human inﬂuence, has a score(X)=115.87.
Therefore, the PPloss of Yover Xis 16 %. This value is obtained by:
PP(X,Y)=/parenleftbigg
1−score(Y)
score(X)/parenrightbigg
×100 % , (9)
where Yis the solution considering subjective evaluations and Xis the fully automatic
solution, i.e., without any interaction.
4.2 Empirical study settings
This subsection initially presents the instances used in the experiments, including the
number of requirements and interdependencies, number of stakeholders and budget
deﬁnition. Then, the main concepts of the machine learning techniques employed in
the tests are presented, as well as their respective parameters. The IGA parameters are
also presented, alongside a brief discussion about the statistical techniques employedto analyze the obtained results.
4.2.1 Instances conﬁguration
The instances set is composed with both artiﬁcial and real-world data. The artiﬁcial
data was randomly generated and designed to represent different scenarios of next
release planning. The number of requirements varies between 50, 100, 150 and 200.The speciﬁc importance of each requirement is ranged from a discrete value between
1 and 5. The number of stakeholders was randomly generated within a discrete range
of 1 to 8. The weight of each stakeholder is given by a continuous random valuebetween 0 and 1, so that the sum of the weights of stakeholders is equal to 1. The
artiﬁcial instances name is in the format I_R, where R is the number of requirements.
For example, if an instance has 50 requirements, it will be named I_50.
The real-world instances employed in this empirical study are the same in Karim
and Ruhe (2014 ), respectively named as Word Processor and ReleasePlanner. The
ﬁrst one is based on a word processor software, being composed of 50 requirementsand 4 customers. The second one is based on a decision support system and has
25 requirements and 9 customers. For all instances, including the artiﬁcial ones, the
budget was considered to be 60 % of the maximum release cost.
123
Autom Softw Eng (2017) 24:623–671 639
Interdependencies between requirements present a considerable impact in the
search space of the NRP Carlshamre et al. 2001 ). The interdependency density indi-
cates the percentage of requirements that have dependencies in a particular instance.For the artiﬁcial instances, the interdependencies were randomly created at a den-
sity of 50 % following a procedure that is described next. Two requirements are
randomly chosen, and the type of interdependency (REQUIRE( r
i,rj) or AND( ri,
rj)) they will have between themselves is also chosen at random. This process is
repeated until the 50 % interdependency density is reached. For example, given a
release with 100 requirements, 50 requirements will have at least one type of inter-dependency and each of these requirements will have a maximum of 50 dependents.
The real-world instances have originally 82 and 40 % of interdependency density,
respectively.
4.2.2 Machine learning techniques settings
With respect to the details regarding the learning process, the API provided by the
Waikato Environment for Knowledge Analysis (WEKA) was emplyoed ( Hall et al.
2009 ). WEKA is an open source platform that is characterized by a high degree of
portability and modiﬁability. Aiming at having a more generic analysis, two techniques
based on different learning strategies were chosen: least median square (LMS) andmultilayer perceptron (MLP).
First of all, when the expected outcome and the attributes of a particular learning
model are numeric, linear regression can be considered a natural technique to beused. Linear Regression performs standard least-squares multiple linear regression
and can optionally perform attribute selection. Such feature is accomplished either by
greedily using backward elimination or by building a full model from all attributes anddropping the terms one by one, in a decreasing order of their standardized coefﬁcients,
until a stopping criteria is reached ( Witten and Frank 2005 ). The usual least-squares
regression models are seriously affected by outliers in the data. As an attempt tominimize this issue, the least median square (LMS) is a robust linear regression method
that minimizes the median (rather than the mean) of the squares of divergences from
the regression line ( Rousseeuw 1984 ). It repeatedly applies standard linear regression
to subsamples of the data and outputs the solution that has the smallest median-squared
error.
As deﬁned in Witten and Frank (2005 ), the main idea is to express the class as a
linear combination of the attributes, with predetermined weights:
x=w
0+w1a1+w2a2+···+ wkak, (10)
where xis the class; a1,a2,..., akare the attribute values; and w0,w1,...,w kare
the weights. These weights are calculated from the training data. The ﬁrst training
sample will have a class, say x(1), and attribute values a(1)
1,a(1)
2,..., a(1)
kwhere the
superscript denotes that it is the ﬁrst sample. Moreover, it is notationally convenient
to assume an extra attribute a0, with a value that is always 1.
123
640 Autom Softw Eng (2017) 24:623–671
The predicted value for the ﬁrst example’s class can be written as:
w0a(1)
0+w1a(1)
1+w2a(1)
2+···+ wka(1)
k=k/summationdisplay
j=0wja(1)
j, (11)
The difference between the predicted and actual values for a certain sample is of
particular interest for the LMS technique. The method of linear regression chooses the
coefﬁcients wjto minimize the median of the sum of the squares of these differences
over all the training samples. Suppose there are ntraining samples; denote the ith one
with a superscript (i). Then, the sum of the squares of the differences is:
n/summationdisplay
i=1⎛
⎝x(i)−/summationdisplay
j=0wja(i)
1⎞
⎠2
, (12)
where the expression inside the parentheses is the difference between the ith samples’s
actual class and its predicted class. The median of the sum of squares is what has to
be minimized through a properly selection of coefﬁcients.
The multilayer perceptron (MLP) is a neural network that is trained using the
backpropagation algorithm and, consequently, is capable of expressing a rich variety
of nonlinear decision surfaces. Its main characteristics are: (a) the model of each neuronor network processing element has a nonlinear activation function; (b) it presents, at
least, one intermediate layer that is not part of the input or output; and (c) it has a high
degree of connectivity between its network processing elements, deﬁned thorough thesynaptic weights ( Haykin 2001 ).
The backpropagation algorithm is able to learn the weights for a given multilayer
network with a ﬁxed set of units and interconnections. It is a version of the genericgradient descent, which attempts to minimize the squared error between the network
output values and the target values for these outputs ( Witten and Frank 2005 ). To use
gradient descent to ﬁnd the weights of a multilayer perceptron, the derivative of thesquared error must be determined with respect to each weight in the network. Accord-
ing to Mitchell (1997 ), the gradient speciﬁes the direction of the steepest increase of
the error E, and the weight update rule is be given by:
w
i←wi+Δw i, (13)
where
Δw i=η/summationdisplay
d∈D(td−Od)xid, (14)
being nas a positive constant called the learning rate, which determines the step size in
the gradient descent search. Dis the dataset of training samples, tdis the target output
for training samples d, and odis the output of the linear unit for training samples d.
Thus, xiddenotes the single input component xifor a training samples d.
123
Autom Softw Eng (2017) 24:623–671 641
Table 1 Machine learning techniques settings
Learning technique Parameter settings
Least median square Sample size (-S): 4
Used seed to generate samples (-G): 0Multilayer perceptron learning rate (-L): 0.3
Momentum rate (-M): 0.2
Number of epochs (-N): 500
Multilayer perceptron percentage size of validation set (-V): 0
The used value to seed the random number generator (-S): 0
The consecutive number of errors allowed for validation,
testing beforethe network terminates (-E): 20
The hidden layers to be created for the network
(-H): (attributes + classes) / 2
The work in Mitchell (1997 ) summarizes the gradient descent algorithm for training
linear units as follows: pick an initial random weight vector, apply the linear unit to
all training examples, then compute Awifor each weight according to Eq. 14. Update
each weight wiby adding Awi, then repeat this process.
Finally, Table 1presents the machine learning techniques parametrization used in
the empirical tests. More details regarding each parameter are available in Witten and
Frank (2005 ).
4.2.3 Interactive genetic algorithm settings
All IGA’s settings were empirically obtained by preliminary experiments. They are:
number of individuals that is double of the number of requirements; 100 generations;
90 % crossover rate; 1 /Nmutation rate, where Nis the number of requirements; 20 %
of elitism rate.
To account for the inherent variation in stochastic optimization algorithms, for
each weight conﬁguration, number of interactions, instance and machine learningtechnique, the IGA was executed 30 times, collecting both quality metrics ( SD,SF
and PP) and respective averages from the obtained results. In the end, more than
55,000 executions of the IGA were performed.
Aiming at analyzing the results in a sound manner, guidelines suggested by Arcuri
and Briand (2011 ) and the statistical computing tool R ( R-Project 2016 ) were consid-
ered. Statistical difference is measured with the Mann–Whitney Utest considering a
95 % conﬁdence level, while the Vargha and Delaney’s ˆA
12test is used to measure
theeffect sizes .T h e ˆA12statistics measures the probability that a run with a particular
algorithm 1 yields better values than a algorithm 2. This work assumes LMS 1and
MLP 2; therefore, ˆA12=0 . 2 6i n score , for example, indicates that, in 26 % of the time
the LMS 1reaches higher values than MLP 2. If there is no difference between two
algorithms performances, then ˆA12= 0.5. All instances and results of the empirical
123
642 Autom Softw Eng (2017) 24:623–671
study, including the ones that had to be omitted due to space constraints, are available
online.1
4.3 Experimental design
The empirical study realized in this work was divided in two different experiments:
Artiﬁcial and Participant-based. Basically, the ﬁrst one is conducted with a simulator
representing the DM, while the second one employs software engineering practition-ers.
4.3.1 Artiﬁcial experimentIn order to represent the DM’s role during the IGA interaction, a simulator was devel-
oped. The main purpose of this simulator is not to faithfully simulate a human being,but rather demonstrate the inﬂuence of a certain evaluation proﬁle in the search process
when faced with different scenarios of next release planning.
Similar to the used idea in Shackelford and Corne (2004 ) and Tonella et al. (2013 ),
the human tacit assessment is simulated by creating a “target solution” which repre-
sents a solution that the DM would consider “ideal” or “gold standard”. Such solution
has the same structure of an individual and, consequently, comprises of a subset of
the selected requirements to be implemented, respecting the budget and the interde-
pendency constraints. The requirements present in the target solution are randomlychosen. In this experiment all instances were tested, where each one has the same
speciﬁc target solution for the all 30 algorithm executions.
Throughout the interactions in the search process, the simulator provides a SE
considering the target solution that is established for the respective instance. The
evaluation given to a particular solution is proportional to how ideal it is, attending the
implicit preference encompassed by the target solution. If the included requirementsin a candidate individual are totally different from the target solution, the evaluation
is minimal. On the other hand, when the individual is equal to the target solution,
the evaluation is maximum. The evaluations are proportionally given for the otherpossibilities. Thus, the following equation is used by the simulator to determine a SE
for a candidate solution:
simulatedSE (X,P)=/parenleftBigg/summationtext
N
j|1≤j≤N∧xj=pj1
N/parenrightBigg
×SEmax, (15)
where, Xis a candidate solution, Pis the target solution, and Nis the total number
of requirements in the solution. Decision variable xjindicates if the requirement rj
is included in the candidate solution, while pjindicates whether the requirement rj
belongs to the target solution P. Finally, SEmaxis the highest value SE can assume.
For this empirical study, the minimum simulatedSE (X,P)is 0 and the maximum is
100.
1http://goes.uece.br/allyssonaraujo/architecture4inrp .
123
Autom Softw Eng (2017) 24:623–671 643
4.3.2 Participant-based experiment
This experiment aims at verifying the behavior and feasibility of the architecture when
it is used by software engineering practitioners. A group of 5 participants was invited
to act as decision makers in the experiments. The members of the group have between
2 and 10 years of experience in the software engineering industry, adding up to a totalof 30, and an average of 6 years of experience. Regarding the software development
experience, on a scale of “low”, “moderate”? and “high”, four participants rated at
“moderate” and one at “high”. In terms of experience in requirements selection, fourparticipants rated at “moderate” and one at “low”. All participants have graduated in
computer science or related areas, and have received or are ﬁnishing post-graduate
degree.
Before the experiment, each participant was separately briefed about (i) the task
they were supposed to perform, which was selecting requirements to be implementedin the next release, (ii) the architecture and interactive approach they would be using
and (iii) the Word Processor instance they would be working with. More speciﬁcally,
at ﬁrst moment, the details of the next release problem were explained, includingits motivation and major aspects such as multiple stakeholders, score and budget
constraint. After this step, the proposed approach was presented through the expla-
nation of all components depicted in Fig. 1. In the ﬁnal stage, initially a period of
tool familiarization with a simple NRP instance was given and, ﬁnally, the Word
Processor instance was presented. This real-world instance was chosen because it
presents more details than ReleasePlanner, thus, being considered to be more intu-itive. This explanation lasted between 35 and 40 minutes. Based on these details, a
simple requirements speciﬁcation document was produced and handled for all par-
ticipants. This document consists of all requirement descriptions, the importancevalues given by the stakeholders and implementation efforts. Also, the total avail-
able budget and the relevance of each stakeholder to the company was presented. Due
to the space constraint, just a sample piece of this document is reproduced in the
Table 2.
Table 2 A sample piece of the requirements speciﬁcation document shown to the participants
Budget 850.20 Weight 9 3 5 7
# Description Cost Md. Fazlul Jim Rick Samantha Total
Alam
ChowdhuryLi Bertuzzi Holmes
1 Create a new ﬁle 66.00 8 9 9 9 35
2 Open an existing ﬁle 76.00 8 9 9 9 35
3 Close current ﬁle 11.00 8 9 9 1 27
4 Save a ﬁle 63.00 8 9 9 9 35
........................
50 Search a text
in the document…7.00 1 7 9 6 23
123
644 Autom Softw Eng (2017) 24:623–671
Fig. 4 Graphical user interface
Next, it was explained to each participant that he/she would need to perform the
role of a requirements engineer in a hypothetical company in which the software to
be developed is a Word Processor. Then, each participant would use the presented
tool to select a set of requirements to be implemented in the next release, wherethe subjective evaluations could be based on the information detailed in the require-
ments speciﬁcation document. As an attempt to assess the behavior of the approach
when facing different evaluation proﬁles, participants had complete freedom to adoptany judgement criteria when giving subjective evaluations to each candidate solu-
tion.
Many of the issues faced when implementing an IEC technique can be resolved or
avoided by careful design and evaluation of the existing experience and environment
of the potential users. Good visualization is a key to the success of the IEC; therefore,signiﬁcant effort should be put into the user interface design ( Shackelford 2007 ). Thus,
a graphical user interface (GUI) was developed to provide a better and user-friendly
environment for the participants to interact with the implemented approach. As seenin Fig. 4, the requirements included in a solution to be evaluated are presented in
the left-hand side, while the not selected requirements are displayed in the right-hand
side. The participant gives the SE to the solution by adjusting the slide on the bottomof the screen. As it was previously explained, a set of solutions will be iteratively
presented to the participant until the number of interactions is reached. In the end, the
ﬁnal solution is presented to the participant.
As stated earlier, before the search process, it is necessary to deﬁne the weights α
andβin the ﬁtness function, regarding the inﬂuence of the score function and the SE
value during the search process, respectively. It is also necessary to indicate how manyinteractions the DM will perform during the optimization process. Speciﬁcally, this
empirical study used α=1 andβ=1 as weight conﬁguration in the ﬁtness function,
and a total of 50 interactions for each participant was established.
123
Autom Softw Eng (2017) 24:623–671 645
interactions10
GAinteractions+10
interactions+10
interactions+10
interactions+10Sp
0
Sp
50
Sp
40
Sp
30
Sp
20
Sp
100IGA10 IGA20IGA30 IGA40 IGA50pppp p pTraining set
Evolutionary process
without interactionsHuman evaluations Model evaluations Training process
Fig. 5 Participant-based experiment procedure
In order to better exploit the experiments performed with the participants, a simple
procedure was designed to evaluate the approach’s performance when used with adifferent number of interactions, even though each participant interacted with the sys-
tem only 50 times. Such a procedure is depicted in Fig. 5, and consists in training the
learning model at different interactive cycles during the optimization process, conse-quently, replacing the DM at different moments. Consider IGA
p
i, where prepresents
the number of the participant and ithe number of interactions using the interactive
genetic algorithm. Similarly, Sp
irepresents the solution generated by a participant p
with iinteractions. This way, the results with different number of interactions can be
properly compared.
At ﬁrst, a solution without any human inﬂuence is generated, called Sp
0. As can be
seen, there are no interactions at this point; therefore, a standard Genetic Algorithm
is used. This solution will be used to conduct comparisons between interactive andnon-interactive methods, and also to provide the score
maxto be used in the score
normalization (Eq. 6). After this solution is captured, the ﬁrst interactive cycle com-
posed by the ﬁrst 10 interactions is initiated. As previously presented, each interactionrepresents a candidate solution that receives a SE from the DM. Consequently, these
10 solutions and their respective subjective evaluations are included in the training
dataset. Then, the remainder of the evolutionary process will be realized consideringa learning model constructed with these 10 samples until a ﬁnal solution S
p
10is found.
After these ﬁrst 10 interactions are reached, the same population continues to be used
for a second interactive cycle of 10 interactions. At this point, the training dataset willbe composed by the samples collected in the ﬁrst one 10 interactions complemented
by the 10 more samples captured in the second cycle. Naturally, the Learning Model
will be constructed in this moment considering 20 samples to ﬁnd a ﬁnal solution Sp
20.
This process is repeated until the 50 interactions are reached, in other words, the ﬁve
cycles of 10 interactions is completed. In the end, 6 different solutions are found for
each participant p, considering the different number of interactions i.
123
646 Autom Softw Eng (2017) 24:623–671
4.3.3 Research questions
Three research questions were designed to assess and analyze the behavior of the
proposed approach. They are presented as follows:
–RQ 1(Sanity check) :Does the proposed architecture incorporate the subjective
evaluations in the ﬁnal solution?
In order to answer this question, there is analyzed if the ﬁnal solutions are
inﬂuenced by the interactions throughout the evolutionary process. The metricemployed was the Similarity Degree , where percentually represents how similar
a candidate solution is when compared to the target solution. Given the fact that
is a exhaustive test which uses a huge number of interactions and weight conﬁgu-rations, just the Artiﬁcial Experiment was considered and, consequently, just the
simulator.
–RQ
2(Interactivity trade-off) :What is the trade-off between the DM’s satisfaction
and the impact on score values?
As explained in Sect. 3, the DM preferences towards the selected requirements
may be not inﬂuenced by the score value. Thus, it is natural to have some trade-
off regarding the loss in the overall stakeholder’s satisfaction to achieve a better
solution from the DM subjective point of view. The metrics used to evaluate this
aspect were the PPand SF. The ﬁrst one indicates how much is lost in explicit
information in order to incorporate implicit preferences, while the second metric
shows the gain, in percentage terms, in SDby comparing a solution with DM inﬂu-
ence and another one without DM inﬂuence. Such as previous research question,
only the Artiﬁcial Experiment was considered.
–RQ 3(Comparison) :Does the proposed architecture improve the DM’s satisfac-
tion when compared to the non-interactive approach?
To answer this question, a participant-based experiment was conducted aiming
to verify the feasibility of the architecture when it is used by professionals withdifferent evaluation proﬁles. The metrics ﬁtness ( F), SE, SFand PPwere used
in the experiment. The SDit was not considered because there is not a previous
target solution to represents the subjectively ideal solution to each participant, aswell as simulated in the artiﬁcial experiment.
4.4 Results and analysis
The results for the empirical study are presented in this section using the analysis of
the previous three presented research questions.
–RQ
1(Sanity check) :Does the proposed architecture incorporate the subjective
evaluations in the ﬁnal solution?
The analysis conducted in this question aims at investigating the inﬂuence of the
number of interactions in the evolutionary process and, consequently, the Similarity
Degree (SD) achieved by the ﬁnal solution. The higher SDvalue, more similar the
candidate solution is to the target solution. Table 3presents the average SDvalue for
the 30 runs of the IGA with different number of interactions, considering all instances
123
Autom Softw Eng (2017) 24:623–671 647
Table 3 Average of similarity degree values for 30 runs and different number of interactions ( i)u s i n gL M S 1and MLP 2withα=1a n dβ=1
i I_50 I_100 I_150
p LMS 1 p MLP 2ˆA12 p LMS 1 p MLP 2ˆA12 p LMS 1 p MLP 2ˆA12
10 – 62.00 – 65.47 0.26 – 55.23 – 57.57 0.30 - 58.62 – 58.64 0.48
20 /triangle 63.00 /trianglesolid 68.73 0.22 /triangle 55.33 /trianglesolid 61.40 0.15 /triangle 58.73 /triangle 60.29 0.35
30 /trianglesolid 69.40 /triangle 70.53 0.43 /trianglesolid 58.67 /triangle 63.80 0.20 /triangle 60.07 /trianglesolid 62.38 0.33
40 /trianglesolid 74.20 /triangle 70.73 0.71 /trianglesolid 62.00 /trianglesolid 67.40 0.19 /trianglesolid 62.80 /triangle 63.31 0.46
50 /trianglesolid 79.60 /triangle 72.07 0.86 /trianglesolid 67.73 /triangle 68.57 0.43 /trianglesolid 64.78 /triangleinv 61.98 0.69
60 /triangle 80.47 /triangle 73.53 0.86 /trianglesolid 71.03 /triangleinv 67.20 0.59 /trianglesolid 67.67 /triangle 64.93 0.64
70 /triangleinv 79.67 /trianglesolid 76.67 0.69 /trianglesolid 75.37 /triangle 68.10 0.78 /triangle 68.98 /triangle 65.33 0.66
80 /triangleinv 79.60 /triangledownsld 73.13 0.85 /triangle 78.50 /trianglesolid 71.80 0.79 /trianglesolid 71.40 /triangleinv 64.51 0.84
90 /triangle 80.47 /triangle 74.80 0.83 /trianglesolid 82.30 /triangle 73.00 0.90 /trianglesolid 75.09 /triangle 66.38 0.95
100 /triangle 81.93 /triangle 75.53 0.88 /triangle 83.47 /triangle 74.57 0.90 /triangle 75.96 /triangle 67.04 0.90
200 /triangleinv 80.53 /triangle 76.80 0.72 /triangle 83.83 /triangleinv 73.83 0.90 /trianglesolid 83.53 /triangle 69.73 0.99
300 /triangle 82.27 /triangleinv 76.00 0.83 /triangle 84.17 /triangleinv 72.50 0.94 /triangle 84.00 /triangleinv 69.53 0.98
400 /triangle 83.07 /triangle 76.40 0.87 /triangleinv 82.43 /triangleinv 70.33 0.91 /triangleinv 83.09 /triangle 71.04 0.92
500 /triangleinv 81.73 /trianglesolid 78.40 0.71 /triangle 83.43 /triangle 71.83 0.97 /triangle 84.16 /triangleinv 68.09 1.00
123
648 Autom Softw Eng (2017) 24:623–671
Table 3 continued
i I_200 Word Processor ReleasePlanner
p LMS 1 p MLP 2ˆA12 p LMS 1 p MLP 2ˆA12 p LMS 1 p MLP 2ˆA12
10 – 55.08 – 56.30 0.39 – 49.33 – 56.67 0.20 – 70.93 – 76.53 0.33
20 /triangleinv 54.48 /triangle 56.73 0.29 /trianglesolid 56.53 /trianglesolid 66.60 0.18 /trianglesolid 77.33 /trianglesolid 82.93 0.33
30 /trianglesolid 56.72 /triangleinv 56.72 0.46 /trianglesolid 60.80 /trianglesolid 69.33 0.22 /trianglesolid 90.00 /triangle 83.73 0.72
40 /triangle 58.25 /trianglesolid 60.00 0.35 /trianglesolid 75.33 /triangle 69.80 0.72 /triangle 91.33 /triangle 84.53 0.72
50 /trianglesolid 60.77 /triangle 61.75 0.43 /trianglesolid 82.93 /triangle 71.80 0.99 /triangle 94.53 /triangleinv 84.40 0.85
60 /triangleinv 60.50 /triangleinv 60.65 0.48 /triangleinv 81.87 /triangleinv 71.73 0.96 /triangleinv 90.80 /triangleinv 82.93 0.75
70 /trianglesolid 63.63 /triangleinv 59.97 0.65 /triangle 82.27 /triangle 72.13 0.94 /triangleinv 88.80 /triangle 84.67 0.64
80 /trianglesolid 65.47 /triangle 60.85 0.71 /triangle 82.80 /triangleinv 70.33 0.99 /triangle 90.40 /triangleinv 84.67 0.64
90 /trianglesolid 68.10 /triangle 62.50 0.71 /triangleinv 82.73 /triangle 72.40 0.96 /triangle 92.67 /triangleinv 84.40 0.74
100 /triangle 69.98 /triangleinv 61.90 0.82 /triangleinv 82.40 /triangle 73.00 0.94 /triangleinv 90.40 /triangleinv 84.27 0.71
200 /trianglesolid 82.27 /triangle 65.72 0.98 /triangle 82.60 /triangle 73.73 0.97 /triangle 92.40 /triangle 85.73 0.72
300 /triangle 83.37 /triangleinv 64.85 1.00 /triangle 82.93 /trianglesolid 75.33 0.96 /triangleinv 90.67 /triangle 89.07 0.56
400 /triangleinv 82.35 /triangle 65.65 0.99 /triangleinv 82.07 /triangle 77.40 0.76 /triangleinv 90.53 /triangle 90.67 0.50
500 /triangle 82.82 /triangleinv 65.47 0.99 /triangle 82.13 /triangleinv 76.00 0.81 /triangleinv 87.07 /triangle 93.60 0.32
The pcolumn indicates statistical difference between SDvalues considering a 95 % conﬁdence level, where the symbol /trianglemeans the average is higher but not signiﬁcantly
different, /triangleinv(lower but not signiﬁcantly different), /trianglesolid(signiﬁcantly higher) and /triangledownsld(signiﬁcantly lower). The ˆA12column presents the effect size measure for the two machine
learning techniques with the same number of interactions. Values in bold represent statistical difference
123
Autom Softw Eng (2017) 24:623–671 649
40%50%60%70%80%90%100%
 0  100  200  300  400  500Similarity Degree
Number of interactionsI_50
I_100
I_150I_200
Word Processor
ReleasePlanner
40%50%60%70%80%90%100%
 0  100  200  300  400  500Similarity Degree
Number of interactionsI_50
I_100
I_150I_200
Word Processor
ReleasePlanner
(a) (b)
Fig. 6 Relation between similarity degree and number of interactions. aMachine learning technique: LMS.
bmachine learning technique: MLP
Table 4 Number of interactions for each instance and machine learning technique
I_50 I_100 I_150 I_200 Word Processor ReleasePlanner
LMS 60 100 200 300 50 40
MLP 200 200 200 200 200 200
and the two machine learning techniques. The sets of 30 values for each particular
number of interactions are statistically compared in order to assess whether a different
number of interactions yield a statistically different SDvalue.
When looking at the LMS results, it is possible to notice that, for every instance,
there is no signiﬁcant gain in SDafter a certain number of interactions which, varies
according to the instance size. For example, considering I_50, this stability is achieved
after 50 interactions, while 200 interactions are needed for I_200. Such behavior can
be visualized in Fig. 6(a), which shows the considerable SDincrease in the ﬁrst 200
interactions and, after a while, the mentioned stabilization.
Regarding the MLP results, the signiﬁcant gains in SDcan be veriﬁed and only
occur using a smaller number of interactions. The non-signiﬁcant differences for bigger
numbers of interactions make it difﬁcult to precisely identify the moment in which SD
stabilizes. As presented in Fig. 6(b), such stabilization occurs at about 200 interactions
for most of the instances. Furthermore in this analysis, LMS outperforms MLP in 80 %
of the results. However, the MLP outperforms the LMS in 92 % when the number ofinteractions is <30.
The previously mentioned stabilization is closely associated with the machine learn-
ing technique and the size of the instance. Therefore, it is natural that the bigger theinstance, the more difﬁcult it will be to learn the implicit preferences. These stabiliza-
tion values will be used in the next two research questions. For instance, for I_50 using
LMS, the conducted analyses will be performed with a ﬁxed number of 60 interactionsbecause this is the number in which occurs the stabilization of the SDfor this instance
and machine learning technique. The number of interactions deﬁned to all instances
and machine learning techniques which will be further used are presented in Table 4.
123
650 Autom Softw Eng (2017) 24:623–671
In conclusion, it was observed the SDvalue has an intrinsic relation with the number
of interactions. This can be explained because as the number of interactions increase,
more samples (individuals and SEvalues) are included in the training dataset. Conse-
quently, with a learning model properly adjusted, it is natural that the ﬁnal solutions
will be more similar to the target solution, i.e., more suitable in subjective aspects. In
turn, these conclusions suggest the proposed approach passes the sanity check whenit demonstrates that the implicit preferences given by the simulator are incorporated
in the solutions.
–RQ
2(Interactivity trade-off) :What is the trade-off between the DM’s satisfaction
and the impact on score values?
As previously discussed, it is natural to have some trade-off related to the loss in
score to achieve a better solution in terms of the DM’s subjective satisfaction. Thus,
to provide an analysis of this trade-off, two complementary results will be detailed:t h eg a i ni n Similarity factor (SF) and the loss in Price of preference (PP). Through
the ﬁrst metric it is possible to measure the proportional gain in SD, while the second
helps to shed light in the expected impact in score .
First, the SFresults will be analyzed while the βweight is increased in the ﬁtness
function. A higher SFvalue indicates a higher gain in SD. The experiments were
performed in such a way that the αweight of the score function was ﬁxed as 1, and
theβweight of the SE function was varied from 0 to 1 with uniform intervals of 0.1. In
other words, different scenarios are considered, ranging from no inﬂuence of the DM
(α=1 andβ=0) to conﬁgurations in which the subjective evaluation is equivalent
to the score function ( α=1 and β=1). Table 5presents the average of SFvalues
for 30 runs, considering different βweights and using both LMS and MLP.
For example, analyzing the real-word instance Word Processor in the conﬁguration
ofβ=0.5, a SFvalue of 23.35 and 27.32 % was obtained for LMS and MLP,
respectively. When the βweight was doubled to 1, the SFvalues had a considerable
increase of 72.12 and 54.89 %, respectively. Generally, when comparing solutions
without DM inﬂuence ( α=1 andβ=0) and solutions inﬂuenced by him/her ( α=1
andβ=1), the average gain of SFfor all instances using LMS is 53.78 %, while using
MLP is 34.06 %. The results for all instances are similar, which clearly demonstrates
that SFvalues signiﬁcantly grows as βincreases.
Figure 7shows the SFvalues obtained with LMS and MLP considering the propor-
tional increase in β. These results reinforces the capability of the proposed approach
at incorporating the subjective evaluations and, consequently, satisfy the DM’s pref-
erences. In addition, it shows that the inﬂuence of the DM’s preferences in the searchprocess can be adjusted by the weights conﬁguration in the ﬁtness function according
to speciﬁc needs of different software projects.
As previously discussed, the incorporation of the DM subjective knowledge in the
ﬁnal solution usually accrues a loss in the score function. Therefore, an analysis of
thePrice of Preference (PP) is suitable. A higher PPvalue indicates a bigger loss
inscore . Similarly to the SFanalysis, the experiments were performed in which the
αweight of the score function was ﬁxed in 1, and the βweight of the SE function
varied from 0 to 1 with 0.1 intervals. Table 6presents average PPvalues for 30 runs,
considering different conﬁgurations of βweight and using LMS and MLP.
123
Autom Softw Eng (2017) 24:623–671 651
Table 5 Average of SFvalues for 30 runs and different settings of βusing LMS 1and MLP 2
β I_50 I_100 I_150
pLMS 1 pMLP 2ˆA12 pLMS 1 pMLP 2ˆA12 pLMS 1 pMLP 2ˆA12
0.1/trianglesolid5.27% /trianglesolid10.54% 0.34 /trianglesolid8.80% /trianglesolid10.30% 0.45 /trianglesolid5.62% /trianglesolid5.57% 0.50
0.2/triangle4.21% /trianglesolid6.39% 0.40 /trianglesolid17.60% /trianglesolid22.60% 0.32/trianglesolid17.39% /trianglesolid6.97% 0.85
0.3/trianglesolid12.24% /trianglesolid10.19% 0.58 /trianglesolid25.49% /trianglesolid21.05% 0.63 /trianglesolid19.69% /trianglesolid10.55% 0.79
0.4/trianglesolid21.65% /trianglesolid15.26% 0.65/trianglesolid31.47% /trianglesolid30.24% 0.54 /trianglesolid27.80% /trianglesolid9.64% 0.94
0.5/trianglesolid22.25% /trianglesolid21.38% 0.54 /trianglesolid37.84% /trianglesolid30.38% 0.64/trianglesolid32.47% /trianglesolid13.21% 0.91
0.6/trianglesolid24.70% /trianglesolid19.09% 0.63 /trianglesolid40.25% /trianglesolid32.00% 0.69/trianglesolid33.92% /trianglesolid14.55% 0.91
0.7/trianglesolid32.51% /trianglesolid23.75% 0.67/trianglesolid47.98% /trianglesolid37.94% 0.67/trianglesolid37.37% /trianglesolid18.89% 0.92
0.8/trianglesolid30.98% /trianglesolid21.32% 0.71/trianglesolid52.81% /trianglesolid34.73% 0.84/trianglesolid41.42% /trianglesolid17.38% 0.95
0.9/trianglesolid28.01% /trianglesolid26.07% 0.57 /trianglesolid49.73% /trianglesolid33.08% 0.81/trianglesolid42.28% /trianglesolid22.17% 0.88
1.0/trianglesolid36.86% /trianglesolid28.00% 0.69/trianglesolid56.68% /trianglesolid41.58% 0.79/trianglesolid46.46% /trianglesolid20.57% 0.97
β I_200 Word Processor ReleasePlanner
pLMS 1 pMLP 2ˆA12 pLMS 1 pMLP 2ˆA12 pLMS 1 pMLP 2ˆA12
0.1/trianglesolid8.41% /trianglesolid4.66% 0.66/triangle1.51% /triangle0.72% 0.55 /trianglesolid16.65% /trianglesolid12.47% 0.46
0.2/trianglesolid17.09% /trianglesolid9.26% 0.73/trianglesolid2.90% /trianglesolid5.77% 0.43 /trianglesolid28.54% /trianglesolid25.17% 0.48
0.3/trianglesolid23.38% /trianglesolid8.93% 0.87/trianglesolid8.99% /trianglesolid11.90% 0.39 /trianglesolid32.06% /trianglesolid31.26% 0.43
0.4/trianglesolid31.54% /trianglesolid14.23% 0.90/trianglesolid12.23% /trianglesolid18.15% 0.36 /trianglesolid33.11% /trianglesolid34.81% 0.29
0.5/trianglesolid37.64% /trianglesolid13.84% 0.98/trianglesolid23.35% /trianglesolid27.32% 0.36 /trianglesolid34.87% /trianglesolid33.78% 0.41
0.6/trianglesolid42.56% /trianglesolid12.27% 0.99/trianglesolid35.31% /trianglesolid39.22% 0.46 /trianglesolid36.97% /trianglesolid36.08% 0.34
0.7/trianglesolid46.19% /trianglesolid12.93% 0.96/trianglesolid45.49% /trianglesolid42.77% 0.54 /trianglesolid39.23% /trianglesolid36.96% 0.45
0.8/trianglesolid51.41% /trianglesolid21.35% 0.97/trianglesolid49.02% /trianglesolid47.35% 0.51 /trianglesolid45.33% /trianglesolid35.53% 0.60
0.9/trianglesolid52.42% /trianglesolid19.52% 0.97/trianglesolid65.48% /trianglesolid53.05% 0.76/trianglesolid38.32% /trianglesolid40.38% 0.40
1.0/trianglesolid56.79% /trianglesolid21.63% 0.98/trianglesolid72.12% /trianglesolid54.89% 0.89/trianglesolid51.70% /trianglesolid37.66% 0.65
The pcolumn indicates statistical difference between SFvalues considering a 95 % conﬁdence level, where
the symbol /trianglemeans the average is higher but not signiﬁcantly different, /triangleinv(lower but not signiﬁcantly
different), /trianglesolid(signiﬁcantly higher) and /triangledownsld(signiﬁcantly lower). The ˆA12column presents effect size measure
for the two machine learning techniques with the same βconﬁguration. Values in bold represent statistical
differences
Analyzing the instance I_150 in the conﬁguration of β=0.5, the PPloss was
2.67 and 1.01 % for LMS and MLP, respectively. Doubling this weight to β=1,
the results are 6.00 and 2.26 % for the LMS and MLP, respectively. In summary, the
average PPloss for all instances using LMS was 10.62 %, while employing MLP was
6.22 %. These values are obtained when comparing solutions generated without any
DM inﬂuence ( α=1 andβ=0) with solutions generated using the same weight for
functions score and SE ( α=1 andβ=1).
Thus, it is important to highlight that the loss in PPusing MLP being smaller than in
the LMS one, and is a direct consequence of the reached SFin the previous analysis.
In other words, as LMS reaches a higher SFand can incorporate most of the DM
123
652 Autom Softw Eng (2017) 24:623–671
0%20%40%60%80%100%
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Similarity Factor
βI_50
I_100
I_150I_200
Word Processor
ReleasePlanner
0%20%40%60%80%100%
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Similarity Factor
βI_50
I_100
I_150I_200
Word Processor
ReleasePlanner
(a)( b )
Fig. 7 Relation between the increase of the βweight and similarity factor .amachine learning technique:
LMS. bmachine learning technique: MLP
preferences, the PPloss is naturally proportional. Similarly to the SFvalues, the PP
values also signiﬁcantly grow in relation with the βincrease in the most of instances.
Figure 8presents an overview of the PPbehavior with LMS and MLP when the
βweight of SE in the ﬁtness function is incremented. As can be seen, the solutions
found for almost all instances present a similar increasing tendency of PPvalues for
both machine learning techniques. Thus, the higher is the inﬂuence of the implicitpreferences in the search process, the smaller will be the score function. Such as in
theSFanalysis, in the scenario where the loss in score needs to be avoided, another
weight conﬁguration that is more suitable for the speciﬁc software project can beemployed. However, even in the case in which the weights are balanced ( α=1 and
β=1), the PPloss is, in average, merely 10.62 and 6.22 % for the LMS and MLP,
respectively. This PPloss can be considered small, since there is a substantial gain in
SFof 72.12 and 54.89 % for both machine learning techniques.
From the previous analyses, two important ﬁndings were obtained: (i) the capability
of the proposal at incorporating the subjective evaluations and, consequently, including
the implicit preferences; and (ii) how much is lost in terms of score due to the inclusion
of subjective evaluations.
In order to provide a better visualization of the trade-off between the metrics SD
andPP, the Fig. 9presents their relation considering all instances and both machine
learning techniques. The right-hand side of the ﬁgures shows how much is reached ofSD, while the left-hand side reﬂects how much is lost in score for all instances. For
example, looking at instance I_100 and using the LMS, one needs to lose only 7.0 %
ofscore in order to reach a SDof 82.3 %. Using MLP the score loss was 5.0 % in
order to reach a 73.8 % of SD.
In synthesis, when using LMS, the average loss of score for all instances is 10.62 %
and the average gain in SDis 83.99 %. By adopting MLP, the average values were
6.22 and 74.26 % for the lost of score and gain in SD, respectively. Therefore, it
can be concluded that the proposed architecture is capable to incorporate the DM’s
preferences in the iNRP, with a considerably small loss in score.
–RQ
3(Comparison) :Does the proposed architecture improve the DM’s satisfac-
tion when compared to the non-interactive approach?
123
Autom Softw Eng (2017) 24:623–671 653
Table 6 Average of PPfor 30 runs and different conﬁguration of the βweight using LMS 1and MLP 2
withα=1
β I_50 I_100 I_150
pLMS 1 pMLP 2ˆA12 pLMS 1 pMLP 2ˆA12 pLMS 1 pMLP 2ˆA12
0.1/triangledownsld−0.72% /triangle0.91% 0.34/triangleinv−0.24% /triangle0.33% 0.45 /triangleinv−0.30% /triangle0.11% 0.46
0.2/triangleinv−0.02% /triangle0.43% 0.41 /trianglesolid0.85% /triangleinv−0.31% 0.59 /triangleinv−1.00% /triangleinv−0.12% 0.43
0.3/trianglesolid0.87% /trianglesolid0.83% 0.54 /trianglesolid1.79% /trianglesolid1.76% 0.53 /triangle0.96% /triangle0.24% 0.57
0.4/trianglesolid3.43% /trianglesolid1.30% 0.72 /trianglesolid1.29% /trianglesolid1.58% 0.5 /triangle0.25% /triangle1.24% 0.40
0.5/trianglesolid4.24% /trianglesolid2.51% 0.63 /trianglesolid3.71% /trianglesolid1.48% 0.8/trianglesolid2.67% /trianglesolid1.01% 0.64
0.6/trianglesolid4.87% /trianglesolid3.27% 0.73/trianglesolid4.91% /trianglesolid2.94% 0.72/trianglesolid3.13% /triangle0.47% 0.74
0.7/trianglesolid6.34% /trianglesolid4.39% 0.68/trianglesolid5.77% /trianglesolid3.85% 0.75/trianglesolid3.68% /triangle0.35% 0.84
0.8/trianglesolid7.54% /trianglesolid4.83% 0.72/trianglesolid7.69% /trianglesolid3.06% 0.90/trianglesolid4.69% /trianglesolid1.73% 0.76
0.9/trianglesolid8.01% /trianglesolid6.24% 0.66 /trianglesolid7.76% /trianglesolid3.48% 0.86/trianglesolid5.64% /triangle1.07% 0.86
1.0/trianglesolid8.86% /trianglesolid6.46% 0.72/trianglesolid8.71% /trianglesolid5.03% 0.82/trianglesolid6.00% /trianglesolid2.62% 0.88
β I_200 Word Processor ReleasePlanner
pLMS 1 pMLP 2ˆA12 pLMS 1 pMLP 2ˆA12 pLMS 1 pMLP 2ˆA12
0.1/trianglesolid0.25% /triangle0.20% 0.53 /trianglesolid0.69% /triangleinv−0.08% 0.63 /trianglesolid0.07% /triangledownsld−0.44% 0.57
0.2/triangle0.14% /triangle0.45% 0.46 /triangle0.17% /triangle0.23% 0.50 /trianglesolid0.82% /trianglesolid0.95% 0.52
0.3/trianglesolid1.05% /triangle1.15% 0.49 /triangle0.90% /triangle0.74% 0.44 /trianglesolid1.20% /trianglesolid1.83% 0.38
0.4/trianglesolid2.08% /trianglesolid0.88% 0.64/trianglesolid1.90% /trianglesolid2.67% 0.38 /trianglesolid1.59% /trianglesolid1.46% 0.48
0.5/trianglesolid4.36% /triangle1.11% 0.84/trianglesolid4.80% /trianglesolid4.99% 0.44 /trianglesolid2.89% /trianglesolid2.00% 0.55
0.6/trianglesolid5.15% /trianglesolid1.74% 0.89/trianglesolid8.65% /trianglesolid9.04% 0.52 /trianglesolid2.99% /trianglesolid2.75% 0.49
0.7/trianglesolid5.65% /trianglesolid2.10% 0.85/trianglesolid12.22% /trianglesolid10.11% 0.67 /trianglesolid3.58% /trianglesolid2.67% 0.48
0.8/trianglesolid6.87% /trianglesolid3.17% 0.90/trianglesolid13.08% /trianglesolid12.69% 0.45 /trianglesolid6.67% /trianglesolid2.61% 0.60
0.9/trianglesolid7.59% /trianglesolid2.85% 0.93/trianglesolid20.15% /trianglesolid15.54% 0.76/trianglesolid4.98% /trianglesolid3.61% 0.48
1.0/trianglesolid8.12% /trianglesolid3.87% 0.90/trianglesolid24.63% /trianglesolid15.95% 0.97/trianglesolid7.43% /trianglesolid3.42% 0.66
The pcolumn indicates statistical difference between PPvalues considering a 95 % conﬁdence level, where
the symbol /trianglemeans the average is higher but not signiﬁcantly different, /triangleinv(lower but not signiﬁcantly
different), /trianglesolid(signiﬁcantly higher) and /triangledownsld(signiﬁcantly lower). The ˆA12column presents effect size measure
for the two machine learning techniques with the same βconﬁguration. Values in bold represent statistical
differences
As explained in Sect. 4.3.2 , a group of software engineering practitioners was
invited to use the proposed approach in a Participant-based Experiment. The obtained
results for all participants are reported in Table 7.
As can be seen, there are no SFand PPvalues for the SP
0conﬁguration. This is
natural because it is a non-interactive solution. Nevertheless, its values in Fitness and
SE were calculated. The ﬁrst one was measured by normalizing the value of the score
function to 100 and adding the SEvalue.
In terms of solution quality, analyzing speciﬁcally the Participant 4, the SP
50with
MLP was the best one among all interactive solutions (including other participants).
Comparing its ﬁtness value with the result of SP
0, it is veriﬁed an improvement of
123
654 Autom Softw Eng (2017) 24:623–671
−5%0%5%10%15%20%25%30%
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Price of Preference
βI_50
I_100
I_150I_200
Word Processor
ReleasePlanner
−5%0%5%10%15%20%25%30%
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Price of Preference
βI_50
I_100
I_150I_200
Word Processor
ReleasePlanner
(a) (b)
Fig. 8 Relation between price of preference and the increase of the βweight. amachine learning technique:
LMS. bmachine learning technique: MLP
I_50
I_100
I_150I_200
Word Processor
ReleasePlanner
− 4 0 − 2 00 2 04 06 08 01 0 0Score loss SD
−8.9
−8.7
−6
−8.1
−24.6
−7.479.6
82.3
83.5
82.3
82.9
90.0
−6.5
−5.0
−3
−3.9
−16.0
−3.476.8
73.8
69.7
65.7
73.7
85.7I_50
I_100I_150
I_200
Word Processor
ReleasePlanner
− 4 0 − 2 00 2 04 06 08 01 0 0Score loss
SD(a)
(b)
Fig. 9 Relation between similarity degree andprice of preference .amachine learning technique: LMS. b
machine learning technique: MLP
36.9 % and a score loss of 4.4 % ( PPline). On the other hand, 94.17 was the ﬁtness
value of the worst solution, which is the SP
20with LMS. The quality of this solution is
32.7 % worse than SP
0. A discussion about the performance of LMS and MLP on the
Participant-based Experiment will be presented later in Sect. 4.5.
Considering the ﬁtness ( Fline) average results, the values of MLP from SP
20toSP
50,
and, the values of LMS for SP
40and SP
50were higher than the ﬁtness average obtained
123
Autom Softw Eng (2017) 24:623–671 655
Table 7 Values of ﬁtness ( F), SE, SFandPPfor all participants with different number of interactions and each machine learning technique
Participant ( p) Sp
0Sp
10Sp
20Sp
30Sp
40Sp
50
LMS 1 MLP 2 LMS 1 MLP 2 LMS 1 MLP 2 LMS 1 MLP 2 LMS 1 MLP 2
1 F 140 .00 96 .95 117 .71 94 .17 174 .27 107 .22 184 .12 108 .15 183 .70 173 .67 191 .69
SE 40 .00 25 .00 19 .00 57 .00 62 .00 65 .00 56 .00 85 .00 85 .00 34 .00 95 .00
SF −− 37.50 −52.50 42 .50 55 .00 62 .50 40 .00 85 .00 112 .50 −15.00 137 .50
PP − 0.10 0 .84 3 .00 8 .95 3 .16 13 .00 6 .188 12 .860 23 .20 4 .41
2 F 162 .00 165 .68 189 .00 173 .04 190 .01 186 .00 184 .90 174 .94 173 .38 176 .53 182 .11
SE 62 .00 62 .00 61 .00 75 .00 75 .00 73 .00 89 .00 75 .00 87 .00 75 .00 85 .00
SF − 0.00 −1.61 20 .97 20 .97 17 .74 43 .55 20 .97 40 .32 20 .97 37 .10
PP − 7.58 10 .77 7 .62 10 .07 7 .44 15 .104 27 .538 26 .622 23 .465 17 .89
3 F 161 .00 142 .69 187 .14 141 .40 156 .18 185 .03 173 .85 170 .49 186 .94 171 .85 189 .18
SE 61 .00 88 .00 100 .00 91 .00 71 .00 56 .00 93 .00 26 .00 76 .00 51 .00 45 .00
SF − 44.26 63 .93 49 .18 16 .39 −8.20 52 .46 −57.38 24 .59 −16.39 −26.23
PP − 13.51 9 .88 5 .81 7 .40 12 .05 7 .62 26 .61 10 .08 25 .20 7 .76
4 F 200 .00 124 .51 185 .89 187 .43 187 .72 121 .05 188 .29 174 .10 190 .41 178 .27 180 .81
SE 100 .00 82 .00 100 .00 100 .00 100 .00 100 .00 100 .00 100 .00 82 .00 91 .00 82 .00
SF −− 18.00 0 .00 0 .00 0 .00 0 .00 0 .00 0 .00 −18.00 −9.00 −18.00
PP − 7.87 11 .42 9 .84 10 .12 8 .63 9 .54 24 .08 10 .72 19 .61 20 .21
5 F 173 .00 119 .55 136 .41 100 .25 150 .22 150 .22 181 .35 143 .93 162 .75 186 .06 177 .39
SE 73 .00 76 .00 76 .00 63 .00 78 .00 74 .00 84 .00 75 .00 93 .00 78 .00 82 .00
123
656 Autom Softw Eng (2017) 24:623–671
Table 7 continued
Participant ( p) Sp
0Sp
10Sp
20Sp
30Sp
40Sp
50
LMS 1 MLP 2 LMS 1 MLP 2 LMS 1 MLP 2 LMS 1 MLP 2 LMS 1 MLP 2
SF − 4.11 4 .11 −13.70 6 .85 4 .11 15 .07 2 .74 27 .40 6 .85 12 .33
PP − 12.86 8 .82 34 .11 13 .81 15 .29 2 .65 14 .58 14 .79 13 .48 19 .88
Average F 147 .20 115 .48 143 .49 131 .82 149 .23 141 .46 156 .88 149 .69 159 .70 149 .34 164 .90
SE 74 .00 66 .60 71 .20 77 .20 77 .20 73 .60 84 .40 72 .20 84 .60 65 .80 77 .80
SF −− 1.43 2 .79 19 .79 19 .84 15 .23 30 .22 10 .27 37 .36 −2.52 28 .54
PP − 8.38 8 .35 12 .08 10 .07 9 .32 9 .58 19 .80 15 .02 20 .99 14 .03
123
Autom Softw Eng (2017) 24:623–671 657
 0 20 40 60 80 100
Participant 1 Participant 2 Participant 3 Participant 4 Participant 5Subjective EvaluationSP
0 SP10SP20SP30SP40SP50
 0 20 40 60 80 100
Participant 1 Participant 2 Participant 3 Participant 4 Participant 5Subjective EvaluationSP
0SP10SP20SP30SP40SP50(a)
(b)
Fig. 10 Results of SE for each participant with several number of interactions. amachine learning tech-
nique: LMS. bmachine learning technique: MLP
bySP
0. Thus, in spite of the loss in score denoted by PP, an overall improvement in
solution quality by the inclusion of DM interactions was observed.
Looking at SE and PPaverage of the results of SP
50with MLP technique, it is noticed
the values of both metrics were similar to the results obtained on Artiﬁcial Experiment
(Tables 3and6) considering the same instance (Word Processor) with 50 interactions.
It is important to remember that a simulator was used in Artiﬁcial Experiment, then
SDmetric in these tables is compared with SEvalue. This observation points out to the
consistence of proposed approach when used by software engineering practitioners incomparison with the main conclusions achieved in the previous experiment.
In order to summarize the participants behavior throughout the experiment, Fig. 10a
presents the SE given by the participant to the ﬁnal solutions, as well as the numberof required interactions.
Analyzing the Fig. 10a, which presents the LMS results, it is noticed that for all
participants, there were at least two solutions that are better or at least as good as S
p
0
(solution generated without interactions). Looking at the results of Participant 2, all
interactive solutions are better than Sp
0, with an exception of Sp
10that had the same
value of Sp
0.
Similarly, Fig. 10b presents the obtained values using the MLP technique. It is
possible to realize that, for each participant, at least four interactive solutions present
higher SE than the Sp
0solution, with exception of Participant 4, for whom three solu-
123
658 Autom Softw Eng (2017) 24:623–671
−5%5%15%25%35%
 10  20  30  40  50
Number of interactionsSF PP
−5%5%15%25%35%
 10  20  30  40  50
Number of interactionsSF PP
Fig. 11 Relation between similarity factor andprice of preference considering the average of all participants
for each number of interactions. amachine learning technique: LMS. bmachine learning technique: MLP
tions were equal to Sp
0. In the speciﬁc case of Participant 5, all solutions inﬂuenced
by him/her were superior than the non-interactive one.
Some interesting results to be highlighted are the ones obtained by the Participant
4. He/she provided a maximum subjective evaluation for Sp
0, thus, it was impossible
for any other solutions to be better in terms of SE. This should not be a problem in a
real usage of the proposed approach, because it is expected that if the non interactive
solution already satisﬁes all the DM’s desires, he/she would simply accept it.
Besides SEresults, the trade-off between satisfaction of the DM’s preferences and
thescore loss is relevant in this experiment. Thus, Fig. 11a and b show the SFandPP
averages of the solutions obtained by all participants with each number of interactions.
Speciﬁcally analyzing the results obtained with 50 interactions, LMS reached
around 20 and 0 % of PPandSF, respectively, while MLP achieved almost 30 % of SF
and<15 % of PP. More generally, in average, the SFvalues using LMS were higher
than the PPones in two of the ﬁve different number of interactions. Furthermore, SF
values from MLP were bigger than PPvalues in four cases.
4.5 Discussion
Based on the data collected and analyzed in the empirical study, some interesting ﬁnd-
ings can be discussed. The ﬁrst one is the relationship between the achieved results and
the machine learning technique employed. In the Artiﬁcial Experiment (Sect. 4.3.1 ), a
simulator was used to evaluate the IGA candidate solutions. The criteria adopted by thesimulator to evaluate the solutions is always the same, in other words, the subjective
evaluations are directly related to the number of requirements present in the candidate
solution that are also present in the target solution. If the requirements included ina candidate individual are totally different from the target solution, the evaluation is
minimal. Similarly, when the individual is equal to the target solution, the evaluation is
maximum. As can be seen, the subjective evaluations provided by the simulator followa linear proportion. On the other hand, the participant-based experiment (Sect. 4.3.2 )
employs real human evaluations which are deﬁned as multi-criteria and of nonlinear
character ( Piegat and Sałabun 2012 ).
123
Autom Softw Eng (2017) 24:623–671 659
Hence, since the LMS is a robust regression linear model ( Rousseeuw 1984 ), it is
expected that it will reach good results when considering the simulator in the artiﬁcial
experiment. Linear regression is a simple method that has been widely used in statisti-cal applications. However, according to Witten and Frank (2005 ), there are two serious
disadvantages to median-based regression techniques: (i) it can only represent linear
boundaries between classes, which makes them too simple for many practical appli-cations and (ii) they incur high computational cost, which also makes them infeasible
for practical problems.
In contrast, the MLP technique, powered by the backpropagation training rule, is
able to perform nonlinear regression; therefore, overcoming the LMS weaknesses. If
the training examples are not linearly separable, the training rule converges toward a
best-ﬁt approximation to the target concept ( Witten and Frank 2005 ;Mitchell 1997 ).
As stated in Palit and Popovic (2006 ), the practical use of neural networks has been
recognized mainly because of distinguished features such as (i) general nonlinearmapping between a subset of the past time series values and the future time series
values and (ii) capability of learning and generalization from examples using a data-
driven self-adaptive approach. These aspects theoretically underlie the conclusions ofthe empirical study suggesting why the LMS achieved better results in the Artiﬁcial
Experiment, while the MLP outperforms the LMS in the Participant-based Experiment.
Another aspect that can be highlighted are the insights obtained into how people
go about using the system. Firtly, all ﬁve participants were invited to sign a term
of consent and to respond a questionnaire. On a scale of “insufﬁcient”, “indifferent”
and “sufﬁcient”, all the participants considered the brieﬁng (described in Sect. 4.3.2 )
before the experiment as “sufﬁcient”. When asked to rate how effective they found the
experience of interactively selecting the requirements for the next release, all profes-
sionals considered as “effective”. The scale used to this answer was “very ineffective”,“ineffective”, “indifferent”, “effective” and “very effective”. Complementing and rein-
forcing this achievement, it was asked if they would use the proposed approach in their
workplaces. Following a scale of 1 (“no way) to 5 (“certainly”), one participant rated
at 3, three participants at 4 and one at 5. These results may suggest two conclusions: (i)
the proposed approach was considered distinguished by software engineering practi-tioners and (ii) it encourages the application of the presented approach in a real-world
scenario of requirements planning.
The participants were also asked to rate how much tiring was to evaluate the can-
didate solutions during the IGA evolution. On a scale of 0 to 9, one participant rated
at 4, two at 5 and two at 7, which generates an average of 5.6. As stated in Wang et al.
(2006 ), “it is pretty hard to measure the fatigue reduction in the case of IEC, since
IEC deals with subjective evaluation values that depend on the application task and
the subject’s perceived value of the task”. There are some pieces of work that propose
approaches to reduce the human effort without compromising the results and consis-tencies, such as Kamalian et al. (2006 ),Hsu and Huang (2005 ) and Wang et al. (2006 ).
Even though these papers claim to reduce the effort undertaken, but the analysis of
“how much” fatigue was mitigated is very context-depending. A straightforward wayto assess some kind of improvement in human fatigue through the usage of the learning
model would be the clock time taken to complete the task. However, this would not
be the most appropriate way to analyze the human fatigue. It is reasonable to believe
123
660 Autom Softw Eng (2017) 24:623–671
that a certain participant may spend more time than another and still report less fatigue
or, in other case, evaluate less solutions than other participant and indicates a higher
fatigue. Also, it is possible that the participant could undertake the experiment withsome previous tiredness (physical or psychological) which can have a direct inﬂuence
in his/her fatigue report. Deﬁnitely, this is an important aspect to be properly discussed
and reﬁned.
Regarding the “free text” comments participants were allowed to give about the
experience, it can be highlighted the need to integrate detailed requirements speciﬁca-
tions in the user graphic interface. Others suggestions to the interface were proposed,such as increase the slide button and a more intuitive use of colors.
Finally, the answers of each research question discussed in the empirical study can
be formalized as:
–RQ
1:the solutions provided by the approach are properly inﬂuenced by the inter-
actions throughout the evolutionary process, given the higher values obtainedin Similarity Degree. This inﬂuence produces solutions that incorporate the sub-
jective evaluations and, consequently, are more suitable to the Decision Maker
preferences.
–RQ
2:the loss in score in the solutions is considerably small, especially when
taking into account the high gain in Similarity Factor. This conclusion suggests
that the approach is able to generate solutions as good as those produced without
any human inﬂuence, but satisfying most of the Decision Maker preferences. In
addition, this trade-off can be conveniently calibrated according to the softwareprojects speciﬁc needs through a detailed weights conﬁguration.
–RQ
3:even considering participants with different and unknown evaluation pro-
ﬁles, the proposed architecture was able to improve their subjective satisfaction.This fact can be justiﬁed through the higher SEvalues obtained in the Participant-
based Experiment. Furthermore, the found results shown architecture consistence
regarding to the conclusions achieved in the Artiﬁcial Experiment.
4.6 Threats to validity
A list of threats to the validity of empirical studies in Search Based Software Engi-
neering is presented in de Oliveira Barros and Neto (2011 ). Such threats are classiﬁed
as: (i) conclusion validity threats, (ii) internal validity threats, (iii) construct validity
threats and (iv) external validity threats. The above the threats that may effect the
validity of the experiments performed in this paper will be discussed, including whatwas eventually designed to mitigate the effects of each one.
Regarding the conclusion validity threats , due to the stochastic nature of the IGA,
each execution can achieve different results. Therefore, in order to ensure a fair com-parison for each weight conﬁguration, number of interactions, instances and machine
learning technique, 30 runs were performed. According to Arcuri and Briand (2014 ),
this is an acceptable number of executions for empirical studies in SBSE, even though agreater number of runs would have provided greater accuracy. Aiming at precisely pro-
viding conclusions regarding the achieved results, statistical analyses were performed
using the Mann–Whitney Utest to measure the difference between the samples with
123
Autom Softw Eng (2017) 24:623–671 661
a 95 % conﬁdence level and, complementing this one, the Vargha and Delaney’s ˆA12
to highlight the effect size . Despite being extensively explored problem in the litera-
ture, there is no other approach using an IGA to solve the NRP. Given the lack of arecognized benchmark example, this work is focused in comparing the results reached
by the proposed architecture with the ones generated without human inﬂuence, which
can be considered a small scale investigation. Another important challenge to be dis-cussed concerns to how evaluate and estimate the reduction of fatigue. This work
asked the participants an numerical value which can express his/her tiredness during
the experiment. Another more reﬁned metric may provide more useful insights.
Concerning the internal validity threats , the parametrization procedure adopted to
the empirical study is properly presented in the Sects. 4.2.2 and4.2.3 , which present
all the conﬁgurations regarding both the machine learning technique and IGA. With
an aim to have a more generic analysis, the LMS and MLP were chosen due to the
different learning strategies of each one. These main differences were discussed in theirrespective topics. Although the results were obtained from a preliminary empirical
study, the tuning parametrization process was not conducted as suggested in Arcuri
and Fraser (2011 ), which may indicate better conﬁgurations and, consequently, to
produce superior results for some speciﬁc instance. Throughout the paper, there are
no details regarding the code instrumentation, however, to mitigate this threat and
facilitate the experiments replication, the source code and all evaluated instances canbe found in the supporting web page.
2Regarding the data collection procedure, all
details of the design and generation of artiﬁcial instances, as well as the real-world
instances particularities, are described in the Sect. 4.2.1 . Furthermore, Participant-
based Experiment are highly dependent of the developed scenario and, consequently,
the users engagement. In order to mitigate this problem, before the experiment, each
participant was separately briefed about the main details related to the experiment.Nevertheless, providing to the participants a more elaborate scenario thats is as similar
as possible to their workplaces is a considerable challenge.
In relation to the construct validity threats , this paper does not perform a study on
computational cost of execution or performance of the used algorithm. Aiming to be
more meaningful and appropriate to the empirical study, it was necessary to developa suite of metrics for the interactive perspective. All these metrics were precisely
deﬁned and can be reused in other works involving interactive optimization in SBSE.
However, a speciﬁc metric for the learning perspective would be useful to consider.The proposed iNRP formulation is a generalization based on the mathematical model
proposed in Baker et al. (2006 ), which can be considered a simpliﬁcation of a real next
release planning situation. Another issue to be discussed is the choice of the targetsolution that determines the performance of the SDandSFmetrics being compared. As
explained in Sect. 4.3.1 , the requirements present in the target solution are randomly
chosen, but of course there is no warranty that this is indeed the optimal selection.Regarding the type of the SE, a numerical value range was used, but other ways of
evaluation may be show as more intuitive (e.g., “bad” to “excellent” or “one star” to
“ﬁve star” rating).
2http://goes.uece.br/allyssonaraujo/architecture4inrp .
123
662 Autom Softw Eng (2017) 24:623–671
Finally, concerning external validity threats , artiﬁcial instances with 25, 50, 100,
150 and 200 requirements and real-word instances with 25 and 50 requirements
were used in the empirical study. Experiments with bigger instances would haveprovided more generalizable results. Moreover, the number of professionals in the
Participant-based Experiment could be bigger to provide a more reliable evaluation
of the approach. To cope with this threat, participants with reasonable software engi-neering practice were invited to discuss the system usage.
5 Related work
In this section, the work related to this research is discussed. Firstly, the work directlyrelated to the Next Release Problem, followed by the work that applies interactive
optimization in Search Based Software Engineering. Finally, some strategies designed
to handle human fatigue are also presented.
5.1 Next release problem
The ﬁrst single objective formulation of the NRP was presented in Bagnall et al. (2001 ).
This approach considers the existence of more than one customer with different levelsof relevance to the company, which is indicated by his/her respective weight. Each
customer then indicates which requirements he/she wants to be implemented in the next
release. The customer is considered satisﬁed if all his/her requirements are selected forthe next release. Also, each requirement presents a development cost. Thus, the goal is
to select a set of requirements that maximizes the sum of satisﬁed customers’ weights,
while the implementation cost of the release is subject to the available release budget.Regarding the empirical evaluation, it uses exact methods, neighborhood heuristics
and a metaheuristic called Simulated Annealing. It was noticed that the exact method
has found better solutions for smaller instances in viable times, but in bigger instances
the metaheuristic had better performance.
Differently from Bagnall et al. (2001 ), the work Baker et al. (2006 ) argues that each
requirement should have its importance given by the customers. The global importance
of a requirement is calculated by a weighted sum of the speciﬁc importance given by
the customers. Thus, it aims at selecting a set of requirements that maximizes theglobal importance of the release. Such work is considered relevant due to the usage
of real-world data from a great telecommunication company. In order to validate
the approach, Greedy Algorithms and Simulated Annealing were applied, with theresults being compared to the ones produced by an expert. It was concluded that both
algorithms had a better performance than the professional.
Naturally, the NRP was tackled by various approaches. In Jiang et al. (2010 ), the
usage of a hybrid algorithm composed by Ant System and Hill Climbing is proposed.
T h ew o r ki n del Sagrado et al. (2010 ) is related to the use of the Ant Colony Optimiza-
tion to the NRP, and the achieved results are better in solution quality and convergenceterms than the results found by Genetic Algorithms and Simulated Annealing. Such
work was complemented in do Nascimento Ferreira and de Souza (2012 ), when it was
elaborated a comparative study between Ant Colony Optimization, Genetic Algo-
123
Autom Softw Eng (2017) 24:623–671 663
rithm and Simulated Annealing considering requirements interdependencies. This
work concludes that the Ant Colony Optimization reaches better results than other
metaheuristics.
Invan den Akker et al. (2005 ), techniques of integer linear programming were
applied considering some practical aspects of the NRP, such as the list of requirements,
interactions between requirements and their respective costs, and the resources thatthe development team requires. The work on Xuan et al. (2012 ) focuses on solving
big instances of the NRP through an algorithm named “Backbone Algorithm”. The
achieved results are compared with a Simulated Annealing variant known as LMSA.
Generally speaking, the approaches presented above can be considered decision-
making tools, where the DM inserts data, then the tool automates the process and
returns a set of requirements to be implemented in the next release. Due to this autom-
atization, such approaches do not consider implicit preferences during the search
process, and consequently don’t making use of various beneﬁts that human knowl-edge could offer to improve the results.
5.2 Interactive optimization in search-based software engineering
T h ew o r ki n Pitangueira et al. (2015 ) presents a systematic review and mapping study
of SBSE approaches to requirements selection and prioritization. The authors point
out that, in terms of innovation for the area, adding user judgement in the model may
lead to better results, and would also take the empirical studies closer to the softwareengineering reality. This point of view is also reinforced in Zhang et al. (2008 ) and
Harman et al. (2012 ).
In the requirements engineering ﬁeld there are four pieces of work that can be
highlighted. First of all, the ﬁrst version of this work presented in Araújo et al.
(2014 ) primarily assess whether an IGA for the NRP can properly incorporate the
DM knowledge in the ﬁnal solutions. In addition, it has also drafted a ﬁrst version
of the architecture that considers a learning model as an option of replacing the DM
when necessary.
A conceptual proposal is presented in Pitangueira (2015 ), which focuses on improv-
ing the selection of software requirements for a next release. It proposes the usage of
a search-based approach interactively with multiple stakeholders in the optimizationprocess through the establishment of a judgment consensus about what is a good Pareto
Front.
InDantas et al. (2015 ), an interactive approach to the software release planning
is proposed in which the search is guided according to a Preferences Base that is
interactively supplied by the DM during the search process. In this approach, the
ﬁtness of a certain candidate solution is penalized according to the importance level ofeach preference that was not satisﬁed. Preliminary results indicates that the solutions
are able to satisfy almost all user preferences, prioritizing the most important ones,
with little score loss.
Finally, Tonella et al. (2010 ) investigates an IGA approach for a real case study
in the requirements prioritization process. The main idea of this paper is to minimize
the amount of “requirements peers” evaluations obtained from the users, making this
123
664 Autom Softw Eng (2017) 24:623–671
approach more scalable and accurate concerning the ﬁnal requirements prioritization.
The results are positive denoting that the performance of the requirements prioritization
process is better when employing the interactive approach. Later, this approach wasextended in Tonella et al. (2013 ), pointing out comparisons with the IAHP, the state
of the art reference algorithm for interactive requirement prioritization.
Regarding software maintenance, an approach to ﬁnd appropriate refactoring sug-
gestions using a set of examples is proposed in Ghannem et al. (2013 ). The ﬁtness
function combines the structural similarity between a candidate design model and
refactoring examples, alongside developer’s ratings for the refactorings proposed dur-ing execution of the IGA. The ﬁtness function of the solution is computed as an average
of its old ﬁtness function and the overall designer’s rating. Initially, the base of exam-
ples and an initial model to be improved are used as inputs. Results showed that this
approach is stable regarding its accuracy, integrity, type and amount of refactorings
per class.
Under the context of software design optimization, an inclusion of the developer
in tasks of software re-modularization is proposed in Bavota et al. (2012 ). In such
work, quality and dependencies between modules are automatically evaluated. Theuser, in turn, evaluates if two components must be in the same module or not. Given
this feedback, a penalty function is applied to penalize solutions that violate the con-
straints imposed by the developers. Despite the effectiveness regarding cohesion, theapproach does not consider the developer’s knowledge when deciding about grouping
the components.
Still considering the software design ﬁeld, there is a large amount of work apply-
ing interactive optimization concepts ( Parmee et al. 2006 ;Simons and Parmee 2010 ;
Simons et al. 2010 ;Simons 2011 ;Simons and Parmee 2012 ;Simons et al. 2014 ;
Simons and Smith 2013 ). It is possible to highlight the work in Simons et al. (2014 )
because it summarizes many ideas and concepts presented in the previous ones. The
research is focused on interactive ant colony optimization in which the search process
is guided by an adaptative model that bring together objective and subjective factors.
Regarding the interaction, the user is invited to give a numeric evaluation (from 1
to 100) for a feasible candidate solution. The solution’s representation is modelledas a UML diagram in which each class is divided into three compartments and con-
nected with other classes through arrows. Concerning the results, the participants of
the experiments rated the proposal as persuasive in the sense that it may be consideredas an interesting direction in the interactive search for problems related to software
design.
With respect to software testing, it is possible to cite the project exploited in Mar-
culescu et al. (2012 ,2013 ,2015 ,?). Generally, it is proposed a system for testing
embedded software by applying a technique that largely automates the generation of
test data while still enabling domain specialists to contribute with their knowledge andexperience. The ﬁtness function is calculated from a set of quality objective scores
for a candidate solution, and a set of weights for those objectives is provided by the
user which are combined into a single ﬁtness. Thus, to guide the search, the domainspecialist decides the relative importance of the quality objectives. In the experiments,
the Differential Evolution was employed as search engine. An industrial evaluation
was conducted, and results showed that the tool complements existing testing meth-
123
Autom Softw Eng (2017) 24:623–671 665
ods, given that the user interaction is essential in developing interesting and useful test
cases.
As presented in Sect. 3.2, this work considers to be of great value three major ques-
tions when deﬁning an Interactive Modeling for a search problem. Thus, Table 8
presents these three aspects under the perspective of the related work discussed
above.
5.3 Treating human fatigue
InKamalian et al. (2006 ) the use of artiﬁcial intelligence techniques is proposed to
predict human evaluations based on previous interactions, analogously to the learning
model proposed in this work. It is proposed the use of fuzzy inference systems and
machine learning techniques to reduce human fatigue. The empirical evaluation wasconducted under the context of microelectromechanical systems, also known as micro-
machines design problem. The fuzzy-system-based predictor was based on four system
rules manually derived from the observation of previous human user tests, reaching areduction in human effort of 51 % in average. Regarding the machine learning, it was
investigated the results of four different approaches, and it was achieved, in average,
a reduction in human effort of 31 %. These approaches achieved good accuracy onvalidation tests, but because of the great diversity in user scoring behavior, they were
unable to achieve equivalent results on the user test data.
InHsu and Huang (2005 ), it is argued that one of the main causes of human fatigue
is the fact that there are occasions where the result preferred by the user does not
exist in the search space. If it is not possible to ensure that the solution idealized by
the user exists in the search space, the search process becomes more difﬁcult and,consequently, the fatigue increases. Given this context, an effective method to create
a search space that meets the customer values (or objectives) is proposed. It integrates
the search space into a customer values-based IEC model to reduce user burden when
designing their preferred products. A case study involving the design of water bottles
was analyzed in order to verify the model’s performance. Overall, the results conﬁrmthat a correct search space contributes to reduce the user fatigue.
Another alternative to ameliorate human fatigue that can be mentioned is the work
inWang et al. (2006 ). Fundamentally, it is proposed the creation of an absolute scale
to improve the prediction of human evaluations in IEC, accelerating the algorithm
convergence, and reducing the amount of human intervention. Thus, a concrete pre-
dictor method of mapping relative data to the absolute scale is proposed. Regardingthe experiments, three methods were compared: an IGA with the proposed predictor
using an absolute scale, an IGA with a conventional predictor using a relative scale,
and an IGA without a predictor. First, the effectiveness of this method was evaluatedusing seven benchmark functions instead of a human user, which was aimed at veri-
fying the convergence speed of an IEC using the proposed absolute rating. Next, the
method was assessed through a subjective test using an IEC based individual emotionretrieval system in order to prove that the proposed predictor is effective in reducing the
user fatigue. The proposed predictor using absolute evaluation had better prediction
performance than conventional predictors, resulting in faster convergence.
123
666 Autom Softw Eng (2017) 24:623–671
Table 8 Interactive modeling to the related work
Paper Problem At which moment are the
preferences from theDM captured?What type and which
preferences areprovided to the search
process?How the preferences are
incorporated andinﬂuence the search
process?
Araújo et al. (2014 ) Next release problem Interactive Implicit. Evaluation about
the solution
requirements selectionAnother objective to be
maximized
Dantas et al. (2015 ) Release planning A priori and Interactive Explicit. Speciﬁc
requirements allocationin
releasesAnother objective to be
maximized
Tonella et al. (2013 ) Requirements prioritization Interactive Explicit. Pairwise
comparison between
requirementsMinimize the number
of pairwise comparisons
Bavota et al. (2012 ) Software re-modularization Interactive Explicit. Add penalties
for artifacts that are not
where they should beSoft constraint in the
ﬁtness function
Ghannem et al. (2013 ) Refactoring Interactive Implicit. Rating the
proposed refactoringsUser rating combined
with structural
measurements
Simons et al. (2014 ) Software design Interactive Implicit Rating candidate
designs based on UML
classesInﬂuence in the heuristic
information of a iACO
Marculescu et al. (2015 ) Test data generation Interactive Explicit. User decides the
relative importance of
the quality objectivesWeighting the different
objectives to be
optimized
123
Autom Softw Eng (2017) 24:623–671 667
6 Conclusions
Selecting requirements for the next release is a complex task in the incremental and
iterative software development model, given the high number of combinations, techni-
cal constraints, multiple objectives and different stakeholders. An interesting approach
to deal with this problem is the interactive optimization, which is a research ﬁeld thatuses the human tacit knowledge in computational search. The use of such optimiza-
tion strategy is primarily recommended when the human inﬂuence can effectively
contribute to the search process enabling the support decision system to absorb theuser’s implicit knowledge without the requirement to formalise this information a
priori.
An initial proposal of the presented architecture has already been introduced in
Araújo et al. (2014 ), in which preliminary results shown that an IGA can successfully
incorporate the user preferences in the ﬁnal solution. The present work signiﬁcantlyextends the previous work, improving both the architecture and empirical study.
Through the combination of the beneﬁts achieved by the Interactive Optimization,
Machine Learning and SBSE, the main objective of this paper is to thoroughly presentand evaluate the architecture to solve an interactive version of the NRP that allows an
inclusion of human knowledge during the IGA evolution. Aiming to not overload the
Decision Maker with a large number of interactions, a learning model was proposedto learn the human behavior and, eventually, replace the DM in the remainder of the
evolutionary process. Thus, the architecture is composed by three different compo-
nents with distinct responsibilities: (a) interactive genetic algorithm, (b) InteractiveModule and (c) learning model. These components communicate among themselves
and each one was properly modelled under the Interactive Next Release Problem
(iNRP) perspective. Regardless of how many stakeholders are involved in the project,the approach focuses in interacting with only one crucial DM that needs to be fully
immersed in the high-level project particularities and responsible to make the last call.
This paper considers both explicit (intrinsic characteristics of the problem) and
implicit preferences (tacit and difﬁcult to previously articulate) in the evaluation func-
tion. The ﬁrst one is captured through the score function that guides the search in
order to achieve solutions that maximize the overall stakeholders satisfaction, in other
words, selecting the requirements they want to be implemented in the next release.
On the other hand, the implicit preference is gathered when the DM provides SE toeach solution during the algorithm evolution. However, it is reasonable to have some
trade-off between these objectives, because the importance assigned by a speciﬁc
stakeholder to a certain requirement may not agree with the DM’s broad view of theproject. According to the speciﬁc scenario and project needs, it is possible to balance
this trade-off deﬁning the inﬂuence of each objective in the search process through
the weights αandβspeciﬁed in the architectural settings.
Two experiments were performed for the empirical study conducted in this work:
the Artiﬁcial Experiment was veriﬁed with several and exhaustive scenarios, and the
Participant-based Experiment investigated the feasibility of the architecture when it isused by software engineering practitioners. Through the analysis and results achieved
in the empirical study, three main ﬁndings can be highlighted:
123
668 Autom Softw Eng (2017) 24:623–671
– The proposed architecture passes the sanity check when it demonstrates that as
the number of interactions increase, more suitable to the DM preferences the ﬁnal
solution is;
– To incorporate the DM preferences, it is natural to have some score loss given the
trade-off previously established. However, this score loss is considerably small,
making the approach able to generate solutions as good as those produced in afully automatic method, but satisfying most of the DM preferences;
– Even considering software engineering practitioners with different evaluation
proﬁles, the proposed approach can improve their subjective satisfaction in com-parison with a non-interactive solution;
Therefore, some interesting beneﬁts of interactively including the DM in the next
release planning during the evolutionary process can be underlined: (i) the DM could
receive some feedback from the search through the presentation of the most promising
solutions throughout the algorithm evolution; (ii) the capability to incorporate thechanges from the DM’s criteria during the search process; (iii) the DM may get some
insights about the problem, and even dynamically adapt the decision criteria; and (iv)
mitigate the feeling of intellectual exclusion by the user in the analysis, which maycause resistance and lack of conﬁdence in the ﬁnal result.
As future work directions, it is expected to provide other analyses regarding the
learning model performance; develop a strategy to measure the increase or decrease
of fatigue; interact with multiple decision makers where their subjective evaluations
are considered as different objectives to be optimized in a many-objective paradigm;assess different ways for the DM to evaluate the solutions and suit the architecture
for a interactive multi-objective next release problem (iMONRP) formulation aiming
to maximize both score and SE, while minimizing the cost; and ﬁnally, conduct a
extensive empirical study considering other traditional problems exploited in SBSE.
Acknowledgments The authors would like to thank the editorial staff and the anonymous reviewers for
their professional and constructive comments, the participants of the experiment for their availability and,ﬁnally, the members of the Optimization in Software Engineering Group of the State University of Ceará.
References
Aljawawdeh, H.J., Simons, C.L., Odeh, M.: Metaheuristic design pattern: Preference. In: Proceedings of
the Companion Publication of the 2015 on Genetic and Evolutionary Computation Conference, pp.
1257–1260. ACM (2015)
Araújo, A.A., Paixão, M.H.E.: Machine learning for user modeling in an interactive genetic algorithm for the
next release problem. In: Proceedings of the 6th International Symposium on Search-Based SoftwareEngineering (SSBSE ’14), vol. 8636, pp. 228–233. Springer, Fortaleza, Brazil (2014). doi: 10.1007/
978-3-319-09940-8_16
Arcuri, A., Briand, L.: A hitchhiker’s guide to statistical tests for assessing randomized algorithms in
software engineering. Softw. Test. Verif. Reliab. 24(3), 219–250 (2014)
Arcuri, A., Briand, L.C.: A practical guide for using statistical tests to assess randomized algorithms in
software engineering. In: Proceedings of the 33rd International Conference on Software Engineering
(ICSE ’11), pp. 1–10. IEEE, Honolulu, HI, USA (2011). doi: 10.1145/1985793.1985795
Arcuri, A., Fraser, G.: On parameter tuning in search based software engineering. In: Proceedings of the 3rd
International Symposium on Search Based Software Engineering (SSBSE ’11), vol. 6956, pp. 33–47.
Springer, Szeged, Hungary (2011). doi: 10.1007/978-3-642-23716-4_6
123
Autom Softw Eng (2017) 24:623–671 669
Bagnall, A.J., Rayward-Smith, V .J., Whittley, I.M.: The next release problem. Inf. Softw. Technol. 43(14),
883–890 (2001). doi: 10.1016/S0950-5849(01)00194-X
Baker, P., Harman, M., Steinhöfel, K., Skaliotis, A.: Search based approaches to component selection and
prioritization for the next release problem. In: Proceedings of the 22nd IEEE International Conference
on Software Maintenance (ICSM ’06), pp. 176–185. IEEE, Philadelphia, Pennsylvania (2006). doi: 10.
1109/ICSM.2006.56
Bavota, G., Carnevale, F., Lucia, A.D., Penta, M.D., Oliveto, R.: Putting the developer in-the-loop: An
interactive ga for software re-modularization. In: Proceedings of the 4th International Symposium on
Search Based Software Engineering (SSBSE ’12), vol. 7515, pp. 75–89. Springer, Riva del Garda,
Italy (2012). doi: 10.1007/978-3-642-33119-0_7
Carlshamre, P., Sandahl, K., Lindvall, M., Regnell, B., et al.: An industrial survey of requirements interde-
pendencies in software product release planning. In: Proceedings Fifth IEEE International Symposiumon Requirements Engineering, 2001, pp. 84–91. IEEE (2001)
Cho, S.B.: Towards creative evolutionary systems with interactive genetic algorithm. Appl. Intell. 16(2),
129–138 (2002)
Dantas, A., Yeltsin, I., Araújo, A.A., Souza, J.: Interactive software release planning with preferences base.
In: Proceedings of the 7th International Symposium on Search-Based Software Engineering (SSBSE’15), pp. 341–346. Springer, Bergamo, Italy (2015). doi: 10.1007/978-3-319-22183-0_32
de Barros, M.O, Neto, A.C.D.: A survey of empirical investigations on ssbse papers. In: Proceedings of
the 3rd International Symposium on Search Based Software Engineering (SSBSE ’11), vol. 6956, pp.268–268. Springer, Szeged, Hungary (2011). doi: 10.1007/978-3-642-23716-4_24
do Nascimento Ferreira, T., de Souza, J.T.: An aco approach for the next release problem with depen-
dency among requirements. In: Proceedings of the 3rd Brazilian Workshop on Search-Based Software
Engineering (WESB ’12). Natal, RN, Brazil (2012)
del Sagrado, J., del Águila, I.M., Orellana, F.J.: Ant colony optimization for the next release problem—a
comparative study. In: Proceedings of the 2nd International Symposium on Search Based Software
Engineering (SSBSE ’10), pp. 67–76. IEEE, Benevento, Italy (2010). doi: 10.1109/SSBSE.2010.18
Ferrucci, F., Harman, M., Sarro, F.: Search-based software project management. In: Software Project Man-
agement in a Changing World, pp. 373–399. Springer (2014). doi: 10.1007/978-3-642-55035-5_15
Ghannem, A., Boussaidi, G.E., Kessentini, M.: Model refactoring using interactive genetic algorithm. In:
Proceedings of the 5th International Symposium on Search Based Software Engineering (SSBSE ’13),
vol. 8084, pp. 96–110. Springer, St. Petersburg, Russia (2013). doi: 10.1007/978-3-642-39742-4_9
Glass, R.L.: Facts and Fallacies of Software Engineering. Addison-Wesley Professional, Boston (2002)Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., Witten, I.H.: The weka data mining software:
an update. SIGKDD Explor. Newsl. 11(1), 10–18 (2009). doi: 10.1145/1656274.1656278
Han, J., Kamber, M., Pei, J.: Data Mining: Concepts and Techniques. Elsevier, Amsterdam (2011)
Harman, M.: The current state and future of search based software engineering. In: Proceedings of Interna-
tional Conference on Software Engineering / Future of Software Engineering 2007 (ICSE/FOSE ’07),pp. 342–357. IEEE, Minneapolis, Minnesota, USA (2007). doi: 10.1109/FOSE.2007.29
Harman, M.: Search based software engineering for program comprehension. In: Proceedings of the 15th
IEEE International Conference on Program Comprehension (ICPC ’07), pp. 3–13. IEEE, Banff,
Alberta, Canada (2007). doi: 10.1109/ICPC.2007.35
Harman, M., Clark, J.A.: Metrics are ﬁtness functions too. In: Proceedings of the 10th IEEE International
Symposium on Software Metrics (METRICS ’04), pp. 58–69. IEEE, Chicago, USA (2004). doi: 10.
1109/METRIC.2004.1357891
Harman, M., McMinn, P., de Souza, J.T., Yoo, S.: Search based software engineering: techniques, taxonomy,
tutorial. Empir. Softw. Eng. Verif. 7007 , 1–59 (2012). doi: 10.1007/978-3-642-25231-0_1
Haykin, S.S.: Redes Neurais. Bookman, Porto Alegre (2001)Hsu, F.C., Huang, P.: Providing an appropriate search space to solve the fatigue problem in interactive
evolutionary computation. New Gener. Comput. 23(2), 115–127 (2005)
Jiang, H., Zhang, J., Xuan, J., Re, Z., Hu, Y .: A hybrid aco algorithm for the next release problem.
In: Proceedings of the 2nd International Conference on Software Engineering and Data Mining
(SEDM ’10), pp. 166–171. IEEE, Chengdu, China (2010). http://ieeexplore.ieee.org/xpls/abs_all.
jsp?arnumber=5542931
Kamalian, R., Yeh, E., Zhang, Y ., Agogino, A.M., Takagi, H.: Reducing human fatigue in interactive evolu-
tionary computation through fuzzy systems and machine learning systems. In: 2006 IEEE InternationalConference on Fuzzy Systems, pp. 678–684. IEEE (2006)
123
670 Autom Softw Eng (2017) 24:623–671
Karim, M.R., Ruhe, G.: Bi-objective genetic search for release planning in support of themes. In: Proceedings
of the 6th International Symposium on Search-Based Software Engineering (SSBSE ’14), vol. 8636,pp. 123–137. Springer, Fortaleza, Brazil (2014). doi: 10.1007/978-3-319-09940-8_9
Marculescu, B., Feldt, R., Torkar, R.: A concept for an interactive search-based software testing system. In:
Proceedings of the 4th International Symposium on Search Based Software Engineering (SSBSE ’12),
vol. 7515, pp. 273–278. Springer, Riva del Garda, Italy (2012). doi: 10.1007/978-3-642-33119-0_21
Marculescu, B., Feldt, R., Torkar, R.: Objective re-weighting to guide an interactive search based software
testing system. In: Proceedings of the 12th International Conference on Machine Learning and Appli-
cations (ICMLA ’13), pp. 102–107. IEEE, Miami, Florida, USA (2013). doi: 10.1109/ICMLA.2013.
113
Marculescu, B., Feldt, R., Torkar, R., Poulding, S.: An initial industrial evaluation of interactive search-
based testing for embedded software. Appl. Soft Comput. 29, 26–39 (2015). doi: 10.1016/j.asoc.2014.
12.025
Marculescu, B., Poulding, S., Feldt, R., Petersen, K., Torkar, R.: Tester interactivity makes a difference in
search-based software testing: A controlled experiment. arXiv preprint arXiv:1512.04812 (2015)
Miettinen, K.: Nonlinear Multiobjective Optimization, vol. 12. Springer, Norwell (1999)
Miettinen, K., Hakanen, J., Podkopaev, D.: Interactive nonlinear multiobjective optimization methods. In:
Multiple Criteria Decision Analysis, pp. 927–976. Springer (2016)
Mitchell, T.M.: Machine Learning, 1st edn. McGraw-Hill Inc, New York (1997)
Palit, A.K., Popovic, D.: Computational intelligence in time series forecasting. Theory Eng. Appl. (2006).
doi:10.1007/1-84628-184-9
Parmee, I., Hall, A., Miles, J., Noyes, J., Simons, C., et al.: Discovery in design: Developing a people-
centred computational approach. In: DS 36: Proceedings DESIGN 2006, the 9th International Design
Conference, Dubrovnik, Croatia (2006)
Piegat, A., Sałabun, W.: Nonlinearity of human multi-criteria in decision-making. J. Theor. Appl. Comput.
Sci.6(3), 36–49 (2012)
Pitangueira, A.M.: Incorporating preferences from multiple stakeholders in software requirements selection
an interactive search-based approach. In: 2015 IEEE 23rd International Requirements Engineering
Conference (RE), pp. 382–387. IEEE (2015)
Pitangueira, A.M., Maciel, R.S.P., de Oliveira Barros, M.: Software requirements selection and prioritization
using sbse approaches: a systematic review and mapping of the literature. J. Syst. Softw. 103, 267–280
(2015). doi: 10.1016/j.jss.2014.09.038
R-Project: http://www.r-project.org/ (2014). Accessed Apr 2016
Rousseeuw, P.J.: Least median of squares regression. J. Am. Stat. Assoc. 79(388), 871–880 (1984)
Schachter, D.L.: Implicit memory: history and current status. J. Exp. Psychol. 13(3), 501–518 (1987)
Semet, Y .: Interactive evolutionary computation: a survey of existing theory. University of Illinois, (2002)
Shackelford, M.: Implementation issues for an interactive evolutionary computation system. In: Proceedings
of the 9th annual conference companion on Genetic and evolutionary computation, pp. 2933–2936.ACM (2007)
Shackelford, M., Corne, D.: A technique for evaluation of interactive evolutionary systems. In: Adaptive
Computing in Design and Manufacture VI, pp. 197–208. Springer (2004)
Simons, C.: Interactive evolutionary computing in early lifecycle software engineering design. Ph.D. thesis,
University of the West of England (2011)
Simons, C.L., Parmee, I.C.: Dynamic parameter control of interactive local search in uml software design.
In: 2010 IEEE International Conference on Systems Man and Cybernetics (SMC), pp. 3397–3404.
IEEE (2010)
Simons, C.L., Parmee, I.C.: Elegant object-oriented software design via interactive, evolutionary compu-
tation. IEEE Trans. Syst. Man Cybern. Part C 42(6), 1797–1805 (2012). doi: 10.1109/TSMCC.2012.
2225103
Simons, C.L., Parmee, I.C., Gwynllyw, R.: Interactive, evolutionary search in upstream object-oriented
class design. IEEE Trans. Softw. Eng. 36(6), 798–816 (2010). doi: 10.1109/TSE.2010.34
Simons, C.L., Smith, J.: A comparison of meta-heuristic search for interactive software design. Soft Comput.
17(11), 2147–2162 (2013). doi: 10.1007/s00500-013-1039-1
Simons, C.L., Smith, J., White, P.: Interactive ant colony optimization (iaco) for early lifecycle software
design. Swarm Intell. 8(2), 139–157 (2014)
Simons, C.L., Smith, J., White, P.: Interactive ant colony optimization (iaco) for early lifecycle software
design. Swarm Intell. 8(2), 139–157 (2014). doi: 10.1007/s11721-014-0094-2
123
Autom Softw Eng (2017) 24:623–671 671
Takagi, H.: Interactive evolutionary computation: System optimization based on human subjective evalu-
ation. In: IEEE International Conference on Intelligent Engineering Systems (INES’98), pp. 17–19(1998)
Takagi, H.: Interactive evolutionary computation: fusion of the capabilities of ec optimization and human
evaluation. Proc. IEEE 89(9), 1275–1296 (2001)
Tonella, P., Susi, A., Palma, F.: Using interactive ga for requirements prioritization. In: Proceedings of the
2nd International Symposium on Search Based Software Engineering (SSBSE ’10), pp. 57–66. IEEE,Benevento, Italy (2010). doi: 10.1109/SSBSE.2010.17
Tonella, P., Susi, A., Palma, F.: Interactive requirements prioritization using a genetic algorithm. Inf. Softw.
Technol. 55(1), 173–187 (2013). doi: 10.1016/j.infsof.2012.07.003
van den Akker, J., Brinkkemper, S., Diepen, G., Versendaal, J.: Determination of the next release of a
software product: an approach using integer linear programming. In: Proceeding of the 11th Interna-tional Workshop on Requirements Engineering: Foundation for Software Quality (REFSQ ’05). Porto,
Portugal (2005). http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.85.27%95
Wang, S., Wang, X., Takagi, H.: User fatigue reduction by an absolute rating data-trained predictor in IEC.
In: IEEE Congress on Evolutionary Computation, 2006. CEC 2006, pp. 2195–2200. IEEE (2006)
Witten, I.H., Frank, E.: Data Mining: Practical Machine Learning Tools and Techniques. Morgan Kaufmann,
Amsterdam (2005)
Xuan, J., Jiang, H., Ren, Z., Luo, Z.: Solving the large scale next release problem with a backbone based
multilevel algorithm. IEEE Trans. Softw. Eng. 38(5), 1195–1212 (2012). doi: 10.1109/TSE.2011.92
Zhang, D., Tsai, J.J.: Machine learning and software engineering. Softw. Qual. J. 11(2), 87–119 (2003)
Zhang, Y ., Finkelstein, A., Harman, M.: Search based requirements optimisation: existing work and chal-
lenges. In: Proceedings of the 14th International Working Conference, Requirements Engineering:
Foundation for Software Quality (RefsQ ’08), vol. 5025, pp. 88–94. Springer, Montpellier, France
(2008). doi: 10.1007/978-3-540-69062-7_8
123

